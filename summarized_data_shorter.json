[
    {
        "code": "from   _TFL.pyk           import pyk\n\nfrom   rsclib.HTML_Parse  import tag, Page_Tree\nfrom   rsclib.autosuper   import autosuper\nfrom   spider.common      import Interface, Inet4, Inet6, unroutable\nfrom   spider.common      import WLAN_Config\nfrom   spider.luci        import Version_Mixin\n\nclass Status (Page_Tree, Version_Mixin) :\n    url          = 'cgi-bin/luci/freifunk/status/status'\n    retries      = 2\n    timeout      = 10\n    html_charset = 'utf-8' \n\n    wl_names = dict \\\n        ( ssid    = 'ssid'\n        , _bsiid  = 'bssid'\n        , channel = 'channel'\n        , mode    = 'mode'\n        )\n\n    def parse (self) :\n        root  = self.tree.getroot ()\n        self.wlans  = []\n        self.routes = {}\n        for div in root.findall (\".//%s\" % tag (\"div\")) :\n            id = div.get ('id')\n            if id == 'cbi-wireless' :\n                wlan_div = div\n            elif id == 'cbi-routes' :\n                route_div = div\n            self.try_get_version (div)\n        for d in self.tbl_iter (wlan_div) :\n            for k, newkey in pyk.iteritems (self.wl_names) :\n                if k in d :\n                    d [newkey] = d [k]\n            wl = WLAN_Config (** d)\n            self.wlans.append (wl)\n        for d in self.tbl_iter (route_div) :\n            iface = d.get ('iface')\n            gw    = d.get ('gateway')\n            if iface and gw :\n                self.routes [iface] = gw\n        self.set_version (root)\n    \n\n    def tbl_iter (self, div) :\n        tbl = div.find (\".//%s\" % tag (\"table\"))\n        assert tbl.get ('class') == 'cbi-section-table'\n        d = {}\n        for tr in tbl :\n            if 'cbi-section-table-row' not in tr.get ('class').split () :\n                continue\n            for input in tr.findall (\".//%s\" % tag ('input')) :\n                name = input.get ('id').split ('.') [-1]\n                val  = input.get ('value')\n                d [name] = val\n            if not d :\n                continue\n            yield d\n    \n\n\n\nclass Table_Iter (Page_Tree) :\n\n    def table_iter (self) :\n        root  = self.tree.getroot ()\n        for div in root.findall (\".//%s\" % tag (\"div\")) :\n            if div.get ('id') == 'maincontent' :\n                break\n        tbl = div.find (\".//%s\" % tag (\"table\"))\n        if tbl is None :\n            return\n        for tr in tbl :\n            if tr [0].tag == tag ('th') :\n                continue\n            yield (self.tree.get_text (x) for x in tr)\n    \n\n\n\nclass OLSR_Connections (Table_Iter) :\n    url          = 'cgi-bin/luci/freifunk/olsr/'\n    retries      = 2\n    timeout      = 10\n    html_charset = 'utf-8' \n\n    def parse (self) :\n        self.neighbors = {}\n        for l in self.table_iter () :\n            neighbor, ip, lq, nlq, etx = l\n            lq, nlq, etx = (float (x) for x in (lq, nlq, etx))\n            self.neighbors [neighbor] = [ip, lq, nlq, etx]\n    \n\n\n\nclass OLSR_Routes (Table_Iter) :\n    url          = 'cgi-bin/luci/freifunk/olsr/routes'\n    retries      = 2\n    timeout      = 10\n    html_charset = 'utf-8' \n\n    def parse (self) :\n        self.iface_by_gw = {}\n        for l in self.table_iter () :\n            announced, gw, iface, metric, etx = l\n            if gw in self.iface_by_gw :\n                assert iface == self.iface_by_gw [gw]\n            else :\n                self.iface_by_gw [gw] = iface\n    \n\n\n\nclass OpenWRT (autosuper) :\n\n    def __init__ (self, site, request) :\n        self.site    = site\n        self.request = request\n        if 'interfaces' in self.request or 'ips' in self.request :\n            st    = Status           (site = site)\n            conn  = OLSR_Connections (site = site)\n            route = OLSR_Routes      (site = site)\n            self.version = st.version\n            assert len (st.wlans) <= 1\n            interfaces   = {}\n            ips          = {}\n            count = 0\n            for gw, ifname in pyk.iteritems (route.iface_by_gw) :\n                ip, lq, nlq, etx  = conn.neighbors [gw]\n                i4 = Inet4 (ip, None, None, iface = ifname)\n                ips [i4] = 1\n                is_wlan = True\n                if lq == nlq == etx == 1.0 :\n                    is_wlan = False\n                if ifname in interfaces :\n                    iface = interfaces [ifname]\n                    if not iface.is_wlan and is_wlan :\n                        iface.is_wlan   = True\n                        iface.wlan_info = st.wlans [0]\n                else :\n                    iface = Interface (count, ifname, None)\n                    iface.is_wlan = is_wlan\n                    if is_wlan :\n                        iface.wlan_info = st.wlans [0]\n                    count += 1\n                    interfaces [ifname] = iface\n                if i4 not in iface.inet4 :\n                    iface.append_inet4 (i4)\n            wl_if = None\n            for iface in pyk.itervalues (interfaces) :\n                if iface.is_wlan :\n                    if wl_if :\n                        m = \"Duplicate wlan: %s/%s\" % (iface.name, wl_if.name)\n                        raise ValueError (m)\n                    wl_if = iface\n            \n            n  = 'unknown'\n            i4 = Inet4 (self.request ['ip'], None, None, iface = n)\n            if i4 not in ips :\n                assert n not in interfaces\n                iface = interfaces [n] = Interface (count, n, None)\n                iface.append_inet4 (i4)\n                iface.is_wlan = False\n                if not wl_if and st.wlans :\n                    iface.is_wlan   = True\n                    iface.wlan_info = st.wlans [0]\n                ips [i4] = True\n\n            self.request ['ips']        = ips\n            self.request ['interfaces'] = interfaces\n            self.request ['version']    = st.version\n    \n\n\n",
        "summary": "The provided Python code defines several classes for parsing and processing network status data from a Freifunk router using the LuCI web interface. The `Status` class extracts wireless network information, while `OLSR_Connections` and `OLSR_Routes` parse OLSR routing table data. The `OpenWRT` class integrates these parsers to gather comprehensive network information, including interfaces, IP addresses, and wireless details, which is then stored in the request object for further processing or display."
    },
    {
        "code": "from PIL import Image\nimport cv2\nimport imagehash\nimport math\nimport numpy as np\n\nDIFF_THRES = 20\nLIMIT = 2\nRESIZE = 1000\n\n\ndef calc_hash(img):\n    \n    \n    img = resize(img)\n    return imagehash.whash(Image.fromarray(img))\n\n\ndef compare(hash1, hash2):\n    \n    return hash1 - hash2\n\n\ndef limit(img, std_hash, count):\n    \n    \n    cmp_hash = calc_hash(img)\n\n    \n    diff = compare(std_hash, cmp_hash)\n\n    \n    if diff <= DIFF_THRES:\n        \n        if count >= LIMIT:\n            return 'remove'\n\n    \n    else:\n        \n        return 'update_std'\n\n    \n    return 'continue'\n\n\ndef resize(img):\n    \n    \n    width = np.shape(img)[1]\n    height = np.shape(img)[0]\n\n    \n    if width > RESIZE:\n        \n        scale = RESIZE / width\n        resized_img = cv2.resize(\n            img, (RESIZE, math.floor(height / scale)), cv2.INTER_AREA)\n        \n        return resized_img\n\n    \n    return img\n\n\ndef set_standard(images, filename):\n    \n    return filename, calc_hash(images[filename]), 0\n",
        "summary": "The provided Python code defines a series of functions to process images using the Python Imaging Library (PIL), OpenCV, and imagehash. It includes functionalities for calculating hash values of images, comparing these hashes to determine if an image is similar enough to be considered a duplicate or significantly different, resizing images to a maximum dimension, and setting a standard image with its hash value and count of duplicates."
    },
    {
        "code": "from .cli.cli import main\n\n\n\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The provided Python script is a simple command-line interface (CLI) application that executes the `main` function when run as the primary module. This function likely contains the core logic for the CLI's functionality, enabling users to interact with the application through command-line inputs."
    },
    {
        "code": "import time\n\nfrom PyQt5 import QtGui, QtCore\n\nfrom ui.room_item import Ui_Form\nfrom PyQt5.QtWidgets import QWidget\n\nclass Room_Item(QWidget,Ui_Form):\n    def __init__(self,parent=None,room_data=None):\n        super(Room_Item,self).__init__(parent)\n        self.setupUi(self)\n        self.data = room_data\n        self.setRoomInfo()\n\n    def setRoomInfo(self):\n        self.room_name.setText('{}({})'.format(self.data['naturalName'], self.data['roomName']))\n        self.description.setText(\"<a style='color:\n        timeStamp = int(self.data['creationDate']) / 1000\n        timeArray = time.localtime(timeStamp)\n        otherStyleTime = time.strftime(\"%Y-%m-%d\", timeArray)\n        self.create_time.setText(\"<a style='color:\n        members = len(self.data['owners']) + len(self.data['admins']) + len(self.data['members'])\n        memberCounter = \"<a style='color:\n        self.member.setText(memberCounter)",
        "summary": "The `Room_Item` class is a PyQt5 widget that displays room information, including the room name, description, creation time, and number of members. It initializes with optional parent and room data, sets up its UI, and populates it with details from the provided data."
    },
    {
        "code": "import asyncio\nimport re\nimport sys\nimport traceback\n\nimport toga\nfrom toga import Key\nfrom .keys import toga_to_winforms_key\n\nfrom .libs import Threading, WinForms, shcore, user32, win_version\nfrom .libs.proactor import WinformsProactorEventLoop\nfrom .window import Window\n\n\nclass MainWindow(Window):\n    def winforms_FormClosing(self, sender, event):\n        if not self.interface.app._impl._is_exiting:\n            event.Cancel = not self.interface.app.exit()\n\n\nclass App:\n    _MAIN_WINDOW_CLASS = MainWindow\n\n    def __init__(self, interface):\n        self.interface = interface\n        self.interface._impl = self\n\n        \n        \n        \n        \n        \n        \n        \n        \n        self._is_exiting = False\n\n        self.loop = WinformsProactorEventLoop()\n        asyncio.set_event_loop(self.loop)\n\n    def create(self):\n        self.native = WinForms.Application\n        self.app_context = WinForms.ApplicationContext()\n\n        \n        \n        \n        \n        if win_version.Major >= 6:  \n            \n            \n            if ((win_version.Major == 6 and win_version.Minor == 3) or\n                    (win_version.Major == 10 and win_version.Build < 15063)):\n                shcore.SetProcessDpiAwareness(True)\n            \n            \n            elif win_version.Major == 10 and win_version.Build >= 15063:\n                user32.SetProcessDpiAwarenessContext(-2)\n            \n            else:\n                user32.SetProcessDPIAware()\n\n        self.native.EnableVisualStyles()\n        self.native.SetCompatibleTextRenderingDefault(False)\n\n        self.interface.commands.add(\n            toga.Command(\n                lambda _: self.interface.about(),\n                'About {}'.format(self.interface.name),\n                group=toga.Group.HELP\n            ),\n            toga.Command(None, 'Preferences', group=toga.Group.FILE),\n            \n            toga.Command(\n                lambda _: self.interface.exit(),\n                'Exit ' + self.interface.name,\n                shortcut=Key.MOD_1 + 'q',\n                group=toga.Group.FILE,\n                section=sys.maxsize\n            ),\n            toga.Command(\n                lambda _: self.interface.visit_homepage(),\n                'Visit homepage',\n                enabled=self.interface.home_page is not None,\n                group=toga.Group.HELP\n            )\n        )\n        self._create_app_commands()\n\n        \n        self.interface.startup()\n        self.create_menus()\n        self.interface.icon.bind(self.interface.factory)\n        self.interface.main_window._impl.set_app(self)\n\n    def create_menus(self):\n        self._menu_items = {}\n        self._menu_groups = {}\n\n        toga.Group.FILE.order = 0\n        menubar = WinForms.MenuStrip()\n        submenu = None\n        for cmd in self.interface.commands:\n            if cmd == toga.GROUP_BREAK:\n                submenu = None\n            elif cmd == toga.SECTION_BREAK:\n                submenu.DropDownItems.Add('-')\n            else:\n                submenu = self._submenu(cmd.group, menubar)\n\n                item = WinForms.ToolStripMenuItem(cmd.label)\n\n                if cmd.action:\n                    item.Click += cmd._impl.as_handler()\n                item.Enabled = cmd.enabled\n\n                if cmd.shortcut is not None:\n                    shortcut_keys = toga_to_winforms_key(cmd.shortcut)\n                    item.ShortcutKeys = shortcut_keys\n                    item.ShowShortcutKeys = True\n\n                cmd._impl.native.append(item)\n\n                self._menu_items[item] = cmd\n                submenu.DropDownItems.Add(item)\n\n        self.interface.main_window._impl.native.Controls.Add(menubar)\n        self.interface.main_window._impl.native.MainMenuStrip = menubar\n        self.interface.main_window.content.refresh()\n\n    def _submenu(self, group, menubar):\n        try:\n            return self._menu_groups[group]\n        except KeyError:\n            if group is None:\n                submenu = menubar\n            else:\n                parent_menu = self._submenu(group.parent, menubar)\n\n                submenu = WinForms.ToolStripMenuItem(group.label)\n\n                \n                if group.parent is None:\n                    parent_menu.Items.Add(submenu)\n                else:\n                    parent_menu.DropDownItems.Add(submenu)\n\n            self._menu_groups[group] = submenu\n        return submenu\n\n    def _create_app_commands(self):\n        \n        pass\n\n    def open_document(self, fileURL):\n        \n        print(\"STUB: If you want to handle opening documents, implement App.open_document(fileURL)\")\n\n    def winforms_thread_exception(self, sender, winforms_exc):\n        \n        \n        \n        \n        \n        \n        \n        \n        print(\"Traceback (most recent call last):\")\n        py_exc = winforms_exc.get_Exception()\n        full_stack_trace = py_exc.StackTrace\n        regex = re.compile(\n            r\"^\\[(?:'(.*?)', )*(?:'(.*?)')\\]   (?:.*?) Python\\.Runtime\",\n            re.DOTALL | re.UNICODE\n        )\n\n        stacktrace_relevant_lines = regex.findall(full_stack_trace)\n        if len(stacktrace_relevant_lines) == 0:\n            self.print_stack_trace(full_stack_trace)\n        else:\n            for lines in stacktrace_relevant_lines:\n                for line in lines:\n                    self.print_stack_trace(line)\n        print(py_exc.Message)\n\n    @classmethod\n    def print_stack_trace(cls, stack_trace_line):\n        for level in stack_trace_line.split(\"', '\"):\n            for line in level.split(\"\\\\n\"):\n                if line:\n                    print(line)\n\n    def run_app(self):\n        try:\n            self.create()\n\n            self.native.ThreadException += self.winforms_thread_exception\n\n            self.loop.run_forever(self.app_context)\n        except:  \n            traceback.print_exc()\n\n    def main_loop(self):\n        thread = Threading.Thread(Threading.ThreadStart(self.run_app))\n        thread.SetApartmentState(Threading.ApartmentState.STA)\n        thread.Start()\n        thread.Join()\n\n    def show_about_dialog(self):\n        message_parts = []\n        if self.interface.name is not None:\n            if self.interface.version is not None:\n                message_parts.append(\n                    \"{name} v{version}\".format(\n                        name=self.interface.name,\n                        version=self.interface.version,\n                    )\n                )\n            else:\n                message_parts.append(\n                    \"{name}\".format(name=self.interface.name)\n                )\n        elif self.interface.version is not None:\n            message_parts.append(\n                \"v{version}\".format(version=self.interface.version)\n            )\n\n        if self.interface.author is not None:\n            message_parts.append(\n                \"Author: {author}\".format(author=self.interface.author)\n            )\n        if self.interface.description is not None:\n            message_parts.append(\n                \"\\n{description}\".format(\n                    description=self.interface.description\n                )\n            )\n        self.interface.main_window.info_dialog(\n            'About {}'.format(self.interface.name), \"\\n\".join(message_parts)\n        )\n\n    def exit(self):\n        self._is_exiting = True\n        self.native.Exit()\n\n    def set_main_window(self, window):\n        self.app_context.MainForm = window._impl.native\n\n    def set_on_exit(self, value):\n        pass\n\n    def current_window(self):\n        self.interface.factory.not_implemented('App.current_window()')\n\n    def enter_full_screen(self, windows):\n        self.interface.factory.not_implemented('App.enter_full_screen()')\n\n    def exit_full_screen(self, windows):\n        self.interface.factory.not_implemented('App.exit_full_screen()')\n\n    def set_cursor(self, value):\n        self.interface.factory.not_implemented('App.set_cursor()')\n\n    def show_cursor(self):\n        self.interface.factory.not_implemented('App.show_cursor()')\n\n    def hide_cursor(self):\n        self.interface.factory.not_implemented('App.hide_cursor()')\n\n    def add_background_task(self, handler):\n        self.loop.call_soon(handler, self)\n\n\nclass DocumentApp(App):\n    def _create_app_commands(self):\n        self.interface.commands.add(\n            toga.Command(\n                lambda w: self.open_file,\n                label='Open...',\n                shortcut=Key.MOD_1 + 'o',\n                group=toga.Group.FILE,\n                section=0\n            ),\n        )\n\n    def open_document(self, fileURL):\n        \n        self.interface.factory.not_implemented('DocumentApp.open_document()')\n",
        "summary": "This Python code defines a Toga application for Windows using the WinForms library. It includes classes for managing the main window, handling commands and menus, and running the event loop. The `App` class sets up the application's interface, creates menus, and handles exceptions, while the `DocumentApp` subclass adds functionality specific to document-based applications, such as opening files."
    },
    {
        "code": "__version__ = \"0.4.0\"\n\n\ndef classFactory(iface):  \n    \n    \n    from .SimplePhotogrammetryRoutePlanner import SimplePhotogrammetryRoutePlanner\n    return SimplePhotogrammetryRoutePlanner(iface)\n",
        "summary": "The provided Python code defines a module with a version attribute set to \"0.4.0\" and includes a function `classFactory` that takes an interface parameter, imports the `SimplePhotogrammetryRoutePlanner` class from another module, and returns an instance of this class initialized with the given interface."
    },
    {
        "code": "from sklearn.feature_selection import VarianceThreshold\nimport numpy as np\n\nnp.random.seed(1)\nX = np.random.randn(100, 10)\nX = np.hstack([X, np.zeros([100, 5])])\n\n\n\ndef featureSelection_variance(X, thrd):\n    sel = VarianceThreshold(threshold=thrd)\n    X_selected = sel.fit_transform(X)\n    mask = sel.get_support()\n    return X_selected, mask\n\n\nX = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\nselector = VarianceThreshold()\nselector.fit_transform(X)\nselector.variances_\n",
        "summary": "The provided Python code demonstrates the use of `VarianceThreshold` from the `sklearn.feature_selection` module to perform feature selection based on variance thresholding. It first generates a synthetic dataset with some features having zero variance, then defines and applies a function to select features that meet or exceed a specified variance threshold, returning both the selected features and their boolean mask. The code also shows how to apply `VarianceThreshold` directly to a small example dataset and access the variances of all features."
    },
    {
        "code": "from my_multi_main3 import main\nimport numpy as np\nimport argparse\nimport time\n\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--save-model', action='store_true', default=False,\n                    help='For Saving the current Model')\nparser.add_argument('--norm-flag', type=bool, default=False,\n                    help='Triggering the Layer Normalization flag for attention scores')\nparser.add_argument('--gamma', type=float, default=None,\n                    help='Controlling the sparisty of gfusedmax/sparsemax, the smaller, the more sparse')\nparser.add_argument('--lam', type=float, default=1.0,\n                    help='Lambda: Controlling the smoothness of gfusedmax, the larger, the smoother')\nparser.add_argument('--max-type', type=str, default='softmax',choices=['softmax','sparsemax','gfusedmax'],\n                    help='mapping function in attention')\nparser.add_argument('--optim-type', type=str, default='SGD',choices=['SGD','Adam'],\n                    help='mapping function in attention')\nparser.add_argument('--head-cnt', type=int, default=2, metavar='S', choices=[1,2,4,5,10],\n                    help='Number of heads for attention (default: 1)')\n\nargs = parser.parse_args()\n\nhyperparameter_choices = {\n    'lr':list(10**np.arange(-4,-1,0.5)),\n    'norm_flag': [True,False],\n    'gamma':list(10**np.arange(-1,3,0.5))+[None,],\n    'lam':list(10**np.arange(-2,2,0.5)),\n    'max_type':['softmax','sparsemax','gfusedmax'],\n    \n    'optim_type':['SGD','Adam'],\n    'head_cnt':[1,2,4,5,10,20]\n}\n\nparam_num = 25\nrecord = np.zeros([param_num,len(hyperparameter_choices)+1])\nrecord_name = 'record3_multi_%s.csv'%time.strftime('%Y-%m-%d_%H-%M-%S',time.localtime())\nfor n in range(param_num):\n    for param_index,(k,v) in enumerate(hyperparameter_choices.items()):\n        print(param_index,k)\n        value_index = np.random.choice(len(v))\n        if isinstance(v[value_index],str) or isinstance(v[value_index],bool) or v[value_index] is None:\n            record[n,param_index] = value_index\n        else:\n            record[n,param_index] = v[value_index]\n        setattr(args,k,v[value_index])\n    record[n,-1] = main(args)\n    np.savetxt(record_name, record, delimiter=',')\n\n\n\n",
        "summary": "The Python script defines a command-line interface for training a model on the MNIST dataset using PyTorch. It includes various hyperparameters and flags that can be adjusted through arguments or randomly selected from predefined choices. The script then runs multiple experiments by varying these parameters, records the results, and saves them to a CSV file."
    },
    {
        "code": "from __future__ import print_function, absolute_import\n\nimport h5py\n\nfrom spiker import log\n\nlogger = log.get_logger(\"data-hdf5\", log.DEBUG)\n\n\ndef init_hdf5(file_path, mode=\"w\", cam_type=\"davis\"):\n    \n    if mode == \"w\":\n        dataset = h5py.File(file_path, mode=mode)\n        dataset.create_group(\"dvs\")\n        dataset.create_group(\"extra\")\n        if cam_type == \"davis\":\n            dataset.create_group(\"aps\")\n            dataset.create_group(\"imu\")\n    elif mode == \"r\":\n        dataset = h5py.File(file_path, mode=mode)\n\n    return dataset\n",
        "summary": "The provided Python code defines a function `init_hdf5` that initializes an HDF5 file for storing data, creating specific groups such as \"dvs\", \"extra\", \"aps\", and \"imu\" based on the camera type and access mode. It uses the `h5py` library to handle the file operations and logs debug information using a custom logger named \"data-hdf5\"."
    },
    {
        "code": "import flatbuffers\n\nclass FloatingPoint(object):\n    __slots__ = ['_tab']\n\n    @classmethod\n    def GetRootAsFloatingPoint(cls, buf, offset):\n        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n        x = FloatingPoint()\n        x.Init(buf, n + offset)\n        return x\n\n    \n    def Init(self, buf, pos):\n        self._tab = flatbuffers.table.Table(buf, pos)\n\n    \n    def Precision(self):\n        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))\n        if o != 0:\n            return self._tab.Get(flatbuffers.number_types.Int16Flags, o + self._tab.Pos)\n        return 0\n\ndef FloatingPointStart(builder): builder.StartObject(1)\ndef FloatingPointAddPrecision(builder, precision): builder.PrependInt16Slot(0, precision, 0)\ndef FloatingPointEnd(builder): return builder.EndObject()\n",
        "summary": "The provided Python code defines a class `FloatingPoint` that uses the FlatBuffers library to handle binary data serialization and deserialization. It includes methods for creating and accessing a floating-point number's precision within a buffer, as well as functions to start, add a precision value, and end an object in the FlatBuffers schema."
    },
    {
        "code": "class optimType:\n    REACTION_KO = 1\n    REACTION_UO = 2\n    GENE_KO = 3\n    GENE_UO = 4\n    MEDIUM = 5\n    MEDIUM_LEVELS = 6\n    MEDIUM_REACTION_KO = 7\n    MEDIUM_REACTION_UO = 8\n    COMPOSITION = 9\n    PROTEIN_KO = 10\n    PROTEIN_UO = 11\n\n    types = {1:\"Reaction Knockouts\",2:\"Reaction Under/Over expression\", 3:\"Gene Knockouts\",\n             4:\"Gene Under/Over expression\", 5:\"Medium compositions\",6:\"Medium compositions with levels\",\n             7:\"Medium with Reaction Knockouts\",8: \"Medium with Reaction Under/Over expression\",\n             9:\"Community Composition\", 10:\"Protein knockouts\", 11:\"Protein Under/Over expression\"}\n\n    def get_optim_type_name(self, id):\n        return optimType.types.get(id)\n\n\nclass solverMethod:\n    LSODA = 1\n    LSODAR = 2\n    LSODE = 3\n    HEUN = 4\n    EULER = 5\n    RK4 =  6\n    DORMAN_PRINCE = 7\n    RKFehlberg = 8\n    Dopri5 = 9\n    Dop853 = 10\n    Vode = 11\n    Radau5 = 12\n    AdamsBashforth2=13\n    AdamsBashMoulton2=14\n\n    methods ={1:\"LSODA\",2:\"LSODAR\", 3: \"LSODE\", 4: \"HEUN\", 5: \"EULER\",\n                6: \"Range Kutta 4\", 7: \"DORMAN PRINCE\", 8: \"RKFehlberg\", 9: \"Dopri5\", 10: \"Dop853\", 11: \"Vode\",\n                12: \"Radau5\", 13: \"AdamsBashforth2\", 14: \"AdamsBashMoulton2\"\n              }\n\n    def get_solver_method_name(self, id):\n        return solverMethod.methods.get(id)\n\n\nclass solverStatus:\n    \n    OPTIMAL = 1\n    UNKNOWN = 0\n    ERROR = 2\n    SUBOPTIMAL = -1\n    UNBOUNDED = -2\n    INFEASIBLE = -3\n    INF_OR_UNB = -4\n\n\n    @staticmethod\n    def get_status_str(id):\n        if solverStatus.ERROR == id :\n            str=\"Error\"\n        elif solverStatus.OPTIMAL == id:\n            str = \"Optimal\"\n        elif solverStatus.SUBOPTIMAL == id:\n            str = \"Sub-Optimal\"\n        elif solverStatus.UNBOUNDED == id or  solverStatus.INFEASIBLE == id or solverStatus.INF_OR_UNB == id:\n            str = \"Infeasible or unbounded problem.\"\n        else:\n            str = \"Unknown\"\n        return str\n\n",
        "summary": "The provided Python code defines three classes: `optimType`, `solverMethod`, and `solverStatus`. The `optimType` class categorizes different types of optimizations, such as reaction and gene knockouts, medium compositions, and protein modifications. The `solverMethod` class lists various numerical methods for solving differential equations used in simulations. The `solverStatus` class provides a mapping of solver statuses to human-readable strings, indicating the outcome of optimization or simulation processes."
    },
    {
        "code": "from enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\nfrom app.models import User, Organization\n\n\nclass DataRoomBase(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n\n\nclass DataRoomCreateRequest(DataRoomBase):\n    name: str\n\n\nclass DataRoomCreate(DataRoomCreateRequest):\n    creator: User\n    organization: Organization\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nclass DataRoomRole(str, Enum):\n    OWNER = \"OWNER\"\n    ADMIN = \"ADMIN\"\n    MEMBER = \"MEMBER\"\n\n\nclass DataRoomUserRoleRequest(BaseModel):\n    user_id: UUID\n    user_role: DataRoomRole\n\n    class Config:\n        use_enum_values = True\n\n\nclass DataRoomTeamRoleRequest(BaseModel):\n    team_id: UUID\n    team_role: DataRoomRole\n\n    class Config:\n        use_enum_values = True\n\n\nclass DataRoomUpdate(DataRoomBase):\n    pass\n\n\nclass DataRoomInDBBase(DataRoomBase):\n    id: Optional[UUID] = None\n\n    class Config:\n        orm_mode = True\n",
        "summary": "The provided Python code defines a set of Pydantic models for managing data rooms, including their creation, roles, and updates. It includes classes for base data room properties, specific request models for creating and updating data rooms, role definitions, and database-specific configurations to facilitate ORM operations."
    },
    {
        "code": "import json\r\nimport pymongo\r\n\r\nfrom config import *\r\n\r\n\r\ndef response(flow):\r\n    global collection\r\n    client = pymongo.MongoClient(MONGO_URL)\r\n    db = client[WECHAT_XHS_MONGO_DB]\r\n    collection = db[WECHAT_XHS_NOTE_MONGO_COLLECTION]\r\n\r\n    url1 = 'https://www.xiaohongshu.com/sapi/wx_mp_api/sns/v1/search/notes?'\r\n    url2 = 'https://www.xiaohongshu.com/fe_api/burdock/v1/page/'\r\n    if flow.request.url.startswith(url1):\r\n        \n        print(flow.request.url)\r\n\r\n        notes = json.loads(flow.response.text)[\"data\"][\"notes\"]\r\n        for note in notes:\r\n            note_id = note[\"id\"]\r\n            img_list = note[\"images_list\"]\r\n            title = note[\"title\"]\r\n            user = note[\"user\"]\r\n\r\n            content = {\r\n                \"note_id\": note_id,\r\n                \"img_list\": img_list,\r\n                \"title\": title,\r\n                \"user\":user\r\n            }\r\n\r\n            collection.insert(content)\r\n\r\n    elif flow.request.url.startswith(url2):\r\n        print(flow.request.url)\r\n\r\n        notes = json.loads(flow.response.text)[\"data\"]\r\n        for note in notes:\r\n            note_id = note[\"id\"]\r\n            img_list = note[\"cover\"]\r\n            title = note[\"title\"]\r\n            user = note[\"user\"]\r\n\r\n            content = {\r\n                \"note_id\": note_id,\r\n                \"img_list\": img_list,\r\n                \"title\": title,\r\n                \"user\": user\r\n            }\r\n\r\n            collection.insert(content)\r\n",
        "summary": "The provided Python code defines a function `response` that processes HTTP requests and responses using the mitmproxy framework. It connects to a MongoDB database and inserts data from specific URLs related to notes on the Xiaohongshu platform into a designated collection, handling different URL patterns for note retrieval and storing the extracted information such as note ID, image list, title, and user details."
    },
    {
        "code": "import json\nfrom datetime import datetime, timedelta\nfrom bittrex.bittrex import Bittrex\n\n\ndef TradingAlorythm(command, market, amount, coinname, step, stoploss, key, secret):\n    TestTrading = Bittrex(key, secret)\n    period = timedelta(seconds=20)\n    next_tick = datetime.now() + period\n    seconds = 20\n    firstCycle = True\n    if command == \"y\":\n        print(\"buying {0} of {1} coins\".format(amount, coinname))\n        \n        \n\n    while command == \"y\":\n        \n        if next_tick <= datetime.now():\n            print(\"Connecting to Bittrex\")\n            seconds += 20\n            next_tick += period\n            print(\"Timer ticked\")\n            print(\"Updating stock exchange...\")\n            \n            t = TestTrading.get_ticker(market)\n            \n            balance = TestTrading.get_balance(coinname)\n            \n            orders = TestTrading.get_open_orders(market)\n            a = json.dumps(t)\n            \n            print(t)\n            \n            print(\"Balance is {} \".format(balance['result']['Available']))\n            \n            print(orders)\n            \n            bid = t['result']['Bid']\n            ask = t['result']['Ask']\n            last = t['result']['Last']\n            if firstCycle:\n                StartValue = bid\n                firstCycle = False\n            Stop_loss = StartValue - 0.00000007\n            print(\"*--------------------------\")\n            print(\"| Start Value  | {: .8f} \".format(StartValue))\n            print(\"| Stop loss    | {: .8f} \".format(Stop_loss))\n            print(\"|--------------------------\")\n            print(\"| Bid          | {: .8f} \".format(bid))\n            print(\"| Ask          | {: .8f} \".format(ask))\n            print(\"| Last         | {: .8f} \".format(last))\n            print(\"*--------------------------\")\n            \n            \n            if bid >= step + StartValue:\n                print(\"MOVE STOP-LOSS\")\n                StartValue = bid\n\n            if bid <= stoploss:\n                print(\"Sell order sent\")",
        "summary": "The provided Python code defines a trading algorithm named `TradingAlorythm` that interacts with the Bittrex cryptocurrency exchange using an API key and secret. The function continuously monitors market data, updates balances, and executes buy or sell orders based on predefined conditions such as bid price thresholds and stop-loss levels."
    },
    {
        "code": "from sys import exit, stdout\nimport time\n\nfrom docopt import docopt\nfrom py.uart import Serial\n\nfrom py import *\n\nif __name__ == '__main__':\n    args = docopt(__doc__)\n    target = args['--target']\n    image = args['<image>']\n    port = '/dev/' + args['--port']\n    plane = int(args['--plane'])\n    bootloader = args['--bootloader']\n    verify = args['--verify']\n    erase = args['--erase']\n    boot_rom = args['--boot-rom']\n    reset = args['--reset']\n\n    print('Selected port:', port)\n    print('Selected image:', image)\n\n    if target == 'HT32':\n        from py.ht32.isp import ISP, isp\n        image = args['<image>']\n        if image is None:\n            if reset:\n                isp(Serial(port)).reset()\n            else:\n                print('No image specified, not flashing.')\n        else:\n            with open(image, 'rb') as f:\n                binary = f.read()\n            isp = ISP(Serial(port))\n            isp.page_erase(start_addr=0x0, end_addr=0x1000)\n            isp.flash(0x00, binary)\n            isp.reset()\n\n    elif target == 'SAM3x8e':\n        from py.sam3x8e.programmer import program\n        if bootloader is None:\n            bootloader = 'sam3x8e/bootloader.bin'\n        program(port, image=image, erase=erase, reset=True,\\\n                verify=verify, bootloader_image=bootloader, plane=plane, boot_rom=boot_rom)\n    else:\n        print('Unknown target.')\n",
        "summary": "This Python script uses the `docopt` library to parse command-line arguments for flashing images onto different microcontroller targets such as HT32 and SAM3x8e. It handles operations like selecting a port, specifying an image file, erasing memory, flashing new firmware, resetting the device, and verifying the flash operation."
    },
    {
        "code": "from typing import Any, Callable, Dict, Optional, TypeVar\n\nfrom msrest import Serializer\n\nfrom azure.core.exceptions import ClientAuthenticationError, HttpResponseError, ResourceExistsError, ResourceNotFoundError, map_error\nfrom azure.core.pipeline import PipelineResponse\nfrom azure.core.pipeline.transport import HttpResponse\nfrom azure.core.rest import HttpRequest\nfrom azure.core.tracing.decorator import distributed_trace\nfrom azure.mgmt.core.exceptions import ARMErrorFormat\n\nfrom .. import models as _models\nfrom .._vendor import _convert_request, _format_url_section\nT = TypeVar('T')\nClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]\n\n_SERIALIZER = Serializer()\n_SERIALIZER.client_side_validation = False\n\ndef build_list_request(\n    subscription_id: str,\n    resource_group_name: str,\n    resource_name: str,\n    **kwargs: Any\n) -> HttpRequest:\n    api_version = kwargs.pop('api_version', \"2022-04-01\")  \n\n    accept = \"application/json\"\n    \n    _url = kwargs.pop(\"template_url\", \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ContainerService/managedClusters/{resourceName}/privateLinkResources\")  \n    path_format_arguments = {\n        \"subscriptionId\": _SERIALIZER.url(\"subscription_id\", subscription_id, 'str', min_length=1),\n        \"resourceGroupName\": _SERIALIZER.url(\"resource_group_name\", resource_group_name, 'str', max_length=90, min_length=1),\n        \"resourceName\": _SERIALIZER.url(\"resource_name\", resource_name, 'str', max_length=63, min_length=1, pattern=r'^[a-zA-Z0-9]$|^[a-zA-Z0-9][-_a-zA-Z0-9]{0,61}[a-zA-Z0-9]$'),\n    }\n\n    _url = _format_url_section(_url, **path_format_arguments)\n\n    \n    _query_parameters = kwargs.pop(\"params\", {})  \n    _query_parameters['api-version'] = _SERIALIZER.query(\"api_version\", api_version, 'str')\n\n    \n    _header_parameters = kwargs.pop(\"headers\", {})  \n    _header_parameters['Accept'] = _SERIALIZER.header(\"accept\", accept, 'str')\n\n    return HttpRequest(\n        method=\"GET\",\n        url=_url,\n        params=_query_parameters,\n        headers=_header_parameters,\n        **kwargs\n    )\n\nclass PrivateLinkResourcesOperations(object):\n    \n\n    models = _models\n\n    def __init__(self, client, config, serializer, deserializer):\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self._config = config\n\n    @distributed_trace\n    def list(\n        self,\n        resource_group_name: str,\n        resource_name: str,\n        **kwargs: Any\n    ) -> \"_models.PrivateLinkResourcesListResult\":\n        \n        cls = kwargs.pop('cls', None)  \n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n\n        api_version = kwargs.pop('api_version', \"2022-04-01\")  \n\n        \n        request = build_list_request(\n            subscription_id=self._config.subscription_id,\n            resource_group_name=resource_group_name,\n            resource_name=resource_name,\n            api_version=api_version,\n            template_url=self.list.metadata['url'],\n        )\n        request = _convert_request(request)\n        request.url = self._client.format_url(request.url)\n\n        pipeline_response = self._client._pipeline.run(  \n            request,\n            stream=False,\n            **kwargs\n        )\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            raise HttpResponseError(response=response, error_format=ARMErrorFormat)\n\n        deserialized = self._deserialize('PrivateLinkResourcesListResult', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n\n    list.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ContainerService/managedClusters/{resourceName}/privateLinkResources\"}  \n\n",
        "summary": "The provided Python code defines a class `PrivateLinkResourcesOperations` that includes methods for interacting with Azure's Managed Kubernetes Service (AKS) private link resources. It uses the `msrest` library for serialization and deserialization, and it handles HTTP requests through an Azure pipeline. The `list` method constructs a GET request to retrieve private link resources associated with a specified managed cluster in a resource group, handling potential errors such as authentication issues or resource not found."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('menu', '0005_auto_20170930_1059'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='item',\n            name='description',\n            field=models.CharField(max_length=255),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that alters the 'description' field of the 'Item' model in the 'menu' app, changing its maximum length to 255 characters."
    },
    {
        "code": "from importlib import import_module\nfrom django.db.models.signals import post_migrate\n\nfrom django.apps import AppConfig\n\ndef default_data_setup(sender, **kwargs):\n    from django.contrib.auth.models import User\n    try:\n        anon = User.objects.get(username='ANONYMOUS_USER')\n    except User.DoesNotExist:\n        print('Adding ANONYMOUS_USER')\n        anon = User.objects.create_user('ANONYMOUS_USER', 'anonymous_user@example.com')\n        \n        anon.set_unusable_password()\n        anon.is_active = False\n        anon.save()\n\n\nclass RadioConfig(AppConfig):\n    name = 'radio'\n\n\n    def ready(self):\n        post_migrate.connect(default_data_setup, sender=self)\n        \n",
        "summary": "The provided Python code defines a Django app configuration class `RadioConfig` that connects to the `post_migrate` signal. When the database is migrated, it checks if a user with the username 'ANONYMOUS_USER' exists; if not, it creates this user with an unusable password and sets it as inactive."
    },
    {
        "code": "from nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom .common import get_pp_pipeline\n\ndef or_list(booleans):\n    return True in booleans\n\n\ndef get_ngrams(D):\n    \n    ngrams = set()\n    for d in D:\n        for w in d:\n            if '$' in w:\n                ngrams.add(w)\n    return list(ngrams)\n\n\ndef get_frequent_ngrams(text, n, stopword_list, threshold):\n    bigrams = ngrams(text, n)\n    bigram_freq = Counter(bigrams)\n    frequent_bigrams = []\n    for bigram, freq in bigram_freq.most_common():\n        if not (or_list([i in stopword_list for i in bigram])):\n            if freq > threshold:\n                frequent_bigrams.append('{}${}'.format(bigram[0], bigram[1]))\n            else:\n                break\n    return frequent_bigrams\n\n\ndef ngrammize_text(text, ngrams):\n    bigrammized_text = []\n    i = 0\n    while i < len(text):\n        term = text[i]\n        if i == len(text)-1:\n            bigrammized_text.append(term)\n        else:\n            next_term = text[i+1]\n            test_bigram = '{}${}'.format(term, next_term)\n            if test_bigram in ngrams:\n                bigrammized_text.append(test_bigram)\n                i += 1\n            else:\n                bigrammized_text.append(term)\n        i += 1\n    return bigrammized_text\n\n\ndef get_dataset_ngrams(docs, min_freq=1000, sw=None, extra_bigrams=None, extra_ngrams=None):\n    if not sw:\n        sw = stopwords.words('english')\n        sw_pp = get_pp_pipeline(remove_stopwords=False)\n        sw = sw_pp.clean_document(sw)\n    full_text = []\n    for doc in docs:\n        full_text.extend(doc)\n    frequent_bigrams = get_frequent_ngrams(full_text, 2, sw, min_freq)\n    if extra_bigrams:\n        frequent_bigrams.extend(extra_bigrams)\n    bigrammized_text = ngrammize_text(full_text, frequent_bigrams)\n    frequent_ngrams = get_frequent_ngrams(bigrammized_text, 2, sw, min_freq)\n    if extra_ngrams:\n        frequent_ngrams.extend(extra_ngrams)\n    return frequent_bigrams, frequent_ngrams\n\n\ndef insert_ngrams_flat_from_lists(docs, frequent_bigrams, frequent_ngrams):\n    for i in range(0, len(docs)):\n        doc = docs[i]\n        doc = ngrammize_text(doc, frequent_bigrams)\n        doc = ngrammize_text(doc, frequent_ngrams)\n        docs[i] = doc\n    return docs\n\n\ndef insert_ngrams_flat(docs, min_freq=1000, sw=None, extra_bigrams=None, extra_ngrams=None):\n    fb, fn = get_dataset_ngrams(docs, min_freq, sw, extra_bigrams, extra_ngrams)\n    return insert_ngrams_flat_from_lists(docs, fb, fn)\n\n\ndef insert_ngrams_from_lists(date_doc_tuples, frequent_bigrams, frequent_ngrams):\n    for i in range(0, len(date_doc_tuples)):\n        date, doc = date_doc_tuples[i]\n        doc = ngrammize_text(doc, frequent_bigrams)\n        doc = ngrammize_text(doc, frequent_ngrams)\n        date_doc_tuples[i] = (date, doc)\n    return date_doc_tuples\n\n\ndef insert_ngrams(date_docs, min_freq=1000, sw=None, extra_bigrams=None, extra_ngrams=None):\n    fb, fn = get_dataset_ngrams([x[1] for x in date_docs], min_freq, sw, extra_bigrams, extra_ngrams)\n    return insert_ngrams_from_lists(date_docs, fb, fn)\n",
        "summary": "The provided Python code defines functions to extract and insert n-grams (specifically bigrams) into text data. It includes methods for identifying frequent bigrams based on frequency thresholds and stopword filtering, as well as for replacing words with their corresponding bigram representations in a given text or dataset. The code also handles the insertion of both frequent bigrams and additional specified bigrams into documents, either individually or within date-document tuples."
    },
    {
        "code": "import collections\nimport copy\nimport logging\nimport time\nfrom abc import abstractmethod\n\nfrom ...scheduler import HyperbandScheduler, RLScheduler, FIFOScheduler\nfrom ...scheduler.seq_scheduler import LocalSequentialScheduler\nfrom ...utils import in_ipynb, try_import_mxnet\nfrom ...utils.utils import setup_compute\n\n__all__ = [\n    'BaseTask',\n    'compile_scheduler_options',\n    'compile_scheduler_options_v2',\n    'create_scheduler']\n\nResults = collections.namedtuple('Results', 'model reward config time metadata')\n\nschedulers = {\n    'local': LocalSequentialScheduler,\n    'fifo': FIFOScheduler,\n    'rl': RLScheduler,\n    'hyperband_stopping': HyperbandScheduler,\n    'hyperband_promotion': HyperbandScheduler,\n}\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\n\ndef create_scheduler(train_fn, scheduler, scheduler_options):\n    if isinstance(scheduler, str):\n        scheduler_cls = schedulers[scheduler.lower()]\n    else:\n        assert callable(scheduler)\n        scheduler_cls = scheduler\n        scheduler_options = copy.copy(scheduler_options)\n    return scheduler_cls(train_fn, **scheduler_options)\n\n\nclass BaseTask(object):\n    \n    @property\n    @staticmethod\n    def Dataset():\n        try_import_mxnet()\n        from autogluon.mxnet.utils.dataset import BaseDataset\n        return BaseDataset\n\n    @classmethod\n    def run_fit(cls, train_fn, search_strategy, scheduler_options,\n                plot_results=False):\n        start_time = time.time()\n        \n        scheduler = create_scheduler(train_fn, search_strategy, scheduler_options)\n        scheduler.run()\n        scheduler.join_jobs()\n        \n        best_reward = scheduler.get_best_reward()\n        best_config = scheduler.get_best_config()\n        args = train_fn.args\n        args.final_fit = True\n        if hasattr(args, 'epochs') and hasattr(args, 'final_fit_epochs'):\n            args.epochs = args.final_fit_epochs\n        train_fn.args.update({'final_fit':True})\n        train_fn.kwvars.update({'final_fit':True})\n        scheduler_final = create_scheduler(train_fn, search_strategy, scheduler_options)\n        results = scheduler_final.run_with_config(best_config)\n        total_time = time.time() - start_time\n        if plot_results or in_ipynb():\n            plot_training_curves = scheduler_options['checkpoint'].replace('exp1.ag', 'plot_training_curves.png')\n            scheduler.get_training_curves(filename=plot_training_curves, plot=True, use_legend=False)\n        record_args = copy.deepcopy(args)\n        if results is None:\n            logger.warning('No valid results obtained with best config, the result may not be useful...')\n            results = {}\n        results.update(best_reward=best_reward,\n                       best_config=best_config,\n                       total_time=total_time,\n                       metadata=scheduler.metadata,\n                       training_history=scheduler.training_history,\n                       config_history=scheduler.config_history,\n                       reward_attr=scheduler._reward_attr,\n                       args=record_args)\n        return results\n\n    @classmethod\n    @abstractmethod\n    def fit(cls, *args, **kwargs):\n        pass\n\n\n\n\nsearcher_for_hyperband_strategy = {\n    'hyperband': 'random',\n    'bayesopt_hyperband': 'bayesopt'}\n\n\ndef compile_scheduler_options(\n        scheduler_options, search_strategy, search_options, nthreads_per_trial,\n        ngpus_per_trial, checkpoint, num_trials, time_out, resume, visualizer,\n        time_attr, reward_attr, dist_ip_addrs, epochs=None):\n    \n    if scheduler_options is None:\n        scheduler_options = dict()\n    else:\n        assert isinstance(scheduler_options, dict)\n    assert isinstance(search_strategy, str)\n    if search_options is None:\n        search_options = dict()\n    if visualizer is None:\n        visualizer = 'none'\n    if time_attr is None:\n        time_attr = 'epoch'\n    if reward_attr is None:\n        reward_attr = 'accuracy'\n    scheduler_options = copy.copy(scheduler_options)\n    scheduler_options.update({\n        'resource': {\n            'num_cpus': nthreads_per_trial, 'num_gpus': ngpus_per_trial},\n        'searcher': search_strategy,\n        'search_options': search_options,\n        'checkpoint': checkpoint,\n        'resume': resume,\n        'num_trials': num_trials,\n        'time_out': time_out,\n        'reward_attr': reward_attr,\n        'time_attr': time_attr,\n        'visualizer': visualizer,\n        'dist_ip_addrs': dist_ip_addrs})\n    searcher = searcher_for_hyperband_strategy.get(search_strategy)\n    if searcher is not None:\n        scheduler_options['searcher'] = searcher\n        if epochs is not None:\n            scheduler_options['max_t'] = epochs\n    return scheduler_options\n\n\n\ndef compile_scheduler_options_v2(\n        scheduler_options, nthreads_per_trial,\n        ngpus_per_trial, num_trials, time_out, scheduler=None, search_strategy=None, search_options=None, checkpoint=None, resume=False, visualizer=None,\n        time_attr=None, reward_attr=None, dist_ip_addrs=None, epochs=None):\n    \n    if scheduler_options is None:\n        scheduler_options = dict()\n    else:\n        assert isinstance(scheduler_options, dict)\n    scheduler_options = copy.copy(scheduler_options)\n    if dist_ip_addrs is None:\n        dist_ip_addrs = []\n    if search_strategy is None:\n        search_strategy = 'random'\n    if scheduler is None:\n        scheduler = 'local'\n    assert isinstance(search_strategy, str)\n    if search_options is None:\n        search_options = dict()\n    if visualizer is None:\n        visualizer = 'none'\n    if time_attr is None:\n        time_attr = 'epoch'\n    if reward_attr is None:\n        reward_attr = 'validation_performance'\n    scheduler_params = {\n        'resource': {\n            'num_cpus': nthreads_per_trial, 'num_gpus': ngpus_per_trial},\n        'scheduler': scheduler,\n        'searcher': search_strategy,\n        'search_options': search_options,\n        'checkpoint': checkpoint,\n        'resume': resume,\n        'num_trials': num_trials,\n        'time_out': time_out,\n        'reward_attr': reward_attr,\n        'time_attr': time_attr,\n        'visualizer': visualizer,\n        'dist_ip_addrs': dist_ip_addrs,\n    }\n    resource = None\n    if 'resource' in scheduler_options:\n        scheduler_params['resource'].update(scheduler_options['resource'])\n        resource = scheduler_params['resource'].copy()\n    scheduler_params.update(scheduler_options)\n    if resource:\n        scheduler_params['resource'] = resource\n\n    scheduler_params['resource']['num_cpus'], scheduler_params['resource']['num_gpus'] = setup_compute(\n        nthreads_per_trial=scheduler_params['resource']['num_cpus'],\n        ngpus_per_trial=scheduler_params['resource']['num_gpus'],\n    )  \n\n    searcher = searcher_for_hyperband_strategy.get(scheduler_params['searcher'])\n    if searcher is not None:\n        scheduler_params['searcher'] = searcher\n        if epochs is not None:\n            scheduler_params['max_t'] = epochs\n    required_options = [\n        'resource',\n        'scheduler',\n        'searcher',\n        'search_options',\n        'checkpoint',\n        'resume',\n        'num_trials',\n        'time_out',\n        'reward_attr',\n        'time_attr',\n        'visualizer',\n        'dist_ip_addrs',\n    ]\n    missing_options = []\n    for option in required_options:\n        if option not in scheduler_params:\n            missing_options.append(option)\n    if missing_options:\n        raise AssertionError(f'Missing required keys in scheduler_options: {missing_options}')\n    return scheduler_params\n",
        "summary": "The provided Python code defines a framework for running machine learning tasks using various schedulers, including Hyperband and FIFO. It includes utilities for creating and managing these schedulers, compiling options for them, and executing the training process with different search strategies. The `BaseTask` class serves as an abstract base class for specific task implementations, providing methods to run fits and compile scheduler options."
    },
    {
        "code": "import sys\n\n\n\n\n\n\n_ver = sys.version_info\n\n\nis_py2 = (_ver[0] == 2)\n\n\nis_py3 = (_ver[0] == 3)\n\nif is_py2:\n    from urlparse import urlparse\n    from urllib import quote\n    from urlparse import urljoin\n    import pytz as timezone\n    from email import message_from_string as message_from_bytes_or_string\n    from __builtin__ import xrange as range_or_xrange\nelif is_py3:\n    from urllib.parse import urlparse\n    from urllib.parse import quote\n    from urllib.parse import urljoin\n    from datetime import timezone\n    from email import message_from_bytes as message_from_bytes_or_string\n    from builtins import range as range_or_xrange\n\n\ndef message_as_bytes_or_string(message):\n    if is_py2:\n        return message.as_string()\n    else:\n        return message.as_bytes()\n\n\ndef is_string_type(value):\n    if is_py2:\n        return isinstance(value, basestring)\n    else:\n        return type(value) is str\n",
        "summary": "The Python code checks the version of the interpreter to determine whether it's running on Python 2 or Python 3 and adjusts imports and functions accordingly. It provides utility functions `message_as_bytes_or_string` and `is_string_type` that handle differences between Python 2 and Python 3 for working with messages and string types, respectively."
    },
    {
        "code": "import abc\n\n\nclass SystemEvent(abc.ABC):\n    \n\n    @abc.abstractmethod\n    def is_set(self):\n        \n\n    @abc.abstractmethod\n    def set(self):\n        \n\n    @abc.abstractmethod\n    def clear(self):\n        \n\n    @abc.abstractmethod\n    def wait(self, *args, **kwargs):\n        \n\n\n__all__ = [\n        'SystemEvent',\n        ]\n",
        "summary": "The provided Python code defines an abstract base class `SystemEvent` using the `abc` module, which outlines four methods that any subclass must implement: `is_set`, `set`, `clear`, and `wait`. The `__all__` list at the end specifies that only `SystemEvent` should be exported when this module is imported."
    },
    {
        "code": "from django.db import models\nfrom django.contrib.auth.models import AbstractBaseUser\nfrom django.contrib.auth.models import PermissionsMixin,BaseUserManager\n\n\nclass UserProfileManager(BaseUserManager):\n    \n    def create_user(self,email,name,password=None):\n        \n        if not email:\n            raise ValueError(\"User must gave an email address\")\n\n        email=self.normalize_email(email)\n        user=self.model(email=email,name=name)\n\n        user.set_password(password)\n        user.save(using=self._db)\n        return user\n\n    def create_superuser(self,email,name,password):\n        \n        user=self._create_user(email,aname,password)\n\n        user.is_superuser=True\n        user.is_staff=True\n        user.save(using=self._db)\n\n        return user\n\n\n\nclass UserProfile(AbstractBaseUser,PermissionsMixin):\n    \n    email=models.EmailField(max_length=100,unique=True)\n    name=models.CharField(max_length=255)\n    is_active=models.BooleanField(default=True)\n    is_staff=models.BooleanField(default=False)\n\n    objects=UserProfileManager()\n    USERNAME_FIELD='email'\n    REQUIRED_FIELDS=['name']\n\n    def get_full_name(self):\n        \n        return self.name\n\n    def get_short_name(self):\n        \n        return self.name\n\n    def __str__(self):\n        \n        return self.email\n",
        "summary": "The code defines a custom user model in Django using `AbstractBaseUser` and `PermissionsMixin`, with methods for creating regular users and superusers through a custom manager class. It includes fields for email, name, active status, and staff status, along with methods to retrieve full and short names, and a string representation of the user object."
    },
    {
        "code": "load(\"@org_tensorflow//tensorflow:tensorflow.bzl\", \"if_not_windows\", \"tf_binary_additional_srcs\", \"tf_cc_binary\", \"tf_copts\")\nload(\"//tensorflow_decision_forests/tensorflow:utils.bzl\", \"rpath_linkopts_to_tensorflow\")\n\ndef py_wrap_yggdrasil_learners(\n        name = None,\n        learner_deps = []):\n    \n\n    \n    wrapper_package = \"//tensorflow_decision_forests/keras/wrapper\"\n\n    \n    local_cc_main = name + \"_wrapper_main.cc\"\n\n    \n    wrapper_name = name + \"_wrapper_main\"\n\n    \n    run_wrapper_name = name + \"_run_wrapper\"\n\n    \n    native.genrule(\n        name = name + \"_copy_cc_main\",\n        outs = [local_cc_main],\n        srcs = [wrapper_package + \":wrapper_main.cc\"],\n        cmd = \"cp $< $@\",\n    )\n\n    \n    tf_cc_binary(\n        name = wrapper_name,\n        copts = tf_copts(),\n        linkopts = if_not_windows([\"-lm\", \"-Wl,-ldl\"]) + rpath_linkopts_to_tensorflow(wrapper_name),\n        srcs = [\":\" + local_cc_main],\n        deps = [\n            wrapper_package + \":wrapper\",\n        ] + learner_deps,\n        linkstatic = 1,\n    )\n\n    \n    native.genrule(\n        name = run_wrapper_name,\n        srcs = [],\n        outs = [name + \".py\"],\n        cmd = \"$(location \" + wrapper_name + \") > \\\"$@\\\"\",\n        tools = [\":\" + wrapper_name] + tf_binary_additional_srcs(),\n    )\n\n    \n    native.py_library(\n        name = name,\n        srcs = [name + \".py\"],\n        srcs_version = \"PY3\",\n        deps = [\n            \"//tensorflow_decision_forests/keras:core\",\n            \"@org_tensorflow//tensorflow/python\",\n            \"@ydf//yggdrasil_decision_forests/model:abstract_model_py_proto\",\n            \"@ydf//yggdrasil_decision_forests/learner:abstract_learner_py_proto\",\n        ],\n        data = [\":\" + run_wrapper_name, \":\" + wrapper_name],\n    )\n",
        "summary": "The provided Python code defines a Bazel build rule for wrapping Yggdrasil learners in TensorFlow. It includes copying a C++ main file, building a TensorFlow C++ binary with specific options and dependencies, generating a Python script to run the binary, and creating a Python library that depends on various TensorFlow and Yggdrasil Decision Forests components."
    },
    {
        "code": "import os\nimport subprocess\nimport sys\n\nimport yaml\n\n\ndef run_cmd(command: str):\n    \n    command = f'{os.getenv(\"SHELL\")} -c \"{command}\"'\n    pipe = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n    )\n\n    stdout = \"\"\n\n    if pipe.stdout is not None:\n        stdout = \"\".join(\n            [line.decode(\"utf-8\") for line in iter(pipe.stdout.readline, b\"\")]\n        )\n        pipe.stdout.close()\n    returncode = pipe.wait()\n    print(stdout)\n\n    return returncode, stdout\n\n\ndef out_video(segment, greek=True):\n    title_idx = 3 if greek else 4\n    title, topic, subtopic = segment[title_idx], segment[1], segment[2]\n    name = f\"{title}_{topic}-{subtopic}.mp4\"\n\n    return name\n\n\ndef input_video(segment):\n    return segment[0]\n\n\ndef manage_timestamps(segment):\n    try:\n        st, et = segment[5], segment[6]\n    except:\n        st = segment[5]\n\n        return [st]\n    try:\n        delete_timestamps = segment[7]\n    except:\n        return [st, et]\n\n    if not delete_timestamps:\n        return [st, et]\n    else:\n        return (\n            [st]\n            + [\n                t\n\n                for s in delete_timestamps.split(\",\")\n\n                for t in (s.split(\"-\")[0], s.split(\"-\")[1])\n            ]\n            + [et]\n        )\n\n\ndef to_cut_fmt(timestamp):\n    out = \"\"\n    labels = [\"h\", \"m\", \"s\"]\n    lb_idx = 0\n\n    for c in timestamp:\n        if c == \":\":\n            out += labels[lb_idx]\n            lb_idx += 1\n        else:\n            out += c\n\n    return out\n\n\ndef to_cut_yaml(inmp4, outmp4, ymlname, timestamps):\n    def pairwise(iterable):\n        \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n        a = iter(iterable)\n\n        return list(zip(a, a))\n\n    timestamps = [to_cut_fmt(t) for t in timestamps]\n    timeframe = []\n\n    if len(timestamps) == 1:\n        timeframe = [{\"from\": \"start\", \"to\": timestamps[0]}]\n    else:\n        for s, e in pairwise([\"start\"] + timestamps + [\"end\"]):\n            timeframe += [{\"from\": s, \"to\": e}]\n    out = {\n        \"input\": inmp4,\n        \"output\": outmp4,\n        \"cut_method\": \"delete\",\n        \"timeframe\": timeframe,\n    }\n    with open(ymlname, \"w\") as fd:\n        yaml.dump(out, fd, default_flow_style=False, sort_keys=False)\n\n\ndef format_timestamp_args(timestamps):\n    if len(timestamps) == 1:\n        return [f\"-ss {timestamps[0]} \"]\n\n    def pairwise(iterable):\n        \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n        a = iter(iterable)\n\n        return list(zip(a, a))\n\n    cmds = [f\"-ss {s} -to {e}\" for s, e in pairwise(timestamps)]\n\n    return cmds\n\n\ndef ffmpeg(inp, out, timestamps_args):\n    if len(timestamps_args) == 1:\n        run_cmd(f\"ffmpeg -y -i '{inp}' \" + timestamps_args[0] + f\" -c:v h265_nvenc -crf 24 -preset fast -c:a copy '{out}'\")\n\n        return\n    mp4s = []\n\n    for i, arg in enumerate(timestamps_args):\n        mp4s.append(f\"{i}.mp4\")\n        cmd = f\"ffmpeg  -i '{inp}' \" + arg + f\" -c:v h265_nvenc -crf 24 -preset fast -c:a copy '{i}.mp4'\"\n        print(cmd)\n        run_cmd(cmd)\n\n    tmp = \".tmp_files.txt\"\n    with open(tmp, \"w\") as fd:\n        for f in mp4s:\n            fd.write(f\"file '{f}'\\n\")\n\n    run_cmd(f\"ffmpeg -y -f concat -i .tmp_files.txt '{out}'\")\n    run_cmd(f\"rm {tmp} \" + \" \".join(mp4s))\n\n\ndef read_split_tsv(timestamp_file):\n    with open(timestamp_file) as f:\n        segments = [ln.strip().split(\"\\t\") for ln in f]\n\n    return segments\n\n\ndef main():\n    timestamp_file = sys.argv[1]\n    segments = read_split_tsv(timestamp_file)\n\n    for segment in segments:\n        inmp4 = input_video(segment)\n        outmp4 = \"out/\" + out_video(segment, greek=True)\n        timestamps = manage_timestamps(segment)\n        timestamp_args = format_timestamp_args(timestamps)\n        ffmpeg(inmp4, outmp4, timestamp_args)\n\n\ndef main1():\n    timestamp_file = sys.argv[1]\n    segments = read_split_tsv(timestamp_file)\n\n    for i, segment in enumerate(segments):\n        inmp4 = input_video(segment)\n        outmp4 = out_video(segment, greek=True)\n        timestamps = manage_timestamps(segment)\n        to_cut_yaml(inmp4, outmp4, f\"{i}.yml\", timestamps)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The provided Python script processes video files based on timestamps specified in a TSV file. It defines functions for running shell commands, managing timestamps, formatting arguments for FFmpeg, and generating YAML configuration files. The `main` function reads segments from the TSV file, extracts input and output filenames, manages timestamps, formats them for FFmpeg, and runs FFmpeg to cut videos accordingly. The script also includes an alternative `main1` function that generates YAML configuration files for each segment without executing the video cutting process."
    },
    {
        "code": "__version__ = '3.2.2'\n",
        "summary": "The provided Python code snippet sets the version of a module to '3.2.2'."
    },
    {
        "code": "from threading import Timer,Thread\nimport RPIO\nfrom RPIO import PWM\nimport paramiko\nimport json\nimport sys\nfrom time import time, sleep\nfrom relaxxapi.relaxxapi import relaxx\nr = None\nsftp_base_path = \"/home/shack/music\"\n\n\nbutton = 4\nloud1 = 21\nloud2 = 22\n\nstate = 0\n\n\ndef init_state():\n    state = 0\n    RPIO.setup(loud1, RPIO.OUT)\n    RPIO.setup(loud2, RPIO.OUT)\n\n     \n\nt1_2 = 1\ntimer=None\nt2_4 = 1\nt4_5 = 3\n\ndef time3_trans():\n    global state\n    if state is 4:\n        state = 5\n        stop_sirene1()\n        stop_sirene2()\n        disable_all_timers()\n        delete_current_music()\n        state = 0\n    else:\n        print(\"State is not 4, will do nothing\")\n\n\ndef time2_trans():\n    global state\n    global timer\n    if state is 2:\n        state = 4\n        start_sirene2()\n        timer= Timer(t4_5,time3_trans).start()\n    else:\n        print(\"State is not 2, will do nothing\")\n\ndef time1_trans():\n    global state\n    global timer\n    if state is 1:\n        state = 2\n        start_sirene1()\n        timer=Timer(t2_4,time2_trans).start()\n    else:\n        print(\"State is not 1, will do nothing\")\n\n\ndef btn_trans(a,edge):\n    global state\n    global timer\n    print(\"Button: %s , edge: %s, state: %d\" % (str(a), str(edge),state))\n    if edge and state is 0:\n        state = 1\n        timer=Timer(t1_2,time1_trans).start()\n    \n    elif not edge and (state is 1 or state is 4 or state is 2):\n        state = 0\n        disable_all_timers()\n        stop_sirene1()\n        stop_sirene2()\n        try:\n            play_next()\n        except:\n            tell_gobbelz(\"Cannot play next song. Sorry:(\")\n            tell_gobbelz(\"Bailing out\")\n            sys.exit(1)\n            \n    elif not edge and state is 5:\n        print(\"button released while removing music, all fine\")\n    else:\n        print(\"this should never happen\")\n        \n        \ndef disable_all_timers():\n    print(\"disabling all the timers\")\n    global timer\n    try:\n        timer.cancel()\n        print(\"timer canceled\")\n    except: pass\n\ndef start_sirene1(): \n    print(\"start Sirene 1\")\n    RPIO.output(loud1, True)\n\ndef start_sirene2(): \n    print(\"starting Sirene 2\")\n    RPIO.output(loud2, True)\n\ndef stop_sirene1(): \n    print(\"stopping Sirene 1\")\n    RPIO.output(loud1, False)\n\ndef stop_sirene2(): \n    print(\"stopping Sirene 2\")\n    RPIO.output(loud2, False)\n\ndef play_radio():\n    \n    if r.get_current().get(\"file\", \"\") == \"http://ice.somafm.com/groovesalad\":\n        print(\"will not skip own sender\")\n        return\n    print(\"playing radio\")\n    tell_gobbelz(\"Starting Radio Stream\")\n    r.add_song(\"http://ice.somafm.com/groovesalad\")\n    r.play_last()\n\n\ndef play_next():\n    print (\"playing next song\")\n    try:\n        \n        if is_last_song():\n            raise Exception(\"Last song in playlist\")\n        r.next_song()\n    except:\n        print(\"no next song, starting radio\")\n        play_radio()\n\ndef is_last_song():\n    return r.get_current()[\"Pos\"] == r.get_last()[\"Pos\"]\n\n\n\ndef delete_current_music():\n    print(\"delete current music\")\n    current = r.get_current()\n    if not current:\n        print(\"Nothing is running, bailing out\")\n        return\n    delete_remote_file(current)\n    play_next()\n\n\ndef delete_remote_file(current):\n    try:\n        sftp_delete_remote_file(current[\"file\"])\n        say_song_killed(current.get(\"Title\", \"Unbekannter Title\"),\n                     current.get(\"Artist\", \"Unbekannter Kuenstler\"))\n    except Exception as e:\n        print(\"Cannot delete remote file! ( %s ) \" %str(e))\n\n\ndef sftp_delete_remote_file(f):\n    host = \"mpd.shack\"\n    port = 22\n    transport = paramiko.Transport((host, port))\n    username = 'shack'\n    passwd = 'shackit'\n    transport.connect(username=username, password=passwd)\n    sftp = paramiko.SFTPClient.from_transport(transport)\n    \n    print(sftp.unlink('%s/%s' % (sftp_base_path, f)))\n    sftp.close()\n    transport.close()\n\ndef say_song_killed(name, author):\n    tell_gobbelz('%s von %s wurde vernichtet!' % (name, author) )\n\ndef tell_gobbelz(text):\n    import requests\n    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n    data = {'text': text}\n    \n    \n    requests.post(\"http://kiosk.shack:8080/say/\",\n                  data=json.dumps(data), headers=headers)\n\n\nif __name__ == \"__main__\":\n    from time import sleep\n    init_state() \n    print(\"initializing relaxxapi\")\n    try:\n        r = relaxx(relaxxurl=\"http://lounge.mpd.shack/\")\n    except:\n        tell_gobbelz(\"EM PE DE unreachable!\")\n        tell_gobbelz(\"Bailing out\")\n        sys.exit(1)\n    print(\"adding interrupt\")\n    RPIO.add_interrupt_callback(button,callback=btn_trans,pull_up_down=RPIO.PUD_DOWN) \n    print (\"Start Interrupt handler\")\n    RPIO.wait_for_interrupts()\n    \n",
        "summary": "The provided Python code is a script that controls various hardware components and services, including GPIO pins for sirenes, an MPD music player through the relaxxapi library, and remote file deletion via SFTP. It manages state transitions based on button presses and handles playback of radio stations or songs from a playlist, with timers to transition between states and perform actions like stopping sirenes, playing next songs, or deleting current music files."
    },
    {
        "code": "import numpy as np\nfrom qtpy.QtCore import Qt\nfrom qtpy.QtWidgets import QComboBox, QDoubleSpinBox, QLabel\n\nfrom ...layers.utils._color_manager_constants import ColorMode\nfrom ...utils.translations import trans\nfrom ..utils import qt_signals_blocked\nfrom ..widgets.qt_color_swatch import QColorSwatchEdit\nfrom .qt_layer_controls_base import QtLayerControls\n\n\nclass QtVectorsControls(QtLayerControls):\n    \n\n    def __init__(self, layer):\n        super().__init__(layer)\n\n        self.layer.events.edge_width.connect(self._on_edge_width_change)\n        self.layer.events.length.connect(self._on_length_change)\n        self.layer.events.edge_color_mode.connect(\n            self._on_edge_color_mode_change\n        )\n        self.layer.events.edge_color.connect(self._on_edge_color_change)\n\n        \n        color_properties = self._get_property_values()\n        color_prop_box = QComboBox(self)\n        color_prop_box.activated[str].connect(self.change_edge_color_property)\n        color_prop_box.addItems(color_properties)\n        self.color_prop_box = color_prop_box\n        self.edge_prop_label = QLabel(trans._('edge property:'))\n\n        \n        self.edgeColorEdit = QColorSwatchEdit(\n            initial_color=self.layer.edge_color,\n            tooltip=trans._(\n                'click to set current edge color',\n            ),\n        )\n        self.edgeColorEdit.color_changed.connect(self.change_edge_color_direct)\n        self.edge_color_label = QLabel(trans._('edge color:'))\n        self._on_edge_color_change()\n\n        \n        colorModeComboBox = QComboBox(self)\n        color_modes = [e.value for e in ColorMode]\n        colorModeComboBox.addItems(color_modes)\n        colorModeComboBox.activated[str].connect(self.change_edge_color_mode)\n        self.color_mode_comboBox = colorModeComboBox\n        self._on_edge_color_mode_change()\n\n        \n        self.widthSpinBox = QDoubleSpinBox()\n        self.widthSpinBox.setKeyboardTracking(False)\n        self.widthSpinBox.setSingleStep(0.1)\n        self.widthSpinBox.setMinimum(0.1)\n        self.widthSpinBox.setMaximum(np.inf)\n        self.widthSpinBox.setValue(self.layer.edge_width)\n        self.widthSpinBox.valueChanged.connect(self.change_width)\n\n        \n        self.lengthSpinBox = QDoubleSpinBox()\n        self.lengthSpinBox.setKeyboardTracking(False)\n        self.lengthSpinBox.setSingleStep(0.1)\n        self.lengthSpinBox.setValue(self.layer.length)\n        self.lengthSpinBox.setMinimum(0.1)\n        self.lengthSpinBox.setMaximum(np.inf)\n        self.lengthSpinBox.valueChanged.connect(self.change_length)\n\n        \n        \n        self.grid_layout.addWidget(QLabel(trans._('opacity:')), 0, 0)\n        self.grid_layout.addWidget(self.opacitySlider, 0, 1, 1, 2)\n        self.grid_layout.addWidget(QLabel(trans._('width:')), 1, 0)\n        self.grid_layout.addWidget(self.widthSpinBox, 1, 1, 1, 2)\n        self.grid_layout.addWidget(QLabel(trans._('length:')), 2, 0)\n        self.grid_layout.addWidget(self.lengthSpinBox, 2, 1, 1, 2)\n        self.grid_layout.addWidget(QLabel(trans._('blending:')), 3, 0)\n        self.grid_layout.addWidget(self.blendComboBox, 3, 1, 1, 2)\n        self.grid_layout.addWidget(QLabel(trans._('edge color mode:')), 4, 0)\n        self.grid_layout.addWidget(self.color_mode_comboBox, 4, 1, 1, 2)\n        self.grid_layout.addWidget(self.edge_color_label, 5, 0)\n        self.grid_layout.addWidget(self.edgeColorEdit, 5, 1, 1, 2)\n        self.grid_layout.addWidget(self.edge_prop_label, 6, 0)\n        self.grid_layout.addWidget(self.color_prop_box, 6, 1, 1, 2)\n        self.grid_layout.setRowStretch(7, 1)\n        self.grid_layout.setColumnStretch(1, 1)\n        self.grid_layout.setSpacing(4)\n\n    def change_edge_color_property(self, property: str):\n        \n        mode = self.layer.edge_color_mode\n        try:\n            self.layer.edge_color = property\n            self.layer.edge_color_mode = mode\n        except TypeError:\n            \n            \n            self._on_edge_color_mode_change()\n            raise\n\n    def change_edge_color_mode(self, mode: str):\n        \n        old_mode = self.layer.edge_color_mode\n        with self.layer.events.edge_color_mode.blocker():\n            try:\n                self.layer.edge_color_mode = mode\n                self._update_edge_color_gui(mode)\n\n            except ValueError:\n                \n                self.layer.edge_color_mode = old_mode\n                raise\n\n    def change_edge_color_direct(self, color: np.ndarray):\n        \n        self.layer.edge_color = color\n\n    def change_width(self, value):\n        \n        self.layer.edge_width = value\n        self.widthSpinBox.clearFocus()\n        self.setFocus()\n\n    def change_length(self, value):\n        \n        self.layer.length = value\n        self.lengthSpinBox.clearFocus()\n        self.setFocus()\n\n    def _update_edge_color_gui(self, mode: str):\n        \n        if mode in ('cycle', 'colormap'):\n            self.edgeColorEdit.setHidden(True)\n            self.edge_color_label.setHidden(True)\n            self.color_prop_box.setHidden(False)\n            self.edge_prop_label.setHidden(False)\n\n        elif mode == 'direct':\n            self.edgeColorEdit.setHidden(False)\n            self.edge_color_label.setHidden(False)\n            self.color_prop_box.setHidden(True)\n            self.edge_prop_label.setHidden(True)\n\n    def _get_property_values(self):\n        \n        property_choices = [*self.layer._property_choices]\n        properties = [*self.layer.properties]\n        property_values = np.union1d(property_choices, properties)\n\n        return property_values\n\n    def _on_length_change(self):\n        \n        with self.layer.events.length.blocker():\n            self.lengthSpinBox.setValue(self.layer.length)\n\n    def _on_edge_width_change(self):\n        \n        with self.layer.events.edge_width.blocker():\n            self.widthSpinBox.setValue(self.layer.edge_width)\n\n    def _on_edge_color_mode_change(self):\n        \n        with qt_signals_blocked(self.color_mode_comboBox):\n            mode = self.layer._edge.color_mode\n            index = self.color_mode_comboBox.findText(\n                mode, Qt.MatchFixedString\n            )\n            self.color_mode_comboBox.setCurrentIndex(index)\n\n            self._update_edge_color_gui(mode)\n\n    def _on_edge_color_change(self):\n        \n        if (\n            self.layer._edge.color_mode == ColorMode.DIRECT\n            and len(self.layer.data) > 0\n        ):\n            with qt_signals_blocked(self.edgeColorEdit):\n                self.edgeColorEdit.setColor(self.layer.edge_color[0])\n        elif self.layer._edge.color_mode in (\n            ColorMode.CYCLE,\n            ColorMode.COLORMAP,\n        ):\n            with qt_signals_blocked(self.color_prop_box):\n                prop = self.layer._edge.color_properties.name\n                index = self.color_prop_box.findText(prop, Qt.MatchFixedString)\n                self.color_prop_box.setCurrentIndex(index)\n",
        "summary": "The `QtVectorsControls` class extends a base layer controls class to provide specific UI elements and functionality for vector layers in a graphical user interface. It includes widgets for adjusting edge width, length, color mode, and direct edge color, as well as handling events related to these properties."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport unittest\n\nimport isi_sdk_8_0_1\nfrom isi_sdk_8_0_1.models.hardware_tapes_devices import HardwareTapesDevices  \nfrom isi_sdk_8_0_1.rest import ApiException\n\n\nclass TestHardwareTapesDevices(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testHardwareTapesDevices(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script defines a unit test class `TestHardwareTapesDevices` that inherits from `unittest.TestCase`. The class includes methods for setting up and tearing down the test environment, as well as a placeholder method `testHardwareTapesDevices` which currently does nothing. The script imports necessary modules from the `isi_sdk_8_0_1` package to interact with hardware tapes devices, but no actual tests are implemented in this example."
    },
    {
        "code": "from dash import dcc, html\nfrom dash.dependencies import Input, Output\n\nfrom app import app\nfrom layouts import index, record, watch, replay, about\n\nfrom callbacks.record import *\nfrom callbacks.watch import *\nfrom callbacks.replay import *\n\nlayout = html.Article([\n    dcc.Location(id='url', refresh=False),  \n    html.Section(id='page-content'),  \n])\n\n\n@app.callback(Output('page-content', 'children'),\n              Input('url', 'pathname'))\ndef display_page(pathname):\n    if pathname == '/':\n        return index.layout\n    if pathname == '/record':\n        return record.layout\n    if pathname == '/watch':\n        return watch.layout\n    if pathname == '/replay':\n        return replay.layout\n    if pathname == '/about':\n        return about.layout\n    \n    \n    \n    \n\n\napp.config.suppress_callback_exceptions = True  \n\nif __name__ == '__main__':\n    import asyncio\n    from dash_xinet.server import run_server\n\n    port = 7777\n    \n    \n    run = run_server(app, layout,\n                     port=port, debug=True\n                     )\n    asyncio.run(run)\nelse:\n    app.layout = layout\n    server = app.server  \n",
        "summary": "This Python code sets up a Dash web application with multiple pages (index, record, watch, replay, about) and corresponding callback functions to handle user interactions. The application routes URLs to the appropriate page layouts and runs on port 7777 in debug mode."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\n\nimport cv2\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.rpn.bbox_transform import clip_boxes\nfrom model.nms.nms_wrapper import nms\nfrom model.rpn.bbox_transform import bbox_transform_inv\nfrom model.utils.net_utils import save_net, load_net, vis_detections\n\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\ntry:\n    xrange          \nexcept NameError:\n    xrange = range  \n\n\ndef parse_args():\n  \n  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n  parser.add_argument('--dataset', dest='dataset',\n                      help='training dataset',\n                      default='pascal_voc', type=str)\n  parser.add_argument('--cfg', dest='cfg_file',\n                      help='optional config file',\n                      default='cfgs/vgg16.yml', type=str)\n  parser.add_argument('--net', dest='net',\n                      help='vgg16, res50, res101, res152',\n                      default='res101', type=str)\n  parser.add_argument('--set', dest='set_cfgs',\n                      help='set config keys', default=None,\n                      nargs=argparse.REMAINDER)\n  parser.add_argument('--load_dir', dest='load_dir',\n                      help='directory to load models', default=\"models\",\n                      type=str)\n  parser.add_argument('--cuda', dest='cuda',\n                      help='whether use CUDA',\n                      action='store_true')\n  parser.add_argument('--ls', dest='large_scale',\n                      help='whether use large imag scale',\n                      action='store_true')\n  parser.add_argument('--mGPUs', dest='mGPUs',\n                      help='whether use multiple GPUs',\n                      action='store_true')\n  parser.add_argument('--cag', dest='class_agnostic',\n                      help='whether perform class_agnostic bbox regression',\n                      action='store_true')\n  parser.add_argument('--parallel_type', dest='parallel_type',\n                      help='which part of model to parallel, 0: all, 1: model before roi pooling',\n                      default=0, type=int)\n  parser.add_argument('--checksession', dest='checksession',\n                      help='checksession to load model',\n                      default=1, type=int)\n  parser.add_argument('--checkepoch', dest='checkepoch',\n                      help='checkepoch to load network',\n                      default=1, type=int)\n  parser.add_argument('--checkpoint', dest='checkpoint',\n                      help='checkpoint to load network',\n                      default=10021, type=int)\n  parser.add_argument('--vis', dest='vis',\n                      help='visualization mode',\n                      action='store_true')\n  parser.add_argument('--input_dir', dest='input_dir',\n                      help='directory to save models',\n                      type=str)\n  args = parser.parse_args()\n  return args\n\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\n\nif __name__ == '__main__':\n\n  args = parse_args()\n\n  print('Called with args:')\n  print(args)\n\n  if torch.cuda.is_available() and not args.cuda:\n    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\n  np.random.seed(cfg.RNG_SEED)\n  if args.dataset == \"pascal_voc\":\n      args.imdb_name = \"voc_2007_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"pascal_voc_0712\":\n      args.imdb_name = \"voc_2007_trainval+voc_2012_trainval\"\n      args.imdbval_name = \"voc_2007_test\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"coco\":\n      args.imdb_name = \"coco_2014_train+coco_2014_valminusminival\"\n      args.imdbval_name = \"coco_2014_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"imagenet\":\n      args.imdb_name = \"imagenet_train\"\n      args.imdbval_name = \"imagenet_val\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n  elif args.dataset == \"vg\":\n      args.imdb_name = \"vg_150-50-50_minitrain\"\n      args.imdbval_name = \"vg_150-50-50_minival\"\n      args.set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n\n  args.cfg_file = \"cfgs/{}/{}_ls.yml\".format(args.dataset, args.net) if args.large_scale else \"cfgs/{}/{}.yml\".format(\n      args.dataset, args.net)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print('Using config:')\n  pprint.pprint(cfg)\n\n  cfg.TRAIN.USE_FLIPPED = False\n  imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdbval_name, False)\n  imdb.competition_mode(on=True)\n\n  print('{:d} roidb entries'.format(len(roidb)))\n\n  input_dir = args.input_dir\n  if not os.path.exists(input_dir):\n    raise Exception('There is no input directory for loading network from ' + input_dir)\n  load_name = os.path.join(input_dir,\n    'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n\n  \n  if args.net == 'vgg16':\n    fasterRCNN = vgg16(imdb.classes, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res101':\n    fasterRCNN = resnet(imdb.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res50':\n    fasterRCNN = resnet(imdb.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == 'res152':\n    fasterRCNN = resnet(imdb.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n  else:\n    print(\"network is not defined\")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n\n  print(\"load checkpoint %s\" % (load_name))\n  checkpoint = torch.load(load_name)\n  fasterRCNN.load_state_dict(checkpoint['model'])\n  if 'pooling_mode' in checkpoint.keys():\n    cfg.POOLING_MODE = checkpoint['pooling_mode']\n\n\n  print('load model successfully!')\n  \n  im_data = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  num_boxes = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  \n  if args.cuda:\n    im_data = im_data.cuda()\n    im_info = im_info.cuda()\n    num_boxes = num_boxes.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  \n  im_data = Variable(im_data)\n  im_info = Variable(im_info)\n  num_boxes = Variable(num_boxes)\n  gt_boxes = Variable(gt_boxes)\n\n  if args.cuda:\n    cfg.CUDA = True\n\n  if args.cuda:\n    fasterRCNN.cuda()\n\n  start = time.time()\n  max_per_image = 100\n\n  vis = args.vis\n\n  if vis:\n    thresh = 0.05\n  else:\n    thresh = 0.0\n\n  save_name = 'faster_rcnn_10'\n  num_images = len(imdb.image_index)\n  all_boxes = [[[] for _ in xrange(num_images)]\n               for _ in xrange(imdb.num_classes)]\n\n  output_dir = get_output_dir(imdb, save_name)\n  dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n                        imdb.num_classes, training=False, normalize = False)\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n                            shuffle=False, num_workers=0,\n                            pin_memory=True)\n\n  data_iter = iter(dataloader)\n\n  _t = {'im_detect': time.time(), 'misc': time.time()}\n  det_file = os.path.join(output_dir, 'detections.pkl')\n\n  fasterRCNN.eval()\n  empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n  for i in range(num_images):\n\n      data = next(data_iter)\n      im_data.data.resize_(data[0].size()).copy_(data[0])\n      im_info.data.resize_(data[1].size()).copy_(data[1])\n      gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n      num_boxes.data.resize_(data[3].size()).copy_(data[3])\n\n      det_tic = time.time()\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, RCNN_loss_bbox, \\\n      rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n\n      scores = cls_prob.data\n      boxes = rois.data[:, :, 1:5]\n\n      if cfg.TEST.BBOX_REG:\n          \n          box_deltas = bbox_pred.data\n          if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n          \n            if args.class_agnostic:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4)\n            else:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n\n          pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n          pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n      else:\n          \n          _ = torch.from_numpy(np.tile(boxes, (1, scores.shape[1])))\n          pred_boxes = _.cuda() if args.cuda > 0 else _\n\n      pred_boxes /= data[1][0][2].item()\n\n      scores = scores.squeeze()\n      pred_boxes = pred_boxes.squeeze()\n      det_toc = time.time()\n      detect_time = det_toc - det_tic\n      misc_tic = time.time()\n      if vis:\n          im = cv2.imread(imdb.image_path_at(i))\n          im2show = np.copy(im)\n      for j in xrange(1, imdb.num_classes):\n          inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n          \n          if inds.numel() > 0:\n            cls_scores = scores[:,j][inds]\n            _, order = torch.sort(cls_scores, 0, True)\n            if args.class_agnostic:\n              cls_boxes = pred_boxes[inds, :]\n            else:\n              cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n\n            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n            \n            cls_dets = cls_dets[order]\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep.view(-1).long()]\n            if vis:\n              im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n            all_boxes[j][i] = cls_dets.cpu().numpy()\n          else:\n            all_boxes[j][i] = empty_array\n\n      \n      if max_per_image > 0:\n          image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                    for j in xrange(1, imdb.num_classes)])\n          if len(image_scores) > max_per_image:\n              image_thresh = np.sort(image_scores)[-max_per_image]\n              for j in xrange(1, imdb.num_classes):\n                  keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                  all_boxes[j][i] = all_boxes[j][i][keep, :]\n\n      misc_toc = time.time()\n      nms_time = misc_toc - misc_tic\n\n      sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n          .format(i + 1, num_images, detect_time, nms_time))\n      sys.stdout.flush()\n\n      if vis:\n          cv2.imwrite('result.png', im2show)\n          pdb.set_trace()\n          \n          \n\n  with open(det_file, 'wb') as f:\n      pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n  print('Evaluating detections')\n  imdb.evaluate_detections(all_boxes, output_dir)\n\n  end = time.time()\n  print(\"test time: %0.4fs\" % (end - start))\n",
        "summary": "This code is a Python script for object detection using a pre-trained deep learning model. It loads an image from the specified dataset, runs it through the model to predict object locations and classes, and then visualizes the results if requested. Here's a breakdown of what each part does:\n\n1. **Imports**: The script imports necessary libraries such as OpenCV for image processing, NumPy for numerical operations, PyTorch for deep learning, and other custom modules like `cfg`, `imdb`, `nms`, etc.\n\n2. **Configuration**: It loads configuration settings from a file named 'cfg.py' which contains parameters like network architecture, training details, and evaluation criteria.\n\n3. **Dataset Initialization**: The script initializes an instance of the dataset class (`imdb`) based on the provided arguments (e.g., dataset name). This class handles loading images, annotations, and other metadata.\n\n4. **Model Loading**: It loads a pre-trained model from a file specified by `args.load_name`. If no specific model is provided, it defaults to using a model trained on Pascal VOC 2007.\n\n5. **Evaluation Mode**: The script sets the model to evaluation mode (`model.eval()`) which disables dropout and batch normalization layers during inference.\n\n6. **Image Loading and Preprocessing**: For each image in the dataset:\n   - It reads the image using OpenCV.\n   - Resizes and normalizes the image according to the model's requirements.\n   - Converts the image to a PyTorch tensor and moves it to the appropriate device (CPU or GPU).\n\n7. **Forward Pass**: The script runs the preprocessed image through the model to get predictions:\n   - It computes region proposals (RPN) and their associated scores.\n   - For each proposal, it predicts bounding box coordinates and class probabilities.\n\n8. **Post-Processing**:\n   - If bounding box regression is enabled, it applies inverse transformations to convert predicted deltas back to actual bounding boxes.\n   - Clips the bounding boxes to ensure they stay within the image boundaries.\n   - Applies non-maximum suppression (NMS) to remove overlapping detections and keep only the most confident ones.\n\n9. **Visualization**: If visualization is enabled (`vis=True`), it draws bounding boxes and class labels on the original image and saves it as 'result.png'.\n\n10. **Saving Results**: The script saves all detected bounding boxes in a pickle file for later evaluation.\n\n11. **Evaluation**: Finally, it evaluates the model's performance using the `evaluate_detections` method of the dataset instance, which compares predicted results with ground truth annotations and outputs metrics like precision, recall, and mAP (mean Average Precision).\n\n12. **Timing**: The script measures and prints the total time taken for inference.\n\nThis script is typically used in scenarios where you need to deploy a pre-trained object detection model on new images or datasets without retraining it from scratch."
    },
    {
        "code": "import click\nfrom flask.cli import FlaskGroup\nfrom . import create_app\n\n\n@click.group(cls=FlaskGroup, create_app=create_app)\ndef main():\n    \n\n\nif __name__ == \"__main__\":  \n    main()\n",
        "summary": "The provided Python code defines a command-line interface (CLI) for a Flask application using the `click` and `flask.cli` libraries. It creates a CLI group that invokes the `create_app` function to initialize and run the Flask app when executed."
    },
    {
        "code": "import pytorch_lightning as pl\nfrom loss.loss import get_loss\nfrom optimizer.optimizer import get_optimizer\nfrom scheduler.scheduler import get_scheduler\n\nimport torch\nimport numpy as np\nfrom pytorch_lightning.metrics import Accuracy\nimport segmentation_models_pytorch as smp\n\nfrom utils.utils import load_obj\nimport albumentations as A\nfrom utils.preprocessing import *\nimport shutil\n\n\n\nclass LitClassifier(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super().__init__()\n        self.save_hyperparameters(hparams)\n        self.model = model\n        self.criteria = get_loss(hparams.training.loss)\n        \n        self.dice =  smp.utils.losses.DiceLoss(activation='sigmoid')\n\n    def forward(self, x):\n        \n        return self.model(x)\n\n    def configure_optimizers(self):\n        optimizer = get_optimizer(self.model.parameters(), self.hparams.training.optimizer)\n\n        scheduler = get_scheduler(optimizer, self.hparams.training.scheduler)\n        \n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        if self.hparams.dataset.mixup:\n            num_batch = self.hparams.dataset.batch_size\n            alpha = 0.2\n            \n            \n            \n            \n            rnd = torch.from_numpy(np.random.beta(alpha,alpha,1)).type_as(x)\n            x = x[:int(num_batch/2)]*rnd + x[int(num_batch/2):]*(1-rnd)\n        y_hat = self.model(x)\n        if self.hparams.dataset.mixup:\n            loss = self.criteria(y_hat, y[:int(num_batch/2)])*rnd + self.criteria(y_hat, y[int(num_batch/2):])*(1-rnd)\n        else:\n            loss = self.criteria(y_hat, y)\n        self.log('train_loss', loss, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criteria(y_hat, y)\n        dice = 1-self.dice(y_hat, y)\n\n        \n        \n\n        return {\n            \"val_loss\": loss,\n            \"val_dice\": dice\n            }\n    \n    def validation_epoch_end(self, outputs):\n        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        avg_val_dice = torch.stack([x[\"val_dice\"] for x in outputs]).mean()\n\n        self.log('val_loss', avg_val_loss)\n        self.log('val_dice', avg_val_dice)\n        \n        \n\n        \n\n        \n\n        \n        \n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criteria(y_hat, y)\n        self.log('test_loss', loss)\n\n    \n",
        "summary": "The provided Python code defines a PyTorch Lightning module named `LitClassifier` for training and evaluating a segmentation model. It includes methods for initializing the model, configuring optimizers and schedulers, performing forward passes, computing losses during training and validation, and logging metrics such as loss and Dice score."
    },
    {
        "code": "from pathlib import Path\nimport os\nimport re\nimport sys\nimport pandas as pd\nimport numpy as np\n\nPCODES = dict([\n    \n    (1011, 'Amsterdam'),\n    (1625, 'Hoorn|Zwaag'),\n    (1811, 'Alkmaar'),\n    (7471, 'Goor'),\n    (7556, 'Hengelo'),\n    (7903, 'Hoogeveen'),\n    (7942, 'Meppel'),\n    (8011, 'Zwolle'),\n    (8232, 'Lelystad'),\n    (8442, 'Heerenveen'),\n    (8911, 'Leeuwarden'),\n    (9291, 'Kollum'),\n    (9501, 'Stadskanaal'),\n    (9726, 'Groningen'),\n\n    \n    (2406, 'Alphen a/d Rijn'),\n    (2515, 'Den Haag'),\n    (3013, 'Rotterdam'),\n    (3511, 'Utrecht'),\n    (3901, 'Veenendaal'),\n    ((7137, 7131), 'Lichtenvoorde|Groenlo'),\n    (7311, 'Apeldoorn'),\n\n    \n    (4325, 'Renesse'),\n    (4462, 'Goes'),\n    (4701, 'Roosendaal'),\n    (5038, 'Tilburg'),\n    (5401, 'Uden'),\n    (5611, 'Eindhoven'),\n    (5801, 'Oostrum'),\n    (6101, 'Echt'),\n    (6229, 'Maastricht'),\n    (6541, 'Nijmegen'),\n    ])\n\n\ndef get_bad_scan_times():\n    \n    df = pd.read_csv('data-ggd/ggd_bad_scans.txt', comment='\n    tstamps = pd.to_datetime(df['Timestamp']).to_list()\n    return tstamps\n\ndef _mean_time(ts_list):\n    \n    ts0 = ts_list[0]\n    delta_sum = pd.Timedelta(0)\n    for ts in ts_list:\n        delta_sum += (ts -ts0)\n    ts_mean = ts0 + delta_sum / len(ts_list)\n    return ts_mean\n\n\ndef _delta_time_hhmm(hm):\n    \n    return pd.Timedelta(f'{hm}:00')\n\n\ndef _summary_to_scores(summary):\n    \n\n    \n    scores = {k: '?' for k in PCODES}\n    multi_pcs = {}  \n    for pc in PCODES:\n        if isinstance(pc, tuple):\n            for pc1 in pc:\n                multi_pcs[pc1] = pc\n\n    qtms = []\n    dhm = _delta_time_hhmm\n    for pc4, vlist in summary.items():\n        pc4 = int(pc4)\n        if pc4 not in scores:\n            if pc4 in multi_pcs:\n                pc4_key = multi_pcs[pc4]\n            else:\n                print(f'{pc4} not in list...')\n                continue\n        else:\n            pc4_key = pc4\n        if len(vlist) == 0:\n            scores[pc4_key] = 7\n            continue\n        qtm = _mean_time([v[0] for v in vlist]) \n        qtms.append(qtm)\n        atm = min(v[1] for v in vlist) \n        qtm_00 = pd.Timestamp(qtm.strftime('%Y-%m-%dT00:00'))\n        thresholds = [\n            (3, qtm_00 + dhm('23:59')),\n            (4, qtm + dhm('24:00')),\n            (5, qtm_00 + dhm('48:00')),\n            (6, qtm + dhm('48:00')),\n            (6.3, qtm_00 + dhm('72:00')),\n            (6.7, qtm + dhm('72:00')),\n            (7, atm)\n            ]\n        if qtm.hour < 9:\n            thresholds.insert(0, (1, qtm_00 + dhm('13:00')))\n        elif qtm.hour < 13:\n            thresholds.insert(0, (1, qtm + dhm('4:00')))\n        elif qtm.hour < 17:\n            thresholds.insert(0, (1, qtm_00 + dhm('24:00')))\n            thresholds.insert(1, (2, qtm + dhm('20:00')))\n        else:\n            thresholds.insert(0, (1, qtm_00 + dhm('24:00')))\n            thresholds.insert(1, (2, qtm_00 + dhm('37:00')))\n\n        for s, tm in thresholds:\n            if atm < tm:\n                scores[pc4_key] = s\n                break\n    if len(qtms) == 0:\n        qtm_mid = pd.Timestamp(None)\n    else:\n        qtm_min = min(qtms)\n        qtm_mid = qtm_min + (max(qtms) - qtm_min)/2\n    return scores, qtm_mid\n\n\ndef _get_min_wait(summary):\n    \n    wtimes = []\n    for _, vlist in summary.items():\n        wtimes_this = [atm - qtm for qtm, atm in vlist]\n        wtimes.append(\n            min(wtimes_this) if wtimes_this else pd.Timedelta(99, 'h')\n            )\n    minwait = min(wtimes) if wtimes else 999\n    medwait = pd.Timedelta(np.median(wtimes))\n    return minwait, medwait\n\n\ndef load_csv(csv_fname):\n    \n    df = pd.read_csv(csv_fname, comment='\n    df['req_pc4'] = df['req_pc4'].astype(int)\n\n    for c in df.columns:\n        if c.endswith('_time') or c.endswith('_date'):\n            df[c] = pd.to_datetime(df[c])\n        else:\n            df.loc[df[c].isna(), c] = None\n\n    \n    start_tms = df.loc[df['scan_time'].diff() > pd.Timedelta('10 min'), 'scan_time']\n    start_tms = [df.iloc[0]['scan_time']] + list(start_tms)\n    start_tms += [df.iloc[-1]['scan_time'] + pd.Timedelta('1 min')]\n    return df, start_tms\n\ndef load_multi_csvs(csv_fnames):\n    \n    dfs = []\n    start_tms = []\n    for f in csv_fnames:\n        df, st = load_csv(f)\n        dfs.append(df)\n        start_tms.extend(st[:-1])\n    df = pd.concat(dfs).reset_index()\n    start_tms.append(df.iloc[-1]['scan_time'] + pd.Timedelta('1 min'))\n    return df, start_tms\n\n\ndef get_scan_scores(df, tm_range):\n    \n    mask = (df['scan_time'] >= tm_range[0]) & (df['scan_time'] < tm_range[1])\n    df1 = df.loc[mask]\n    summary = {}\n    for pc4, city_re in PCODES.items():\n        pc4_tup = (pc4,) if isinstance(pc4, int) else pc4\n        options = []\n        req_pc4 = None\n        for _, row in df1.loc[df1['req_pc4'].isin(pc4_tup)].iterrows():\n            req_pc4 = int(row['req_pc4'])\n            for i in range(3):\n                addr = row[f'opt{i}_short_addr']\n                if addr and re.match(f'{city_re}$', addr[5:]):\n                    options.append((row['scan_time'], row[f'opt{i}_time']))\n        if req_pc4 is not None:\n            summary[req_pc4] = options\n    scores, tstamp = _summary_to_scores(summary)\n    if pd.isna(tstamp):\n        tstamp = df1.iloc[len(df1)//2]['scan_time']\n    minwait, medwait = _get_min_wait(summary)\n    if medwait == 999:\n        medwait = pd.Timedelta(None)\n    return tstamp, scores, minwait, medwait\n\n\ndef get_scan_scores_df(df, tm_ranges, decimal_comma=True):\n    \n    n = len(tm_ranges)\n    records = []\n    index = []\n    minwait_hs = []\n    medwait_hs = []\n    bad_stimes = get_bad_scan_times()\n    for i in range(n-1):\n        tm_ra = tm_ranges[i:i+2]\n        is_ok = True\n        for tm in bad_stimes:\n            if tm_ra[0] <= tm < tm_ra[1]:\n                is_ok = False\n                break\n        if not is_ok:\n            print(f'Dropped scan at {tm_ra[0].strftime(\"%Y-%m-%d %H:%M\")}')\n            continue\n        tm, scores, minwait, medwait = get_scan_scores(df, tm_ra)\n        records.append(scores)\n        index.append(tm)\n        minwait_hs.append(minwait.total_seconds() / 3600)\n        medwait_hs.append(medwait.total_seconds() / 3600)\n\n    dates = [t.strftime('%Y-%m-%d') for t in index]\n    times = [t.strftime('%H:%M') for t in index]\n    sdf = pd.DataFrame.from_records(records)\n    sdf.insert(0, 'Time', times)\n    sdf.insert(0, 'Date', dates)\n    sdf['min_wait_h'] = np.around(minwait_hs, 2)\n    sdf['med_wait_h'] = np.around(medwait_hs, 2)\n    sdf.loc[sdf['min_wait_h'].isna(), 'min_wait_h'] = 999\n    sdf.columns = [\n        ('/'.join([str(x) for x in c]) if isinstance(c, tuple) else c)\n        for c in sdf.columns\n        ]\n    if decimal_comma:\n        for c in sdf.columns[2:]:\n            sdf[c] = sdf[c].astype(str)\n            sdf[c] = sdf[c].str.replace('.', ',', regex=False)\n            sdf[c] = sdf[c].str.replace(',0$', '', regex=False)\n            sdf[c] = sdf[c].str.replace('?', '', regex=False)\n\n    return sdf\n\n\nif __name__ == '__main__':\n\n    in_spyder = ('SPYDER_ARGS' in os.environ)\n    csv_fnames = sorted(Path('data-ggd').glob('ggd_scan-????-W??.csv'))\n    do_all = ('--all' in sys.argv)\n    do_all = do_all or in_spyder and input('(A)ll or latest?').lower() == 'a'\n    if do_all:\n        df, start_tms = load_multi_csvs(csv_fnames)\n        sdf = get_scan_scores_df(df, start_tms).iloc[::-1]\n    else:\n        df, start_tms = load_csv(csv_fnames[-1])\n        sdf = get_scan_scores_df(df, start_tms[-2:])\n    print(sdf)\n    if len(sdf) > 1:\n        sdf.to_clipboard(index=False)\n        print('Copied to clipboard including headers')\n    elif len(sdf) == 1:\n        sdf.iloc[[0], 2:].to_clipboard(header=False, index=False)\n        print('Copied to clipboard, scores only.')\n    else:\n        print('No output.')\n\n    if not in_spyder:\n        \n        input('Press Enter to quit and clear clipboard.')\n\n",
        "summary": "This Python script is designed to analyze data from multiple CSV files containing scan results. The main functionalities include:\n\n1. **Loading Data**: It can load data from a single CSV file or multiple CSV files located in the 'data-ggd' directory.\n\n2. **Filtering Data**: It filters the data based on specified time ranges, which are determined by scanning through the loaded data to identify suitable intervals.\n\n3. **Scoring and Analysis**:\n   - For each identified interval, it summarizes the scan results for different regions (cities).\n   - It calculates scores based on these summaries.\n   - It determines the minimum wait time and median wait time for scans in each region.\n\n4. **Output Formatting**: The script formats the output into a DataFrame that includes dates, times, scores, and wait times. If running outside of an IDE like Spyder, it also allows copying the results to the clipboard.\n\n5. **User Interaction**: It provides options to either process all available data or just the latest file. This is particularly useful when running in environments where user input might be required (like a command-line interface).\n\n### Key Components:\n\n- **Data Loading Functions**:\n  - `load_csv`: Loads data from a single CSV file.\n  - `load_multi_csvs`: Loads data from multiple CSV files.\n\n- **Scoring and Analysis Functions**:\n  - `get_scan_scores`: Analyzes scan data within a specified time range.\n  - `_summary_to_scores`: Converts summary data into scores for different regions.\n  - `_get_min_wait`: Calculates the minimum wait time for scans in each region.\n\n- **Output Formatting**:\n  - `get_scan_scores_df`: Formats the analysis results into a DataFrame with appropriate headers and units (hours).\n\n### Usage:\n\nThe script can be run directly from the command line. It will prompt the user to choose between processing all available data or just the latest file if running in an environment where user input is required.\n\n### Example Output:\n\n```\n  Date   Time  /1024/1024  /1024/1024  /1024/1024  /1024/1024  /1024/1024  min_wait_h  med_wait_h\n0  2023-04-01  15:00  9.876,  8.765,  7.654,  6.543,  5.432,  4.321,  3.210,  2.109,  1.098\n1  2023-04-01  16:00  8.765,  7.654,  6.543,  5.432,  4.321,  3.210,  2.109,  1.098\n```\n\nThis output shows the scores for different regions and wait times in hours.\n\n### Notes:\n\n- The script assumes that the CSV files have a specific structure with columns like 'scan_time', 'req_pc4', 'opt{i}_short_addr', and 'opt{i}_time'.\n- It uses regular expressions to match addresses against city codes.\n- The script handles user input for processing all data or just the latest file, which is particularly useful in environments where interactive prompts are required."
    },
    {
        "code": "from re import T\nimport sys\nimport fire\nimport questionary\nfrom pathlib import Path\nimport csv \n\nfrom qualifier.utils.fileio import (\n    load_csv,\n    save_csv,\n) \n\nfrom qualifier.utils.calculators import (\n    calculate_monthly_debt_ratio,\n    calculate_loan_to_value_ratio,\n)\n\nfrom qualifier.filters.max_loan_size import filter_max_loan_size\nfrom qualifier.filters.credit_score import filter_credit_score\nfrom qualifier.filters.debt_to_income import filter_debt_to_income\nfrom qualifier.filters.loan_to_value import filter_loan_to_value\n\ndef load_bank_data():\n    \n\n    csvpath = questionary.text(\"Enter a file path to a rate-sheet (.csv):\").ask()\n    csvpath = Path(csvpath)\n    if not csvpath.exists():\n        sys.exit(f\"Oops! Can't find this path: {csvpath}\")\n\n    return load_csv(csvpath)\n\n\ndef get_applicant_info():\n    \n\n    credit_score = questionary.text(\"What's your credit score?\").ask()\n    debt = questionary.text(\"What's your current amount of monthly debt?\").ask()\n    income = questionary.text(\"What's your total monthly income?\").ask()\n    loan_amount = questionary.text(\"What's your desired loan amount?\").ask()\n    home_value = questionary.text(\"What's your home value?\").ask()\n\n    credit_score = int(credit_score)\n    debt = float(debt)\n    income = float(income)\n    loan_amount = float(loan_amount)\n    home_value = float(home_value)\n\n    return credit_score, debt, income, loan_amount, home_value\n\n\ndef find_qualifying_loans(bank_data, credit_score, debt, income, loan, home_value):\n    \n\n    \n    monthly_debt_ratio = calculate_monthly_debt_ratio(debt, income)\n    print(f\"The monthly debt to income ratio is {monthly_debt_ratio:.02f}\")\n\n    \n    loan_to_value_ratio = calculate_loan_to_value_ratio(loan, home_value)\n    print(f\"The loan to value ratio is {loan_to_value_ratio:.02f}.\")\n\n    \n    bank_data_filtered = filter_max_loan_size(loan, bank_data)\n    bank_data_filtered = filter_credit_score(credit_score, bank_data_filtered)\n    bank_data_filtered = filter_debt_to_income(monthly_debt_ratio, bank_data_filtered)\n    bank_data_filtered = filter_loan_to_value(loan_to_value_ratio, bank_data_filtered)\n\n    print(f\"Found {len(bank_data_filtered)} qualifying loans\")\n\n    return bank_data_filtered\n\n\ndef save_qualifying_loans(qualifying_loans):\n    \n    \n    \n\n    choice = questionary.confirm (\"Would you like to save the qualifying loans?\").ask()\n\n    if (choice == T) :\n        filepath = questionary.text (\"Please enter the file path\").ask()\n        save_csv(qualifying_loans, filepath)\n\n\ndef run():\n    \n\n    \n    bank_data = load_bank_data()\n\n    \n    credit_score, debt, income, loan_amount, home_value = get_applicant_info()\n\n    \n    qualifying_loans = find_qualifying_loans(\n        bank_data, credit_score, debt, income, loan_amount, home_value\n    )\n\n    \n    save_qualifying_loans(qualifying_loans)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(run)\n\n",
        "summary": "The provided Python script is a command-line application that helps users find and potentially save qualifying loans based on their financial information. It prompts the user for credit score, debt, income, loan amount, and home value, then filters potential loans from a dataset using various criteria such as maximum loan size, credit score, debt-to-income ratio, and loan-to-value ratio. If any qualifying loans are found, the script offers to save them to a CSV file."
    },
    {
        "code": "from pathlib import Path\n\nimport pytest\n\nfrom pants.backend.codegen.export_codegen_goal import ExportCodegen\nfrom pants.backend.codegen.export_codegen_goal import rules as write_codegen_rules\nfrom pants.core.target_types import FilesSources, ResourcesSources\nfrom pants.core.util_rules import distdir\nfrom pants.engine.fs import CreateDigest, FileContent, Snapshot\nfrom pants.engine.rules import Get, rule\nfrom pants.engine.target import GeneratedSources, GenerateSourcesRequest, Sources, Target\nfrom pants.engine.unions import UnionRule\nfrom pants.testutil.rule_runner import RuleRunner\n\n\nclass Gen1Sources(Sources):\n    pass\n\n\nclass Gen2Sources(Sources):\n    pass\n\n\nclass Gen1Target(Target):\n    alias = \"gen1\"\n    core_fields = (Gen1Sources,)\n\n\nclass Gen2Target(Target):\n    alias = \"gen2\"\n    core_fields = (Gen2Sources,)\n\n\nclass Gen1Request(GenerateSourcesRequest):\n    input = Gen1Sources\n    output = FilesSources\n\n\nclass Gen2Request(GenerateSourcesRequest):\n    input = Gen2Sources\n    output = ResourcesSources\n\n\n@rule\nasync def gen1(_: Gen1Request) -> GeneratedSources:\n    result = await Get(Snapshot, CreateDigest([FileContent(\"assets/README.md\", b\"Hello!\")]))\n    return GeneratedSources(result)\n\n\n@rule\nasync def gen2(_: Gen2Request) -> GeneratedSources:\n    result = await Get(Snapshot, CreateDigest([FileContent(\"src/haskell/app.hs\", b\"10 * 4\")]))\n    return GeneratedSources(result)\n\n\n@pytest.fixture\ndef rule_runner() -> RuleRunner:\n    return RuleRunner(\n        rules=[\n            *write_codegen_rules(),\n            gen1,\n            gen2,\n            UnionRule(GenerateSourcesRequest, Gen1Request),\n            UnionRule(GenerateSourcesRequest, Gen2Request),\n            *distdir.rules(),\n        ],\n        target_types=[Gen1Target, Gen2Target],\n    )\n\n\ndef test_no_codegen_targets(rule_runner: RuleRunner, caplog) -> None:\n    result = rule_runner.run_goal_rule(ExportCodegen)\n    assert result.exit_code == 0\n    assert len(caplog.records) == 1\n    assert \"No codegen files/targets matched. All codegen target types: gen1, gen2\" in caplog.text\n\n\ndef test_export_codegen(rule_runner: RuleRunner) -> None:\n    rule_runner.add_to_build_file(\"\", \"gen1(name='gen1')\\ngen2(name='gen2')\\n\")\n    result = rule_runner.run_goal_rule(ExportCodegen, args=[\"::\"])\n    assert result.exit_code == 0\n    parent_dir = Path(rule_runner.build_root, \"dist\", \"codegen\")\n    assert (parent_dir / \"assets\" / \"README.md\").read_text() == \"Hello!\"\n    assert (parent_dir / \"src\" / \"haskell\" / \"app.hs\").read_text() == \"10 * 4\"\n",
        "summary": "The provided Python code defines custom targets and rules for generating sources in a Pants build system. It includes specific target types (`Gen1Target` and `Gen2Target`) with associated request classes (`Gen1Request` and `Gen2Request`). The rules (`gen1` and `gen2`) handle the generation of files and resources based on these requests. The code also features a test suite using `pytest` to validate the functionality of the generated sources and their export through an `ExportCodegen` goal."
    },
    {
        "code": "from collections import defaultdict\nfrom operator import itemgetter\n\n\nfrom movies_analyzer.Movies import Movies\nfrom movies_analyzer.RecommendationDataset import RecommendationDataSet\nfrom movies_recommender.Recommender import Recommender\nfrom surprise import SVD, KNNBasic\n\nfrom movies_recommender.utils import get_top_n\n\n\nclass RecommenderSVD(Recommender):\n    def __init__(self, recommendation_dataset: RecommendationDataSet):\n        super(RecommenderSVD, self).__init__(recommendation_dataset.movies)\n        self.algorithm = SVD()\n        self.recommendation_dataset = recommendation_dataset\n\n    def fit(self, dataset):\n        return self.algorithm.fit(dataset)\n\n    def test(self, test_set):\n        return self.algorithm.test(test_set)\n\n    def get_recommendation(self, watched, k=20):\n        \n        new_user_id, full_dataset = self.recommendation_dataset.get_dataset_with_extended_user(watched)\n        inner_user_id = full_dataset.to_inner_uid(new_user_id)\n\n        \n        \n        self.algorithm.fit(full_dataset)\n\n        \n        watched = {full_dataset.to_inner_iid(key): value for key,value in watched.items()}\n\n        \n        test_items = [\n            self.algorithm.predict(new_user_id, full_dataset.to_raw_iid(i))\n            for i in range(0, full_dataset.n_items)\n            if i not in watched\n        ]\n\n        topn_items = [i[0] for i in get_top_n(test_items, n=k, minimum_rating=1.0)[new_user_id]]\n        return self.movies.get_movie_by_movie_ids(topn_items)\n\n\nif __name__ == '__main__':\n    from movies_recommender.Recommender import test_recommendation\n    from movies_recommender.RecommenderSVD import RecommenderSVD\n    from movies_analyzer.RecommendationDataset import RecommendationDataSet\n    from movies_analyzer.Movies import Movies\n\n    movies = Movies()\n    recommendation_dataset = RecommendationDataSet(movies=movies)\n    recommender = RecommenderSVD(recommendation_dataset)\n\n    assert recommender.__module__[:len('movies_recommender.')] == 'movies_recommender.'\n    test_recommendation(recommender, recommendation_dataset, \n                        example_items=['arek','mateusz'], anti_test=True)\n\n    \n",
        "summary": "The provided Python code defines a class `RecommenderSVD` that extends the base `Recommender` class to implement a movie recommendation system using the Singular Value Decomposition (SVD) algorithm from the Surprise library. The class includes methods for fitting the model, testing its performance on a test set, and generating recommendations for new users based on their watched movies."
    },
    {
        "code": "from .rpsl import Rpsl\n\n\nclass Maintainer(Rpsl):\n\n    def __init__(self):\n        self.handle = None\n        self.description = list()\n        self.update_to = list()\n        self.maintainer_notify = list()\n        self.authentication = list()\n        super().__init__()\n\n    def html(self, heading_level=1):\n        return super().html(\n            title='Maintainer',\n            attributes=[\n                (None, self.handle),\n                ('Description', self.description),\n                ('Update To', self.update_to),\n                ('Maintainer Notify', self.maintainer_notify),\n                ('Authentication', self.authentication),\n                ('Organisation', self.organisation),\n                ('Admin Contact', self.admin_contact),\n                ('Technical Contact', self.technical_contact),\n                ('Remarks', self.remarks),\n                ('Notify', self.notify),\n                ('Maintained By', self.maintained_by),\n                ('Modified', self.modified),\n                ('Type', self.type_),\n            ]\n        )\n",
        "summary": "The `Maintainer` class extends the `Rpsl` class and is designed to handle maintainer information in a specific format, providing methods for initialization and HTML representation of maintainer details."
    },
    {
        "code": "from django.db.models import query\n\nfrom .query import SafeDeleteQuery\nfrom functools import partial, reduce\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.db.models import Max, Min, F\nfrom django.utils.module_loading import import_string\n\n\ndef get_lookup_value(obj, field):\n    return reduce(lambda i, f: getattr(i, f), field.split(LOOKUP_SEP), obj)\n\n\nclass SafeDeleteQueryset(query.QuerySet):\n    \n\n    def __init__(self, model=None, query=None, using=None, hints=None):\n        super(SafeDeleteQueryset, self).__init__(model=model, query=query, using=using, hints=hints)\n        self.query = query or SafeDeleteQuery(self.model)\n\n    def delete(self, force_policy=None):\n        \n        assert self.query.can_filter(), \"Cannot use 'limit' or 'offset' with delete.\"\n        \n        for obj in self.all():\n            obj.delete(force_policy=force_policy)\n        self._result_cache = None\n    delete.alters_data = True\n\n    def undelete(self, force_policy=None):\n        \n        assert self.query.can_filter(), \"Cannot use 'limit' or 'offset' with undelete.\"\n        \n        for obj in self.all():\n            obj.undelete(force_policy=force_policy)\n        self._result_cache = None\n    undelete.alters_data = True\n\n    def all(self, force_visibility=None):\n        \n        if force_visibility is not None:\n            self.query._safedelete_force_visibility = force_visibility\n        return super(SafeDeleteQueryset, self).all()\n\n    def filter(self, *args, **kwargs):\n        \n        queryset = self._clone()\n        queryset.query.check_field_filter(**kwargs)\n        return super(SafeDeleteQueryset, queryset).filter(*args, **kwargs)\n\n\nclass OrderedSafeDeleteQueryset(SafeDeleteQueryset):\n    \n\n    def _get_order_field_name(self):\n        return self.model.order_field_name\n\n    def _get_order_field_lookup(self, lookup):\n        order_field_name = self._get_order_field_name()\n        return LOOKUP_SEP.join([order_field_name, lookup])\n\n    def _get_order_with_respect_to(self):\n        model = self.model\n        order_with_respect_to = model.order_with_respect_to\n        if isinstance(order_with_respect_to, str):\n            order_with_respect_to = (order_with_respect_to,)\n        if order_with_respect_to is None:\n            raise AssertionError(\n                (\n                    'ordered model admin \"{0}\" has not specified \"order_with_respect_to\"; note that this '\n                    \"should go in the model body, and is not to be confused with the Meta property of the same name, \"\n                    \"which is independent Django functionality\"\n                ).format(model)\n            )\n        return order_with_respect_to\n\n    def get_max_order(self):\n        order_field_name = self._get_order_field_name()\n        return self.aggregate(Max(order_field_name)).get(\n            self._get_order_field_lookup(\"max\")\n        )\n\n    def get_min_order(self):\n        order_field_name = self._get_order_field_name()\n        return self.aggregate(Min(order_field_name)).get(\n            self._get_order_field_lookup(\"min\")\n        )\n\n    def get_next_order(self):\n        order = self.get_max_order()\n        return order + 1 if order is not None else 0\n\n    def above(self, order, inclusive=False):\n        \n        lookup = \"gte\" if inclusive else \"gt\"\n        return self.filter(**{self._get_order_field_lookup(lookup): order})\n\n    def above_instance(self, ref, inclusive=False):\n        \n        order_field_name = self._get_order_field_name()\n        order = getattr(ref, order_field_name)\n        return self.above(order, inclusive=inclusive)\n\n    def below(self, order, inclusive=False):\n        \n        lookup = \"lte\" if inclusive else \"lt\"\n        return self.filter(**{self._get_order_field_lookup(lookup): order})\n\n    def below_instance(self, ref, inclusive=False):\n        \n        order_field_name = self._get_order_field_name()\n        order = getattr(ref, order_field_name)\n        return self.below(order, inclusive=inclusive)\n\n    def decrease_order(self, **extra_kwargs):\n        \n        order_field_name = self._get_order_field_name()\n        update_kwargs = {order_field_name: F(order_field_name) - 1}\n        if extra_kwargs:\n            update_kwargs.update(extra_kwargs)\n        return self.update(**update_kwargs)\n\n    def increase_order(self, **extra_kwargs):\n        \n        order_field_name = self._get_order_field_name()\n        update_kwargs = {order_field_name: F(order_field_name) + 1}\n        if extra_kwargs:\n            update_kwargs.update(extra_kwargs)\n        return self.update(**update_kwargs)\n\n    def bulk_create(self, objs, batch_size=None):\n        order_field_name = self._get_order_field_name()\n        order_with_respect_to = self.model.order_with_respect_to\n        objs = list(objs)\n        if order_with_respect_to:\n            order_with_respect_to_mapping = {}\n            order_with_respect_to = self._get_order_with_respect_to()\n            for obj in objs:\n                key = tuple(\n                    get_lookup_value(obj, field) for field in order_with_respect_to\n                )\n                if key in order_with_respect_to_mapping:\n                    order_with_respect_to_mapping[key] += 1\n                else:\n                    order_with_respect_to_mapping[\n                        key\n                    ] = self.filter_by_order_with_respect_to(obj).get_next_order()\n                setattr(obj, order_field_name, order_with_respect_to_mapping[key])\n        else:\n            for order, obj in enumerate(objs, self.get_next_order()):\n                setattr(obj, order_field_name, order)\n        return super().bulk_create(objs, batch_size=batch_size)\n\n    def _get_order_with_respect_to_filter_kwargs(self, ref):\n        order_with_respect_to = self._get_order_with_respect_to()\n        _get_lookup_value = partial(get_lookup_value, ref)\n        return {field: _get_lookup_value(field) for field in order_with_respect_to}\n\n    _get_order_with_respect_to_filter_kwargs.queryset_only = False\n\n    def filter_by_order_with_respect_to(self, ref):\n        order_with_respect_to = self.model.order_with_respect_to\n        if order_with_respect_to:\n            filter_kwargs = self._get_order_with_respect_to_filter_kwargs(ref)\n            return self.filter(**filter_kwargs)\n        return self\n\n\n",
        "summary": "The provided Python code defines a custom Django QuerySet class, `SafeDeleteQueryset`, which extends the standard Django QuerySet to include methods for safely deleting and undeleting objects while respecting soft delete policies. It also includes additional functionality in the `OrderedSafeDeleteQueryset` subclass to manage ordered models by providing methods to manipulate their order fields efficiently."
    },
    {
        "code": "import numpy as np\nfrom itertools import product\nfrom markovGames.gameDefs.mdpDefs import Policy\n\n\ndef getAllDetPol(numStates, numActions):\n    detProbs = [np.array([1 if j == i else 0 for j in range(numActions)]) for i in range(numActions)]\n    return product(detProbs, repeat=numStates)\n\n\ndef getPolList(states, acSet):\n    \n    numStates = len(states)\n    numActions = len(acSet)\n    detPol = getAllDetPol(numStates, numActions)\n    return [Policy(states, pol, acSet) for pol in detPol]\n\n\ndef prodPolList(states, listActions):\n    \n    polList = [getPolList(states, ac) for ac in listActions]\n    return polList\n\n\ndef getPayoff(utilMap, listAcSet):\n    \n    \n    def utilInd(index):\n        jointAc = [listAcSet[j][ind] for j, ind in enumerate(index)]\n        val = utilMap(jointAc)\n        return val\n\n    numPL = [len(pL) for pL in listAcSet]\n    payoff = np.zeros(numPL)\n    for ind in product(*[range(nI) for nI in numPL]):\n        payoff[ind] = utilInd(ind)\n    return payoff\n\n\ndef getArgOpt(tensor):\n    return np.unravel_index(np.argmax(tensor), tensor.shape)\n\n\ndef bruteFindNash(payoffList):\n    TOLERANCE = 1e-7\n    cpnes = list(np.argwhere(payoffList[0] > np.amax(payoffList[0], 0) - TOLERANCE))\n    cpnes = [tuple(cpne) for cpne in cpnes]\n    N = len(payoffList)\n\n    for i in range(1, N):\n        pMat = payoffList[i]\n        for cpne in cpnes[:]:\n            ind = cpne[:i] + (slice(None),) + cpne[i + 1:]\n            if pMat[cpne] < np.max(pMat[ind]) - TOLERANCE:\n                cpnes.pop(cpnes.index(cpne))\n    return cpnes\n\n\ndef getEfficiency(cpnes, welfareMat):\n    \n    pneWelf = [welfareMat[cpne] for cpne in cpnes]\n    opt = np.max(welfareMat)\n    priceRatios = [float(pne) / opt for pne in pneWelf]\n    return priceRatios\n\n\ndef getPoA(cpnes, welfareMat):\n    return min(getEfficiency(cpnes, welfareMat))\n",
        "summary": "The provided Python code defines functions to generate all deterministic policies for a given number of states and actions, compute payoffs based on utility mappings, find Nash equilibria through brute force, evaluate the efficiency of these equilibria relative to an optimal welfare state, and calculate the Price of Anarchy (PoA) as the minimum efficiency ratio."
    },
    {
        "code": "from setuptools import find_packages, setup\nimport io\n\ndef readfile(filename):\n    with io.open(filename, encoding=\"utf-8\") as stream:\n        return stream.read().split()\n\n\n\n\n\n\nrequiers = .split(\"\\n\")\n\n\n\nversion = readfile(\"VERSION\")[0].strip()\n\nwith open('README.md') as f:\n    long_description = f.read()\n\n\n\nNAME = \"cloudmesh-john\"\nDESCRIPTION = \"A command called john and foo for the cloudmesh shell\"\nAUTHOR = \"Gregor von Laszewski\"\nAUTHOR_EMAIL = \"laszewski@gmail.com\"\nURL = \"https://github.com/cloudmesh/cloudmesh-john\"\n\n\nsetup(\n    name=NAME,\n    author=AUTHOR,\n    author_email=AUTHOR_EMAIL,\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    version=version,\n    license=\"Apache 2.0\",\n    url=URL,\n    packages=find_packages(),\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Environment :: Web Environment\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 2\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    install_requires=requiers,\n    tests_require=[\n        \"flake8\",\n        \"coverage\",\n    ],\n    zip_safe=False,\n    namespace_packages=['cloudmesh'],\n)\n",
        "summary": "This Python script sets up a package using setuptools, defining metadata such as the name, version, and dependencies. It reads the project's version from a file, includes a README for long description, and specifies classifiers for distribution on PyPI."
    },
    {
        "code": "def checkCtdFileName(ctd=None, confile='.XMLCON'):\n    import Tkinter, tkFileDialog\n    import os\n    import time\n    import sys\n    import codecs\n    root = Tkinter.Tk()\n    root.withdraw() \n    \n    if ctd == None:\n        sys.exit(\"No CTD given, when calling checkCtdFileName() add a CTD number like checkCtdFileName(ctd='1044')\")\n    \n    file_path = tkFileDialog.askopenfilename(title=\"Open file\", initialdir=\"C:\\\\ctd\\\\temp\",\n    filetypes=[(\"hdr files\",\".hdr\")]) \n\n\n        \n    \n    fname = os.path.basename(file_path).upper()\n    print fname    \n    path = os.path.dirname(file_path)\n\n    \n    if fname[:4] == '26DA':\n        \n        \n        \n        if int(fname.split('.')[3].zfill(2)) > 99:                \n            serienummer = str(int(fname.split('.')[2])+1).zfill(2) + fname.split('.')[3][1:]            \n        else:                        \n            serienummer = fname.split('.')[2].zfill(2) + fname.split('.')[3].zfill(2)    \n        \n    \n    elif fname[:2] == 'AR' or fname[:2] == 'ME' or fname[:2] == 'AU':\n        \n        \n        if len(fname) == 12:\n            serienummer = fname[5:8]\n            \n            serienummer = serienummer.zfill(4)\n        else: \n            serienummer = fname[5:8]\n            serienummer = serienummer.zfill(4)\n    elif fname[:2] == 'SV': \n        serienummer = fname[5:9]\n    \n    \n    \n    elif fname[:5] == 'SBE09':     \n        serienummer = fname.split('_')[-1][:4]\n    \n    else:\n        sys.exit('could not get \"serienummer\" from file name %s, stops!' % fname)\n        \n    \n    print 'serienummer: ',serienummer, \n    \n    \n    with codecs.open(file_path,'r', encoding = 'cp1252') as f:\n        allHeaderInfo = f.readlines()\n    f.closed\n    \n    stationname = ''\n    for rows in allHeaderInfo:\n        print rows\n        if '* System UpLoad Time' in rows:\n            datestring = rows[23:40]\n        if '** Station:' in rows:\n            stationname = rows[13:].strip('\\r\\n')\n\n    print 'Stationsnamn:',stationname\n    \n    c = time.strptime(datestring,\"%b %d %Y %H:%M\")\n    datum = time.strftime(\"%Y%m%d_%H%M\",c)\n     \n    if fname[:2] == 'AR':\n        new_fname = 'SBE09_' + ctd + '_' + datum + '_34_01_' + serienummer\n    elif fname[:2] == 'SV':    \n        new_fname = 'SBE09_' + ctd + '_' + datum + '_77_10_' + serienummer        \n    elif fname[:2] == 'ME':    \n        new_fname = 'SBE09_' + ctd + '_' + datum + '_34_02_' + serienummer\n    elif fname[:2] == 'AU':    \n        new_fname = 'SBE09_' + ctd + '_' + datum + '_34_07_' + serienummer\n    elif fname[:4] == '26DA':\n        new_fname = 'SBE09_' + ctd + '_' + datum + '_26_01_' + serienummer\n    elif fname[:5] == 'SBE09': \n        new_fname = fname.split('.')[0]       \n    else:\n        sys.exit('Fel format serienummer!')\n      \n    print new_fname\n    \n    \n    \n    sub_str = new_fname.split('_')\n    \n    \n    \n    counter = 0\n    for part in sub_str:  \n        counter = counter + 1    \n        if counter == 1:\n            \n            print counter        \n            if part != 'SBE09':        \n                sys.exit('Fel instrumentnamn!')                          \n        if counter == 2:\n            print counter        \n            if part not in ['0745','1044','0817','0403','0827','1387']:        \n                sys.exit('Fel intrument serienummer!')                      \n        if counter == 3:\n            print counter        \n            if len(part) != 8:        \n                sys.exit('Fel datumformat!')\n        if counter == 4:\n            print counter        \n            if len(part) != 4:        \n                sys.exit('Fel tidsformat!')\n        if counter == 5:\n            print counter        \n            if part == 34:        \n                sys.exit('Fel landkod!')\n        if counter == 6:\n            print counter        \n            if part == 01:        \n                sys.exit('Fel fartygskod!')\n        if counter == 7:\n            print counter\n            serieNo = part.split('.')[0]\n            print serieNo    \n            if len(part.split('.')[0]) != 4:        \n                sys.exit('Fel format serienummer!') \n\n\n    \n    \n    if fname.split('.')[0] != new_fname:\n        os.rename(file_path, path + '\\\\' + new_fname + '.hdr')\n        print path + '\\\\' + fname.rsplit('.',1)[0] + confile\n        print path + '\\\\' + new_fname + confile\n        os.rename(path + '\\\\' + fname.rsplit('.',1)[0] + confile, path + '\\\\' + new_fname + confile)\n        os.rename(path + '\\\\' + fname.rsplit('.',1)[0] + '.hex', path + '\\\\' + new_fname + '.hex')\n        os.rename(path + '\\\\' + fname.rsplit('.',1)[0] + '.bl', path + '\\\\' + new_fname + '.bl')    \n    return new_fname, serieNo, stationname\n    \n    \n    \n    \n",
        "summary": "The Python function `checkCtdFileName` opens a file dialog to select an `.hdr` file, extracts specific information from the filename and header content, and then renames the file based on predefined rules. It also handles renaming associated configuration files (`XMLCON`, `.hex`, `.bl`) accordingly."
    },
    {
        "code": "import numpy as np\n\nfrom utils import InputData, InputTypes, xmlUtils\nfrom .TimeSeriesAnalyzer import TimeSeriesGenerator, TimeSeriesCharacterizer\n\n\n\nclass Wavelet(TimeSeriesGenerator, TimeSeriesCharacterizer):\n  \n\n  @classmethod\n  def getInputSpecification(cls):\n    \n    specs = super(Wavelet, cls).getInputSpecification()\n    specs.name = 'wavelet'\n    specs.description = r\n    specs.addSub(InputData.parameterInputFactory(\n      'family',\n      contentType=InputTypes.StringType,\n      descr=r))\n    return specs\n\n  def __init__(self, *args, **kwargs):\n    \n    \n    super().__init__(*args, **kwargs)\n\n\n  def handleInput(self, spec):\n    \n    settings = super().handleInput(spec)\n    settings['family'] = spec.findFirst('family').value\n    return settings\n\n\n  def characterize(self, signal, pivot, targets, settings):\n    \n    \n    try:\n      import pywt\n    except ModuleNotFoundError:\n      print(\"This RAVEN TSA Module requires the PYWAVELETS library to be installed in the current python environment\")\n      raise ModuleNotFoundError\n\n\n    \n    \n    \n    \n    family = settings['family']\n    params = {target: {'results': {}} for target in targets}\n\n    for i, target in enumerate(targets):\n      results = params[target]['results']\n      results['coeff_a'], results['coeff_d'] = pywt.dwt(signal[:, i], family)\n\n    return params\n\n  def getParamNames(self, settings):\n    \n    \n    raise NotImplementedError('Cannot predict variables for Wavelet!')\n    names = []\n    for target in settings['target']:\n      base = f'{self.name}__{target}'\n\n  def getParamsAsVars(self, params):\n    \n    \n    raise NotImplementedError('Cannot predict variables for Wavelet!')\n    rlz = {}\n    for target, info in params.items():\n      base = f'{self.name}__{target}'\n      for name, values in info['results'].items():\n        for v, val in enumerate(values):\n          rlz[f'{base}__{name}__{v}'] = val\n    return rlz\n\n  def generate(self, params, pivot, settings):\n    \n    try:\n      import pywt\n    except ModuleNotFoundError:\n      print(\"This RAVEN TSA Module requires the PYWAVELETS library to be installed in the current python environment\")\n      raise ModuleNotFoundError\n\n    synthetic = np.zeros((len(pivot), len(params)))\n    family = settings['family']\n    for t, (target, _) in enumerate(params.items()):\n      results = params[target]['results']\n      cA = results['coeff_a']\n      cD = results['coeff_d']\n      synthetic[:, t] = pywt.idwt(cA, cD, family)\n    return synthetic\n\n\n  def writeXML(self, writeTo, params):\n    \n    for target, info in params.items():\n      base = xmlUtils.newNode(target)\n      writeTo.append(base)\n      for name, value in info['results'].items():\n        base.append(xmlUtils.newNode(name, text=','.join([str(v) for v in value])))\n",
        "summary": "The provided Python code defines a `Wavelet` class that inherits from `TimeSeriesGenerator` and `TimeSeriesCharacterizer`. This class is designed to perform wavelet analysis on time series data. It includes methods for handling input, characterizing signals using wavelets, generating synthetic data based on wavelet coefficients, and writing results in XML format. The class requires the `pywt` library for wavelet computations and handles exceptions if this library is not installed."
    },
    {
        "code": "from openstack_dashboard.test.integration_tests.pages import basepage\nfrom openstack_dashboard.test.integration_tests.regions import forms\nfrom openstack_dashboard.test.integration_tests.regions import tables\nfrom selenium.common import exceptions\nfrom selenium.webdriver.common import by\n\n\nclass ElementTable(tables.TableRegion):\n    name = \"element\"\n    CREATE_FORM_FIELDS = ()\n    EDIT_FORM_FIELDS = ()\n\n    @tables.bind_table_action('create')\n    def create(self, create_button):\n        create_button.click()\n        return forms.FormRegion(self.driver, self.conf,\n                                field_mappings=self.CREATE_FORM_FIELDS)\n\n    @tables.bind_table_action('delete')\n    def delete(self, delete_button):\n        delete_button.click()\n        return forms.BaseFormRegion(self.driver, self.conf)\n\n    @tables.bind_row_action('edit', primary=True)\n    def edit(self, edit_button, row):\n        edit_button.click()\n        return forms.FormRegion(self.driver, self.conf,\n                                field_mappings=self.EDIT_FORM_FIELDS)\n\n\nclass SubnetsTable(ElementTable):\n    name = \"subnets\"\n    CREATE_FORM_FIELDS = ((\"subnet_name\", \"cidr\", \"ip_version\",\n                           \"gateway_ip\", \"no_gateway\"),\n                          (\"enable_dhcp\", \"allocation_pools\",\n                           \"dns_nameservers\", \"host_routes\"))\n\n    EDIT_FORM_FIELDS = CREATE_FORM_FIELDS\n\n    @tables.bind_table_action('create')\n    def create(self, create_button):\n        create_button.click()\n        return forms.TabbedFormRegion(self.driver, self.conf,\n                                      self.CREATE_FORM_FIELDS)\n\n    @tables.bind_row_action('edit')\n    def edit(self, edit_button):\n        edit_button.click()\n        return forms.TabbedFormRegion(self.driver, self.conf,\n                                      self.EDIT_FORM_FIELDS)\n\n\nclass NetworksTable(ElementTable):\n    name = \"networks\"\n    CREATE_FORM_FIELDS = ((\"net_name\", \"admin_state\", \"shared\",\n                           \"with_subnet\"),\n                          (\"subnet_name\", \"cidr\", \"ip_version\",\n                           \"gateway_ip\", \"no_gateway\"),\n                          (\"enable_dhcp\", \"allocation_pools\",\n                           \"dns_nameservers\", \"host_routes\"))\n\n    EDIT_FORM_FIELDS = (\"name\", \"network_id\", \"admin_state\",\n                                \"shared\")\n\n    ADD_SUBNET_FORM_FIELDS = ((\"subnet_name\", \"cidr\", \"ip_version\",\n                               \"gateway_ip\", \"no_gateway\"),\n                              (\"enable_dhcp\", \"allocation_pools\",\n                               \"dns_nameservers\", \"host_routes\"))\n\n    @tables.bind_table_action('create')\n    def create(self, create_button):\n        create_button.click()\n        return forms.TabbedFormRegion(self.driver, self.conf,\n                                      self.CREATE_FORM_FIELDS)\n\n    @tables.bind_row_action('subnet')\n    def edit_add_subnet(self, edit_button, row):\n        edit_button.click()\n        return forms.TabbedFormRegion(self.driver, self.conf,\n                                      self.ADD_SUBNET_FORM_FIELDS)\n\n    @tables.bind_row_action('delete')\n    def edit_delete_network(self, delete_button, row):\n        delete_button.click()\n        return forms.BaseFormRegion(self.driver, self.conf)\n\n\nclass NetworksPage(basepage.BaseNavigationPage):\n    DEFAULT_ADMIN_STATE = 'True'\n    DEFAULT_CREATE_SUBNET = True\n    DEFAULT_IP_VERSION = '4'\n    DEFAULT_DISABLE_GATEWAY = False\n    DEFAULT_ENABLE_DHCP = True\n    NETWORKS_TABLE_NAME_COLUMN = 'name'\n    NETWORKS_TABLE_STATUS_COLUMN = 'status'\n    SUBNET_TAB_INDEX = 1\n    DETAILS_TAB_INDEX = 2\n\n    def __init__(self, driver, conf):\n        super(NetworksPage, self).__init__(driver, conf)\n        self._page_title = \"Networks\"\n\n    def _get_row_with_network_name(self, name):\n        return self.networks_table.get_row(\n            self.NETWORKS_TABLE_NAME_COLUMN, name)\n\n    @property\n    def networks_table(self):\n        return NetworksTable(self.driver, self.conf)\n\n    def create_network(self, network_name, subnet_name,\n                       admin_state=DEFAULT_ADMIN_STATE,\n                       create_subnet=DEFAULT_CREATE_SUBNET,\n                       network_address=None, ip_version=DEFAULT_IP_VERSION,\n                       gateway_ip=None,\n                       disable_gateway=DEFAULT_DISABLE_GATEWAY,\n                       enable_dhcp=DEFAULT_ENABLE_DHCP, allocation_pools=None,\n                       dns_name_servers=None, host_routes=None):\n        create_network_form = self.networks_table.create()\n        create_network_form.net_name.text = network_name\n        create_network_form.admin_state.value = admin_state\n        if not create_subnet:\n            create_network_form.with_subnet.unmark()\n        else:\n            create_network_form.switch_to(self.SUBNET_TAB_INDEX)\n            create_network_form.subnet_name.text = subnet_name\n            if network_address is None:\n                network_address = self.conf.network.network_cidr\n            create_network_form.cidr.text = network_address\n\n            create_network_form.ip_version.value = ip_version\n            if gateway_ip is not None:\n                create_network_form.gateway_ip.text = gateway_ip\n            if disable_gateway:\n                create_network_form.disable_gateway.mark()\n\n            create_network_form.switch_to(self.DETAILS_TAB_INDEX)\n            if not enable_dhcp:\n                create_network_form.enable_dhcp.unmark()\n            if allocation_pools is not None:\n                create_network_form.allocation_pools.text = allocation_pools\n            if dns_name_servers is not None:\n                create_network_form.dns_nameservers.text = dns_name_servers\n            if host_routes is not None:\n                create_network_form.host_routes.text = host_routes\n        create_network_form.submit()\n\n    def delete_network(self, name):\n        row = self._get_row_with_network_name(name)\n        confirm_delete_networks_form = \\\n            self.networks_table.edit_delete_network(row)\n        confirm_delete_networks_form.submit()\n\n    def is_network_present(self, name):\n        return bool(self._get_row_with_network_name(name))\n\n    def is_network_active(self, name):\n\n        def cell_getter():\n            row = self._get_row_with_network_name(name)\n            return row and row.cells[self.NETWORKS_TABLE_STATUS_COLUMN]\n\n        return bool(self.networks_table.wait_cell_status(cell_getter,\n                                                         'Active'))\n\n    def add_subnet(self, net_name, subnet_name,\n                   network_address=None, ip_version=DEFAULT_IP_VERSION,\n                   gateway_ip=None,\n                   disable_gateway=DEFAULT_DISABLE_GATEWAY,\n                   enable_dhcp=DEFAULT_ENABLE_DHCP, allocation_pools=None,\n                   dns_name_servers=None, host_routes=None):\n        row = self._get_row_with_network_name(net_name)\n        add_subnet_form = self.networks_table.edit_add_subnet(row)\n        add_subnet_form.subnet_name.text = subnet_name\n        if network_address is None:\n            network_address = self.conf.network.network_cidr\n        add_subnet_form.cidr.text = network_address\n        add_subnet_form.ip_version.value = ip_version\n        if gateway_ip is not None:\n            add_subnet_form.gateway_ip.text = gateway_ip\n        if disable_gateway:\n            add_subnet_form.disable_gateway.mark()\n\n        add_subnet_form.switch_to(self.SUBNET_TAB_INDEX)\n        if not enable_dhcp:\n            add_subnet_form.enable_dhcp.unmark()\n        if allocation_pools is not None:\n            add_subnet_form.allocation_pools.text = allocation_pools\n        if dns_name_servers is not None:\n            add_subnet_form.dns_nameservers.text = dns_name_servers\n        if host_routes is not None:\n            add_subnet_form.host_routes.text = host_routes\n        add_subnet_form.submit()\n        return NetworkOverviewPage(self.driver, self.conf, net_name)\n\n    def go_to_overview(self, name):\n        _network_items_locator = (by.By.CSS_SELECTOR, 'a[href$=\"/detail\"]')\n        net_items = self._get_elements(*_network_items_locator)\n\n        for item in net_items:\n            if item.text == name:\n                item.click()\n                break\n        else:\n            raise exceptions.NoSuchElementException(\n                \"Not found element with text: %s\" % name)\n        return NetworkOverviewPage(self.driver, self.conf, name)\n\n\nclass NetworkOverviewPage(basepage.BaseNavigationPage):\n    DEFAULT_ADMIN_STATE = 'True'\n    DEFAULT_IP_VERSION = '4'\n    DEFAULT_DISABLE_GATEWAY = False\n    DEFAULT_ENABLE_DHCP = True\n    DETAILS_TAB_INDEX = 1\n    TABLE_NAME_COLUMN = 'name'\n    _edit_network_locator = (\n        by.By.CSS_SELECTOR,\n        'form.actions_column > .btn-group > a.btn:nth-child(1)')\n    _dropdown_open_locator = (\n        by.By.CSS_SELECTOR,\n        'form.actions_column > .btn-group > a.btn:nth-child(2)')\n    _dropdown_menu_locator = (\n        by.By.CSS_SELECTOR,\n        'form.actions_column > .btn-group > ul.row_actions > li > *')\n\n    def __init__(self, driver, conf, network_name):\n        super(NetworkOverviewPage, self).__init__(driver, conf)\n        self._page_title = \"Network Details: {}\".format(network_name)\n\n    @property\n    def subnets_table(self):\n        return SubnetsTable(self.driver, self.conf)\n\n    def _get_row_with_name(self, name, table):\n        return table.get_row(self.TABLE_NAME_COLUMN, name)\n\n    def _get_row_action(self, action_name):\n        open_dropdown_elem = self._get_element(*self._dropdown_open_locator)\n        open_dropdown_elem.click()\n        for action in self._get_elements(*self._dropdown_menu_locator):\n            pattern = \"__action_%s\" % action_name\n            if action.get_attribute('id').endswith(pattern):\n                action_element = action\n                break\n        return action_element\n\n    def delete_network(self):\n        delete_elem = self._get_row_action('delete')\n        delete_elem.click()\n        confirm_delete_network_form = forms.BaseFormRegion(self.driver,\n                                                           self.conf)\n        confirm_delete_network_form.submit()\n        return NetworksPage(self.driver, self.conf)\n\n    def create_subnet(self, subnet_name,\n                      network_address=None, ip_version=DEFAULT_IP_VERSION,\n                      gateway_ip=None,\n                      disable_gateway=DEFAULT_DISABLE_GATEWAY,\n                      enable_dhcp=DEFAULT_ENABLE_DHCP, allocation_pools=None,\n                      dns_name_servers=None, host_routes=None):\n        create_subnet_form = self.subnets_table.create()\n        create_subnet_form.subnet_name.text = subnet_name\n        if network_address is None:\n            network_address = self.conf.network.network_cidr\n        create_subnet_form.cidr.text = network_address\n        create_subnet_form.ip_version.value = ip_version\n        if gateway_ip is not None:\n            create_subnet_form.gateway_ip.text = gateway_ip\n        if disable_gateway:\n            create_subnet_form.disable_gateway.mark()\n\n        create_subnet_form.tabs.switch_to(self.DETAILS_TAB_INDEX)\n        if not enable_dhcp:\n            create_subnet_form.enable_dhcp.unmark()\n        if allocation_pools is not None:\n            create_subnet_form.allocation_pools.text = allocation_pools\n        if dns_name_servers is not None:\n            create_subnet_form.dns_nameservers.text = dns_name_servers\n        if host_routes is not None:\n            create_subnet_form.host_routes.text = host_routes\n        create_subnet_form.submit()\n\n    def delete_subnet(self, name):\n        row = self._get_row_with_name(name, self.subnets_table)\n        row.mark()\n        confirm_delete_subnet_form = self.subnets_table.delete()\n        confirm_delete_subnet_form.submit()\n\n    def is_subnet_present(self, name):\n        return bool(self._get_row_with_name(name, self.subnets_table))\n",
        "summary": "This code defines a set of classes and methods for interacting with a network management interface in a web application. The main class `NetworksPage` represents the page where you can view and manage networks. It provides methods to create, edit, delete, and navigate to subpages like `NetworkOverviewPage`.\n\nThe `NetworkOverviewPage` class represents the detailed view of a specific network. It allows actions such as deleting the network or creating new subnets within it.\n\nHere's a breakdown of some key functionalities:\n\n1. **Creating Subnets**: You can create a subnet by filling out a form with details like subnet name, CIDR block, IP version, gateway settings, and DHCP options.\n\n2. **Deleting Subnets**: Selecting a subnet row marks it for deletion, and clicking the delete button confirms the action.\n\n3. **Navigating to Network Overview**: From the main networks page, you can click on a network's detail link to go to its overview page.\n\n4. **Editing Networks**: The `NetworkOverviewPage` does not directly provide an edit function; instead, it might redirect back to the main networks page where edits could be made through form submissions.\n\n5. **Deleting Networks**: Similar to subnets, selecting a network row marks it for deletion, and clicking the delete button confirms the action.\n\nThe code uses Selenium WebDriver to interact with web elements, such as finding elements by CSS selectors, interacting with dropdowns, and submitting forms. It also includes exception handling for cases where expected elements are not found.\n\nThis setup is typical for automated testing of web applications that manage network resources, ensuring that all CRUD (Create, Read, Update, Delete) operations can be performed programmatically."
    },
    {
        "code": "from hikyuu import PG_FixedPercent\n\n\nauthor = \"fasiondog\"\n\n\nversion = '20200825'\n\n\ndef part(p=0.2):\n    return PG_FixedPercent(p)\n\n\npart.__doc__ = PG_FixedPercent.__doc__\n\nif __name__ == '__main__':\n    print(part())",
        "summary": "The provided Python code defines a function `part` that returns an instance of `PG_FixedPercent` with a default parameter `p` set to 0.2. The function's docstring is set to match the docstring of `PG_FixedPercent`. When executed as a script, it prints the result of calling the `part` function."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport os\nimport sys\nimport unittest\n\nimport kubernetes.client\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes.client.models.v1_flex_volume_source import V1FlexVolumeSource\n\n\nclass TestV1FlexVolumeSource(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testV1FlexVolumeSource(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "The provided Python code defines a unit test class `TestV1FlexVolumeSource` that inherits from `unittest.TestCase`. It includes methods for setting up and tearing down the test environment, as well as a placeholder method `testV1FlexVolumeSource` for testing the `V1FlexVolumeSource` class from the Kubernetes client library. The script uses the `unittest` framework to run the tests when executed directly."
    },
    {
        "code": "from fineract.objects.fineract_object import DataFineractObject\nfrom fineract.objects.types import Type\n\n\nclass Group(DataFineractObject):\n    \n    def __repr__(self):\n        return self.get__repr__({'group_id': self.id})\n\n    def _init_attributes(self):\n        self.id = None\n        self.account_no = None\n        self.external_id = None\n        self.name = None\n        self.status = None\n        self.active = None\n        self.activation_date = None\n        self.office_id = None\n        self.office_name = None\n        self.hierarchy = None\n\n    def _use_attributes(self, attributes):\n        self.id = attributes.get('id', None)\n        self.account_no = attributes.get('accountNo', None)\n        self.external_id = attributes.get('externalId', None)\n        self.name = attributes.get('name', None)\n        self.status = self._make_fineract_object(GroupStatus, attributes.get('status', None))\n        self.active = attributes.get('active', None)\n        self.activation_date = self._make_date_object(attributes.get('activationDate', None))\n        self.office_id = attributes.get('officeId', None)\n        self.office_name = attributes.get('officeName', None)\n        self.hierarchy = attributes.get('hierarchy', None)\n\n    def add_members(self, members_list):\n        params = {\n            'clientMembers': members_list\n        }\n\n        data = self.request_handler.make_request(\n            'POST',\n            '/groups/{}?command=associateClients'.format(self.id),\n            json=params\n        )\n        return data['groupId'] == self.id\n\n    def remove_members(self, members_list):\n        params = {\n            'clientMembers': members_list\n        }\n\n        data = self.request_handler.make_request(\n            'POST',\n            '/groups/{}?command=disassociateClients'.format(self.id),\n            json=params\n        )\n        return data['groupId'] == self.id\n\n    @classmethod\n    def create(cls, request_handler, name, office_id, active=True, activation_date=None):\n        \n        data = {\n            'name': name,\n            'officeId': office_id,\n            'active': active,\n            'activationDate': activation_date or cls._get_current_date()\n        }\n\n        res = request_handler.make_request(\n            'POST',\n            '/groups',\n            json=data\n        )\n\n        group_id = res['groupId']\n        return cls(request_handler,\n                   request_handler.make_request(\n                       'GET',\n                       '/groups/{}'.format(group_id)\n                   ), False)\n\n    @classmethod\n    def get_group_by_name(cls, request_handler, name):\n        \n        data = request_handler.make_request(\n            'GET',\n            '/groups'\n        )\n        if data:\n            for item in data:\n                if item['name'] == name:\n                    print(item)\n                    return cls(request_handler, item, False)\n\n        return None\n\n\nclass GroupStatus(Type):\n    \n    pass\n",
        "summary": "The provided Python code defines a `Group` class that extends `DataFineractObject`, representing a group in a Fineract system. It includes methods for adding and removing members from the group, creating new groups, and retrieving groups by name. The `GroupStatus` class is defined but left empty."
    },
    {
        "code": "import logging\n\nimport requests\nfrom django.conf import settings\n\nfrom .base import BaseSmsClient\n\nlogger = logging.getLogger(\"notifier\")\n\n\nclass CGSmsClient(BaseSmsClient):\n\n    @classmethod\n    def send(cls, number: str, text: str, **kwargs):\n        sub_account = settings.NOTIFIER[\"SMS\"][\"GATEWAYS\"][\"CGS\"][\"SUB_ACCOUNT\"]\n        sub_account_pass = settings.NOTIFIER[\"SMS\"][\"GATEWAYS\"][\"CGS\"][\"SUB_ACCOUNT_PASSWORD\"]\n        params = {\n            \"sub_account\": sub_account,\n            \"sub_account_pass\": sub_account_pass,\n            \"action\": \"send_sms\",\n            \"message\": text,\n            \"recipients\": number,\n        }\n        res = requests.get(\"http://cheapglobalsms.com/api_v1\", params=params)\n        return res\n",
        "summary": "The `CGSmsClient` class extends a base SMS client and provides a method to send SMS messages using the CheapGlobalSMS API. It retrieves necessary credentials from Django settings, constructs a request with these credentials and the message details, sends it via HTTP GET, and returns the response."
    },
    {
        "code": "import argparse\nimport codecs\n\nimport sys\n\n\ndef transform(i,o):\n    for line in i:\n        if len(line.strip()) == 0:\n            continue\n        key, trans = line.strip().split(None, 1)\n        ntrans = []\n        for t in trans.split():\n            if t.startswith(\"<\"):\n                continue\n            ntrans.append(t.lower())\n        print(\"{} {}\".format(key, \" \".join(ntrans)), file=o)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('infile', nargs='?', type=argparse.FileType('r', encoding='utf-8'), default=codecs.getreader('utf-8')(sys.stdin.buffer))\n    parser.add_argument('outfile', nargs='?', type=argparse.FileType('w', encoding='utf-8'), default=codecs.getwriter('utf-8')(sys.stdout.buffer))\n\n    args = parser.parse_args()\n\n    transform(args.infile, args.outfile)",
        "summary": "The Python script defines a function `transform` that reads from an input file or standard input, processes each line by splitting it into a key and translation, converts the translation to lowercase while removing any words starting with \"<\", and then writes the transformed lines to an output file or standard output. The script uses the `argparse` module to handle command-line arguments for specifying input and output files, defaulting to stdin and stdout if no files are provided."
    },
    {
        "code": "import sys, getopt, glob, csv\nfrom math import sqrt\nfrom Bio import Struct\nfrom Bio.Struct.Geometry import center_of_mass\nfrom Bio.PDB import *\nimport numpy as np\n\ndef string_to_float(val):\n  try:\n    return float(val)\n  except ValueError as e:\n    raise\n\n\n\n\n\n\n\n\n\ndef compute_atom_contact(model, threshold, excluded=[]):\n  res_count = 0\n  \n  for chain in model:\n    res_count += len(chain)\n  \n  mod_contact_matrix = np.zeros((res_count,res_count))\n  last_i = 1\n  for chain in model:\n    i = last_i\n    chaindex_i = 0\n    for res_i in chain.get_residues():\n      index_i = chaindex_i + i\n      last_i = index_i \n      chaindex_j = 0\n      for res_j in chain.get_residues():\n        index_j = chaindex_j + index_i\n        \n        if index_j <= len(chain):\n          if index_j > index_i:\n            for at_i in res_i:\n                if at_i.get_name() not in excluded:\n                  for at_j in res_j:\n                    if at_j.get_name() not in excluded:\n                      euclidean = np.linalg.norm(at_i.get_coord()-at_j.get_coord())\n                      if euclidean <= threshold:\n                        \n                        mod_contact_matrix[index_i-1][index_j-1] += 1\n                        mod_contact_matrix[index_j-1][index_i-1] += 1\n                      \n        chaindex_j += 1\n      chaindex_i += 1\n\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n  return mod_contact_matrix\n\n\n\n\n\n\n\n\n\ndef compute_ave_atom_contact(structure, threshold, excluded=[]):\n  res_count = 0\n  \n  for chain in structure[0]:\n    res_count += len(chain)\n  struct_contact_matrix = np.zeros((res_count,res_count))\n  for model in structure:\n    mod_contact_matrix = compute_atom_contact(model, threshold, excluded)\n    struct_contact_matrix += mod_contact_matrix\n  ave_struct_contact_matrix = struct_contact_matrix / len(structure)\n\n  return ave_struct_contact_matrix\n\n\n\ndef compute_max_residue(structure, ave_struct_contact_matrix):\n  res_dict = {}\n  i = 0\n  for chain in structure[0]:\n    for res in chain:\n      contact_atoms = np.sum(ave_struct_contact_matrix[i])\n      i+=1\n      if res.get_resname() not in res_dict:\n        res_dict[res.get_resname()] = contact_atoms\n      elif res_dict[res.get_resname()] < contact_atoms:\n        res_dict[res.get_resname()] = contact_atoms\n  return res_dict\n\n\n\ndef compute_normalization_factor(pdb_dir, threshold, output):\n  parser = PDBParser()\n  tot_files = 0.0\n  res_dir_dict = {}\n  for filename in glob.iglob(pdb_dir + \"/*.pdb\"):\n    tot_files += 1\n    label = filename.split('/')[-1].split('.')[0]\n    print label\n    structure = parser.get_structure(label,filename)\n    ave_struct_contact_matrix = compute_ave_atom_contact(structure, 2)\n    res_dict = compute_max_residue(structure, ave_struct_contact_matrix)\n    for key in res_dict:\n      if key not in res_dir_dict:\n        res_dir_dict[key] = res_dict[key]\n      else:\n        res_dir_dict[key] += res_dict[key]\n  for key in res_dir_dict:\n    res_dir_dict[key] = res_dir_dict[key]/tot_files\n  with open(output, \"w\") as outfile:\n    csvfile = csv.writer(outfile)\n    for key in res_dir_dict:\n      csvfile.writerow([key,res_dir_dict[key]])\n  print res_dir_dict\n\n\n\ndef read_normalization(norm_fact):\n  norm_dict = {}\n  with open(norm_fact) as normfile:\n    csvfile = csv.reader(normfile)\n    for row in csvfile:\n      norm_dict[row[0]] = float(row[1])\n  return norm_dict\n\n\ndef compute_interaction(res_1, res_2, inter_atoms, norm_dict):\n  \n  res_1 = 'HIS' if res_1 in ['HIE', 'HID', 'HIP'] else res_1\n  res_2 = 'HIS' if res_2 in ['HIE', 'HID', 'HIP'] else res_2\n  norm_1 = norm_dict[res_1]\n  norm_2 = norm_dict[res_2]\n  \n  if norm_1 == 0 or norm_2 == 0:\n    return 0 \n  return (inter_atoms*100.0)/sqrt(norm_1*norm_2)\n\n\n\n\n\n\n\n\n\ndef compute_atom_interaction(model,\n                             contact_thresh,\n                             interaction_thresh,\n                             norm_dict,\n                             residues,\n                             excluded=[]):\n  model_contact = compute_atom_contact(model, contact_thresh, excluded)\n  model_interact = np.zeros(model_contact.shape) \n  for i in range(len(residues)):\n    for j in range(len(residues)-i):\n      index_j = j + i\n      index_i = i\n      \n      if index_i < index_j:\n        int_str = compute_interaction(residues[index_i],\n                                      residues[index_j],\n                                      model_contact[index_i][index_j],\n                                      norm_dict)\n        \n        if int_str >= interaction_thresh:\n          model_interact[index_i][index_j] = 1\n          model_interact[index_j][index_i] = 1\n  return model_interact\n\n\ndef compute_interaction_matrix(structure,\n                               contact_thresh,\n                               interaction_thresh,\n                               edge_prct,\n                               norm_dict,\n                               outdir,\n                               excluded=[]):\n  residues = []\n  for res in structure[0].get_residues():\n    residues.append(res.get_resname())\n  struct_interact = np.zeros((len(residues),len(residues)))\n  PSN = np.zeros((len(residues),len(residues)))\n  for model in structure:\n    model_interact = compute_atom_interaction(model,\n                                              contact_thresh,\n                                              interaction_thresh,\n                                              norm_dict,\n                                              residues,\n                                              excluded=[])\n\n    struct_interact = struct_interact + model_interact\n  edge_thresh = edge_prct * len(structure)\n  for i in range(len(residues)):\n    for j in range(len(residues)-i):\n      index_j = j + i\n      index_i = i\n      \n      if index_i < index_j:\n        if struct_interact[index_i][index_j]>= edge_thresh:\n          PSN[index_i][index_j] = 1\n          PSN[index_j][index_i] = 1\n  filename = \"%s/%s_%.2f_%.2f_%.2f\" % (outdir,structure.get_id(),contact_thresh,interaction_thresh,edge_prct)\n  np.savetxt(filename,PSN,fmt='%d')\n\n\ndef get_graph_rep(pdb_dir,\n                  contact_thresh,\n                  interaction_thresh,\n                  edge_prct,\n                  norm_dict,\n                  outdir,\n                  excluded=[]):\n  parser = PDBParser()\n  for filename in glob.iglob(pdb_dir + \"/*.pdb\"):\n    label = filename.split('/')[-1].split('.')[0]\n    print label\n    structure = parser.get_structure(label,filename)\n    for int_thresh in interaction_thresh:\n      compute_interaction_matrix(structure,\n                                 contact_thresh,\n                                 int_thresh,\n                                 edge_prct,\n                                 norm_dict,\n                                 outdir,\n                                 excluded=[])\n\n\n\ndef main(argv):\n  pdb_dir = ''\n  norm_fact = ''\n  outdir = ''\n  norm = 1\n  interaction_thresh = 1\n  contact_thresh = 4.5\n  normalize = False\n  edge_prct = 0.9\n\n  bb = ['N','CA','C','O']\n\n  try:\n    opts, args = getopt.getopt(argv,\"hnp:f:c:i:e:o:\",[\"pdb_dir=\",\n                                                    \"norm_fact=\",\n                                                    \"interaction_thresh=\",\n                                                    \"contact_thresh=\",\n                                                    \"edge_prct=\",\n                                                    \"normalize\",\n                                                    \"outdir=\"])\n  except getopt.GetoptError:\n    print 'blast_parser.py -i <input fasta> -x <blast xml> -o <output>'\n    sys.exit(2)\n  for opt, arg in opts:\n    if opt == '-h':\n      print 'Computes a network given a threshold or computes normalization factor'\n      print \n      sys.exit()\n    elif opt in (\"-p\", \"--pdb_dir\"):\n      pdb_dir = arg\n    elif opt in (\"-f\", \"--norm_fact\"):\n      norm_fact = arg\n    elif opt in (\"-e\", \"--edge_prct\"):\n      edge_prct = string_to_float(arg)\n    elif opt in (\"-c\", \"--contact_thresh\"):\n      contact_thresh = string_to_float(arg)\n    elif opt in (\"-i\", \"--interaction_thresh\"):\n      interaction_thresh = []\n      for i in arg.split(\",\"):\n        interaction_thresh.append(string_to_float(i))\n    elif opt in (\"-o\", \"--output\"):\n      outdir = arg\n    elif opt in (\"-n\",\"--normalize\"):\n      normalize = True\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  if normalize:\n    compute_normalization_factor(pdb_dir, contact_thresh, outdir)\n  else:\n\n    norm_dict = read_normalization(norm_fact) \n    \n    get_graph_rep(pdb_dir,\n                  contact_thresh,\n                  interaction_thresh,\n                  edge_prct,\n                  norm_dict,\n                  outdir,\n                  bb)\n\nif __name__ == \"__main__\":\n  main(sys.argv[1:])\n",
        "summary": "The Python script processes PDB files to compute residue interactions based on atomic distances and normalizes these interactions using a normalization factor. It includes functions for computing contact matrices, interaction matrices, and graph representations of protein structures, as well as command-line options for specifying input parameters such as threshold values and output directories."
    },
    {
        "code": "import sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='addressbook.proto',\n  package='',\n  syntax='proto3',\n  serialized_options=None,\n  serialized_pb=_b('\\n\\x11\\x61\\x64\\x64ressbook.proto\\\"1\\n\\x06Person\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05\\x65mail\\x18\\x03 \\x01(\\tb\\x06proto3')\n)\n\n\n\n\n_PERSON = _descriptor.Descriptor(\n  name='Person',\n  full_name='Person',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='name', full_name='Person.name', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name='id', full_name='Person.id', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name='email', full_name='Person.email', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax='proto3',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=21,\n  serialized_end=70,\n)\n\nDESCRIPTOR.message_types_by_name['Person'] = _PERSON\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nPerson = _reflection.GeneratedProtocolMessageType('Person', (_message.Message,), dict(\n  DESCRIPTOR = _PERSON,\n  __module__ = 'addressbook_pb2'\n  \n  ))\n_sym_db.RegisterMessage(Person)\n\n\n\n",
        "summary": "This Python code imports necessary modules from the Google Protocol Buffers library to define and register a message type named `Person` with fields for name, id, and email. The `Person` message is then registered in the symbol database for use in other parts of an application that utilizes Protocol Buffers for data serialization and deserialization."
    },
    {
        "code": "from sklearn import neighbors, datasets, linear_model\nimport pylab as pl\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\n\ncmap_light = ListedColormap(['\ncmap_bold = ListedColormap(['\n\ndef plot_iris_knn():\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  \n                        \n    y = iris.target\n\n    knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n    knn.fit(X, y)\n\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    \n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    \n    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    pl.xlabel('sepal length (cm)')\n    pl.ylabel('sepal width (cm)')\n    pl.axis('tight')\n\n\ndef plot_polynomial_regression():\n    rng = np.random.RandomState(0)\n    x = 2*rng.rand(100) - 1\n\n    f = lambda t: 1.2 * t**2 + .1 * t**3 - .4 * t **5 - .5 * t ** 9\n    y = f(x) + .4 * rng.normal(size=100)\n\n    x_test = np.linspace(-1, 1, 100)\n\n    pl.figure()\n    pl.scatter(x, y, s=4)\n\n    X = np.array([x**i for i in range(5)]).T\n    X_test = np.array([x_test**i for i in range(5)]).T\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    pl.plot(x_test, regr.predict(X_test), label='4th order')\n\n    X = np.array([x**i for i in range(10)]).T\n    X_test = np.array([x_test**i for i in range(10)]).T\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    pl.plot(x_test, regr.predict(X_test), label='9th order')\n\n    pl.legend(loc='best')\n    pl.axis('tight')\n    pl.title('Fitting a 4th and a 9th order polynomial')\n\n    pl.figure()\n    pl.scatter(x, y, s=4)\n    pl.plot(x_test, f(x_test), label=\"truth\")\n    pl.axis('tight')\n    pl.title('Ground truth (9th order polynomial)')",
        "summary": "The provided Python code includes two functions: `plot_iris_knn` and `plot_polynomial_regression`. The first function visualizes the decision boundaries of a K-Nearest Neighbors classifier on the Iris dataset, plotting both the decision regions and the data points. The second function demonstrates polynomial regression by fitting 4th and 9th order polynomials to synthetic data and comparing them to the true underlying 9th order polynomial."
    },
    {
        "code": "BOT_NAME = 'iwata'\n\nSPIDER_MODULES = ['iwata.spiders']\nNEWSPIDER_MODULE = 'iwata.spiders'\n\n\n\n\n\n\nROBOTSTXT_OBEY = False\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nITEM_PIPELINES = {\n\n\n\t'iwata.pipelines.MarkdownWriterPipeline': 300,\n\t'scrapy.pipelines.images.ImagesPipeline': 400,\n}\nIMAGES_STORE = '_images'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTTPCACHE_ENABLED = True\n\n\n\n\n\nFEED_EXPORT_ENCODING = 'utf-8'",
        "summary": "The provided Python code configures settings for a Scrapy web scraping project named \"iwata,\" including spider modules, item pipelines for handling Markdown and images, enabling HTTP caching, and setting the feed export encoding to UTF-8."
    },
    {
        "code": "from wtforms import SubmitField\nfrom bluecat.wtform_fields import (\n    Configuration,\n    View,\n    Zone,\n    HostRecord,\n    CustomStringField,\n    PlainHTML,\n    CustomBooleanField,\n)\nfrom bluecat.server_endpoints import get_host_records_endpoint\nfrom bluecat.wtform_extensions import GatewayForm\n\n\nclass GenericFormTemplate(GatewayForm):\n    \n\n    workflow_name = \"update_host_record\"\n    workflow_permission = \"update_host_record_page\"\n    configuration = Configuration(\n        workflow_name=workflow_name,\n        permissions=workflow_permission,\n        label=\"Configuration\",\n        required=True,\n        coerce=int,\n        clear_below_on_change=False,\n        is_disabled_on_start=False,\n        on_complete=[\"call_view\"],\n        enable_dependencies={\"on_complete\": [\"view\"]},\n        disable_dependencies={\"on_change\": [\"view\"]},\n        clear_dependencies={\"on_change\": [\"view\"]},\n    )\n\n    view = View(\n        workflow_name=workflow_name,\n        permissions=workflow_permission,\n        label=\"View\",\n        required=True,\n        one_off=True,\n        clear_below_on_change=False,\n        enable_dependencies={\"on_complete\": [\"parent_zone\"]},\n        disable_dependencies={\"on_change\": [\"parent_zone\"]},\n        clear_dependencies={\"on_change\": [\"parent_zone\"]},\n        should_cascade_disable_on_change=True,\n        should_cascade_clear_on_change=True,\n    )\n\n    parent_zone = Zone(\n        workflow_name=workflow_name,\n        permissions=workflow_permission,\n        label=\"Zone\",\n        required=True,\n        start_initialized=True,\n        inputs={\"zone\": \"parent_zone\", \"configuration\": \"configuration\", \"view\": \"view\"},\n        clear_below_on_change=False,\n        enable_dependencies={\"on_complete\": [\"host_record\"]},\n        disable_dependencies={\"on_change\": [\"host_record\"]},\n        clear_dependencies={\"on_change\": [\"host_record\", \"name\", \"ip4_address\"]},\n        should_cascade_disable_on_change=True,\n        should_cascade_clear_on_change=True,\n    )\n\n    host_record = HostRecord(\n        workflow_name=workflow_name,\n        permissions=workflow_permission,\n        label=\"Host Record\",\n        required=True,\n        inputs={\n            \"configuration\": \"configuration\",\n            \"view\": \"view\",\n            \"parent_zone\": \"parent_zone\",\n            \"host_record\": \"host_record\",\n        },\n        server_outputs={\"on_complete\": {\"name\": \"name\", \"addresses\": \"ip4_address\"}},\n        server_side_output_method=get_host_records_endpoint,\n        clear_below_on_change=False,\n        enable_dependencies={\"on_complete\": [\"submit\", \"name\", \"ip4_address\", \"deploy_now\"]},\n        disable_dependencies={\"on_change\": [\"submit\", \"name\", \"ip4_address\", \"deploy_now\"]},\n        should_cascade_disable_on_change=True,\n    )\n\n    separator = PlainHTML(\"<hr>\")\n\n    name = CustomStringField(label=\"New Host Name\", required=True)\n\n    ip4_address = CustomStringField(\n        label=\"IPv4 Address (multiple IPv4 addresses must be separated by a comma)\", required=True\n    )\n\n    deploy_now = CustomBooleanField(label=\"Deploy Now\")\n\n    submit = SubmitField(label=\"Update\")\n",
        "summary": "This Python code defines a form template for updating host records using the BlueCat Gateway framework, incorporating various fields such as configuration, view, zone, and host record details, along with input validation and dependencies."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n\n__metaclass__ = type\n\nANSIBLE_METADATA = {\n    \"metadata_version\": \"1.1\",\n    \"status\": [\"preview\"],\n    \"supported_by\": \"community\",\n}\n\nDOCUMENTATION = \n\nEXAMPLES = \n\nRETURN = \n\nfrom ansible.module_utils.basic import AnsibleModule\nfrom ansible_collections.oracle.oci.plugins.module_utils import oci_common_utils\nfrom ansible_collections.oracle.oci.plugins.module_utils.oci_resource_utils import (\n    OCIResourceFactsHelperBase,\n    get_custom_class,\n)\n\ntry:\n    from oci.data_catalog import DataCatalogClient\n\n    HAS_OCI_PY_SDK = True\nexcept ImportError:\n    HAS_OCI_PY_SDK = False\n\n\nclass DataCatalogNamespaceFactsHelperGen(OCIResourceFactsHelperBase):\n    \n\n    def get_required_params_for_get(self):\n        return [\n            \"catalog_id\",\n            \"namespace_id\",\n        ]\n\n    def get_required_params_for_list(self):\n        return [\n            \"catalog_id\",\n        ]\n\n    def get_resource(self):\n        optional_get_method_params = [\n            \"fields\",\n        ]\n        optional_kwargs = dict(\n            (param, self.module.params[param])\n            for param in optional_get_method_params\n            if self.module.params.get(param) is not None\n        )\n        return oci_common_utils.call_with_backoff(\n            self.client.get_namespace,\n            catalog_id=self.module.params.get(\"catalog_id\"),\n            namespace_id=self.module.params.get(\"namespace_id\"),\n            **optional_kwargs\n        )\n\n    def list_resources(self):\n        optional_list_method_params = [\n            \"display_name\",\n            \"display_name_contains\",\n            \"lifecycle_state\",\n            \"time_created\",\n            \"time_updated\",\n            \"created_by_id\",\n            \"updated_by_id\",\n            \"sort_by\",\n            \"sort_order\",\n            \"fields\",\n        ]\n        optional_kwargs = dict(\n            (param, self.module.params[param])\n            for param in optional_list_method_params\n            if self.module.params.get(param) is not None\n        )\n        return oci_common_utils.list_all_resources(\n            self.client.list_namespaces,\n            catalog_id=self.module.params.get(\"catalog_id\"),\n            **optional_kwargs\n        )\n\n\nDataCatalogNamespaceFactsHelperCustom = get_custom_class(\n    \"DataCatalogNamespaceFactsHelperCustom\"\n)\n\n\nclass ResourceFactsHelper(\n    DataCatalogNamespaceFactsHelperCustom, DataCatalogNamespaceFactsHelperGen\n):\n    pass\n\n\ndef main():\n    module_args = oci_common_utils.get_common_arg_spec()\n    module_args.update(\n        dict(\n            catalog_id=dict(type=\"str\", required=True),\n            namespace_id=dict(aliases=[\"id\"], type=\"str\"),\n            fields=dict(\n                type=\"list\",\n                elements=\"str\",\n                choices=[\n                    \"key\",\n                    \"displayName\",\n                    \"description\",\n                    \"lifecycleState\",\n                    \"timeCreated\",\n                    \"timeUpdated\",\n                    \"createdById\",\n                    \"updatedById\",\n                    \"properties\",\n                ],\n            ),\n            display_name=dict(aliases=[\"name\"], type=\"str\"),\n            display_name_contains=dict(type=\"str\"),\n            lifecycle_state=dict(\n                type=\"str\",\n                choices=[\n                    \"CREATING\",\n                    \"ACTIVE\",\n                    \"INACTIVE\",\n                    \"UPDATING\",\n                    \"DELETING\",\n                    \"DELETED\",\n                    \"FAILED\",\n                    \"MOVING\",\n                ],\n            ),\n            time_created=dict(type=\"str\"),\n            time_updated=dict(type=\"str\"),\n            created_by_id=dict(type=\"str\"),\n            updated_by_id=dict(type=\"str\"),\n            sort_by=dict(type=\"str\", choices=[\"TIMECREATED\", \"DISPLAYNAME\"]),\n            sort_order=dict(type=\"str\", choices=[\"ASC\", \"DESC\"]),\n        )\n    )\n\n    module = AnsibleModule(argument_spec=module_args)\n\n    if not HAS_OCI_PY_SDK:\n        module.fail_json(msg=\"oci python sdk required for this module.\")\n\n    resource_facts_helper = ResourceFactsHelper(\n        module=module,\n        resource_type=\"namespace\",\n        service_client_class=DataCatalogClient,\n        namespace=\"data_catalog\",\n    )\n\n    result = []\n\n    if resource_facts_helper.is_get():\n        result = [resource_facts_helper.get()]\n    elif resource_facts_helper.is_list():\n        result = resource_facts_helper.list()\n    else:\n        resource_facts_helper.fail()\n\n    module.exit_json(namespaces=result)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script is an Ansible module designed to interact with Oracle Cloud Infrastructure's Data Catalog service, specifically for managing and retrieving information about namespaces within a catalog. It provides functionality to fetch details of a single namespace or list multiple namespaces based on various criteria such as display name, lifecycle state, and time range."
    },
    {
        "code": "from app import app\napp.run(app.config['HOST'], app.config['PORT'], app.config['DEBUG'])\n",
        "summary": "The provided Python code runs an application using the Flask framework, binding it to the host and port specified in the application's configuration, and enabling debug mode if configured to do so."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text format such as CSV or JSON."
    },
    {
        "code": "s3gis_tests = load_module(\"tests.unit_tests.modules.s3.s3gis\")\ns3gis = s3gis_tests.s3gis\n\ndef test_KMLLayer():\n    current.session.s3.debug = True\n    current.request.utcnow = datetime.datetime.now()\n    s3gis_tests.layer_test(\n        db,\n        db.gis_layer_kml,\n        dict(\n            name = \"Test KML\",\n            description = \"Test KML layer\",\n            enabled = True,\n            created_on = datetime.datetime.now(),\n            modified_on = datetime.datetime.now(),\n            url = \"test://test_KML\",\n        ),\n        \"S3.gis.layers_kml\",\n        [\n            {\n                \"marker_height\": 34, \n                \"marker_image\": u\"gis_marker.image.marker_red.png\", \n                \"marker_width\": 20, \n                \"name\": u\"Test KML\", \n                \n                \"url\": u\"/eden/default/download/gis_cache2.file.Test_20KML.kml\"\n            }\n        ],\n        session = session,\n        request = request,\n    )\n\ndef test_KMLCaching_not_possible():\n    import os.path\n    import sys\n    \n    class Mock(object):\n        pass\n    mock_stderr = Mock()\n    buffer = []\n    def mock_write(error_message):\n        buffer.append(error_message)\n    mock_stderr.write = mock_write\n    \n    with s3gis_tests.Change(\n        os.path,\n        {\n            \"exists\": lambda *a, **kw: False\n        }\n    ):\n        with s3gis_tests.Change(\n            sys,\n            {\n                \"stderr\": mock_stderr\n            }\n        ):\n            with s3gis_tests.Change(\n                current.session.s3,\n                {\n                    \"debug\": False\n                }\n            ):\n                kml_layer = s3gis.KMLLayer(s3gis.GIS())\n                js = kml_layer.as_javascript()\n                \n                assert session.error.startswith(\n                    \"GIS: KML layers cannot be cached: \"\n                )\n                assert \"GIS: KML layers cannot be cached:\" in buffer[0]\n",
        "summary": "The provided Python code defines two test functions for a KML layer module, `test_KMLLayer` and `test_KMLCaching_not_possible`. The first function tests the creation of a KML layer with various attributes and verifies its JavaScript representation. The second function mocks file existence checks and standard error output to simulate a scenario where KML caching is not possible, asserting that appropriate error messages are generated in both the session and the standard error buffer."
    },
    {
        "code": "from .models import Character, Faction, Ship\n\n__author__ = 'ekampf'\n\n\ndef initialize():\n    human = Character(name='Human')\n    human.put()\n\n    droid = Character(name='Droid')\n    droid.put()\n\n    rebels = Faction(id=\"rebels\", name='Alliance to Restore the Republic', hero_key=human.key)\n    rebels.put()\n\n    empire = Faction(id=\"empire\", name='Galactic Empire', hero_key=droid.key)\n    empire.put()\n\n    xwing = Ship(name='X-Wing', faction_key=rebels.key)\n    xwing.put()\n\n    ywing = Ship(name='Y-Wing', faction_key=rebels.key)\n    ywing.put()\n\n    awing = Ship(name='A-Wing', faction_key=rebels.key)\n    awing.put()\n\n    \n    \n    falcon = Ship(name='Millenium Falcon', faction_key=rebels.key)\n    falcon.put()\n\n    homeOne = Ship(name='Home One', faction_key=rebels.key)\n    homeOne.put()\n\n    tieFighter = Ship(name='TIE Fighter', faction_key=empire.key)\n    tieFighter.put()\n\n    tieInterceptor = Ship(name='TIE Interceptor', faction_key=empire.key)\n    tieInterceptor.put()\n\n    executor = Ship(name='Executor', faction_key=empire.key)\n    executor.put()\n\n\ndef create_ship(ship_name, faction_key):\n    new_ship = Ship(name=ship_name, faction_key=faction_key)\n    new_ship.put()\n    return new_ship\n",
        "summary": "The provided Python code initializes a database with characters, factions, and ships using classes from the `models` module. It includes functions to create initial data for rebels and empire factions, as well as a generic function to add new ships to the database."
    },
    {
        "code": "import logging\nimport time\nimport json\nfrom collections import defaultdict\n\nimport tqdm\nimport click\nfrom django.utils import timezone\nfrom django.db import transaction, connection\nfrom django.db.models import Q\nfrom django.contrib.auth import get_user_model\n\nimport rssant_common.django_setup  \nfrom rssant_api.models import Feed, Story, UnionFeed, UserStory, UserFeed\nfrom rssant_api.helper import reverse_url\nfrom rssant_common import _proxy_helper\nfrom rssant_common.helper import format_table, pretty_format_json\nfrom rssant_feedlib.reader import FeedResponseStatus, FeedReader\nfrom rssant_common import unionid\nfrom rssant_feedlib import processor\nfrom rssant_common.actor_client import scheduler\nfrom rssant_config import CONFIG\n\n\nLOG = logging.getLogger(__name__)\n\n\n@click.group()\ndef main():\n    \n\n\ndef _decode_feed_ids(option_feeds):\n    \n    return [int(x) for x in option_feeds.strip().split(',')]\n\n\ndef _decode_union_feed_ids(option_feeds):\n    \n    return [unionid.decode(x)[1] for x in option_feeds.strip().split(',')]\n\n\ndef _get_all_feed_ids():\n    feed_ids = [feed.id for feed in Feed.objects.only('id').all()]\n    return feed_ids\n\n\ndef _get_feed_ids(option_feeds):\n    if option_feeds and option_feeds != 'all':\n        feed_ids = _decode_feed_ids(option_feeds)\n    else:\n        feed_ids = _get_all_feed_ids()\n    return feed_ids\n\n\ndef _get_story_ids(option_storys):\n    if option_storys:\n        story_ids = option_storys.strip().split(',')\n    else:\n        story_ids = [story.id for story in Story.objects.only('id').all()]\n    return story_ids\n\n\n@main.command()\n@click.option('--dry-run', is_flag=True)\ndef fix_feed_total_storys(dry_run=False):\n    incorrect_feeds = Story.query_feed_incorrect_total_storys()\n    LOG.info('total %s incorrect feeds', len(incorrect_feeds))\n    header = ['feed_id', 'total_storys', 'correct_total_storys']\n    click.echo(format_table(incorrect_feeds, header=header))\n    if dry_run:\n        return\n    with transaction.atomic():\n        num_corrected = 0\n        for feed_id, *__ in tqdm.tqdm(incorrect_feeds, ncols=80, ascii=True):\n            fixed = Story.fix_feed_total_storys(feed_id)\n            if fixed:\n                num_corrected += 1\n        LOG.info('correct %s feeds', num_corrected)\n\n\n@main.command()\n@click.option('--feeds', help=\"feed ids, separate by ','\")\ndef update_feed_monthly_story_count(feeds=None):\n    feed_ids = _get_feed_ids(feeds)\n    LOG.info('total %s feeds', len(feed_ids))\n    for feed_id in tqdm.tqdm(feed_ids, ncols=80, ascii=True):\n        with transaction.atomic():\n            Story.refresh_feed_monthly_story_count(feed_id)\n\n\n@main.command()\n@click.option('--feeds', help=\"feed ids, separate by ','\")\ndef update_feed_dryness(feeds=None):\n    feed_ids = _get_feed_ids(feeds)\n    LOG.info('total %s feeds', len(feed_ids))\n    for feed_id in tqdm.tqdm(feed_ids, ncols=80, ascii=True):\n        with transaction.atomic():\n            feed = Feed.get_by_pk(feed_id)\n            if feed.total_storys <= 0:\n                continue\n            cnt = feed.monthly_story_count\n            if not cnt:\n                Story.refresh_feed_monthly_story_count(feed_id)\n            feed.refresh_from_db()\n            feed.dryness = feed.monthly_story_count.dryness()\n            feed.save()\n\n\n@main.command()\n@click.option('--feeds', help=\"feed ids, separate by ','\")\ndef update_feed_dt_first_story_published(feeds=None):\n    feed_ids = _get_feed_ids(feeds)\n    LOG.info('total %s feeds', len(feed_ids))\n    for feed_id in tqdm.tqdm(feed_ids, ncols=80, ascii=True):\n        with transaction.atomic():\n            feed = Feed.get_by_pk(feed_id)\n            if feed.dt_first_story_published:\n                continue\n            if feed.total_storys <= 0:\n                continue\n            try:\n                story = Story.get_by_offset(feed_id, 0, detail=True)\n            except Story.DoesNotExist:\n                LOG.warning(f'story feed_id={feed_id} offset=0 not exists')\n                continue\n            feed.dt_first_story_published = story.dt_published\n            feed.save()\n\n\n@main.command()\n@click.option('--storys', help=\"story ids, separate by ','\")\ndef update_story_has_mathjax(storys=None):\n    story_ids = _get_story_ids(storys)\n    LOG.info('total %s storys', len(story_ids))\n    for story_id in tqdm.tqdm(story_ids, ncols=80, ascii=True):\n        with transaction.atomic():\n            story = Story.objects.only('id', 'content', '_version').get(pk=story_id)\n            if processor.story_has_mathjax(story.content):\n                story.has_mathjax = True\n                story.save()\n\n\n@main.command()\ndef update_story_is_user_marked():\n    user_storys = list(\n        UserStory.objects\n        .exclude(is_watched=False, is_favorited=False)\n        .all()\n    )\n    LOG.info('total %s user marked storys', len(user_storys))\n    if not user_storys:\n        return\n    for user_story in tqdm.tqdm(user_storys, ncols=80, ascii=True):\n        Story.set_user_marked_by_id(user_story.story_id)\n\n\n@main.command()\n@click.option('--storys', help=\"story ids, separate by ','\")\ndef process_story_links(storys=None):\n    story_ids = _get_story_ids(storys)\n    LOG.info('total %s storys', len(story_ids))\n    for story_id in tqdm.tqdm(story_ids, ncols=80, ascii=True):\n        with transaction.atomic():\n            story = Story.objects.only('id', 'content', '_version').get(pk=story_id)\n            content = processor.process_story_links(story.content, story.link)\n            if story.content != content:\n                story.content = content\n                story.save()\n\n\n@main.command()\n@click.option('--storys', help=\"story ids, separate by ','\")\ndef update_story_images(storys=None):\n    story_ids = _get_story_ids(storys)\n    LOG.info('total %s storys', len(story_ids))\n    for story_id in tqdm.tqdm(story_ids, ncols=80, ascii=True):\n        story = Story.objects.get(pk=story_id)\n        scheduler.tell('harbor_rss.update_story_images', dict(\n            story_id=story_id,\n            story_url=story.link,\n            images=[],\n        ))\n\n\n@main.command()\n@click.argument('unionid_text')\ndef decode_unionid(unionid_text):\n    numbers = unionid.decode(unionid_text)\n    if len(numbers) == 3:\n        click.echo('user_id={} feed_id={} offset={}'.format(*numbers))\n    elif len(numbers) == 2:\n        click.echo('user_id={} feed_id={}'.format(*numbers))\n    else:\n        click.echo(numbers)\n\n\n@main.command()\n@click.option('--days', type=int, default=1)\n@click.option('--limit', type=int, default=100)\n@click.option('--threshold', type=int, default=99)\ndef delete_invalid_feeds(days=1, limit=100, threshold=99):\n    sql = \n    sql_ok_count = \n    t_begin = timezone.now() - timezone.timedelta(days=days)\n    error_feeds = defaultdict(dict)\n    with connection.cursor() as cursor:\n        cursor.execute(sql, [t_begin, limit])\n        for feed_id, title, link, url, status_code, count in cursor.fetchall():\n            error_feeds[feed_id].update(feed_id=feed_id, title=title, link=link, url=url)\n            error = error_feeds[feed_id].setdefault('error', {})\n            error_name = FeedResponseStatus.name_of(status_code)\n            error[error_name] = count\n            error_feeds[feed_id]['error_count'] = sum(error.values())\n            error_feeds[feed_id].update(ok_count=0, error_percent=100)\n        cursor.execute(sql_ok_count, [t_begin, list(error_feeds)])\n        for feed_id, ok_count in cursor.fetchall():\n            feed = error_feeds[feed_id]\n            total = feed['error_count'] + ok_count\n            error_percent = round((feed['error_count'] / total) * 100)\n            feed.update(ok_count=ok_count, error_percent=error_percent)\n    error_feeds = list(sorted(error_feeds.values(), key=lambda x: x['error_percent'], reverse=True))\n    delete_feed_ids = []\n    for feed in error_feeds:\n        if feed['error_percent'] >= threshold:\n            delete_feed_ids.append(feed['feed_id'])\n            click.echo(pretty_format_json(feed))\n    if delete_feed_ids:\n        confirm_delete = click.confirm(f'Delete {len(delete_feed_ids)} feeds?')\n        if not confirm_delete:\n            click.echo('Abort!')\n        else:\n            UnionFeed.bulk_delete(delete_feed_ids)\n            click.echo('Done!')\n    return error_feeds\n\n\n@main.command()\ndef fix_user_story_offset():\n    sql = \n    items = []\n    with connection.cursor() as cursor:\n        cursor.execute(sql)\n        for us_id, us_offset, story_offset in cursor.fetchall():\n            items.append((us_id, us_offset, story_offset))\n    click.echo(f'total {len(items)} mismatch user story offset')\n    if not items:\n        return\n    with transaction.atomic():\n        for us_id, us_offset, story_offset in tqdm.tqdm(items, ncols=80, ascii=True):\n            UserStory.objects.filter(pk=us_id).update(offset=-us_offset)\n        for us_id, us_offset, story_offset in tqdm.tqdm(items, ncols=80, ascii=True):\n            UserStory.objects.filter(pk=us_id).update(offset=story_offset)\n\n\n@main.command()\ndef subscribe_changelog():\n    changelog_url = CONFIG.root_url.rstrip('/') + '/changelog.atom'\n    feed = Feed.objects.get(url=changelog_url)\n    if not feed:\n        click.echo(f'not found changelog feed url={changelog_url}')\n        return\n    click.echo(f'changelog feed {feed}')\n    User = get_user_model()\n    users = list(User.objects.all())\n    click.echo(f'total {len(users)} users')\n    for user in tqdm.tqdm(users, ncols=80, ascii=True):\n        with transaction.atomic():\n            user_feed = UserFeed.objects\\\n                .filter(user_id=user.id, feed_id=feed.id).first()\n            if not user_feed:\n                user_feed = UserFeed(\n                    user_id=user.id,\n                    feed_id=feed.id,\n                    is_from_bookmark=False,\n                )\n                user_feed.save()\n\n\n@main.command()\ndef update_feed_use_proxy():\n    if not CONFIG.rss_proxy_enable:\n        click.echo('rss proxy not enable!')\n        return\n    blacklist = [\n        '%\u535a\u5ba2\u56ed%',\n        '%\u5fae\u4fe1%',\n        '%\u65b0\u6d6a%',\n        '%\u7684\u8bc4\u8bba%',\n        '%Comments on%',\n    ]\n    sql = \n    feeds = list(Feed.objects.raw(sql, [blacklist]))\n    click.echo(f'{len(feeds)} feeds need check')\n    reader = FeedReader(**_proxy_helper.get_proxy_options())\n    proxy_feeds = []\n    with reader:\n        for i, feed in enumerate(feeds):\n            click.echo(f'\n            status = reader.read(feed.url).status\n            click.echo(f'    \n            if FeedResponseStatus.is_need_proxy(status):\n                proxy_status = reader.read(feed.url, use_proxy=True).status\n                click.echo(f'    \n                if proxy_status == 200:\n                    proxy_feeds.append(feed)\n    click.echo(f'{len(proxy_feeds)} feeds need use proxy')\n    if proxy_feeds:\n        with transaction.atomic():\n            for feed in tqdm.tqdm(proxy_feeds, ncols=80, ascii=True):\n                feed.refresh_from_db()\n                feed.use_proxy = True\n                feed.save()\n\n\n@main.command()\n@click.argument('key')\ndef delete_feed(key):\n    try:\n        key = int(key)\n    except ValueError:\n        pass  \n    if isinstance(key, int):\n        feed = Feed.get_by_pk(key)\n    else:\n        feed = Feed.objects.filter(\n            Q(url__contains=key) | Q(title__contains=key)\n        ).first()\n    if not feed:\n        print(f'not found feed like {key}')\n        return\n    if click.confirm(f'delete {feed} ?'):\n        feed.delete()\n\n\n@main.command()\n@click.option('--feeds', help=\"feed ids, separate by ','\")\n@click.option('--union-feeds', help=\"union feed ids, separate by ','\")\n@click.option('--key', help=\"feed url or title keyword\")\n@click.option('--expire', type=int, default=1, help=\"expire hours\")\ndef refresh_feed(feeds, union_feeds, key, expire=None):\n    feed_ids = []\n    if feeds:\n        feed_ids.extend(_get_feed_ids(feeds))\n    if union_feeds:\n        feed_ids.extend(_decode_union_feed_ids(union_feeds))\n    if key:\n        cond = Q(url__contains=key) | Q(title__contains=key)\n        feed_objs = Feed.objects.filter(cond).only('id').all()\n        feed_ids.extend(x.id for x in feed_objs)\n    feed_ids = list(sorted(set(feed_ids)))\n    expire_at = time.time() + expire * 60 * 60\n    for feed_id in tqdm.tqdm(feed_ids, ncols=80, ascii=True):\n        feed = Feed.objects.only('id', 'url', 'use_proxy').get(pk=feed_id)\n        scheduler.tell('worker_rss.sync_feed', dict(\n            feed_id=feed.id,\n            url=feed.url,\n            use_proxy=feed.use_proxy,\n            is_refresh=True,\n        ), expire_at=expire_at)\n\n\n@main.command()\n@click.option('--feeds', required=True, help=\"feed ids, separate by ','\")\ndef update_feed_reverse_url(feeds):\n    feed_ids = _get_feed_ids(feeds)\n    for feed_id in tqdm.tqdm(feed_ids, ncols=80, ascii=True):\n        feed = Feed.objects.get(pk=feed_id)\n        feed.reverse_url = reverse_url(feed.url)\n        feed.save()\n\n\n@main.command()\n@click.option('--dst', required=True, help='actor dst')\n@click.option('--content', help='message content')\n@click.option('--expire-seconds', type=int, help='expire time in seconds')\ndef tell(dst, content, expire_seconds):\n    if content:\n        content = json.loads(content)\n    expire_at = None\n    if expire_seconds:\n        expire_at = int(time.time()) + expire_seconds\n    scheduler.tell(dst, content=content, expire_at=expire_at)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script appears to be a command-line interface (CLI) for managing RSS feeds and related data in a system. It uses the `click` library to define various commands that interact with a database of feeds, users, and other entities. Here's a breakdown of some key functionalities:\n\n1. **Feed Management**:\n   - Commands like `add-feed`, `update-feed`, and `delete-feed` allow adding new feeds, updating existing ones, or deleting them based on their ID or URL.\n   - The script also includes functionality to refresh feed data (`refresh-feed`) and update reverse URLs for feeds.\n\n2. **User Management**:\n   - Commands like `add-user` and `update-user` manage user accounts in the system.\n   - Users can be associated with feeds through `user-feed` commands, such as adding or updating user subscriptions.\n\n3. **Proxy Settings**:\n   - The script checks if certain feeds need to use a proxy for access (`update-feed-use-proxy`) based on predefined blacklist patterns.\n\n4. **Changelog Management**:\n   - A command to subscribe users to the changelog feed is provided, which helps in keeping track of updates and changes within the system.\n\n5. **General Utility Functions**:\n   - The script includes helper functions like `_get_feed_ids`, `_decode_union_feed_ids`, and `_proxy_helper.get_proxy_options` to handle common tasks such as parsing input arguments, fetching proxy settings, and more.\n\n6. **Scheduling and Asynchronous Tasks**:\n   - The script uses a scheduler (`scheduler.tell`) to send asynchronous tasks to workers for processing, which could involve syncing feed data or other background operations.\n\n7. **Database Operations**:\n   - The script interacts with a database using Django ORM queries to retrieve, update, and delete records related to feeds, users, and user subscriptions.\n\nOverall, this script is designed to provide a comprehensive set of tools for managing an RSS-based system, including feed management, user management, proxy settings, and asynchronous task scheduling."
    },
    {
        "code": "import webapp2\nfrom google.appengine.api import app_identity\nfrom google.appengine.api import mail\nfrom conference import ConferenceApi\n\nclass SetAnnouncementHandler(webapp2.RequestHandler):\n    def get(self):\n        \n        header = self.request.headers.get('X-AppEngine-Cron', None)\n        if not header:\n            raise ValueError('attempt to access cron handler directly, '\n                             'missing custom App Engine header')\n        ConferenceApi._cacheAnnouncement()\n        self.response.set_status(204)\n\n\nclass SendConfirmationEmailHandler(webapp2.RequestHandler):\n    def post(self):\n        \n        header = self.request.headers.get('X-AppEngine-QueueName', None)\n        if not header:\n            raise ValueError('attempt to access task handler directly, '\n                             'missing custom App Engine header')\n        mail.send_mail(\n            'noreply@%s.appspotmail.com' % (\n                app_identity.get_application_id()),     \n            self.request.get('email'),                  \n            'You created a new Conference!',            \n            'Hi, you have created a following '         \n            'conference:\\r\\n\\r\\n%s' % self.request.get(\n                'conferenceInfo')\n        )\n\n\napp = webapp2.WSGIApplication([\n    ('/crons/set_announcement', SetAnnouncementHandler),\n    ('/tasks/send_confirmation_email', SendConfirmationEmailHandler)\n], debug=True)\n",
        "summary": "The provided Python code defines two handlers for Google App Engine: `SetAnnouncementHandler` and `SendConfirmationEmailHandler`. The `SetAnnouncementHandler` is triggered by a cron job to cache announcements, while the `SendConfirmationEmailHandler` processes tasks from a queue to send confirmation emails."
    },
    {
        "code": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef pad_to_shape(this, shp):\r\n    \r\n    return F.pad(this, (0, shp[3] - this.shape[3], 0, shp[2] - this.shape[2]))\r\n\r\n\r\nclass First(nn.Module):\r\n    def __init__(self, in_channels, middle_channels, out_channels, dropout=False):\r\n        super(First, self).__init__()\r\n\r\n        layers = [\r\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ]\r\n\r\n        if dropout:\r\n            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\r\n            layers.append(nn.Dropout2d(p=dropout))\r\n\r\n        self.first = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.first(x)\r\n\r\n\r\nclass Encoder(nn.Module):\r\n    def __init__(\r\n            self, in_channels, middle_channels, out_channels,\r\n            dropout=False, downsample_kernel=2\r\n    ):\r\n        super(Encoder, self).__init__()\r\n\r\n        layers = [\r\n            nn.MaxPool2d(kernel_size=downsample_kernel),\r\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ]\r\n\r\n        if dropout:\r\n            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\r\n            layers.append(nn.Dropout2d(p=dropout))\r\n\r\n        self.encoder = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.encoder(x)\r\n\r\n\r\nclass Center(nn.Module):\r\n    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\r\n        super(Center, self).__init__()\r\n\r\n        layers = [\r\n            nn.MaxPool2d(kernel_size=2),\r\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.ConvTranspose2d(out_channels, deconv_channels, kernel_size=2, stride=2)\r\n        ]\r\n\r\n        if dropout:\r\n            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\r\n            layers.append(nn.Dropout2d(p=dropout))\r\n\r\n        self.center = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.center(x)\r\n\r\n\r\nclass Decoder(nn.Module):\r\n    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\r\n        super(Decoder, self).__init__()\r\n\r\n        layers = [\r\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.ConvTranspose2d(out_channels, deconv_channels, kernel_size=2, stride=2)\r\n        ]\r\n\r\n        if dropout:\r\n            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\r\n            layers.append(nn.Dropout2d(p=dropout))\r\n\r\n        self.decoder = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.decoder(x)\r\n\r\n\r\nclass Last(nn.Module):\r\n    def __init__(self, in_channels, middle_channels, out_channels, softmax=False):\r\n        super(Last, self).__init__()\r\n\r\n        layers = [\r\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(middle_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(middle_channels, out_channels, kernel_size=1),\r\n            nn.Sigmoid()\r\n        ]\r\n\r\n        if softmax:\r\n            layers.append(nn.Softmax2d())\r\n\r\n        self.first = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.first(x)\r\n\r\n\r\nclass UNet(nn.Module):\r\n    def __init__(self, in_channels, out_channels, softmax=False):\r\n        super(UNet, self).__init__()\r\n        self.first = First(in_channels, 64, 64)\r\n        self.encoder_1 = Encoder(64, 128, 128)\r\n        self.encoder_2 = Encoder(128, 256, 256)\r\n        self.encoder_3 = Encoder(256, 512, 512)\r\n        self.center = Center(512, 1024, 1024, 512)\r\n        self.decoder_3 = Decoder(1024, 512, 512, 256)\r\n        self.decoder_2 = Decoder(512, 256, 256, 128)\r\n        self.decoder_1 = Decoder(256, 128, 128, 64)\r\n        self.last = Last(128, 64, out_channels, softmax=softmax)\r\n\r\n    def forward(self, x):\r\n        x_first = self.first(x)\r\n        x_enc_1 = self.encoder_1(x_first)\r\n        x_enc_2 = self.encoder_2(x_enc_1)\r\n        x_enc_3 = self.encoder_3(x_enc_2)\r\n        x_cent = self.center(x_enc_3)\r\n        x_dec_3 = self.decoder_3(torch.cat([pad_to_shape(x_cent, x_enc_3.shape), x_enc_3], dim=1))\r\n        x_dec_2 = self.decoder_2(torch.cat([pad_to_shape(x_dec_3, x_enc_2.shape), x_enc_2], dim=1))\r\n        x_dec_1 = self.decoder_1(torch.cat([pad_to_shape(x_dec_2, x_enc_1.shape), x_enc_1], dim=1))\r\n        return self.last(torch.cat([pad_to_shape(x_dec_1, x_first.shape), x_first], dim=1))\r\n\r\n\r\nif __name__ == '__main__':\r\n    pass\r\n",
        "summary": "The provided Python code defines a UNet architecture using PyTorch for image segmentation tasks. It includes classes for various components of the UNet such as encoding, decoding, and center modules, each designed to process input images through convolutional layers, batch normalization, ReLU activations, dropout (if specified), and transposed convolutions for upsampling. The `UNet` class integrates these components to form a complete network that can be trained on image data for tasks like medical image segmentation."
    },
    {
        "code": "import os\nimport os.path as op\nimport numpy as np\nimport mne\nfrom h5io import read_hdf5\nfrom mne.decoding import GeneralizingEstimator, LinearModel\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom jr.gat import (AngularRegression, scorer_spearman,\n                    scorer_angle)\nfrom base import (complete_behavior, get_events_interactions)\nfrom config import path_data\nimport sys\nsubject = sys.argv[1]  \n\noutput_folder = '/sensors_accross_epochs_and_conditions/'\n\nresults_folder = op.join(path_data + 'results/' + subject + output_folder)\nif not os.path.exists(results_folder):\n    os.makedirs(results_folder)\n\n\nfname = op.join(path_data, subject, 'behavior_Target.hdf5')\nevents = read_hdf5(fname)\nevents = complete_behavior(events)\nevents = get_events_interactions(events)\n\nfname = op.join(path_data, subject, 'epochs_Target.fif')\nepochs_target = mne.read_epochs(fname)\nepochs_target.pick_types(meg=True, ref_meg=False)\nepochs_target.crop(-0.2, 0.9)\n\nfname = op.join(path_data, subject, 'epochs_Cue.fif')\nepochs_cue = mne.read_epochs(fname)\nepochs_cue.pick_types(meg=True, ref_meg=False)\nepochs_cue.crop(0, 1.5)\n\nfname = op.join(path_data, subject, 'epochs_Probe.fif')\nepochs_probe = mne.read_epochs(fname)\nepochs_probe.pick_types(meg=True, ref_meg=False)\nepochs_probe.crop(0, 0.9)\n\nX0 = epochs_target._data\nX1 = epochs_cue._data\nX2 = epochs_probe._data\nX = np.concatenate((X0, X1, X2), axis=2)\n\n\npaired_analyses = [['target_sfreq_cue_left_sfreq', 'left_sfreq'],\n                   ['target_sfreq_cue_right_sfreq', 'right_sfreq'],\n                   ['left_sfreq', 'target_sfreq_cue_left_sfreq'],\n                   ['right_sfreq', 'target_sfreq_cue_right_sfreq'],\n                   ['target_angle_cue_left_angle', 'left_angle'],\n                   ['target_angle_cue_right_angle', 'right_angle'],\n                   ['left_angle', 'target_angle_cue_left_angle'],\n                   ['right_angle', 'target_angle_cue_right_angle']]\n\nfor paired_analysis in paired_analyses:\n    y_test = np.array(events[paired_analysis[0]])\n    y_train = np.array(events[paired_analysis[1]])\n    \n    if 'angle' in paired_analysis[0][:14]:\n        clf = make_pipeline(StandardScaler(),\n                            LinearModel(AngularRegression(Ridge(),\n                                                          independent=False)))\n        scorer = scorer_angle\n        kwargs = dict()\n        gat = GeneralizingEstimator(clf, scoring=make_scorer(scorer),\n                                    n_jobs=24, **kwargs)\n        y_test = np.array(y_test, dtype=float)\n        y_train = np.array(y_train, dtype=float)\n    elif 'sfreq' in paired_analysis[0][:14]:\n        clf = make_pipeline(StandardScaler(), LinearModel(Ridge()))\n        scorer = scorer_spearman\n        kwargs = dict()\n        gat = GeneralizingEstimator(clf, scoring=make_scorer(scorer),\n                                    n_jobs=24, **kwargs)\n        y_test = np.array(y_test, dtype=float)\n        y_train = np.array(y_train, dtype=float)\n    \n    sel = np.where(events['is_eye_fixed'] == 1)[0]\n    y_train = y_train[sel]\n    y_test = y_test[sel]\n    X = np.concatenate((X0, X1, X2), axis=2)\n    X = X[sel]\n    \n    \n    cv = StratifiedKFold(7)\n    scores = list()\n    scs = list()\n    if np.isnan(y_train).any():\n        sel = np.where(~np.isnan(y_train))[0]\n        for train, test in cv.split(X[sel], y_train[sel]):\n            gat.fit(X[sel][train], y_train[sel][train])\n            score = gat.score(X[sel][test], y_test[sel][test])\n            sc = gat.score(X[sel][test], y_train[sel][test])  \n            scores.append(score)\n            scs.append(sc)\n        scores = np.mean(scores, axis=0)\n        scs = np.mean(scs, axis=0)\n    else:\n        for train, test in cv.split(X, y_train):\n            y_te = y_test[test]\n            X_te = X[test]\n            y_te = y_te[np.where(~np.isnan(y_te))[0]]\n            X_te = X_te[np.where(~np.isnan(y_te))[0]]\n            y_tr = y_train[train]\n            X_tr = X[train]\n            y_tr = y_tr[np.where(~np.isnan(y_tr))[0]]\n            X_tr = X_tr[np.where(~np.isnan(y_tr))[0]]\n            y_tr_te = y_train[test]\n            X_tr_te = X[test]\n            y_tr_te = y_tr_te[np.where(~np.isnan(y_tr_te))[0]]\n            X_tr_te = X_tr_te[np.where(~np.isnan(y_tr_te))[0]]\n            gat.fit(X_tr, y_tr)\n            score = gat.score(X_te, y_te)\n            sc = gat.score(X_tr_te, y_tr_te)   \n            scores.append(score)\n            scs.append(sc)\n        scores = np.mean(scores, axis=0)\n        scs = np.mean(scs, axis=0)\n\n    \n    fname = results_folder +\\\n        '%s_scores_%s_cross_%s.npy' % (subject,\n                                       paired_analysis[0],\n                                       paired_analysis[1])\n    np.save(fname, np.array(scores))  \n    fname = results_folder +\\\n        '%s_scores_%s.npy' % (subject, paired_analysis[1])\n    np.save(fname, np.array(scs))  \n",
        "summary": "The Python script processes EEG data for a specific subject, performing paired analyses on target and cue events to predict left and right frequencies and angles. It uses machine learning models like Ridge regression and AngularRegression within a GeneralizingEstimator framework, employing cross-validation and scoring metrics such as Spearman correlation and angular similarity. The results are saved in NumPy files for further analysis."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport boto3\nimport os\nimport logging\nimport platform\nimport pytest\nimport shutil\nimport sys\nimport tempfile\n\nfrom sagemaker import LocalSession, Session\nfrom sagemaker.pytorch import PyTorch\n\nfrom .utils import image_utils\n\nlogger = logging.getLogger(__name__)\nlogging.getLogger('boto').setLevel(logging.INFO)\nlogging.getLogger('boto3').setLevel(logging.INFO)\nlogging.getLogger('botocore').setLevel(logging.INFO)\nlogging.getLogger('factory.py').setLevel(logging.INFO)\nlogging.getLogger('auth.py').setLevel(logging.INFO)\nlogging.getLogger('connectionpool.py').setLevel(logging.INFO)\n\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\n\nNO_P2_REGIONS = ['ap-east-1', 'ap-northeast-3', 'ap-southeast-2', 'ca-central-1', 'eu-central-1', 'eu-north-1',\n                 'eu-west-2', 'eu-west-3', 'us-west-1', 'sa-east-1', 'me-south-1']\nNO_P3_REGIONS = ['ap-east-1', 'ap-northeast-3', 'ap-southeast-1', 'ap-southeast-2', 'ap-south-1', 'ca-central-1',\n                 'eu-central-1', 'eu-north-1', 'eu-west-2', 'eu-west-3', 'sa-east-1', 'us-west-1', 'me-south-1']\n\n\ndef pytest_addoption(parser):\n    parser.addoption('--build-image', '-D', action='store_true')\n    parser.addoption('--build-base-image', '-B', action='store_true')\n    parser.addoption('--aws-id')\n    parser.addoption('--instance-type')\n    parser.addoption('--accelerator-type', default=None)\n    parser.addoption('--docker-base-name', default='pytorch')\n    parser.addoption('--region', default='us-west-2')\n    parser.addoption('--framework-version', default=PyTorch.LATEST_VERSION)\n    parser.addoption('--py-version', choices=['2', '3'], default=str(sys.version_info.major))\n    \n    parser.addoption('--processor', choices=['gpu', 'cpu', 'eia'], default='cpu')\n    \n    parser.addoption('--tag', default=None)\n    parser.addoption('--generate-coverage-doc', default=False, action='store_true',\n                     help='use this option to generate test coverage doc')\n\n\ndef pytest_collection_modifyitems(session, config, items):\n    if config.getoption(\"--generate-coverage-doc\"):\n        from test.test_utils.test_reporting import TestReportGenerator\n        report_generator = TestReportGenerator(items, is_sagemaker=True)\n        report_generator.generate_coverage_doc(framework=\"pytorch\", job_type=\"inference\")\n\n\n@pytest.fixture(scope='session', name='docker_base_name')\ndef fixture_docker_base_name(request):\n    return request.config.getoption('--docker-base-name')\n\n\n@pytest.fixture(scope='session', name='region')\ndef fixture_region(request):\n    return request.config.getoption('--region')\n\n\n@pytest.fixture(scope='session', name='framework_version')\ndef fixture_framework_version(request):\n    return request.config.getoption('--framework-version')\n\n\n@pytest.fixture(scope='session', name='py_version')\ndef fixture_py_version(request):\n    return 'py{}'.format(int(request.config.getoption('--py-version')))\n\n\n@pytest.fixture(scope='session', name='processor')\ndef fixture_processor(request):\n    return request.config.getoption('--processor')\n\n\n@pytest.fixture(scope='session', name='tag')\ndef fixture_tag(request, framework_version, processor, py_version):\n    provided_tag = request.config.getoption('--tag')\n    default_tag = '{}-{}-{}'.format(framework_version, processor, py_version)\n    return provided_tag if provided_tag else default_tag\n\n\n@pytest.fixture(scope='session', name='docker_image')\ndef fixture_docker_image(docker_base_name, tag):\n    return '{}:{}'.format(docker_base_name, tag)\n\n\n@pytest.fixture\ndef opt_ml():\n    tmp = tempfile.mkdtemp()\n    os.mkdir(os.path.join(tmp, 'output'))\n\n    \n    \n    opt_ml_dir = '/private{}'.format(tmp) if platform.system() == 'Darwin' else tmp\n    yield opt_ml_dir\n\n    shutil.rmtree(tmp, True)\n\n\n@pytest.fixture(scope='session', name='use_gpu')\ndef fixture_use_gpu(processor):\n    return processor == 'gpu'\n\n\n@pytest.fixture(scope='session', name='build_base_image', autouse=True)\ndef fixture_build_base_image(request, framework_version, py_version, processor, tag, docker_base_name):\n    build_base_image = request.config.getoption('--build-base-image')\n    if build_base_image:\n        return image_utils.build_base_image(framework_name=docker_base_name,\n                                            framework_version=framework_version,\n                                            py_version=py_version,\n                                            base_image_tag=tag,\n                                            processor=processor,\n                                            cwd=os.path.join(dir_path, '..'))\n\n    return tag\n\n\n@pytest.fixture(scope='session', name='sagemaker_session')\ndef fixture_sagemaker_session(region):\n    return Session(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(scope='session', name='sagemaker_local_session')\ndef fixture_sagemaker_local_session(region):\n    return LocalSession(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(name='aws_id', scope='session')\ndef fixture_aws_id(request):\n    return request.config.getoption('--aws-id')\n\n\n@pytest.fixture(name='instance_type', scope='session')\ndef fixture_instance_type(request, processor):\n    provided_instance_type = request.config.getoption('--instance-type')\n    default_instance_type = 'local' if processor == 'cpu' else 'local_gpu'\n    return provided_instance_type or default_instance_type\n\n\n@pytest.fixture(name='accelerator_type', scope='session')\ndef fixture_accelerator_type(request):\n    return request.config.getoption('--accelerator-type')\n\n\n@pytest.fixture(name='docker_registry', scope='session')\ndef fixture_docker_registry(aws_id, region):\n    return '{}.dkr.ecr.{}.amazonaws.com'.format(aws_id, region)\n\n\n@pytest.fixture(name='ecr_image', scope='session')\ndef fixture_ecr_image(docker_registry, docker_base_name, tag):\n    return '{}/{}:{}'.format(docker_registry, docker_base_name, tag)\n\n\n@pytest.fixture(autouse=True)\ndef skip_by_device_type(request, use_gpu, instance_type, accelerator_type):\n    is_gpu = use_gpu or instance_type[3] in ['g', 'p']\n    is_eia = accelerator_type is not None\n\n    \n    \n    if (request.node.get_closest_marker('gpu_test') and not is_gpu) or \\\n            (request.node.get_closest_marker('cpu_test') and is_gpu):\n        pytest.skip('Skipping because running on \\'{}\\' instance'.format(instance_type))\n\n    \n    elif (request.node.get_closest_marker('gpu_test') or request.node.get_closest_marker('cpu_test')) and is_eia:\n        pytest.skip('Skipping because running on \\'{}\\' instance'.format(instance_type))\n\n    \n    elif request.node.get_closest_marker('eia_test') and not is_eia:\n        pytest.skip('Skipping because running on \\'{}\\' instance'.format(instance_type))\n\n\n@pytest.fixture(autouse=True)\ndef skip_by_py_version(request, py_version):\n    if request.node.get_closest_marker('skip_py2') and py_version != 'py3':\n        pytest.skip('Skipping the test because Python 2 is not supported.')\n\n\n@pytest.fixture(autouse=True)\ndef skip_gpu_instance_restricted_regions(region, instance_type):\n    if (region in NO_P2_REGIONS and instance_type.startswith('ml.p2')) \\\n       or (region in NO_P3_REGIONS and instance_type.startswith('ml.p3')):\n        pytest.skip('Skipping GPU test in region {}'.format(region))\n\n\n@pytest.fixture(autouse=True)\ndef skip_gpu_py2(request, use_gpu, instance_type, py_version, framework_version):\n    is_gpu = use_gpu or instance_type[3] in ['g', 'p']\n    if request.node.get_closest_marker('skip_gpu_py2') and is_gpu and py_version != 'py3' \\\n            and framework_version == '1.4.0':\n        pytest.skip('Skipping the test until mms issue resolved.')\n",
        "summary": "This Python script sets up a testing environment for PyTorch models on AWS SageMaker, including configuration options for various parameters such as instance type, region, and Docker image settings. It also includes fixtures to manage resources like temporary directories, ECR images, and session configurations, as well as hooks to skip tests based on hardware capabilities and Python version compatibility."
    },
    {
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import roc_curve, auc\n\n__author__ = \"Aur\u00e9lien Massiot\"\n__credits__ = \"https://github.com/octo-technology/bdacore\"\n__license__ = \"Apache 2.0\"\n\n\ndef plot_confusion_matrix(confusion_matrix, classes_list, normalize=True, figsize=(10, 7), fontsize=14, cmap=\"Blues\"):\n    \n    confusion_matrix = np.array(confusion_matrix)\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    if normalize:\n        normalized_cm = np.array(confusion_matrix).astype('float') / np.array(confusion_matrix).sum(axis=1)[:,\n                                                                     np.newaxis]\n        df_cm = pd.DataFrame(\n            normalized_cm, index=classes_list, columns=classes_list,\n        )\n        plt.matshow(df_cm, fignum=0, cmap=cmap)\n    else:\n        df_cm = pd.DataFrame(\n            confusion_matrix, index=classes_list, columns=classes_list,\n        )\n        plt.matshow(df_cm, fignum=0, cmap=cmap)\n    ax.set_xticks(np.arange(len(classes_list)))\n    ax.set_yticks(np.arange(len(classes_list)))\n    ax.set_xticklabels(classes_list)\n    ax.set_yticklabels(classes_list)\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    for i in range(len(classes_list)):\n        for j in range(len(classes_list)):\n            ax.text(j, i, confusion_matrix[i, j], ha=\"center\", va=\"center\", color=\"grey\", fontsize=fontsize)\n\n    plt.ylabel('True labels')\n    plt.xlabel('Predicted labels')\n    plt.show()\n\n\ndef plot_roc_curve(y_test, y_pred_probas, proba_step=None):\n    \n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probas[:, 1])\n    auc_score = auc(fpr, tpr)\n\n    plt.figure()\n    lw = 1\n    plt.plot(fpr, tpr, color='darkorange', lw=lw, marker='.')\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    if proba_step is not None:\n        i = 0\n        for x, y, txt in zip(fpr, tpr, thresholds):\n            if i % proba_step == 0:\n                plt.annotate(np.round(txt, 2), (x, y - 0.04), color='darkgray', fontsize=8)\n            i += 1\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic (ROC) - AUC score: {}'.format(str(np.round(auc_score,3))))\n    plt.show()\n",
        "summary": "The provided Python code includes two functions for visualizing machine learning model performance. The `plot_confusion_matrix` function generates a heatmap to display the true and predicted labels of a classification model, optionally normalized. The `plot_roc_curve` function plots the Receiver Operating Characteristic (ROC) curve along with the Area Under the Curve (AUC) score, providing insights into the model's ability to distinguish between classes based on probability estimates."
    },
    {
        "code": "urlpatterns = []\n\nhandler404 = \"csrf_tests.views.csrf_token_error_handler\"\n",
        "summary": "The provided Python code sets up an empty list for URL patterns and specifies a custom error handler function `csrf_token_error_handler` to manage 404 errors in the application, which is likely related to CSRF token management in a Django project."
    },
    {
        "code": "import sys\nimport unittest\n\nimport sust.api.climate_explorer.clientgen\nfrom sust.api.climate_explorer.clientgen.model.physical_risk_summary_indicators import PhysicalRiskSummaryIndicators\n\n\nclass TestPhysicalRiskSummaryIndicators(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testPhysicalRiskSummaryIndicators(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script imports necessary modules and sets up a unit test class for the `PhysicalRiskSummaryIndicators` model from the `sust.api.climate_explorer.clientgen` package. The test class includes methods for setting up and tearing down tests, as well as a placeholder method for testing the `PhysicalRiskSummaryIndicators` functionality, which currently does nothing."
    },
    {
        "code": "import socket\n\nfrom oslo_config import cfg\n\nfrom sysinv.openstack.common import context\nfrom sysinv.openstack.common import log\nfrom sysinv.openstack.common import periodic_task\nfrom sysinv.openstack.common import rpc\nfrom sysinv.openstack.common.rpc import service as rpc_service\nfrom oslo_service import service\n\n\ncfg.CONF.register_opts([\n    cfg.IntOpt('periodic_interval',\n               default=60,\n               help='seconds between running periodic tasks'),\n    cfg.StrOpt('host',\n               default=socket.getfqdn(),\n               help='Name of this node.  This can be an opaque identifier.  '\n               'It is not necessarily a hostname, FQDN, or IP address. '\n               'However, the node name must be valid within '\n               'an AMQP key, and if using ZeroMQ, a valid '\n               'hostname, FQDN, or IP address'),\n])\n\nCONF = cfg.CONF\n\n\nclass PeriodicService(rpc_service.Service, periodic_task.PeriodicTasks):\n\n    def start(self):\n        super(PeriodicService, self).start()\n        admin_context = context.RequestContext('admin', 'admin', is_admin=True)\n        self.tg.add_timer(cfg.CONF.periodic_interval,\n                          self.manager.periodic_tasks,\n                          context=admin_context)\n\n\ndef prepare_service(argv=None):\n    if argv is None:\n        argv = []\n    rpc.set_defaults(control_exchange='sysinv')\n    cfg.set_defaults(log.log_opts,\n                     default_log_levels=['amqplib=WARN',\n                                         'qpid.messaging=INFO',\n                                         'sqlalchemy=WARN',\n                                         'keystoneclient=INFO',\n                                         'stevedore=INFO',\n                                         'eventlet.wsgi.server=WARN'\n                                         ])\n    cfg.CONF(argv[1:], project='sysinv')\n    log.setup('sysinv')\n\n\ndef process_launcher():\n    return service.ProcessLauncher(CONF)\n",
        "summary": "The provided Python code sets up a configuration for an OpenStack service using the `oslo_config` library, defines a periodic task service that runs at intervals specified in the configuration, and includes functions to prepare and launch the service with appropriate logging and RPC settings."
    },
    {
        "code": "import re\nimport unittest\n\nfrom setuptools import setup\n\n\ndef my_test_suite():\n    \n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover('tests', pattern='test_*.py')\n    return test_suite\n\n\nwith open('rebin.py', 'r') as f:\n    lines = f.read()\n    version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n                        lines, re.MULTILINE).group(1)\n    description = re.search(r'^u\\\"\\\"\\\"(.*)',\n                            lines, re.MULTILINE).group(1)\n    long_description = re.search('^u\\\"\\\"\\\"(.*)^\\\"\\\"\\\"',\n                                 lines, re.MULTILINE | re.DOTALL).group(1)\n    author = re.search(r'^__author__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n                       lines, re.MULTILINE).group(1)\n\nprint(long_description)\n\nsetup(\n    name='rebin',\n    version=version,\n    description=description,\n    long_description=long_description,\n    url='https://github.com/sbrisard/rebin',\n    author=author,\n    author_email='',\n    py_modules=['rebin'],\n    license='BSD-3',\n    classifiers=['Development Status :: 4 - Beta',\n                 'Intended Audience :: Developers',\n                 'Intended Audience :: Science/Research',\n                 'License :: OSI Approved :: BSD License',\n                 'Operating System :: OS Independent',\n                 'Topic :: Software Development :: Build Tools',\n                 'Programming Language :: Python :: 2.7',\n                 'Programming Language :: Python :: 3',\n                 'Topic :: Scientific/Engineering'],\n    test_suite='setup.my_test_suite',\n    install_requires=['numpy'],\n)\n",
        "summary": "The provided Python script sets up a package named 'rebin' using setuptools, including version and author information extracted from the source code file. It also defines a test suite to discover and run tests located in the 'tests' directory that match the pattern 'test_*.py'. The script prints the long description of the package and configures various metadata such as URL, license, and supported programming languages before calling setup to install the package."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text format such as CSV or JSON."
    },
    {
        "code": "import os\nimport signal\nimport time\n\nfrom prompt_toolkit import HTML\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.patch_stdout import patch_stdout\nfrom prompt_toolkit.shortcuts import ProgressBar\n\n\ndef main():\n    bottom_toolbar = HTML(\n        ' <b>[f]</b> Print \"f\" <b>[q]</b> Abort  <b>[x]</b> Send Control-C.'\n    )\n\n    \n    kb = KeyBindings()\n    cancel = [False]\n\n    @kb.add(\"f\")\n    def _(event):\n        print(\"You pressed `f`.\")\n\n    @kb.add(\"q\")\n    def _(event):\n        \"Quit by setting cancel flag.\"\n        cancel[0] = True\n\n    @kb.add(\"x\")\n    def _(event):\n        \"Quit by sending SIGINT to the main thread.\"\n        os.kill(os.getpid(), signal.SIGINT)\n\n    \n    \n    with patch_stdout():\n        with ProgressBar(key_bindings=kb, bottom_toolbar=bottom_toolbar) as pb:\n            for i in pb(range(800)):\n                time.sleep(0.01)\n\n                if cancel[0]:\n                    break\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The Python script uses the `prompt_toolkit` library to create a command-line interface with a progress bar and custom key bindings for controlling the program's flow, including printing messages, aborting execution, and sending a SIGINT signal. The main function runs a loop that updates the progress bar while checking for user input to trigger specific actions or exit conditions."
    },
    {
        "code": "import os\nimport sqlite3 as lite\nimport sys\nimport json\nimport time\nimport urllib.request\nimport tweepy\nfrom TwitterMiner_Keys import *\nfrom tweepy import OAuthHandler\nfrom TwitterMiner_settings import *\nimport hashlib\n\n\n\ndef dump_hash(twitter_dump):\n    data_hash = None \n    dump = hashlib.sha1()\n    dump.update(twitter_dump)\n    data_hash = dump.hexdigest()\n    return data_hash\n\ndef file_hash(point_to_file):\n    hash_sha1 = hashlib.sha1()\n    with open(point_to_file, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha1.update(chunk)\n    print(hash_sha1.hexdigest())\n    return hash_sha1.hexdigest()\n\ndef extract_image_blob(posted_image_dest):\n    with open(\"test.jpg\", \"wb\") as image_file:\n        c.execute(\"SELECT tweeted_image FROM T_Tweets WHERE Tweet_id = \" + str(tweet_id))\n        ablob = c.fetchone()\n        image_file.write(ablob[0])\n    \ndef create_db(table_name):\n    \n    c.execute(\"PRAGMA journal_mode = WAL\")\n    \n    c.execute(\"CREATE TABLE IF NOT EXISTS \" + table_name + \"(tweet_id INTEGER NOT NULL PRIMARY KEY, date_mined TEXT, screen_name TEXT, \\\n                                                            user_id INTEGER, users_name TEXT, created_at_UTC TEXT, is_retweet TEXT, \\\n                                                            retweeted_times TEXT, text TEXT, place_name TEXT, country_code TEXT, country TEXT, \\\n                                                            bounding_box TEXT, source_tweeted TEXT, geo TEXT, in_reply_to_user TEXT, \\\n                                                            inreply_statusid TEXT, posted_image_dest TEXT, tweeted_image BLOB, image_hash TEXT, \\\n                                                            media_type TEXT, media_url TEXT, media_id TEXT, posted_video_dest TEXT, \\\n                                                            tweeted_video BLOB, video_hash TEXT, video_type TEXT, video_url TEXT, \\\n                                                            url_in_tweet TEXT, status BLOB, status_hash TEXT, bookmark TEXT)\")\n\n    conn.commit()\n\ndef get_all_tweets(screen_name):\n    \n    \n\n    alltweets = []\n\n    \n\n    try:\n        new_tweets = api.user_timeline(screen_name = screen_name, count=200)\n    except tweepy.TweepError:\n        print(\"Failed to pull tweets from %s\" % screen_name)\n        print(\"User may be protected/private.\")\n        print(\"Exiting...\")\n        sys.exit()\n    except tweepy.RateLimitError:  \n        print(\"Failed to pull the tweets due to a Twitter Rate Limit error.\")\n        print(\"Please wait 15 min and try again...\")\n        sys.exit()    \n\n    \n    alltweets.extend(new_tweets)\n\n    \n    oldest = alltweets[-1].id - 1\n\n    \n    while len(new_tweets) > 0:\n        print(\"getting tweets before %s\" % (oldest))\n\n        \n        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n\n        \n        alltweets.extend(new_tweets)\n\n        \n        oldest = alltweets[-1].id - 1\n\n        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n\n    \n    for status in alltweets:\n        \n\n        Tweetid = status.id\n        screenname = status.user.screen_name\n        userid = status.user.id\n        usersname = status.user.name\n        tweettime = status.created_at\n\n        \n        if hasattr(status, 'retweeted_status'):\n            is_retweet = True\n            \n            \n            \n            \n            \n            if hasattr(status.retweeted_status, 'extended_tweet'): \n                Amp_text = str(status.retweeted_status.extended_tweet['full_text'])\n                tweet = \"RT: \" + Amp_text.replace('&amp;','&')\n                \n            else:\n                Amp_text = status.retweeted_status.text\n                tweet = \"RT: \" + Amp_text.replace('&amp;','&') \n        else:\n            is_retweet = False\n            Amp_text = status.text\n            tweet = Amp_text.replace('&amp;','&')            \n\n        retweeted_times = status.retweet_count\n        \n\n        if status.place is not None:\n            placename = status.place.full_name\n            countrycode = status.place.country_code\n            country = status.place.country\n            boundingbox = str(status.place.bounding_box.coordinates)\n\n        else:\n            placename = None\n            countrycode = None\n            country = None\n            boundingbox = None\n\n        Tweet_source = status.source\n        geo = status.geo\n        \n        if geo is not None:\n            geo = json.dumps(geo)\n            \n        inreplytouser = status.in_reply_to_screen_name\n        inreply_tostatus = status.in_reply_to_status_id_str\n\n        \n\n        if 'media' in status.entities:\n            image_posted = status.entities['media'][0]['media_url']\n            remove_tweet_url = image_posted.split('/')[-1]\n            posted_image_dest = os.path.join(\"Case_Attachments/\" + casename + \"/tweets/\" + screenname + \"/tweeted_image/\" + remove_tweet_url)\n            image_path = \"Case_Attachments/\" + casename + \"/tweets/\" + screenname + \"/tweeted_image/\"\n\n            if not os.path.exists(image_path):\n                os.makedirs(image_path)\n            try:\n                print(\"Downloading... %s\" % posted_image_dest)\n                urllib.request.urlretrieve(image_posted, filename = posted_image_dest)\n                \n                tweeted_image = open(posted_image_dest, \"rb\").read()\n                \n                image_hash = dump_hash(tweeted_image)\n\n            except urllib.error.URLError as e:\n                print(\"Error downloading file... %s ... from TweetID: %s\" % (posted_image_dest, str(Tweetid)))\n                posted_image_dest = \"ERROR DOWNLOADING FILE\"\n                tweeted_image = None\n                image_hash = None                \n                pass\n\n            except:\n                print(\"Error downloading file... %s ... from TweetID: %s\" % (posted_image_dest, str(Tweetid)))\n                posted_image_dest = \"ERROR DOWNLOADING FILE - Unknown Error\"\n                tweeted_image = None\n                image_hash = None                \n                pass\n\n            mediatype = status.entities['media'][0]['type']\n            mediaurl = status.entities['media'][0]['media_url']\n            mediaid = status.entities['media'][0]['id']        \n\n        else:\n            posted_image_dest = None\n            mediatype = None\n            mediaurl = None\n            mediaid = None\n            tweeted_image = None\n            image_hash = None\n\n        \n        \n\n        if hasattr(status, 'extended_entities'):\n            if 'video_info' in status.extended_entities['media'][0]:\n\n                \n                \n                \n\n                variant_times = len(status.extended_entities['media'][0]['video_info']['variants']) \n\n                bit_rate = -1\n\n                for variant_count in range(0, variant_times): \n\n                    if 'bitrate' in status.extended_entities['media'][0]['video_info']['variants'][variant_count] and \\\n                       bit_rate < status.extended_entities['media'][0]['video_info']['variants'][variant_count]['bitrate']:\n                        bit_rate = status.extended_entities['media'][0]['video_info']['variants'][variant_count]['bitrate']\n                        videourl = status.extended_entities['media'][0]['video_info']['variants'][variant_count]['url']\n                        videotype = status.extended_entities['media'][0]['video_info']['variants'][variant_count]['content_type']\n\n                        remove_video_url = videourl.split('/')[-1]\n                        posted_video_dest = os.path.join(\"Case_Attachments/\" + casename + \"/tweets/\" + screenname + \"/tweeted_video/\" + remove_video_url)\n                        video_path = \"Case_Attachments/\" + casename + \"/tweets/\" + screenname + \"/tweeted_video/\"\n\n                        if not os.path.exists(video_path):\n                            os.makedirs(video_path)\n\n                        try:\n                            print(\"Downloading... %s\" % posted_video_dest)\n                            urllib.request.urlretrieve(videourl, filename = posted_video_dest)\n                            \n                            tweeted_video = open(posted_video_dest, \"rb\").read()\n                            \n                            video_hash = dump_hash(tweeted_video)\n\n                        except urllib.error.URLError as e:\n                            print(\"Error downloading file... %s ... from TweetID: %s\" % (posted_video_dest, str(Tweetid)))\n                            posted_image_dest = \"ERROR DOWNLOADING FILE\"\n                            tweeted_video = None\n                            video_hash = None                            \n                            pass\n\n                        except:\n                            print(\"Error downloading file... %s ... from TweetID: %s\" % (posted_video_dest, str(Tweetid)))\n                            posted_image_dest = \"ERROR DOWNLOADING FILE\"\n                            tweeted_video = None\n                            video_hash = None                            \n                            pass\n\n            else:\n                posted_video_dest = None\n                videotype= None\n                videourl= None\n                tweeted_video = None\n                video_hash = None                \n        else:\n            posted_video_dest = None\n            videotype= None\n            videourl= None\n            tweeted_video = None\n            video_hash = None\n\n        \n\n\n        \n\n        if not status.entities['urls']:\n            url_in_tweet = None\n\n        else:\n            url_in_tweet = str(status.entities['urls'][0]['url'])\n\n        \n\n        now = time.strftime(\"%c\")\n        \n    \n        status_dump = str(status).encode('utf-8')\n    \n        status_hash = dump_hash(status_dump)\n    \n        bookmark = None\n    \n        \n    \n        try:\n            c.execute(\"INSERT INTO \" + table_name + \"(tweet_id, date_mined, screen_name, user_id, users_name, \\\n                                                    created_at_UTC, is_retweet, retweeted_times,text, place_name, \\\n                                                    country_code, country, bounding_box, source_tweeted, geo, \\\n                                                    in_reply_to_user, inreply_statusid, posted_image_dest, \\\n                                                    tweeted_image, image_hash, media_type, media_url, media_id, \\\n                                                    posted_video_dest, tweeted_video, video_hash, video_type, \\\n                                                    video_url, url_in_tweet, status, status_hash, bookmark) \\\n                                                    VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\" , \\\n                                                (Tweetid, \n                                                 now, \n                                                 screenname, \n                                                 userid, \n                                                 usersname, \n                                                 tweettime, \n                                                 is_retweet, \n                                                 retweeted_times, \n                                                 tweet, \n                                                 placename, \n                                                 countrycode, \n                                                 country, \n                                                 boundingbox, \n                                                 Tweet_source, \n                                                 geo, \n                                                 inreplytouser, \n                                                 inreply_tostatus, \n                                                 posted_image_dest, \n                                                 tweeted_image,\n                                                 image_hash, \n                                                 mediatype, \n                                                 mediaurl, \n                                                 mediaid, \n                                                 posted_video_dest, \n                                                 tweeted_video, \n                                                 video_hash, \n                                                 videotype,\n                                                 videourl, \n                                                 url_in_tweet, \n                                                 str(status), \n                                                 status_hash, \n                                                 bookmark))\n            conn.commit()\n            print(str(Tweetid), \"--- Successfully added to the Database\")\n    \n        except lite.IntegrityError:\n            print(str(Tweetid), \"--- Record already Exists\")\t\t\n\n\nif __name__ == '__main__':\n    \n    auth = OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    api = tweepy.API(auth)\n    \n    \n    \n    \n    \n    \n    \n    casename = CASE_NAME\n    \n    dbname = casename + \".db\"    \n\n    conn = lite.connect(dbname)\n\n    c = conn.cursor()\n    \n    screenname = USER_NAME\n    \n    table_name = USER_NAME + \"_Tweets\"\n    \n    create_db(table_name)\n    \n    get_all_tweets(screenname)\n    \n    print(\"\\n Finished collecting Tweets from user --- %s\" % screenname)\n    print(\"Results are stored in \" + casename)\n    \n    ",
        "summary": "This Python script is designed to collect and store Twitter data for a specific user. It uses the Tweepy library to interact with the Twitter API and SQLite to store the collected data. Here's a breakdown of the key components and functionalities:\n\n1. **Authentication**: The script starts by setting up authentication using OAuthHandler from Tweepy, which requires consumer keys and access tokens.\n\n2. **Database Setup**:\n   - A database file is created with a name based on the case name.\n   - A table is created within this database to store tweet data. The table structure includes various fields such as tweet ID, date mined, screen name, user ID, text of the tweet, place information, media URLs, and more.\n\n3. **Data Collection**:\n   - The `get_all_tweets` function is called with a screen name (username) as an argument.\n   - This function retrieves tweets from the specified user using Tweepy's API methods.\n   - For each tweet, it extracts relevant data such as text, place information, media URLs, and more.\n\n4. **Data Storage**:\n   - Each tweet is inserted into the SQLite database table.\n   - The script handles exceptions to avoid inserting duplicate records (using `lite.IntegrityError`).\n\n5. **Execution**:\n   - The script runs when executed directly (`if __name__ == '__main__':`).\n   - It sets up the API, connects to the database, creates the necessary table, and starts collecting tweets.\n\n### Key Points:\n\n- **Tweepy**: Used for interacting with the Twitter API.\n- **SQLite**: For storing the collected data locally.\n- **OAuthHandler**: For handling authentication with Twitter.\n- **IntegrityError**: To handle cases where a record already exists in the database.\n\n### Usage:\n\n1. Replace `consumer_key`, `consumer_secret`, `access_token`, and `access_token_secret` with your actual Twitter API credentials.\n2. Set `CASE_NAME` to the desired name for the case or project.\n3. Set `USER_NAME` to the Twitter username whose tweets you want to collect.\n\n### Example:\n\n```python\n# Replace these with your actual Twitter API credentials\nconsumer_key = 'your_consumer_key'\nconsumer_secret = 'your_consumer_secret'\naccess_token = 'your_access_token'\naccess_token_secret = 'your_access_token_secret'\n\nCASE_NAME = 'my_case'\nUSER_NAME = 'twitter_username_to_collect_tweets_from'\n```\n\n### Notes:\n\n- Ensure you have the necessary permissions to access the Twitter API and store data locally.\n- The script assumes that the user has tweets available. If no tweets are found, it will not create any records in the database.\n\nThis script provides a basic framework for collecting and storing Twitter data. Depending on your needs, you might want to extend or modify it to include additional features such as error handling, logging, or more complex data processing."
    },
    {
        "code": "from django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\nimport django.utils.timezone\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Choice',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('choice_text', models.CharField(max_length=1000)),\n                ('is_correct', models.BooleanField(default=False)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Course',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(default='online course', max_length=30)),\n                ('image', models.ImageField(upload_to='course_images/')),\n                ('description', models.CharField(max_length=1000)),\n                ('pub_date', models.DateField(null=True)),\n                ('total_enrollment', models.IntegerField(default=0)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Enrollment',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('date_enrolled', models.DateField(default=django.utils.timezone.now)),\n                ('mode', models.CharField(choices=[('audit', 'Audit'), ('honor', 'Honor'), ('BETA', 'BETA')], default='audit', max_length=5)),\n                ('rating', models.FloatField(default=5.0)),\n                ('course', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='onlinecourse.course')),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Submission',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('choices', models.ManyToManyField(to='onlinecourse.Choice')),\n                ('enrollment', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='onlinecourse.enrollment')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Question',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('question_text', models.CharField(max_length=1000)),\n                ('grade', models.IntegerField(default=1)),\n                ('course', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='onlinecourse.course')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Lesson',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('title', models.CharField(default='title', max_length=200)),\n                ('order', models.IntegerField(default=0)),\n                ('content', models.TextField()),\n                ('course', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='onlinecourse.course')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Learner',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('occupation', models.CharField(choices=[('student', 'Student'), ('developer', 'Developer'), ('data_scientist', 'Data Scientist'), ('dba', 'Database Admin')], default='student', max_length=20)),\n                ('social_link', models.URLField()),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Instructor',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('full_time', models.BooleanField(default=True)),\n                ('total_learners', models.IntegerField()),\n                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n            ],\n        ),\n        migrations.AddField(\n            model_name='course',\n            name='instructors',\n            field=models.ManyToManyField(to='onlinecourse.Instructor'),\n        ),\n        migrations.AddField(\n            model_name='course',\n            name='users',\n            field=models.ManyToManyField(through='onlinecourse.Enrollment', to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AddField(\n            model_name='choice',\n            name='question',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='onlinecourse.question'),\n        ),\n    ]\n",
        "summary": "This Django migration script defines a series of models for an online course platform, including choices, courses, enrollments, submissions, questions, lessons, learners, and instructors. It establishes relationships between these models and sets up initial data structures for the application."
    },
    {
        "code": "from itertools import product\nimport math\nfrom collections import OrderedDict\nfrom pathlib import Path\nimport logging\n\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport shapely.geometry as sg\nimport googlemaps\n\n\n\nlogger = logging.getLogger()\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n  '%(asctime)s %(name)-12s %(levelname)-8s \\n%(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\nWGS84 = {'init': 'epsg:4326'}\n\nMAX_ELEMENTS = 100\n\ndef flip_coords(xy_list):\n    \n    return [(y, x) for (x, y) in xy_list]\n\ndef make_ids(n, prefix='row_'):\n    \n    k = int(math.log10(n)) + 1  \n    return [prefix + '{num:0{pad}d}'.format(num=i, pad=k) for i in range(n)]\n\ndef to_df(distance_matrix_response, origin_ids=None, destination_ids=None):\n    \n    \n    r = distance_matrix_response\n    columns = ['origin_address', 'destination_address', 'origin_id',\n      'destination_id', 'duration', 'distance']\n    f = pd.DataFrame([], columns=columns)\n\n    \n    if not r['rows']:\n        return f\n\n    f['origin_address'], f['destination_address'] =  zip(\n      *product(r['origin_addresses'], r['destination_addresses']))\n\n    \n    if origin_ids is None:\n        origin_ids = make_ids(len(r['origin_addresses']))\n\n    if destination_ids is None:\n        destination_ids = make_ids(len(r['destination_addresses']))\n\n    f['origin_id'], f['destination_id'] =  zip(\n      *product(origin_ids, destination_ids))\n\n    \n    durs = []\n    dists = []\n    for row in r['rows']:\n        for e in row['elements']:\n            if e['status'] == 'OK':\n                if 'duration_in_traffic' in e:\n                    dur_key = 'duration_in_traffic'\n                else:\n                    dur_key = 'duration'\n                durs.append(e[dur_key]['value'])\n                dists.append(e['distance']['value'])\n            else:\n                durs.append(np.nan)\n                dists.append(np.nan)\n    f['duration'] = durs\n    f['distance'] = dists\n\n    return f\n\ndef point_df_to_gdf(f, x_col='lon', y_col='lat', from_crs=WGS84):\n    \n    f = f.copy()\n    f['geometry'] = f[[x_col, y_col]].apply(lambda p: sg.Point(p), axis=1)\n    f = f.drop([x_col, y_col], axis=1)\n    f = gpd.GeoDataFrame(f)\n    f.crs = from_crs\n    return f\n\ndef point_gdf_to_df(f, x_col='lon', y_col='lat', to_crs=WGS84):\n    \n    f = f.copy()\n    if f.crs is None:\n        raise ValueError('GeoDataFrame needs a crs attribute')\n    if f.crs != to_crs:\n        f = f.to_crs(to_crs)\n\n    f[x_col], f[y_col] = zip(*f['geometry'].map(lambda p: p.coords[0]))\n    del f['geometry']\n    return pd.DataFrame(f)\n\ndef build_distance_matrix_df(client, origins_gdf, destinations_gdf,\n  origin_id_col=None, destination_id_col=None,\n  max_elements=MAX_ELEMENTS, **distance_matrix_kwargs):\n    \n    \n    o_gdf = origins_gdf.copy()\n    d_gdf = destinations_gdf.copy()\n\n    n = o_gdf.shape[0]*d_gdf.shape[0]\n    if n > max_elements:\n        raise ValueError('Number of origins times number of destinations '\n          'is {}, which exceeds threshold of {} elements'.format(\n          n, max_elements))\n\n    \n    if o_gdf.crs != WGS84:\n        o_gdf = o_gdf.to_crs(WGS84)\n    if origin_id_col is None:\n        origin_id_col = 'temp_id'\n        o_gdf[origin_id_col] = make_ids(o_gdf.shape[0])\n\n    o_locs = [geo.coords[0] for geo in o_gdf['geometry']]\n    o_ids = o_gdf[origin_id_col].values\n\n    \n    if d_gdf.crs != WGS84:\n        d_gdf = d_gdf.to_crs(WGS84)\n    if destination_id_col is None:\n        destination_id_col = 'temp_id'\n        d_gdf[destination_id_col] = make_ids(d_gdf.shape[0])\n\n    d_locs = [geo.coords[0] for geo in d_gdf['geometry']]\n    d_ids = d_gdf[destination_id_col].values\n\n    \n    try:\n        r = client.distance_matrix(flip_coords(o_locs),\n          flip_coords(d_locs), **distance_matrix_kwargs)\n        f = to_df(r, o_ids, d_ids)\n    except (googlemaps.exceptions.HTTPError, googlemaps.exceptions.Timeout):\n        \n        f =  pd.DataFrame(columns=[\n            'origin_address',\n            'origin_id',\n            'destination_address',\n            'destination_id',\n            'duration',\n            'distance',\n        ])\n\n    return f\n\ndef run_distance_matrix_job(client, origins_gdf, destinations_gdf, out_dir,\n  origin_id_col=None, destination_id_col=None,\n  max_elements=MAX_ELEMENTS, **distance_matrix_kwargs):\n    \n    o_gdf = origins_gdf.copy()\n    d_gdf = destinations_gdf.copy()\n\n    n_o = o_gdf.shape[0]\n    n_d = d_gdf.shape[0]\n\n    \n    if origin_id_col is None:\n        origin_id_col = 'ersatz_origin_id'\n        o_gdf[origin_id_col] = make_ids(n_o, 'orig_row_')\n\n    if destination_id_col is None:\n        destination_id_col = 'ersatz_destination_id'\n        d_gdf[destination_id_col] = make_ids(n_d, 'dest_row_')\n\n    \n    mode = distance_matrix_kwargs.get('mode', 'driving')\n\n    \n    out_dir = Path(out_dir)\n    if not out_dir.exists():\n        out_dir.mkdir(parents=True)\n\n    \n    \n    \n    \n    \n    for ix, orig_id in o_gdf[[origin_id_col]].itertuples():\n        logger.info('Working on origin {} of {} (id {})'.format(\n          ix + 1, n_o, orig_id))\n\n        \n        \n        \n        \n        for j in range(math.ceil(n_d/max_elements)):\n            n1 = max_elements*j\n            n2 = min(max_elements*(j + 1), n_d)\n            dest_id1, dest_id2 = (\n                d_gdf[destination_id_col].iat[n1],\n                d_gdf[destination_id_col].iat[n2 - 1]\n            )\n            path = Path(out_dir)/'{}_from_{}_to_{}--{}.csv'.format(\n              mode, orig_id, dest_id1, dest_id2)\n            f = build_distance_matrix_df(client, o_gdf.loc[ix:ix],\n              d_gdf.iloc[n1:n2],\n              origin_id_col=origin_id_col,\n              destination_id_col=destination_id_col,\n              **distance_matrix_kwargs)\n            f.to_csv(path, index=False)\n\n            if f.empty:\n                logger.info('* Failed to get data for ' + path.stem)\n\ndef compute_cost(n, cost=0.5/1000, num_freebies=0,\n  daily_limit=100000, chunk_size=MAX_ELEMENTS):\n    \n    d = OrderedDict()\n    d['\n    d['exceeds {!s}-element daily limit?'.format(daily_limit)] = (\n        n > daily_limit)\n    d['estimated cost for job in USD'] = max(0, n - num_freebies)*cost\n    d['estimated duration for job in minutes'] = n/chunk_size/60\n    return pd.Series(d)\n",
        "summary": "The provided Python code defines a suite of functions to interact with Google Maps Distance Matrix API, process geographic data using geopandas and pandas, and compute costs based on the number of requests made. It includes utilities for flipping coordinate order, creating unique IDs, converting between DataFrame and GeoDataFrame formats, building distance matrices, running jobs in chunks due to API limits, and calculating job costs."
    },
    {
        "code": "import unittest\n\nimport gevent\nfrom gevent import sleep\nfrom gevent.queue import Queue\n\nimport mock\nfrom locust import events\nfrom locust.core import Locust, TaskSet, task\nfrom locust.exception import LocustError\nfrom locust.main import parse_options\nfrom locust.rpc import Message\nfrom locust.runners import LocalLocustRunner, MasterLocustRunner\nfrom locust.stats import global_stats, RequestStats\nfrom locust.test.testcases import LocustTestCase\n\n\ndef mocked_rpc_server():\n    class MockedRpcServer(object):\n        queue = Queue()\n        outbox = []\n\n        def __init__(self, host, port):\n            pass\n        \n        @classmethod\n        def mocked_send(cls, message):\n            cls.queue.put(message.serialize())\n            sleep(0)\n        \n        def recv(self):\n            results = self.queue.get()\n            return Message.unserialize(results)\n        \n        def send(self, message):\n            self.outbox.append(message.serialize())\n    \n    return MockedRpcServer\n\n\nclass TestMasterRunner(LocustTestCase):\n    def setUp(self):\n        global_stats.reset_all()\n        self._slave_report_event_handlers = [h for h in events.slave_report._handlers]\n\n        parser, _, _ = parse_options()\n        args = [\n            \"--clients\", \"10\",\n            \"--hatch-rate\", \"10\"\n        ]\n        opts, _ = parser.parse_args(args)\n        self.options = opts\n        \n    def tearDown(self):\n        events.slave_report._handlers = self._slave_report_event_handlers\n    \n    def test_slave_connect(self):\n        class MyTestLocust(Locust):\n            pass\n        \n        with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n            master = MasterLocustRunner(MyTestLocust, self.options)\n            server.mocked_send(Message(\"client_ready\", None, \"zeh_fake_client1\"))\n            self.assertEqual(1, len(master.clients))\n            self.assertTrue(\"zeh_fake_client1\" in master.clients, \"Could not find fake client in master instance's clients dict\")\n            server.mocked_send(Message(\"client_ready\", None, \"zeh_fake_client2\"))\n            server.mocked_send(Message(\"client_ready\", None, \"zeh_fake_client3\"))\n            server.mocked_send(Message(\"client_ready\", None, \"zeh_fake_client4\"))\n            self.assertEqual(4, len(master.clients))\n            \n            server.mocked_send(Message(\"quit\", None, \"zeh_fake_client3\"))\n            self.assertEqual(3, len(master.clients))\n    \n    def test_slave_stats_report_median(self):\n        class MyTestLocust(Locust):\n            pass\n        \n        with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n            master = MasterLocustRunner(MyTestLocust, self.options)\n            server.mocked_send(Message(\"client_ready\", None, \"fake_client\"))\n            \n            master.stats.get(\"/\", \"GET\").log(100, 23455)\n            master.stats.get(\"/\", \"GET\").log(800, 23455)\n            master.stats.get(\"/\", \"GET\").log(700, 23455)\n            \n            data = {\"user_count\":1}\n            events.report_to_master.fire(client_id=\"fake_client\", data=data)\n            master.stats.clear_all()\n            \n            server.mocked_send(Message(\"stats\", data, \"fake_client\"))\n            s = master.stats.get(\"/\", \"GET\")\n            self.assertEqual(700, s.median_response_time)\n    \n    def test_master_total_stats(self):\n        class MyTestLocust(Locust):\n            pass\n        \n        with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n            master = MasterLocustRunner(MyTestLocust, self.options)\n            server.mocked_send(Message(\"client_ready\", None, \"fake_client\"))\n            stats = RequestStats()\n            stats.log_request(\"GET\", \"/1\", 100, 3546)\n            stats.log_request(\"GET\", \"/1\", 800, 56743)\n            stats2 = RequestStats()\n            stats2.log_request(\"GET\", \"/2\", 700, 2201)\n            server.mocked_send(Message(\"stats\", {\n                \"stats\":stats.serialize_stats(), \n                \"stats_total\": stats.total.serialize(),\n                \"errors\":stats.serialize_errors(),\n                \"user_count\": 1,\n            }, \"fake_client\"))\n            server.mocked_send(Message(\"stats\", {\n                \"stats\":stats2.serialize_stats(), \n                \"stats_total\": stats2.total.serialize(),\n                \"errors\":stats2.serialize_errors(),\n                \"user_count\": 2,\n            }, \"fake_client\"))\n            self.assertEqual(700, master.stats.total.median_response_time)\n    \n    def test_master_current_response_times(self):\n        class MyTestLocust(Locust):\n            pass\n        \n        start_time = 1\n        with mock.patch(\"time.time\") as mocked_time:\n            mocked_time.return_value = start_time\n            global_stats.reset_all()\n            with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n                master = MasterLocustRunner(MyTestLocust, self.options)\n                mocked_time.return_value += 1\n                server.mocked_send(Message(\"client_ready\", None, \"fake_client\"))\n                stats = RequestStats()\n                stats.log_request(\"GET\", \"/1\", 100, 3546)\n                stats.log_request(\"GET\", \"/1\", 800, 56743)\n                server.mocked_send(Message(\"stats\", {\n                    \"stats\":stats.serialize_stats(),\n                    \"stats_total\": stats.total.get_stripped_report(),\n                    \"errors\":stats.serialize_errors(),\n                    \"user_count\": 1,\n                }, \"fake_client\"))\n                mocked_time.return_value += 1\n                stats2 = RequestStats()\n                stats2.log_request(\"GET\", \"/2\", 400, 2201)\n                server.mocked_send(Message(\"stats\", {\n                    \"stats\":stats2.serialize_stats(),\n                    \"stats_total\": stats2.total.get_stripped_report(),\n                    \"errors\":stats2.serialize_errors(),\n                    \"user_count\": 2,\n                }, \"fake_client\"))\n                mocked_time.return_value += 4\n                self.assertEqual(400, master.stats.total.get_current_response_time_percentile(0.5))\n                self.assertEqual(800, master.stats.total.get_current_response_time_percentile(0.95))\n                \n                \n                \n                mocked_time.return_value += 10\n                stats.log_request(\"GET\", \"/1\", 20, 1)\n                stats.log_request(\"GET\", \"/1\", 30, 1)\n                stats.log_request(\"GET\", \"/1\", 3000, 1)\n                server.mocked_send(Message(\"stats\", {\n                    \"stats\":stats.serialize_stats(),\n                    \"stats_total\": stats.total.get_stripped_report(),\n                    \"errors\":stats.serialize_errors(),\n                    \"user_count\": 2,\n                }, \"fake_client\"))\n                self.assertEqual(30, master.stats.total.get_current_response_time_percentile(0.5))\n                self.assertEqual(3000, master.stats.total.get_current_response_time_percentile(0.95))\n    \n    def test_spawn_zero_locusts(self):\n        class MyTaskSet(TaskSet):\n            @task\n            def my_task(self):\n                pass\n            \n        class MyTestLocust(Locust):\n            task_set = MyTaskSet\n            min_wait = 100\n            max_wait = 100\n        \n        runner = LocalLocustRunner([MyTestLocust], self.options)\n        \n        timeout = gevent.Timeout(2.0)\n        timeout.start()\n        \n        try:\n            runner.start_hatching(0, 1, wait=True)\n            runner.greenlet.join()\n        except gevent.Timeout:\n            self.fail(\"Got Timeout exception. A locust seems to have been spawned, even though 0 was specified.\")\n        finally:\n            timeout.cancel()\n    \n    def test_spawn_uneven_locusts(self):\n        \n        class MyTestLocust(Locust):\n            pass\n        \n        with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n            master = MasterLocustRunner(MyTestLocust, self.options)\n            for i in range(5):\n                server.mocked_send(Message(\"client_ready\", None, \"fake_client%i\" % i))\n            \n            master.start_hatching(7, 7)\n            self.assertEqual(5, len(server.outbox))\n            \n            num_clients = 0\n            for msg in server.outbox:\n                num_clients += Message.unserialize(msg).data[\"num_clients\"]\n            \n            self.assertEqual(7, num_clients, \"Total number of locusts that would have been spawned is not 7\")\n    \n    def test_spawn_fewer_locusts_than_slaves(self):\n        class MyTestLocust(Locust):\n            pass\n        \n        with mock.patch(\"locust.rpc.rpc.Server\", mocked_rpc_server()) as server:\n            master = MasterLocustRunner(MyTestLocust, self.options)\n            for i in range(5):\n                server.mocked_send(Message(\"client_ready\", None, \"fake_client%i\" % i))\n            \n            master.start_hatching(2, 2)\n            self.assertEqual(5, len(server.outbox))\n            \n            num_clients = 0\n            for msg in server.outbox:\n                num_clients += Message.unserialize(msg).data[\"num_clients\"]\n            \n            self.assertEqual(2, num_clients, \"Total number of locusts that would have been spawned is not 2\")\n    \n    def test_exception_in_task(self):\n        class HeyAnException(Exception):\n            pass\n        \n        class MyLocust(Locust):\n            class task_set(TaskSet):\n                @task\n                def will_error(self):\n                    raise HeyAnException(\":(\")\n        \n        runner = LocalLocustRunner([MyLocust], self.options)\n        \n        l = MyLocust()\n        l._catch_exceptions = False\n        \n        self.assertRaises(HeyAnException, l.run)\n        self.assertRaises(HeyAnException, l.run)\n        self.assertEqual(1, len(runner.exceptions))\n        \n        hash_key, exception = runner.exceptions.popitem()\n        self.assertTrue(\"traceback\" in exception)\n        self.assertTrue(\"HeyAnException\" in exception[\"traceback\"])\n        self.assertEqual(2, exception[\"count\"])\n    \n    def test_exception_is_catched(self):\n        \n        class HeyAnException(Exception):\n            pass\n        \n        class MyTaskSet(TaskSet):\n            def __init__(self, *a, **kw):\n                super(MyTaskSet, self).__init__(*a, **kw)\n                self._task_queue = [\n                    {\"callable\":self.will_error, \"args\":[], \"kwargs\":{}}, \n                    {\"callable\":self.will_stop, \"args\":[], \"kwargs\":{}},\n                ]\n            \n            @task(1)\n            def will_error(self):\n                raise HeyAnException(\":(\")\n            \n            @task(1)\n            def will_stop(self):\n                self.interrupt()\n        \n        class MyLocust(Locust):\n            min_wait = 10\n            max_wait = 10\n            task_set = MyTaskSet\n        \n        runner = LocalLocustRunner([MyLocust], self.options)\n        l = MyLocust()\n        \n        \n        with mock.patch(\"sys.stderr\") as mocked:\n            l.task_set._task_queue = [l.task_set.will_error, l.task_set.will_stop]\n            self.assertRaises(LocustError, l.run) \n            l.task_set._task_queue = [l.task_set.will_error, l.task_set.will_stop]\n            self.assertRaises(LocustError, l.run) \n        self.assertEqual(2, len(mocked.method_calls))\n        \n        \n        self.assertEqual(1, len(runner.exceptions))\n        hash_key, exception = runner.exceptions.popitem()\n        self.assertTrue(\"traceback\" in exception)\n        self.assertTrue(\"HeyAnException\" in exception[\"traceback\"])\n        self.assertEqual(2, exception[\"count\"])\n\n\nclass TestMessageSerializing(unittest.TestCase):\n    def test_message_serialize(self):\n        msg = Message(\"client_ready\", None, \"my_id\")\n        rebuilt = Message.unserialize(msg.serialize())\n        self.assertEqual(msg.type, rebuilt.type)\n        self.assertEqual(msg.data, rebuilt.data)\n        self.assertEqual(msg.node_id, rebuilt.node_id)\n",
        "summary": "The provided code snippet is a test suite for the Locust load testing tool. It includes various unit tests to ensure that different components of Locust function correctly. Below is a breakdown of each test case and its purpose:\n\n1. **Test Cases for LocalRunner**:\n   - `test_spawn_uneven_locusts`: Ensures that when more locusts are requested than available clients, the master distributes them evenly among the available clients.\n   - `test_spawn_fewer_locusts_than_slaves`: Verifies that if fewer locusts are requested than available clients, all locusts are spawned on the available clients.\n   - `test_exception_in_task`: Tests exception handling within tasks. It ensures that exceptions are caught and logged appropriately.\n   - `test_exception_is_catched`: Similar to the previous test but specifically checks that exceptions are caught when Locust is configured to catch them.\n\n2. **Test Cases for MasterRunner**:\n   - `test_spawn_uneven_locusts`: Ensures that locusts are distributed evenly among available clients.\n   - `test_spawn_fewer_locusts_than_slaves`: Verifies that fewer locusts are spawned than available clients.\n   - `test_exception_in_task`: Tests exception handling within tasks in a master environment.\n   - `test_exception_is_catched`: Ensures exceptions are caught and logged when Locust is configured to catch them.\n\n3. **Test Cases for Message Serializing**:\n   - `test_message_serialize`: Verifies that messages can be serialized and deserialized correctly, ensuring data integrity during communication between components.\n\n4. **General Test Cases**:\n   - `test_sum`: A simple test case that sums two numbers to ensure basic arithmetic operations are working as expected.\n   - `test_exception_in_task`: Tests exception handling within tasks in a local runner environment.\n   - `test_exception_is_catched`: Ensures exceptions are caught and logged when Locust is configured to catch them.\n\nEach test case uses the `unittest` framework, which provides a robust testing infrastructure. The tests cover various aspects of Locust's functionality, including task execution, exception handling, message serialization, and client management. This ensures that Locust can handle different scenarios and configurations correctly."
    },
    {
        "code": "import os\nimport datetime\nimport hashlib\nimport pexpect\nfrom config import *\nfrom common import openssl, jsonMessage, gencrl\nfrom OpenSSL import crypto\n\n\n\ndef revokeFromCert(cert):\n    \n    try:\n        x509_obj = crypto.load_certificate(crypto.FILETYPE_PEM, cert)\n        \n        serial = hex(x509_obj.get_serial_number())[2:]\n    except crypto.Error:\n        return jsonMessage(status=-1,\n                           msg=\"[ERROR]: Wrong certificate (X509) format!\")\n\n    \n    path = os.path.join(\n        '/tmp',\n        hashlib.md5(str(datetime.datetime.now()).encode('utf-8')).hexdigest() +\n        \"_revokecert.crt\")\n    with open(path, \"w\") as f:\n        f.write(cert.decode('utf8'))\n\n    return revoking(path, serial)\n\n\n\n\ndef revokeFromSerial(serial):\n    path = os.path.join(CA_NEWCERTS, serial + \".pem\")\n    if not os.path.exists(path):\n        msg = \"[ERROR]: This may be an invalid serial number!\"\n        return jsonMessage(-1, msg)\n\n    return revoking(path, serial)\n\n\ndef revoking(certfile, serial):\n    child = openssl('ca', '-revoke', certfile)\n    ret = child.expect(\n        ['Already revoked', 'Revoking Certificate', pexpect.EOF])\n    if ret == 0:\n        msg = \"[ERROR]: This certificate is revoked!\"\n        return jsonMessage(-1, msg)\n    elif ret == 1:\n        msg = \"Revoke Certificate success! Serial number is \" + serial\n        \n        gencrl()\n        return jsonMessage(0, msg, {\"Serial Number\": serial})\n    elif ret == 2:\n        msg = \"[ERROR]: Revoke failed, unknown error!\"\n        return jsonMessage(-1, msg)\n",
        "summary": "The provided Python code defines functions to revoke certificates based on their content or serial number. It uses OpenSSL for the revocation process and handles various outcomes such as successful revocation, errors related to already revoked certificates, invalid serial numbers, and unknown errors. The code also generates a CRL (Certificate Revocation List) after a successful revocation."
    },
    {
        "code": "import time\r\n\r\nfrom board import SCL, SDA\r\nimport busio\r\nfrom adafruit_neotrellis.neotrellis import NeoTrellis\r\n\r\n\ni2c_bus = busio.I2C(SCL, SDA)\r\n\r\n\ntrellis = NeoTrellis(i2c_bus)\r\n\r\n\nOFF = (0, 0, 0)\r\nRED = (255, 0, 0)\r\nYELLOW = (255, 150, 0)\r\nGREEN = (0, 255, 0)\r\nCYAN = (0, 255, 255)\r\nBLUE = (0, 0, 255)\r\nPURPLE = (180, 0, 255)\r\n\r\n\ndef blink(event):\r\n    \n    if event.edge == NeoTrellis.EDGE_RISING:\r\n        trellis.pixels[event.number] = CYAN\r\n    \n    elif event.edge == NeoTrellis.EDGE_FALLING:\r\n        trellis.pixels[event.number] = OFF\r\n\r\n\r\nfor i in range(16):\r\n    \n    trellis.activate_key(i, NeoTrellis.EDGE_RISING)\r\n    \n    trellis.activate_key(i, NeoTrellis.EDGE_FALLING)\r\n    \n    trellis.callbacks[i] = blink\r\n\r\n    \n    trellis.pixels[i] = PURPLE\r\n    time.sleep(0.05)\r\n\r\nfor i in range(16):\r\n    trellis.pixels[i] = OFF\r\n    time.sleep(0.05)\r\n\r\nwhile True:\r\n    \n    trellis.sync()\r\n    \n    time.sleep(0.02)\r\n",
        "summary": "The Python code initializes a NeoTrellis board connected via I2C, sets up a callback function to change LED colors on button presses, and continuously updates the display in a loop."
    },
    {
        "code": "import unittest\nimport MuPythonLibrary.Uefi.EdkII.VariableFormat as VF\n\n\nclass TestVariableHeader(unittest.TestCase):\n\n    def test_set_name(self):\n        var = VF.VariableHeader()\n\n        test_name = \"MyNewName\"\n        var.set_name(test_name)\n\n        self.assertEqual(var.Name, test_name)\n\n    def test_get_packed_name(self):\n        var = VF.VariableHeader()\n\n        test_name = \"MyNewName\"\n        var.set_name(test_name)\n\n        test_name_packed = bytes.fromhex('4D0079004E00650077004E0061006D0065000000')\n        self.assertEqual(var.get_packed_name(), test_name_packed)\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python code defines a unit test class `TestVariableHeader` that tests methods for setting and getting the name of a variable header using the `MuPythonLibrary.Uefi.EdkII.VariableFormat` module. The tests ensure that the name is correctly set and packed as bytes, verifying the functionality through assertions."
    },
    {
        "code": "import yaql.tests\n\n\nclass TestCommon(yaql.tests.TestCase):\n    def test_null(self):\n        self.assertIsNone(self.eval('null'))\n\n    def test_true(self):\n        res = self.eval('true')\n        self.assertTrue(res)\n        self.assertIsInstance(res, bool)\n\n    def test_false(self):\n        res = self.eval('false')\n        self.assertFalse(res)\n        self.assertIsInstance(res, bool)\n\n    def test_string(self):\n        self.assertEqual('True', self.eval('True'))\n        self.assertEqual('some string', self.eval(\"'some string'\"))\n\n    def test_null_to_null(self):\n        self.assertTrue(self.eval('null = null'))\n        self.assertFalse(self.eval('null != null'))\n        self.assertTrue(self.eval('null <= null'))\n        self.assertTrue(self.eval('null >= null'))\n        self.assertFalse(self.eval('null < null'))\n        self.assertFalse(self.eval('null > null'))\n\n    def test_ordering(self):\n        self.assertTrue(self.eval('null < 0'))\n        self.assertTrue(self.eval('null < true'))\n        self.assertTrue(self.eval('null < false'))\n        self.assertTrue(self.eval('null < a'))\n        self.assertTrue(self.eval('null <= 0'))\n        self.assertFalse(self.eval('null > 0'))\n        self.assertFalse(self.eval('null >= 0'))\n        self.assertTrue(self.eval('null != 0'))\n        self.assertTrue(self.eval('null != false'))\n        self.assertFalse(self.eval('null = false'))\n        self.assertFalse(self.eval('null = 0'))\n        self.assertFalse(self.eval('0 < null'))\n        self.assertFalse(self.eval('0 <= null'))\n        self.assertTrue(self.eval('0 >= null'))\n        self.assertTrue(self.eval('0 > null'))\n\n    def test_max(self):\n        self.assertEqual(5, self.eval('max(1, 5)'))\n        self.assertEqual(-1, self.eval('max(null, -1)'))\n        self.assertIsNone(self.eval('max(null, null)'))\n\n    def test_min(self):\n        self.assertEqual(1, self.eval('min(1, 5)'))\n        self.assertIsNone(self.eval('min(null, -1)'))\n        self.assertIsNone(self.eval('min(null, null)'))\n\n    def test_comparision_of_incomparable(self):\n        self.assertFalse(self.eval('a = 1'))\n        self.assertFalse(self.eval('a = false'))\n        self.assertFalse(self.eval('a = null'))\n        self.assertFalse(self.eval('[a] = [false]'))\n        self.assertTrue(self.eval('a != 1'))\n        self.assertTrue(self.eval('a != false'))\n        self.assertTrue(self.eval('[a] != [false]'))\n        self.assertTrue(self.eval('a != null'))\n",
        "summary": "The provided Python code defines a test class `TestCommon` that inherits from `yaql.tests.TestCase`. It includes several methods to test various aspects of the YAQL (Yet Another Query Language) library, such as handling null values, boolean comparisons, string evaluations, and ordering operations. Each method uses assertions to verify the correctness of the YAQL expressions evaluated within them."
    },
    {
        "code": "from keras.mixed_precision import policy\nfrom keras.saving.saved_model import base_serialization\nfrom keras.saving.saved_model import constants\nfrom keras.saving.saved_model import save_impl\nfrom keras.saving.saved_model import serialized_attributes\nfrom keras.utils import generic_utils\nimport tensorflow.compat.v2 as tf\n\n\nclass LayerSavedModelSaver(base_serialization.SavedModelSaver):\n  \n\n  @property\n  def object_identifier(self):\n    return constants.LAYER_IDENTIFIER\n\n  @property\n  def python_properties(self):\n    \n    return self._python_properties_internal()\n\n  def _python_properties_internal(self):\n    \n    \n    \n    \n    metadata = dict(\n        name=self.obj.name,\n        trainable=self.obj.trainable,\n        expects_training_arg=self.obj._expects_training_arg,  \n        dtype=policy.serialize(self.obj._dtype_policy),  \n        batch_input_shape=getattr(self.obj, '_batch_input_shape', None),\n        stateful=self.obj.stateful,\n        must_restore_from_config=self.obj._must_restore_from_config,  \n    )\n\n    metadata.update(get_serialized(self.obj))\n    if self.obj.input_spec is not None:\n      \n      metadata['input_spec'] = tf.nest.map_structure(\n          lambda x: generic_utils.serialize_keras_object(x) if x else None,\n          self.obj.input_spec)\n    if (self.obj.activity_regularizer is not None and\n        hasattr(self.obj.activity_regularizer, 'get_config')):\n      metadata['activity_regularizer'] = generic_utils.serialize_keras_object(\n          self.obj.activity_regularizer)\n    if self.obj._build_input_shape is not None:  \n      metadata['build_input_shape'] = self.obj._build_input_shape  \n    return metadata\n\n  def objects_to_serialize(self, serialization_cache):\n    return (self._get_serialized_attributes(\n        serialization_cache).objects_to_serialize)\n\n  def functions_to_serialize(self, serialization_cache):\n    return (self._get_serialized_attributes(\n        serialization_cache).functions_to_serialize)\n\n  def _get_serialized_attributes(self, serialization_cache):\n    \n    keras_cache = serialization_cache.setdefault(constants.KERAS_CACHE_KEY, {})\n    if self.obj in keras_cache:\n      return keras_cache[self.obj]\n\n    serialized_attr = keras_cache[self.obj] = (\n        serialized_attributes.SerializedAttributes.new(self.obj))\n\n    if (save_impl.should_skip_serialization(self.obj) or\n        self.obj._must_restore_from_config):  \n      return serialized_attr\n\n    object_dict, function_dict = self._get_serialized_attributes_internal(\n        serialization_cache)\n\n    serialized_attr.set_and_validate_objects(object_dict)\n    serialized_attr.set_and_validate_functions(function_dict)\n    return serialized_attr\n\n  def _get_serialized_attributes_internal(self, serialization_cache):\n    \n    objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\n    \n    \n    functions['_default_save_signature'] = None\n    return objects, functions\n\n\n\n\ndef get_serialized(obj):\n  with generic_utils.skip_failed_serialization():\n    \n    \n    \n    return generic_utils.serialize_keras_object(obj)\n\n\nclass InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n  \n\n  @property\n  def object_identifier(self):\n    return constants.INPUT_LAYER_IDENTIFIER\n\n  @property\n  def python_properties(self):\n\n    return dict(\n        class_name=type(self.obj).__name__,\n        name=self.obj.name,\n        dtype=self.obj.dtype,\n        sparse=self.obj.sparse,\n        ragged=self.obj.ragged,\n        batch_input_shape=self.obj._batch_input_shape,  \n        config=self.obj.get_config())\n\n  def objects_to_serialize(self, serialization_cache):\n    return {}\n\n  def functions_to_serialize(self, serialization_cache):\n    return {}\n\n\nclass RNNSavedModelSaver(LayerSavedModelSaver):\n  \n\n  @property\n  def object_identifier(self):\n    return constants.RNN_LAYER_IDENTIFIER\n\n  def _get_serialized_attributes_internal(self, serialization_cache):\n    objects, functions = (\n        super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(\n            serialization_cache))\n    states = tf.__internal__.tracking.wrap(self.obj.states)\n    \n    \n    \n    \n    \n    \n    if isinstance(states, tuple):\n      states = tf.__internal__.tracking.wrap(list(states))\n    objects['states'] = states\n    return objects, functions\n\n\nclass VocabularySavedModelSaver(LayerSavedModelSaver):\n  \n\n  @property\n  def python_properties(self):\n    \n    metadata = self._python_properties_internal()\n    \n    metadata['config']['vocabulary'] = None\n    \n    metadata['config']['has_input_vocabulary'] = self.obj._has_input_vocabulary  \n    return metadata\n",
        "summary": "The provided Python code defines a set of classes for saving and serializing different types of Keras layers, including `LayerSavedModelSaver`, `InputLayerSavedModelSaver`, `RNNSavedModelSaver`, and `VocabularySavedModelSaver`. Each class handles the serialization of specific layer types by defining properties such as `object_identifier` and `python_properties`, and methods for retrieving objects and functions to serialize. The code also includes utility functions like `get_serialized` for serializing Keras objects while skipping failed serialization attempts."
    },
    {
        "code": "import random\r\nx=random.random()\r\nprint(\"The Random number is\",round(x,3))\r\n",
        "summary": "This Python script generates a random floating-point number between 0 and 1, rounds it to three decimal places, and prints the result."
    },
    {
        "code": "import re\nimport logging\n\nfrom cmframework.apis import cmerror\n\n\nclass CMPluginManager(object):\n\n    def __init__(self, plugins_path):\n        self.pluginlist = {}\n        self.filterdict = {}\n        self.plugins_path = plugins_path\n\n    \n    def load_plugin(self):\n        raise cmerror.CMError('Not implemented')\n\n    \n    def build_input(self, indata, filtername):\n        search_re = re.compile(filtername)\n        if isinstance(indata, dict):\n            filter_data = {}\n            for key, value in indata.iteritems():\n                logging.debug('Matching %s against %s', key, filtername)\n                if search_re.match(key):\n                    filter_data[key] = value\n        else:\n            filter_data = []\n            for key in indata:\n                logging.debug('Matching %s against %s', key, filtername)\n                if search_re.match(key):\n                    filter_data.append(key)\n\n        return filter_data\n",
        "summary": "The `CMPluginManager` class is designed to manage plugins located at a specified path. It includes methods for loading plugins and building input data based on a filter name, using regular expressions for matching keys in dictionaries or lists. The `build_input` method logs debug information during the matching process."
    },
    {
        "code": "from ent2id.Ent2Id import *\n",
        "summary": "The provided Python code imports all functions and classes from the module `Ent2Id` within the package `ent2id`. This allows for direct access to its functionalities without needing to specify the module name in subsequent function calls."
    },
    {
        "code": "from tencentcloud.common.abstract_model import AbstractModel\n\n\nclass EvaluationRequest(AbstractModel):\n    \n\n    def __init__(self):\n        \n        self.SessionId = None\n        self.Image = None\n        self.HcmAppid = None\n        self.Url = None\n        self.SupportHorizontalImage = None\n        self.RejectNonArithmeticImage = None\n        self.IsAsync = None\n        self.EnableDispRelatedVertical = None\n        self.EnableDispMidresult = None\n        self.EnablePdfRecognize = None\n        self.PdfPageIndex = None\n\n\n    def _deserialize(self, params):\n        self.SessionId = params.get(\"SessionId\")\n        self.Image = params.get(\"Image\")\n        self.HcmAppid = params.get(\"HcmAppid\")\n        self.Url = params.get(\"Url\")\n        self.SupportHorizontalImage = params.get(\"SupportHorizontalImage\")\n        self.RejectNonArithmeticImage = params.get(\"RejectNonArithmeticImage\")\n        self.IsAsync = params.get(\"IsAsync\")\n        self.EnableDispRelatedVertical = params.get(\"EnableDispRelatedVertical\")\n        self.EnableDispMidresult = params.get(\"EnableDispMidresult\")\n        self.EnablePdfRecognize = params.get(\"EnablePdfRecognize\")\n        self.PdfPageIndex = params.get(\"PdfPageIndex\")\n\n\nclass EvaluationResponse(AbstractModel):\n    \n\n    def __init__(self):\n        \n        self.SessionId = None\n        self.Items = None\n        self.TaskId = None\n        self.RequestId = None\n\n\n    def _deserialize(self, params):\n        self.SessionId = params.get(\"SessionId\")\n        if params.get(\"Items\") is not None:\n            self.Items = []\n            for item in params.get(\"Items\"):\n                obj = Item()\n                obj._deserialize(item)\n                self.Items.append(obj)\n        self.TaskId = params.get(\"TaskId\")\n        self.RequestId = params.get(\"RequestId\")\n\n\nclass Item(AbstractModel):\n    \n\n    def __init__(self):\n        \n        self.Item = None\n        self.ItemString = None\n        self.ItemCoord = None\n        self.Answer = None\n        self.ExpressionType = None\n\n\n    def _deserialize(self, params):\n        self.Item = params.get(\"Item\")\n        self.ItemString = params.get(\"ItemString\")\n        if params.get(\"ItemCoord\") is not None:\n            self.ItemCoord = ItemCoord()\n            self.ItemCoord._deserialize(params.get(\"ItemCoord\"))\n        self.Answer = params.get(\"Answer\")\n        self.ExpressionType = params.get(\"ExpressionType\")\n\n\nclass ItemCoord(AbstractModel):\n    \n\n    def __init__(self):\n        \n        self.Height = None\n        self.Width = None\n        self.X = None\n        self.Y = None\n\n\n    def _deserialize(self, params):\n        self.Height = params.get(\"Height\")\n        self.Width = params.get(\"Width\")\n        self.X = params.get(\"X\")\n        self.Y = params.get(\"Y\")",
        "summary": "The provided Python code defines classes for a request and response in an API interaction with Tencent Cloud's HCM service, specifically for evaluating arithmetic expressions. The `EvaluationRequest` class includes various parameters such as session ID, image data, and configuration options for processing the input. The `EvaluationResponse` class contains the results of the evaluation, including items identified, their coordinates, answers, and expression types. Additionally, there are classes for deserializing these responses into structured objects."
    },
    {
        "code": "import os\nimport logging\nimport unittest.mock as mock\nfrom unittest import TestCase\n\nfrom esrally import exceptions\nfrom esrally.utils import git\n\n\nclass GitTests(TestCase):\n    def test_is_git_working_copy(self):\n        test_dir = os.path.dirname(os.path.dirname(__file__))\n        \n        self.assertFalse(git.is_working_copy(test_dir))\n        self.assertTrue(git.is_working_copy(os.path.dirname(test_dir)))\n\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_output\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_git_version_too_old(self, run_subprocess_with_logging, run_subprocess):\n        \n        run_subprocess_with_logging.return_value = 64\n        run_subprocess.return_value = \"1.0.0\"\n\n        with self.assertRaises(exceptions.SystemSetupError) as ctx:\n            git.head_revision(\"/src\")\n        self.assertEqual(\"Your git version is [1.0.0] but Rally requires at least git 1.9. Please update git.\", ctx.exception.args[0])\n        run_subprocess_with_logging.assert_called_with(\"git -C /src --version\", level=logging.DEBUG)\n\n    @mock.patch(\"esrally.utils.io.ensure_dir\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_clone_successful(self, run_subprocess_with_logging, ensure_dir):\n        run_subprocess_with_logging.return_value = 0\n        src = \"/src\"\n        remote = \"http://github.com/some/project\"\n\n        git.clone(src, remote)\n\n        ensure_dir.assert_called_with(src)\n        run_subprocess_with_logging.assert_called_with(\"git clone http://github.com/some/project /src\")\n\n    @mock.patch(\"esrally.utils.io.ensure_dir\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_clone_with_error(self, run_subprocess_with_logging, ensure_dir):\n        run_subprocess_with_logging.return_value = 128\n        src = \"/src\"\n        remote = \"http://github.com/some/project\"\n\n        with self.assertRaises(exceptions.SupplyError) as ctx:\n            git.clone(src, remote)\n        self.assertEqual(\"Could not clone from [http://github.com/some/project] to [/src]\", ctx.exception.args[0])\n\n        ensure_dir.assert_called_with(src)\n        run_subprocess_with_logging.assert_called_with(\"git clone http://github.com/some/project /src\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_fetch_successful(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.fetch(\"/src\", remote=\"my-origin\")\n        run_subprocess.assert_called_with(\"git -C /src fetch --prune --quiet my-origin\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_fetch_with_error(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = True\n        with self.assertRaises(exceptions.SupplyError) as ctx:\n            git.fetch(\"/src\", remote=\"my-origin\")\n        self.assertEqual(\"Could not fetch source tree from [my-origin]\", ctx.exception.args[0])\n        run_subprocess.assert_called_with(\"git -C /src fetch --prune --quiet my-origin\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_checkout_successful(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.checkout(\"/src\", \"feature-branch\")\n        run_subprocess.assert_called_with(\"git -C /src checkout --quiet feature-branch\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_checkout_with_error(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = True\n        with self.assertRaises(exceptions.SupplyError) as ctx:\n            git.checkout(\"/src\", \"feature-branch\")\n        self.assertEqual(\"Could not checkout branch [feature-branch]. Do you have uncommitted changes?\", ctx.exception.args[0])\n        run_subprocess.assert_called_with(\"git -C /src checkout --quiet feature-branch\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_rebase(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.rebase(\"/src\", remote=\"my-origin\", branch=\"feature-branch\")\n        calls = [\n            mock.call(\"git -C /src checkout --quiet feature-branch\"),\n            mock.call(\"git -C /src rebase --quiet my-origin/feature-branch\")\n        ]\n        run_subprocess.assert_has_calls(calls)\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_pull(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.pull(\"/src\", remote=\"my-origin\", branch=\"feature-branch\")\n        calls = [\n            mock.call(\"git -C /src fetch --prune --quiet my-origin\"),\n            mock.call(\"git -C /src checkout --quiet feature-branch\"),\n            mock.call(\"git -C /src rebase --quiet my-origin/feature-branch\")\n        ]\n        run_subprocess.assert_has_calls(calls)\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_pull_ts(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.pull_ts(\"/src\", \"20160101T110000Z\")\n        run_subprocess.assert_called_with(\n                \"git -C /src fetch --quiet origin && git -C /src checkout \"\n                \"--quiet `git -C /src rev-list -n 1 --before=\\\"20160101T110000Z\\\" --date=iso8601 origin/master`\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_pull_revision(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = False\n        git.pull_revision(\"/src\", \"3694a07\")\n        run_subprocess.assert_called_with(\"git -C /src fetch --quiet origin && git -C /src checkout --quiet 3694a07\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_output\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_head_revision(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = [\"3694a07\"]\n        self.assertEqual(\"3694a07\", git.head_revision(\"/src\"))\n        run_subprocess.assert_called_with(\"git -C /src rev-parse --short HEAD\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_output\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_list_remote_branches(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = [\"  origin/HEAD\",\n                                       \"  origin/master\",\n                                       \"  origin/5.0.0-alpha1\",\n                                       \"  origin/5\"]\n        self.assertEqual([\"master\", \"5.0.0-alpha1\", \"5\"], git.branches(\"/src\", remote=True))\n        run_subprocess.assert_called_with(\"git -C /src for-each-ref refs/remotes/ --format='%(refname:short)'\")\n\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_output\")\n    @mock.patch(\"esrally.utils.process.run_subprocess_with_logging\")\n    def test_list_local_branches(self, run_subprocess_with_logging, run_subprocess):\n        run_subprocess_with_logging.return_value = 0\n        run_subprocess.return_value = [\"  HEAD\",\n                                       \"  master\",\n                                       \"  5.0.0-alpha1\",\n                                       \"  5\"]\n        self.assertEqual([\"master\", \"5.0.0-alpha1\", \"5\"], git.branches(\"/src\", remote=False))\n        run_subprocess.assert_called_with(\"git -C /src for-each-ref refs/heads/ --format='%(refname:short)'\")\n",
        "summary": "The provided Python code defines a test suite `GitTests` that uses the `unittest` framework to validate various Git operations. These operations include checking if a repository is a Git repository, retrieving the current branch, fetching and pulling updates from remote repositories, rebasing branches, and obtaining specific revisions or timestamps. The tests mock subprocess calls to simulate Git commands and verify their outputs against expected results."
    },
    {
        "code": "import argparse\nimport time\n\nimport ray\n\nray.init(address=\"auto\")\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"num_nodes\", type=int, help=\"Wait for this number of nodes (includes head)\"\n)\n\nparser.add_argument(\"max_time_s\", type=int, help=\"Wait for this number of seconds\")\n\nparser.add_argument(\n    \"--feedback_interval_s\",\n    type=int,\n    default=10,\n    help=\"Wait for this number of seconds\",\n)\n\nargs = parser.parse_args()\n\ncurr_nodes = 0\nstart = time.time()\nnext_feedback = start\nmax_time = start + args.max_time_s\n\nwhile not curr_nodes >= args.num_nodes:\n    now = time.time()\n\n    if now >= max_time:\n        raise RuntimeError(\n            f\"Maximum wait time reached, but only \"\n            f\"{curr_nodes}/{args.num_nodes} nodes came up. Aborting.\"\n        )\n\n    if now >= next_feedback:\n        passed = now - start\n        print(\n            f\"Waiting for more nodes to come up: \"\n            f\"{curr_nodes}/{args.num_nodes} \"\n            f\"({passed:.0f} seconds passed)\"\n        )\n        next_feedback = now + args.feedback_interval_s\n\n    time.sleep(5)\n    curr_nodes = len(ray.nodes())\n\npassed = time.time() - start\nprint(\n    f\"Cluster is up: {curr_nodes}/{args.num_nodes} nodes online after \"\n    f\"{passed:.0f} seconds\"\n)\n",
        "summary": "The Python script uses Ray, a distributed computing framework, to wait for a specified number of nodes in a cluster to come online within a given time frame. It continuously checks the number of nodes every 5 seconds and provides feedback at intervals until either the desired number of nodes is reached or the maximum wait time is exceeded."
    },
    {
        "code": "import itertools\nfrom multiprocessing import Manager\nfrom pyaugmecon.options import Options\n\n\nclass Flag(object):\n    def __init__(self, opts: Options):\n        self.opts = opts\n\n        if self.opts.shared_flag:\n            self.flag = Manager().dict()\n        else:\n            self.flag = {}\n\n    def set(self, flag_range, value, iter):\n        indices = [tuple([n for n in flag_range(o)]) for o in iter]\n        iter = list(itertools.product(*indices))\n        tmp_flag = {}\n\n        for gp in iter:\n            tmp_flag[gp] = value\n\n        self.flag.update(tmp_flag)\n\n    def get(self, i):\n        return self.flag.get(i, 0)\n",
        "summary": "The provided Python code defines a `Flag` class that manages flags using either a shared dictionary (if `shared_flag` is enabled) or a local dictionary. The class includes methods to set and retrieve flag values based on specified ranges and iterations, utilizing the `itertools.product` function for generating combinations of indices."
    },
    {
        "code": "import os\nfrom setuptools import setup, find_packages\n\n\n__version__ = '3.1.0'\n\nrequirements_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'requirements.txt')\nwith open(requirements_path) as requirements_file:\n    requirements = requirements_file.readlines()\n\nkafka = ['confluent-kafka==1.0.0']\n\ncassandra = ['cassandra-driver==3.20.1']\n\nglue = ['boto3==1.10.1']\n\nsnowflake = [\n    'snowflake-connector-python',\n    'snowflake-sqlalchemy'\n]\n\nathena = ['PyAthena[SQLAlchemy]>=1.0.0']\n\n\n\n\nbigquery = [\n    'google-api-python-client>=1.6.0, <2.0.0dev',\n    'google-auth-httplib2>=0.0.1'\n    'google-auth>=1.0.0, <2.0.0dev'\n]\n\njsonpath = ['jsonpath_rw==1.4.0']\n\ndb2 = [\n    'ibm_db==3.0.1',\n    'ibm-db-sa-py3==0.3.1-1'\n]\n\ndruid = [\n    'pydruid'\n]\n\nall_deps = requirements + kafka + cassandra + glue + snowflake + athena + bigquery + jsonpath + db2 + druid\n\nsetup(\n    name='amundsen-databuilder',\n    version=__version__,\n    description='Amundsen Data builder',\n    url='https://www.github.com/amundsen-io/amundsendatabuilder',\n    maintainer='Amundsen TSC',\n    maintainer_email='amundsen-tsc@lists.lfai.foundation',\n    packages=find_packages(exclude=['tests*']),\n    dependency_links=[],\n    install_requires=requirements,\n    python_requires='>=3.6,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*',\n    extras_require={\n        ':python_version==\"2.7\"': ['typing>=3.6'],  \n        'all': all_deps,\n        'kafka': kafka,  \n        'cassandra': cassandra,\n        'glue': glue,\n        'snowflake': snowflake,\n        'athena': athena,\n        'bigquery': bigquery,\n        'jsonpath': jsonpath,\n        'db2': db2,\n        'druid': druid,\n    },\n    classifiers=[\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ],\n)\n",
        "summary": "This Python script sets up a package named `amundsen-databuilder` using `setuptools`, specifying its version, dependencies, and additional requirements for various data integration tools like Kafka, Cassandra, AWS Glue, Snowflake, Athena, BigQuery, JSONPath, DB2, and Druid."
    },
    {
        "code": "import pprint\nimport re  \n\nimport six\n\n\nclass EcommerceProductImage2(object):\n    \n\n    \n    swagger_types = {\n        'id': 'str',\n        'url': 'str',\n        'variant_ids': 'list[str]'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'url': 'url',\n        'variant_ids': 'variant_ids'\n    }\n\n    def __init__(self, id=None, url=None, variant_ids=None):  \n          \n\n        self._id = None\n        self._url = None\n        self._variant_ids = None\n        self.discriminator = None\n\n        if id is not None:\n            self.id = id\n        if url is not None:\n            self.url = url\n        if variant_ids is not None:\n            self.variant_ids = variant_ids\n\n    @property\n    def id(self):\n        \n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \n\n        self._id = id\n\n    @property\n    def url(self):\n        \n        return self._url\n\n    @url.setter\n    def url(self, url):\n        \n\n        self._url = url\n\n    @property\n    def variant_ids(self):\n        \n        return self._variant_ids\n\n    @variant_ids.setter\n    def variant_ids(self, variant_ids):\n        \n\n        self._variant_ids = variant_ids\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(EcommerceProductImage2, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, EcommerceProductImage2):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n",
        "summary": "The provided Python code defines a class `EcommerceProductImage2` that represents an image associated with an ecommerce product. It includes properties for the image's ID, URL, and variant IDs, along with methods to serialize the object to a dictionary, convert it to a string representation, and compare instances for equality."
    },
    {
        "code": "from optparse import OptionParser\nimport json\n\ndef main():\n    usage = \"\" \n    parser = OptionParser(usage=usage)\n    \n    \n    (options, args) = parser.parse_args()\n\n    with open('transfected_sample_raw_metadata.json', 'r') as f:\n        sample_to_key_to_val = json.load(f)\n\n    gene_id_to_symbol = {}\n    gene_id_to_name = {}\n    with open('genes.tsv', 'r') as f:\n        for i,l in enumerate(f):\n            if i == 0:\n                continue\n            toks = l.split()       \n            g_id = toks[0]\n            symbol = toks[1]\n            gene_id_to_symbol[g_id] = symbol\n            if len(toks) == 3:\n                name = toks[2]\n                gene_id_to_name[g_id] = name \n            \n    \n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The Python script uses the `optparse` module for command-line options and processes JSON data from a file named 'transfected_sample_raw_metadata.json'. It also reads a TSV file 'genes.tsv' to map gene IDs to symbols and names, storing these mappings in dictionaries. The script does not perform any operations with the parsed data beyond loading it into memory."
    },
    {
        "code": "import sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='contact.proto',\n  package='',\n  syntax='proto3',\n  serialized_pb=_b('\\n\\rcontact.proto\\\"\\xdb\\x01\\n\\x07\\x43ontact\\x12\\x12\\n\\nfirst_name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tlast_name\\x18\\x02 \\x01(\\t\\x12\\x14\\n\\x0ctwitter_name\\x18\\x03 \\x01(\\t\\x12\\r\\n\\x05\\x65mail\\x18\\x04 \\x01(\\t\\x12\\x13\\n\\x0bgithub_link\\x18\\x05 \\x01(\\t\\x12\\\"\\n\\x04type\\x18\\x06 \\x01(\\x0e\\x32\\x14.Contact.ContactType\\x12\\x11\\n\\timageName\\x18\\x07 \\x01(\\t\\\"8\\n\\x0b\\x43ontactType\\x12\\x0b\\n\\x07SPEAKER\\x10\\x00\\x12\\r\\n\\tATTENDANT\\x10\\x01\\x12\\r\\n\\tVOLUNTEER\\x10\\x02\\\"&\\n\\x08Speakers\\x12\\x1a\\n\\x08\\x63ontacts\\x18\\x01 \\x03(\\x0b\\x32\\x08.Contactb\\x06proto3')\n)\n\n\n\n_CONTACT_CONTACTTYPE = _descriptor.EnumDescriptor(\n  name='ContactType',\n  full_name='Contact.ContactType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='SPEAKER', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='ATTENDANT', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='VOLUNTEER', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=181,\n  serialized_end=237,\n)\n_sym_db.RegisterEnumDescriptor(_CONTACT_CONTACTTYPE)\n\n\n_CONTACT = _descriptor.Descriptor(\n  name='Contact',\n  full_name='Contact',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='first_name', full_name='Contact.first_name', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='last_name', full_name='Contact.last_name', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='twitter_name', full_name='Contact.twitter_name', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='email', full_name='Contact.email', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='github_link', full_name='Contact.github_link', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='type', full_name='Contact.type', index=5,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name='imageName', full_name='Contact.imageName', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(\"\").decode('utf-8'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _CONTACT_CONTACTTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax='proto3',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=18,\n  serialized_end=237,\n)\n\n\n_SPEAKERS = _descriptor.Descriptor(\n  name='Speakers',\n  full_name='Speakers',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name='contacts', full_name='Speakers.contacts', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax='proto3',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=239,\n  serialized_end=277,\n)\n\n_CONTACT.fields_by_name['type'].enum_type = _CONTACT_CONTACTTYPE\n_CONTACT_CONTACTTYPE.containing_type = _CONTACT\n_SPEAKERS.fields_by_name['contacts'].message_type = _CONTACT\nDESCRIPTOR.message_types_by_name['Contact'] = _CONTACT\nDESCRIPTOR.message_types_by_name['Speakers'] = _SPEAKERS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nContact = _reflection.GeneratedProtocolMessageType('Contact', (_message.Message,), dict(\n  DESCRIPTOR = _CONTACT,\n  __module__ = 'contact_pb2'\n  \n  ))\n_sym_db.RegisterMessage(Contact)\n\nSpeakers = _reflection.GeneratedProtocolMessageType('Speakers', (_message.Message,), dict(\n  DESCRIPTOR = _SPEAKERS,\n  __module__ = 'contact_pb2'\n  \n  ))\n_sym_db.RegisterMessage(Speakers)\n\n\n\n",
        "summary": "This Python code defines protocol buffer message classes for a `Contact` and a `Speakers` structure, using the Google Protocol Buffers library. The `Contact` class includes fields for first name, last name, Twitter handle, email, GitHub link, type (which is an enum), and image name, while the `Speakers` class contains a repeated field of `Contact` objects."
    },
    {
        "code": "import aiy.audio\nimport aiy.cloudspeech\nimport aiy.voicehat\nimport RPi.GPIO as GPIO\n\ndef main():\n    recognizer = aiy.cloudspeech.get_recognizer()\n    recognizer.expect_phrase('turn on the light')\n    recognizer.expect_phrase('turn off the light')\n\n    button = aiy.voicehat.get_button()\n    aiy.audio.get_recorder().start()\n\n    GPIO.setmode(GPIO.BCM)\n    GPIO.setwarnings(False)\n    GPIO.setup(05,GPIO.OUT)\n\n    while True: \n      print('press the button and speak')\n      button.wait_for_press()\n      print('eating pizza...')\n      text = recognizer.recognize()\n      if text is None:\n        print('please repeat...')\n      else:\n        print('You said \"', text, '\"')\n        if 'turn on the light' in text: \n          GPIO.output(05,GPIO.HIGH)\n        elif 'turn off the light' in text:\n          GPIO.output(05,GPIO.LOW)\n\nif __name__ == '__main__':\n    main()\n\n",
        "summary": "The Python code sets up a voice-activated system using an AIY Voice Kit for Raspberry Pi. It listens for specific phrases to control a light connected to the GPIO pins of the Raspberry Pi, turning it on or off based on the user's voice commands."
    },
    {
        "code": "test = {\n  'name': 'q3_1_8',\n  'points': 1,\n  'suites': [\n    {\n      'cases': [\n        {\n          'code': r,\n          'hidden': False,\n          'locked': False\n        },\n        {\n          'code': r,\n          'hidden': False,\n          'locked': False\n        },\n        {\n          'code': r,\n          'hidden': False,\n          'locked': False\n        },\n        {\n          'code': r,\n          'hidden': False,\n          'locked': False\n        }\n      ],\n      'scored': True,\n      'setup': '',\n      'teardown': '',\n      'type': 'doctest'\n    }\n  ]\n}\n",
        "summary": "The Python code defines a dictionary named `test` with keys for 'name', 'points', and 'suites'. The 'name' key holds the string 'q3_1_8', 'points' contains an integer value of 1, and 'suites' is a list containing a single dictionary. This inner dictionary represents a test suite with four cases, each having identical code `r` and flags for hidden status and locked state all set to False. The test suite is configured to be scored and uses doctest type."
    },
    {
        "code": "from __future__ import print_function\n\nimport pygion\nfrom pygion import task\n\n@task\ndef main():\n    nprocs = pygion.Tunable.select(pygion.Tunable.GLOBAL_PYS).get()\n    print(\"Number of Python processors: %s\" % nprocs)\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The provided Python code imports the necessary modules from PyGion, defines a task named `main`, and within this task, it retrieves the number of Python processors using a tunable parameter and prints it. The script is executed as the main program by calling the `main` function if the script is run directly."
    },
    {
        "code": "from typing import TYPE_CHECKING\nfrom ooo.oenv.env_const import UNO_ENVIRONMENT, UNO_RUNTIME\n_DYNAMIC = False\nif (not TYPE_CHECKING) and UNO_RUNTIME and UNO_ENVIRONMENT:\n    _DYNAMIC = True\n\nif not TYPE_CHECKING and _DYNAMIC:\n    from com.sun.star.system import XSimpleMailMessage as XSimpleMailMessage\n    setattr(XSimpleMailMessage, '__ooo_ns__', 'com.sun.star.system')\n    setattr(XSimpleMailMessage, '__ooo_full_ns__', 'com.sun.star.system.XSimpleMailMessage')\n    setattr(XSimpleMailMessage, '__ooo_type_name__', 'interface')\nelse:\n    from ...lo.system.x_simple_mail_message import XSimpleMailMessage as XSimpleMailMessage\n\n__all__ = ['XSimpleMailMessage']\n\n",
        "summary": "The Python code checks if the environment is suitable for dynamic imports and, if so, dynamically sets attributes for the `XSimpleMailMessage` class to specify its namespace, full namespace, and type name. If not in a dynamic environment or during type checking, it imports `XSimpleMailMessage` from a specified location and includes it in the module's public interface."
    },
    {
        "code": "import os\nimport pathlib\nimport shutil\nimport subprocess\nimport sys\n\n\nimport nox  \n\nCURRENT_DIRECTORY = pathlib.Path(__file__).parent.absolute()\n\nLOWER_BOUND_CONSTRAINTS_FILE = CURRENT_DIRECTORY / \"constraints.txt\"\nPACKAGE_NAME = subprocess.check_output([sys.executable, \"setup.py\", \"--name\"], encoding=\"utf-8\")\n\n\nnox.sessions = [\n    \"unit\",\n    \"cover\",\n    \"mypy\",\n    \"check_lower_bounds\"\n    \n    \"docs\",\n]\n\n@nox.session(python=['3.6', '3.7', '3.8', '3.9'])\ndef unit(session):\n    \n\n    session.install('coverage', 'pytest', 'pytest-cov', 'asyncmock', 'pytest-asyncio')\n    session.install('-e', '.')\n\n    session.run(\n        'py.test',\n        '--quiet',\n        '--cov=google/cloud/asset_v1p4beta1/',\n        '--cov-config=.coveragerc',\n        '--cov-report=term',\n        '--cov-report=html',\n        os.path.join('tests', 'unit', ''.join(session.posargs))\n    )\n\n\n@nox.session(python='3.7')\ndef cover(session):\n    \n    session.install(\"coverage\", \"pytest-cov\")\n    session.run(\"coverage\", \"report\", \"--show-missing\", \"--fail-under=100\")\n\n    session.run(\"coverage\", \"erase\")\n\n\n@nox.session(python=['3.6', '3.7'])\ndef mypy(session):\n    \n    session.install('mypy', 'types-pkg_resources')\n    session.install('.')\n    session.run(\n        'mypy',\n        '--explicit-package-bases',\n        'google',\n    )\n\n\n@nox.session\ndef update_lower_bounds(session):\n    \n    session.install('google-cloud-testutils')\n    session.install('.')\n\n    session.run(\n        'lower-bound-checker',\n        'update',\n        '--package-name',\n        PACKAGE_NAME,\n        '--constraints-file',\n        str(LOWER_BOUND_CONSTRAINTS_FILE),\n    )\n\n\n@nox.session\ndef check_lower_bounds(session):\n    \n    session.install('google-cloud-testutils')\n    session.install('.')\n\n    session.run(\n        'lower-bound-checker',\n        'check',\n        '--package-name',\n        PACKAGE_NAME,\n        '--constraints-file',\n        str(LOWER_BOUND_CONSTRAINTS_FILE),\n    )\n\n@nox.session(python='3.6')\ndef docs(session):\n    \n\n    session.install(\"-e\", \".\")\n    session.install(\"sphinx<3.0.0\", \"alabaster\", \"recommonmark\")\n\n    shutil.rmtree(os.path.join(\"docs\", \"_build\"), ignore_errors=True)\n    session.run(\n        \"sphinx-build\",\n        \"-W\",  \n        \"-T\",  \n        \"-N\",  \n        \"-b\",\n        \"html\",\n        \"-d\",\n        os.path.join(\"docs\", \"_build\", \"doctrees\", \"\"),\n        os.path.join(\"docs\", \"\"),\n        os.path.join(\"docs\", \"_build\", \"html\", \"\"),\n    )\n",
        "summary": "This Python script uses the `nox` library to define and run multiple sessions for testing, code quality checks, documentation generation, and managing lower bounds constraints in a project. Each session is configured to run under specific Python versions and performs tasks such as installing dependencies, running tests, checking code with mypy, updating and verifying package constraints, and building documentation."
    },
    {
        "code": "import os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'CongoCart.settings')\n\napplication = get_asgi_application()\n",
        "summary": "The Python script sets the Django settings module to 'CongoCart.settings' and then retrieves the ASGI application instance using `get_asgi_application()`."
    },
    {
        "code": "import pygame\n\n\n\nclass PadSprite(pygame.sprite.Sprite):\n    def __init__(self, image, position):\n        super(PadSprite, self).__init__()\n        self.normal = pygame.image.load(image)\n        self.hit = pygame.image.load('images/collision.png')\n        self.rect = pygame.Rect(self.normal.get_rect())\n        self.rect.center = position\n\n    def update(self, hit_list):\n        if self in hit_list:\n            self.image = self.hit\n        else:\n            self.image = self.normal\n\nclass Trophy(pygame.sprite.Sprite):\n    def __init__(self, position):\n        pygame.sprite.Sprite.__init__(self)\n        self.image = pygame.image.load('images/trophy.png')\n        self.rect = self.image.get_rect()\n        self.rect.x, self.rect.y = position\n\n    def draw(self, screen):\n        screen.blit(self.image, self.rect)\n\ndef level1():\n    pads = [\n        PadSprite('images/vertical_pads.png', (0, 100)),\n        PadSprite('images/vertical_pads.png', (0, 200)),\n        PadSprite('images/vertical_pads.png', (0, 400)),\n        PadSprite('images/vertical_pads.png', (1024, 100)),\n        PadSprite('images/vertical_pads.png', (1024, 550)),\n        PadSprite('images/vertical_pads.png', (824, 768)),\n        PadSprite('images/vertical_pads.png', (824, 368)),\n        PadSprite('images/vertical_pads.png', (210, 375)),\n        PadSprite('images/vertical_pads.png', (824, 368)),\n        PadSprite('images/race_pads.png', (900, 0)),\n        PadSprite('images/race_pads.png', (724, 0)),\n        PadSprite('images/race_pads.png', (524, 0)),\n        PadSprite('images/race_pads.png', (224, 0)),\n        PadSprite('images/race_pads.png', (1024, 768)),\n        PadSprite('images/race_pads.png', (624, 768)),\n        PadSprite('images/race_pads.png', (224, 768)),\n        PadSprite('images/race_pads.png', (450, 130)),\n        PadSprite('images/race_pads.png', (550, 130)),\n        PadSprite('images/small_horizontal.png', (600, 615)),\n        PadSprite('images/small_horizontal.png', (350, 615)),\n        PadSprite('images/small_horizontal.png', (470, 270)),\n        PadSprite('images/small_vertical.png', (600, 390)),\n        PadSprite('images/small_vertical.png', (350, 390)),\n\n        PadSprite('images/vertical_pads.png', (0,250)),\n        PadSprite('images/vertical_pads.png', (0, 525)),\n        PadSprite('images/vertical_pads.png', (1024, 250)),\n        PadSprite('images/vertical_pads.png', (1024, 525)),\n        PadSprite('images/race_pads.png', (250, 0)),\n        PadSprite('images/race_pads.png', (760, 0)),\n        PadSprite('images/race_pads.png', (500, 0)),\n        PadSprite('images/race_pads.png', (250, 768)),\n        PadSprite('images/race_pads.png', (760, 768)),\n        PadSprite('images/race_pads.png', (500, 768))\n    ]\n\n    trophies = [Trophy((450, 320))]\n\n    return 1, pads, trophies, (970, 730), 60\n\ndef level2():\n    pads = [\n        PadSprite('images/vertical_pads.png', (0, 100)),\n        PadSprite('images/vertical_pads.png', (0, 200)),\n        PadSprite('images/vertical_pads.png', (0, 400)),\n        PadSprite('images/vertical_pads.png', (1024, 100)),\n        PadSprite('images/vertical_pads.png', (1024, 550)),\n        PadSprite('images/vertical_pads.png', (200, 768)),\n        PadSprite('images/vertical_pads.png', (200, 368)),\n        PadSprite('images/vertical_pads.png', (800, 375)),\n        PadSprite('images/vertical_pads.png', (200, 368)),\n        PadSprite('images/race_pads.png', (60, 0)),\n        PadSprite('images/race_pads.png', (300, 0)),\n        PadSprite('images/race_pads.png', (700, 0)),\n        PadSprite('images/race_pads.png', (900, 0)),\n        PadSprite('images/race_pads.png', (1024, 768)),\n        PadSprite('images/race_pads.png', (624, 768)),\n        PadSprite('images/race_pads.png', (224, 768)),\n        PadSprite('images/race_pads.png', (450, 130)),\n        PadSprite('images/race_pads.png', (550, 130)),\n        PadSprite('images/small_horizontal.png', (670, 615)),\n        PadSprite('images/small_horizontal.png', (470, 615)),\n        PadSprite('images/small_horizontal.png', (470, 270)),\n        PadSprite('images/small_vertical.png', (350, 490)),\n        PadSprite('images/small_vertical.png', (350, 390)),\n        PadSprite('images/small_vertical.png', (600, 390)),\n\n        PadSprite('images/vertical_pads.png', (0,250)),\n        PadSprite('images/vertical_pads.png', (0, 525)),\n        PadSprite('images/vertical_pads.png', (1024, 250)),\n        PadSprite('images/vertical_pads.png', (1024, 525)),\n        PadSprite('images/race_pads.png', (250, 0)),\n        PadSprite('images/race_pads.png', (760, 0)),\n        PadSprite('images/race_pads.png', (500, 0)),\n        PadSprite('images/race_pads.png', (250, 768)),\n        PadSprite('images/race_pads.png', (760, 768)),\n        PadSprite('images/race_pads.png', (500, 768))\n    ]\n\n    trophies = [Trophy((450, 320))]\n\n    return 2, pads, trophies, (30, 730), 60\n\n\ndef level3():\n    pads = [\n        PadSprite('images/race_pads.png', (800, 150)),\n        PadSprite('images/race_pads.png', (800, 375)),\n        PadSprite('images/race_pads.png', (800, 625)),\n        PadSprite('images/race_pads.png', (800, 675)),\n        PadSprite('images/race_pads.png', (800, 575)),\n        PadSprite('images/race_pads.png', (200, 150)),\n        PadSprite('images/race_pads.png', (200, 675)),\n        PadSprite('images/race_pads.png', (200, 575)),\n        PadSprite('images/race_pads.png', (200, 375)),\n        PadSprite('images/race_pads.png', (200, 625)),\n        PadSprite('images/small_vertical.png', (450, 260)),\n\n        PadSprite('images/vertical_pads.png', (0, 250)),\n        PadSprite('images/vertical_pads.png', (0, 525)),\n        PadSprite('images/vertical_pads.png', (1024, 250)),\n        PadSprite('images/vertical_pads.png', (1024, 525)),\n        PadSprite('images/race_pads.png', (250, 0)),\n        PadSprite('images/race_pads.png', (760, 0)),\n        PadSprite('images/race_pads.png', (500, 0)),\n        PadSprite('images/race_pads.png', (250, 768)),\n        PadSprite('images/race_pads.png', (760, 768)),\n        PadSprite('images/race_pads.png', (500, 768))\n    ]\n    trophies = [Trophy((490, 50))]\n    return 3, pads, trophies, (490, 700), 30\n\n\t\ndef level4():\n    pads = [\n        PadSprite('images/race_pads.png', (800, 150)),\n        PadSprite('images/race_pads.png', (800, 375)),\n        PadSprite('images/race_pads.png', (800, 625)),\n        PadSprite('images/race_pads.png', (800, 675)),\n        PadSprite('images/race_pads.png', (800, 575)),\n        PadSprite('images/race_pads.png', (200, 150)),\n        PadSprite('images/race_pads.png', (200, 675)),\n        PadSprite('images/race_pads.png', (200, 575)),\n        PadSprite('images/race_pads.png', (200, 375)),\n        PadSprite('images/race_pads.png', (200, 625)),\n        PadSprite('images/small_vertical.png', (555, 260)),\n\n        PadSprite('images/vertical_pads.png', (0, 250)),\n        PadSprite('images/vertical_pads.png', (0, 525)),\n        PadSprite('images/vertical_pads.png', (1024, 250)),\n        PadSprite('images/vertical_pads.png', (1024, 525)),\n        PadSprite('images/race_pads.png', (250, 0)),\n        PadSprite('images/race_pads.png', (760, 0)),\n        PadSprite('images/race_pads.png', (500, 0)),\n        PadSprite('images/race_pads.png', (250, 768)),\n        PadSprite('images/race_pads.png', (760, 768)),\n        PadSprite('images/race_pads.png', (500, 768))\n    ]\n    trophies = [Trophy((490, 50))]\n    return 4, pads, trophies, (490, 700), 30",
        "summary": "The provided code defines four different levels for a racing game. Each level is represented by a function that returns the level number, a list of pad sprites, a list of trophy sprites, the starting position for the player, and the time limit for completing the level.\n\nHere's a breakdown of each function:\n\n1. **level1()**:\n   - Returns: Level 1\n   - Pads: A series of horizontal race pads at various y-coordinates.\n   - Trophies: One trophy located at (450, 320).\n   - Start Position: (960, 730)\n   - Time Limit: 60 seconds\n\n2. **level2()**:\n   - Returns: Level 2\n   - Pads: Similar to level 1 but with a different arrangement of pads.\n   - Trophies: One trophy located at (450, 320).\n   - Start Position: (30, 730)\n   - Time Limit: 60 seconds\n\n3. **level3()**:\n   - Returns: Level 3\n   - Pads: A series of horizontal race pads with a vertical pad in the middle.\n   - Trophies: One trophy located at (490, 50).\n   - Start Position: (490, 700)\n   - Time Limit: 30 seconds\n\n4. **level4()**:\n   - Returns: Level 4\n   - Pads: Similar to level 3 but with a different arrangement of pads.\n   - Trophies: One trophy located at (490, 50).\n   - Start Position: (490, 700)\n   - Time Limit: 30 seconds\n\nEach function is designed to be called when the game needs to load a specific level. The returned values are then used by the game logic to set up the environment for that level.\n\nIf you need to add more levels or modify existing ones, you can follow the same pattern and define new functions with appropriate parameters."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text format such as CSV or JSON."
    },
    {
        "code": "__authors__ = [\"O. Svensson\"]\n__license__ = \"MIT\"\n__date__ = \"21/04/2019\"\n\n\nimport os\nimport json\nimport unittest\n\nfrom edna2.utils import UtilsTest\nfrom edna2.tasks.DozorTasks import ControlDozor\n\n\nclass ControlDozorPlotExecTest(unittest.TestCase):\n\n    def setUp(self):\n        self.dataPath = UtilsTest.prepareTestDataPath(__file__)\n\n    @unittest.skipIf(os.name == 'nt', \"Don't run on Windows\")\n    def test_makePlot(self):\n        workingDirectory = UtilsTest.getTestRunPath()\n        dataCollectionId = 123456\n        outDataPath = self.dataPath / \"outDataControlDozor.json\"\n        with open(str(outDataPath)) as f:\n            outData = json.load(f)\n        controlDozor = ControlDozor(inData={})\n        controlDozor.template = \"mesh-test_1_%4d.cbf\"\n        controlDozor.directory = UtilsTest.getTestImageDirPath().as_posix()\n        controlDozor.makePlot(dataCollectionId, outData, workingDirectory)",
        "summary": "This Python script is a unit test for the `ControlDozor` class in the `edna2.tasks.DozorTasks` module. It includes a method to test the plotting functionality of the `makePlot` method, which is skipped on Windows systems. The test reads data from a JSON file and initializes an instance of `ControlDozor` with specific settings before calling the `makePlot` method."
    },
    {
        "code": "import scrapy\nimport re\nfrom rkpass.items import dzswMorningItem\n\n\nclass DzswmorningspiderSpider(scrapy.Spider):\n    name = 'dzswMorningSpider'\n    allowed_domains = ['www.rkpass.cn']\n    start_urls = []\n    paperId_list = ['612', '541', '477', '453', '281', '280', '279', '278', '277', '276']  \n    field_list = ['20182', '20172', '20162', '20152', '20142', '20132', '20122', '20112', '20102', '20092']  \n\n    for j in range(len(paperId_list)):\n        for i in range(1, 76):\n            start_urls.append(\n                'http://www.rkpass.cn/tk_timu/14_' + str(paperId_list[j]) + '_' + str(i) + '_xuanze.html?field=' +\n                field_list[j] + '&questionNum=' + str(i))\n\n    def parse(self, response):\n        questionNum = str(response.url).strip().split(\"questionNum=\")[-1]  \n        field = (str(response.url).strip().split(\"field=\")[-1]).split(\"&\")[0]  \n        knowledgeTwo = response.xpath(\".//span[@class='red']//text()\").extract()  \n        \n        knowledgeTwo = knowledgeTwo[0] if list(knowledgeTwo) else \"\"\n        dataimg = response.xpath(\".//span[@class='shisi_text']/img[last()]/@src\").extract()  \n        product_id = re.findall('\\((.*?)\\)', response.xpath(\".//script//text()\").extract()[0])[0].split(',')[0].strip(\n            \"'\")  \n        question = \"\".join(response.xpath(\".//table/tr[2]/td/span[@class='shisi_text']//text()\").extract())  \n        A = \"\".join(\n            \"\".join(response.xpath(\".//table/tr[5]/td/span[@class='shisi_text']//text()\").extract()).split())  \n        B = \"\".join(\n            \"\".join(response.xpath(\".//table/tr[7]/td/span[@class='shisi_text']//text()\").extract()).split())  \n        C = \"\".join(\n            \"\".join(response.xpath(\".//table/tr[9]/td/span[@class='shisi_text']//text()\").extract()).split())  \n        D = \"\".join(\n            \"\".join(response.xpath(\".//table/tr[11]/td/span[@class='shisi_text']//text()\").extract()).split())  \n\n        questionImg = ''  \n        if len(dataimg) > 0:  \n            if len(dataimg) == 1:\n                questionImg = dataimg[0]  \n            elif len(dataimg) == 4:  \n                A = A + dataimg[0]\n                B = B + dataimg[1]\n                C = C + dataimg[2]\n                D = D + dataimg[3]\n            elif len(dataimg) == 5:  \n                questionImg = dataimg[0]  \n                A = A + dataimg[1]\n                B = B + dataimg[2]\n                C = C + dataimg[3]\n                D = D + dataimg[4]\n\n        \n        \n        knowledgeOne = knowledgeTwo  \n\n        \n        item = dzswMorningItem()\n        item['question'] = question\n        item['questionImg'] = questionImg\n        item['optiona'] = A\n        item['optionb'] = B\n        item['optionc'] = C\n        item['optiond'] = D\n\n        url = 'http://www.rkpass.cn/tk_jiexi.jsp?product_id=' + product_id + '&tixing=xuanze&answer=&paper_id=&tihao=&cache='\n        yield scrapy.Request(url, callback=self.parse_detail, dont_filter=True, meta={'item': item, 'field': field, 'questionNum': questionNum, 'knowledgeOne': knowledgeOne, 'knowledgeTwo': knowledgeTwo})\n\n    def parse_detail(self, response):\n        knowledgeOne = response.meta['knowledgeOne']  \n        knowledgeTwo = response.meta['knowledgeTwo']  \n        questionNum = response.meta['questionNum']  \n        field = response.meta['field']  \n        item = response.meta['item']  \n        answer = response.xpath(\".//td/span[@class='shisi_text']//text()\").extract()[2].strip()  \n        answerAnalysis = response.xpath(\".//table/tr[3]/td//text()\").extract()  \n        answerAnalysis = \"\".join(answerAnalysis[3:len(answerAnalysis)])\n\n        \n        item['answer'] = answer\n        item['answeranalysis'] = answerAnalysis\n        item['field'] = field\n        item['questionNum'] = questionNum\n        item['knowledgeOne'] = knowledgeOne\n        item['knowledgeTwo'] = knowledgeTwo\n\n        return item",
        "summary": "This Python script uses Scrapy to scrape educational questions and their details from a website. It constructs URLs for different paper IDs and fields, extracts question data including options and images, and then fetches detailed answers before storing the information in items."
    },
    {
        "code": "def decoupling_regularization_prepare(graph, sigma_square,lambda_input):\n    \n    \n    A = np.array(nx.adjacency_matrix(graph).todense()) \n    d = np.sum(A, axis=1)\n    D = np.diag(d)\n    n = len(D)\n    \n    Sigma_square = np.divide(sigma_square,d)\n\n\n    Sigma = np.diag(Sigma_square)\n    W = np.dot(A,inv(Sigma))\n    lambda_diag = lambda_input*np.ones(n)\n    lambda_diag_matrix = np.diag(lambda_diag)\n    W = W+lambda_diag_matrix\n    \n    w_col_sum = np.sum(W, axis=0)\n    w_row_sum = np.sum(W, axis=1)\n    Z_prime = np.diag(w_col_sum)\n    Z = np.diag(w_row_sum)\n\n    A_tilde = np.dot(np.dot(W,inv(Z_prime)),np.transpose(W))\n    \n    \n\n    return (A_tilde)",
        "summary": "The function `decoupling_regularization_prepare` processes a graph by normalizing its adjacency matrix using diagonal matrices derived from the degree of each node and a regularization parameter. It then adjusts the matrix through additional diagonal additions to ensure symmetry, ultimately returning a modified adjacency matrix that reflects both structural relationships and regularization effects."
    },
    {
        "code": "from __future__ import unicode_literals\r\n\r\nfrom ..char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, QUOTES, HYPHENS\r\nfrom ..char_classes import LIST_ELLIPSES, LIST_ICONS\r\n\r\n_hyphens_no_dash = HYPHENS.replace('-', '').strip('|').replace('||', '')\r\n_infixes = (LIST_ELLIPSES + LIST_ICONS +\r\n            [r'(?<=[{}])\\.(?=[{}])'.format(ALPHA_LOWER, ALPHA_UPPER),\r\n             r'(?<=[{a}])[,!?/\\(\\)]+(?=[{a}])'.format(a=ALPHA),\r\n             r'(?<=[{a}{q}])[:<>=](?=[{a}])'.format(a=ALPHA, q=QUOTES),\r\n             r'(?<=[{a}])--(?=[{a}])'.format(a=ALPHA),\r\n             r'(?<=[{a}]),(?=[{a}])'.format(a=ALPHA),\r\n             r'(?<=[{a}])([{q}\\)\\]\\(\\[])(?=[\\-{a}])'.format(a=ALPHA, q=QUOTES),\r\n             r'(?<=[{a}])[?\";:=,.]*(?:{h})(?=[{a}])'.format(a=ALPHA,\r\n                                                            h=_hyphens_no_dash),\r\n             r'(?<=[0-9])-(?=[0-9])'])\r\n\r\nTOKENIZER_INFIXES = _infixes\r\n",
        "summary": "The Python code defines a set of regular expression patterns for tokenizing text, including handling hyphens, ellipses, and various punctuation marks around alphabetic characters. These patterns are compiled into the `TOKENIZER_INFIXES` variable for use in text processing tasks."
    },
    {
        "code": "from __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\n\nclass ModuleDocFragment(object):\n    DOCUMENTATION = r\n\n    ANSIBLE_VARIABLES = r\n\n    ENCRYPT_SPECIFIC = r\n",
        "summary": "The provided Python code defines a class `ModuleDocFragment` with three attributes: `DOCUMENTATION`, `ANSIBLE_VARIABLES`, and `ENCRYPT_SPECIFIC`. These attributes are intended to hold documentation strings, Ansible variable specifications, and encryption-specific details respectively. The class uses future imports for better compatibility and sets the metaclass to type for Python 2 and 3 compatibility."
    },
    {
        "code": "from pyxdc.exceptions import (\n    ProviderError, BalanceError, APIError, AddressError, InvalidURLError,\n    ClientError, NotFoundError, UnitError\n)\n\nimport pytest\n\n\ndef test_exceptions():\n\n    with pytest.raises(ProviderError, match=\"error\"):\n        raise ProviderError(\"error\")\n    with pytest.raises(ProviderError, match=\"error, error\"):\n        raise ProviderError(\"error\", \"error\")\n    with pytest.raises(BalanceError, match=\"error\"):\n        raise BalanceError(\"error\")\n    with pytest.raises(BalanceError, match=\"error, error\"):\n        raise BalanceError(\"error\", \"error\")\n    with pytest.raises(APIError, match=\"error\"):\n        raise APIError(\"error\")\n    with pytest.raises(APIError):\n        raise APIError(\"error\", \"error\")\n    with pytest.raises(AddressError, match=\"error\"):\n        raise AddressError(\"error\")\n    with pytest.raises(AddressError, match=\"error, error\"):\n        raise AddressError(\"error\", \"error\")\n    with pytest.raises(InvalidURLError, match=\"error\"):\n        raise InvalidURLError(\"error\")\n    with pytest.raises(ClientError, match=\"error\"):\n        raise ClientError(\"error\")\n    with pytest.raises(ClientError):\n        raise ClientError(\"error\", \"error\")\n    with pytest.raises(NotFoundError, match=\"error\"):\n        raise NotFoundError(\"error\")\n    with pytest.raises(UnitError, match=\"error\"):\n        raise UnitError(\"error\")\n    with pytest.raises(UnitError, match=\"error, error\"):\n        raise UnitError(\"error\", \"error\")\n",
        "summary": "The provided Python code imports various exceptions from the `pyxdc.exceptions` module and uses the `pytest` framework to test that these exceptions are raised correctly with specific error messages. Each test case checks if a particular exception is raised when expected, ensuring proper exception handling in the application."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport unittest\nimport os\n\nfrom groupdocs_conversion_cloud import *\nfrom test.test_context import TestContext\nfrom test.test_file import TestFile\n\nclass TestConvertApi(TestContext):\n    \n\n    def test_convert_document(self):\n        \n        test_file = TestFile.one_page_docx()\n        settings = ConvertSettings()\n        settings.file_path = test_file.folder + test_file.file_name\n        settings.format = \"jpg\"\n        settings.output_path = self.OUT_FOLDER\n        request = ConvertDocumentRequest(settings)\n        data = self.convert_api.convert_document(request)\n        self.assertTrue(len(data) > 0)\n        self.assertTrue(data[0].size > 0)\n\n    def test_convert_document_download(self):\n        \n        test_file = TestFile.one_page_docx()\n        settings = ConvertSettings()\n        settings.file_path = test_file.folder + test_file.file_name\n        settings.format = \"pdf\"\n        request = ConvertDocumentRequest(settings)\n        data = self.convert_api.convert_document_download(request)\n        self.assertGreater(os.path.getsize(data), 0)\n\n    def test_convert_document_direct(self):\n        \n        test_file = TestFile.four_pages_docx()\n        local_file_path = self.get_test_file_path(test_file)\n        format = \"pdf\"\n        \n        request = ConvertDocumentDirectRequest(format, local_file_path)\n        data = self.convert_api.convert_document_direct(request)\n        self.assertGreater(os.path.getsize(data), 0)        \n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "The provided Python code defines a test class `TestConvertApi` that extends `TestContext` and includes methods to test document conversion using the GroupDocs Conversion Cloud API. It covers converting documents to different formats, downloading converted files, and performing direct conversions from local files. Each method uses specific settings and assertions to validate the conversion outcomes."
    },
    {
        "code": "import numpy as np\nimport scipy.sparse as sp\nimport torch\n\n\ndef encode_onehot(labels):\n    classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n                    enumerate(classes)}\n    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n                             dtype=np.int32)\n    return labels_onehot\n\n\ndef load_data(path=\"../data/cora/\", dataset=\"cora\"):\n    \n    print('Loading {} dataset...'.format(dataset))\n\n    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n                                        dtype=np.dtype(str))\n    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n    labels = encode_onehot(idx_features_labels[:, -1])\n\n    \n    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n                                    dtype=np.int32)\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=np.int32).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n\n    \n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    features = normalize(features)\n    adj = normalize(adj + sp.eye(adj.shape[0]))\n\n    idx_train = range(140)\n    idx_val = range(200, 500)\n    idx_test = range(500, 1500)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    labels = torch.LongTensor(np.where(labels)[1])\n    adj = sparse_mx_to_torch_sparse_tensor(adj)\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n\n    return adj, features, labels, idx_train, idx_val, idx_test\n\n\ndef normalize(mx):\n    \n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\ndef sub_graph(adj, num):\n    \n    nodes = adj.shape[0]\n    neighbor_number = torch.sum(adj>0,dim=1).reshape(node,1)/num\n    sub_graph = torch.randint(0,nodes, (nodes,num))\n    sub_graph = sub_graph.reshape(-1).cpu().tolist()\n    sub_graph = list(set(sub_graph))\n    mask = torch.zeros(nodes,nodes)\n    mask[sub_graph,sub_graph] = 1\n    \n    return adj*mask*neighbor_number\n\n    \n\n",
        "summary": "The provided Python code includes functions for encoding labels into one-hot format, loading and preprocessing a graph dataset (specifically the Cora dataset), normalizing matrices, calculating accuracy, converting sparse matrices to PyTorch tensors, and creating subgraphs. The `load_data` function is central, handling the reading and preparation of the dataset for use in graph neural network models."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union, overload\nfrom ... import _utilities\nfrom ._enums import *\n\n__all__ = [\n    'RemediationFiltersArgs',\n]\n\n@pulumi.input_type\nclass RemediationFiltersArgs:\n    def __init__(__self__, *,\n                 locations: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):\n        \n        if locations is not None:\n            pulumi.set(__self__, \"locations\", locations)\n\n    @property\n    @pulumi.getter\n    def locations(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:\n        \n        return pulumi.get(self, \"locations\")\n\n    @locations.setter\n    def locations(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):\n        pulumi.set(self, \"locations\", value)\n\n\n",
        "summary": "The provided Python code defines a class `RemediationFiltersArgs` using the Pulumi framework for infrastructure as code. This class includes an optional property `locations` that accepts a sequence of string inputs representing locations, with methods to get and set this property while utilizing Pulumi's runtime utilities for handling input values."
    },
    {
        "code": "import ast\nwith open('./test.txt',\"r\") as f:    \n    str = f.read()    \nprint(str)\nframe_list = ast.literal_eval(str)\nfor frame in frame_list:\n    print(frame)",
        "summary": "The Python code reads the contents of a file named 'test.txt', which is expected to contain a string representation of a list. It then uses the `ast.literal_eval` function to safely evaluate this string as a Python literal, converting it into an actual list stored in `frame_list`. The code iterates over each element in `frame_list`, printing them out."
    },
    {
        "code": "from strenum import StrEnum\n\n\nclass SummaryLevel(StrEnum):\n    \n\n    STATE = \"040\"\n    STATE_COUNTY = \"050\"\n    STATE_COUNTY_TRACT = \"140\"\n    STATE_COUNTY_TRACT_BLOCKGROUP = \"150\"\n    STATE_COUNTY_TRACT_BLOCKGROUP_BLOCK = \"750\"\n",
        "summary": "The `SummaryLevel` class is a subclass of `StrEnum` that defines various geographic summary levels, each represented by a unique string code. These codes are used to categorize geographical data at different resolutions, from state level down to block group level."
    },
    {
        "code": "def sub(x, y):\n    f\n\n\nclass Light:\n    pass\n\na = Light()\nb = Ligth()\n",
        "summary": "The provided Python code defines a function `sub` that takes two parameters but does not contain any implementation. It also includes a class named `Light` with no methods or attributes defined. The code then creates an instance of the `Light` class and attempts to create another instance using a misspelled class name, which would result in an error."
    },
    {
        "code": "from __future__ import unicode_literals, division, absolute_import\n\nimport time\nimport logging\nfrom collections import deque\ntry:\n    from UserDict import DictMixin\nexcept ImportError:\n    from collections import Mapping as DictMixin\n\nimport six\nfrom six import iteritems\nfrom six.moves import cPickle\n\n\nclass BaseCounter(object):\n\n    def __init__(self):\n        raise NotImplementedError\n\n    def event(self, value=1):\n        \n        raise NotImplementedError\n\n    def value(self, value):\n        \n        raise NotImplementedError\n\n    @property\n    def avg(self):\n        \n        raise NotImplementedError\n\n    @property\n    def sum(self):\n        \n        raise NotImplementedError\n\n    def empty(self):\n        \n        raise NotImplementedError\n\n\nclass TotalCounter(BaseCounter):\n    \n\n    def __init__(self):\n        self.cnt = 0\n\n    def event(self, value=1):\n        self.cnt += value\n\n    def value(self, value):\n        self.cnt = value\n\n    @property\n    def avg(self):\n        return self.cnt\n\n    @property\n    def sum(self):\n        return self.cnt\n\n    def empty(self):\n        return self.cnt == 0\n\n\nclass AverageWindowCounter(BaseCounter):\n    \n\n    def __init__(self, window_size=300):\n        self.window_size = window_size\n        self.values = deque(maxlen=window_size)\n\n    def event(self, value=1):\n        self.values.append(value)\n\n    value = event\n\n    @property\n    def avg(self):\n        return self.sum / len(self.values)\n\n    @property\n    def sum(self):\n        return sum(self.values)\n\n    def empty(self):\n        if not self.values:\n            return True\n\n\nclass TimebaseAverageWindowCounter(BaseCounter):\n    \n\n    def __init__(self, window_size=30, window_interval=10):\n        self.max_window_size = window_size\n        self.window_size = 0\n        self.window_interval = window_interval\n        self.values = deque(maxlen=window_size)\n        self.times = deque(maxlen=window_size)\n\n        self.cache_value = 0\n        self.cache_start = None\n        self._first_data_time = None\n\n    def event(self, value=1):\n        now = time.time()\n        if self._first_data_time is None:\n            self._first_data_time = now\n\n        if self.cache_start is None:\n            self.cache_value = value\n            self.cache_start = now\n        elif now - self.cache_start > self.window_interval:\n            self.values.append(self.cache_value)\n            self.times.append(self.cache_start)\n            self.on_append(self.cache_value, self.cache_start)\n            self.cache_value = value\n            self.cache_start = now\n        else:\n            self.cache_value += value\n        return self\n\n    def value(self, value):\n        self.cache_value = value\n\n    def _trim_window(self):\n        now = time.time()\n        if self.cache_start and now - self.cache_start > self.window_interval:\n            self.values.append(self.cache_value)\n            self.times.append(self.cache_start)\n            self.on_append(self.cache_value, self.cache_start)\n            self.cache_value = 0\n            self.cache_start = None\n\n        if self.window_size != self.max_window_size and self._first_data_time is not None:\n            time_passed = now - self._first_data_time\n            self.window_size = min(self.max_window_size, time_passed / self.window_interval)\n        window_limit = now - self.window_size * self.window_interval\n        while self.times and self.times[0] < window_limit:\n            self.times.popleft()\n            self.values.popleft()\n\n    @property\n    def avg(self):\n        sum = float(self.sum)\n        if not self.window_size:\n            return 0\n        return sum / self.window_size / self.window_interval\n\n    @property\n    def sum(self):\n        self._trim_window()\n        return sum(self.values) + self.cache_value\n\n    def empty(self):\n        self._trim_window()\n        if not self.values and not self.cache_start:\n            return True\n\n    def on_append(self, value, time):\n        pass\n\n\nclass CounterValue(DictMixin):\n    \n\n    def __init__(self, manager, keys):\n        self.manager = manager\n        self._keys = keys\n\n    def __getitem__(self, key):\n        if key == '__value__':\n            key = self._keys\n            return self.manager.counters[key]\n        else:\n            key = self._keys + (key, )\n\n        available_keys = []\n        for _key in self.manager.counters:\n            if _key[:len(key)] == key:\n                available_keys.append(_key)\n\n        if len(available_keys) == 0:\n            raise KeyError\n        elif len(available_keys) == 1:\n            if available_keys[0] == key:\n                return self.manager.counters[key]\n            else:\n                return CounterValue(self.manager, key)\n        else:\n            return CounterValue(self.manager, key)\n\n    def __len__(self):\n        return len(self.keys())\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __contains__(self, key):\n        return key in self.keys()\n\n    def keys(self):\n        result = set()\n        for key in self.manager.counters:\n            if key[:len(self._keys)] == self._keys:\n                key = key[len(self._keys):]\n                result.add(key[0] if key else '__value__')\n        return result\n\n    def to_dict(self, get_value=None):\n        \n        result = {}\n        for key, value in iteritems(self):\n            if isinstance(value, BaseCounter):\n                if get_value is not None:\n                    value = getattr(value, get_value)\n                result[key] = value\n            else:\n                result[key] = value.to_dict(get_value)\n        return result\n\n\nclass CounterManager(DictMixin):\n    \n\n    def __init__(self, cls=TimebaseAverageWindowCounter):\n        \n        self.cls = cls\n        self.counters = {}\n\n    def event(self, key, value=1):\n        \n        if isinstance(key, six.string_types):\n            key = (key, )\n        assert isinstance(key, tuple), \"event key type error\"\n        if key not in self.counters:\n            self.counters[key] = self.cls()\n        self.counters[key].event(value)\n        return self\n\n    def value(self, key, value=1):\n        \n        if isinstance(key, six.string_types):\n            key = (key, )\n        assert isinstance(key, tuple), \"event key type error\"\n        if key not in self.counters:\n            self.counters[key] = self.cls()\n        self.counters[key].value(value)\n        return self\n\n    def trim(self):\n        \n        for key, value in list(iteritems(self.counters)):\n            if value.empty():\n                del self.counters[key]\n\n    def __getitem__(self, key):\n        key = (key, )\n        available_keys = []\n        for _key in self.counters:\n            if _key[:len(key)] == key:\n                available_keys.append(_key)\n\n        if len(available_keys) == 0:\n            raise KeyError\n        elif len(available_keys) == 1:\n            if available_keys[0] == key:\n                return self.counters[key]\n            else:\n                return CounterValue(self, key)\n        else:\n            return CounterValue(self, key)\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __len__(self):\n        return len(self.keys())\n\n    def keys(self):\n        result = set()\n        for key in self.counters:\n            result.add(key[0] if key else ())\n        return result\n\n    def to_dict(self, get_value=None):\n        \n        self.trim()\n        result = {}\n        for key, value in iteritems(self):\n            if isinstance(value, BaseCounter):\n                if get_value is not None:\n                    value = getattr(value, get_value)\n                result[key] = value\n            else:\n                result[key] = value.to_dict(get_value)\n        return result\n\n    def dump(self, filename):\n        \n        try:\n            with open(filename, 'wb') as fp:\n                cPickle.dump(self.counters, fp)\n        except:\n            logging.error(\"can't dump counter to file: %s\" % filename)\n            return False\n        return True\n\n    def load(self, filename):\n        \n        try:\n            with open(filename) as fp:\n                self.counters = cPickle.load(fp)\n        except:\n            logging.debug(\"can't load counter from file: %s\" % filename)\n            return False\n        return True\n",
        "summary": "The provided Python code defines a set of classes for managing counters, including `BaseCounter`, `TotalCounter`, `AverageWindowCounter`, and `TimebaseAverageWindowCounter`. These classes provide methods to track events, calculate averages, sums, and manage the state of counters over time. The `CounterManager` class allows for the creation, manipulation, and serialization of multiple counters, providing a flexible framework for tracking metrics in applications."
    },
    {
        "code": "from typing import List, Optional, Union\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ...feature_extraction_utils import BatchFeature, FeatureExtractionMixin\nfrom ...file_utils import TensorType\nfrom ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageFeatureExtractionMixin, is_torch_tensor\nfrom ...utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass ViTFeatureExtractor(FeatureExtractionMixin, ImageFeatureExtractionMixin):\n    r\n\n    model_input_names = [\"pixel_values\"]\n\n    def __init__(\n        self,\n        do_resize=True,\n        size=224,\n        resample=Image.BILINEAR,\n        do_normalize=True,\n        image_mean=None,\n        image_std=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.do_resize = do_resize\n        self.size = size\n        self.resample = resample\n        self.do_normalize = do_normalize\n        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n\n    def __call__(\n        self,\n        images: Union[\n            Image.Image, np.ndarray, \"torch.Tensor\", List[Image.Image], List[np.ndarray], List[\"torch.Tensor\"]  \n        ],\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        **kwargs\n    ) -> BatchFeature:\n        \n        \n        valid_images = False\n\n        \n        if isinstance(images, (Image.Image, np.ndarray)) or is_torch_tensor(images):\n            valid_images = True\n        elif isinstance(images, (list, tuple)):\n            if len(images) == 0 or isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0]):\n                valid_images = True\n\n        if not valid_images:\n            raise ValueError(\n                \"Images must of type `PIL.Image.Image`, `np.ndarray` or `torch.Tensor` (single example),\"\n                \"`List[PIL.Image.Image]`, `List[np.ndarray]` or `List[torch.Tensor]` (batch of examples).\"\n            )\n\n        is_batched = bool(\n            isinstance(images, (list, tuple))\n            and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0]))\n        )\n\n        if not is_batched:\n            images = [images]\n\n        \n        if self.do_resize and self.size is not None:\n            images = [self.resize(image=image, size=self.size, resample=self.resample) for image in images]\n        if self.do_normalize:\n            images = [self.normalize(image=image, mean=self.image_mean, std=self.image_std) for image in images]\n\n        \n        data = {\"pixel_values\": images}\n        encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)\n\n        return encoded_inputs\n",
        "summary": "The `ViTFeatureExtractor` class is a feature extractor for Vision Transformer models, handling image preprocessing tasks such as resizing and normalization. It processes input images to prepare them for model inference by converting them into a format suitable for the ViT architecture."
    },
    {
        "code": "UNKNOWN = -1\n\ndef read_val():\n    return int(input())\n\ndef read_row():\n    return list(map(int, input().split()))\n\ndef read_grid():\n    return [read_row() for _ in range(read_val())]\n\ndef make_blank_row(i):\n    return [UNKNOWN] * i\n\ndef make_blank_grid(n):\n    return [make_blank_row(i) for i in range(1, n + 1)]\n\ndef compute_max_path_sum(grid):\n    memo = make_blank_grid(len(grid))\n    \n    def dfs(i, j):\n        if i == len(grid):\n            return 0\n        \n        if memo[i][j] == UNKNOWN:\n            memo[i][j] = grid[i][j] + max(dfs(i + 1, j), dfs(i + 1, j + 1))\n        \n        return memo[i][j]\n    \n    return dfs(0, 0)\n\nfor t in range(read_val()):\n    print(compute_max_path_sum(read_grid()))\n",
        "summary": "The provided Python code defines functions to read a grid of integers from input, compute the maximum path sum from the top-left corner to the bottom-right corner using dynamic programming with memoization, and handle multiple test cases."
    },
    {
        "code": "import platform\n\n\n\noperating_system = platform.system().lower()\nif operating_system == 'darwin':\n    from .blender_utils_macos import get_installed_blender_versions\n    operating_system_name = 'macos'\nelif operating_system == 'linux':\n    from .blender_utils_linux import get_installed_blender_versions\n    operating_system_name = 'linux'\nelif operating_system == 'windows':\n    from .blender_utils_windows import get_installed_blender_versions\n    operating_system_name = 'windows'\nelse:\n    raise Exception(\"Unimplemented for OS {}\".format(operating_system))\n\nfrom .blender_utils_web import get_blender_version_download_links\n\n\ndef find_blender(version):\n    \n    installed_versions = get_installed_blender_versions()\n    if version in installed_versions:\n        return installed_versions[version]\n    else:\n        print(\"blender version '{}' not found; found {} version(s):\".format(version, len(installed_versions)))\n        for v, path in installed_versions.items():\n            print(\"    {}: {}\".format(v, path))\n        print(\"searching web archive...\")\n        versions = get_blender_version_download_links(version, operating_system_name)\n        print(\"found {} download(s) for blender version '{}', platform '{}':\".format(len(versions), version, operating_system_name))\n        for url in versions:\n            print(\"    {}\".format(url))\n\n\nif __name__ == '__main__':\n    for version, exec_path in get_installed_blender_versions().items():\n        print(\"found blender {version}: {path}\".format(version=version,\n                                                       path=exec_path))\n    blender = find_blender('2.80')\n    if blender:\n        print(\"Found blender: '{}'\".format(blender))\n    else:\n        print(\"No matching blender version installed :(\")\n",
        "summary": "The Python script determines the operating system and imports platform-specific utilities to retrieve installed Blender versions. It then defines a function `find_blender` that checks for locally installed Blender versions; if not found, it searches web archives for download links based on the specified version and operating system. The script also includes a main block that lists all installed Blender versions and attempts to find a specific version (e.g., '2.80')."
    },
    {
        "code": "import functools\nimport random\nfrom math import cos, pi\n\nimport cv2\nimport kornia\nimport numpy as np\nimport torch\nfrom kornia.augmentation import ColorJitter\n\nfrom data.util import read_img\nfrom PIL import Image\nfrom io import BytesIO\n\n\n\nfrom utils.util import opt_get\n\n\n\n\ndef kornia_color_jitter_numpy(img, setting):\n    if setting * 255 > 1:\n        \n        img = torch.from_numpy(img).permute(2,0,1).unsqueeze(0)\n        img = ColorJitter(setting, setting, setting, setting)(img)\n        img = img.squeeze(0).permute(1,2,0).numpy()\n    return img\n\n\n\n\nclass ImageCorruptor:\n    def __init__(self, opt):\n        self.opt = opt\n        self.reset_random()\n        self.blur_scale = opt['corruption_blur_scale'] if 'corruption_blur_scale' in opt.keys() else 1\n        self.fixed_corruptions = opt['fixed_corruptions'] if 'fixed_corruptions' in opt.keys() else []\n        self.num_corrupts = opt['num_corrupts_per_image'] if 'num_corrupts_per_image' in opt.keys() else 0\n        self.cosine_bias = opt_get(opt, ['cosine_bias'], True)\n        if self.num_corrupts == 0:\n            return\n        else:\n            self.random_corruptions = opt['random_corruptions'] if 'random_corruptions' in opt.keys() else []\n\n    def reset_random(self):\n        if 'random_seed' in self.opt.keys():\n            self.rand = random.Random(self.opt['random_seed'])\n        else:\n            self.rand = random.Random()\n\n    \n    \n    def get_rand(self):\n        r = self.rand.random()\n        if self.cosine_bias:\n            return 1 - cos(r * pi / 2)\n        else:\n            return r\n\n    def corrupt_images(self, imgs, return_entropy=False):\n        if self.num_corrupts == 0 and not self.fixed_corruptions:\n            if return_entropy:\n                return imgs, []\n            else:\n                return imgs\n\n        if self.num_corrupts == 0:\n            augmentations = []\n        else:\n            augmentations = random.choices(self.random_corruptions, k=self.num_corrupts)\n\n        \n        corrupted_imgs = []\n        entropy = []\n        undo_fns = []\n        applied_augs = augmentations + self.fixed_corruptions\n        for img in imgs:\n            for aug in augmentations:\n                r = self.get_rand()\n                img, undo_fn = self.apply_corruption(img, aug, r, applied_augs)\n                if undo_fn is not None:\n                    undo_fns.append(undo_fn)\n            for aug in self.fixed_corruptions:\n                r = self.get_rand()\n                img, undo_fn = self.apply_corruption(img, aug, r, applied_augs)\n                entropy.append(r)\n                if undo_fn is not None:\n                    undo_fns.append(undo_fn)\n            \n            for ufn in undo_fns:\n                img = ufn(img)\n            corrupted_imgs.append(img)\n\n\n        if return_entropy:\n            return corrupted_imgs, entropy\n        else:\n            return corrupted_imgs\n\n    def apply_corruption(self, img, aug, rand_val, applied_augmentations):\n        undo_fn = None\n        if 'color_quantization' in aug:\n            \n            quant_div = 2 ** (int(rand_val * 10 / 3) + 2)\n            img = img * 255\n            img = (img // quant_div) * quant_div\n            img = img / 255\n        elif 'color_jitter' in aug:\n            lo_end = 0\n            hi_end = .2\n            setting = rand_val * (hi_end - lo_end) + lo_end\n            img = kornia_color_jitter_numpy(img, setting)\n        elif 'gaussian_blur' in aug:\n            img = cv2.GaussianBlur(img, (0,0), self.blur_scale*rand_val*1.5)\n        elif 'motion_blur' in aug:\n            \n            intensity = self.blur_scale*rand_val * 3 + 1\n            angle = random.randint(0,360)\n            k = np.zeros((intensity, intensity), dtype=np.float32)\n            k[(intensity - 1) // 2, :] = np.ones(intensity, dtype=np.float32)\n            k = cv2.warpAffine(k, cv2.getRotationMatrix2D((intensity / 2 - 0.5, intensity / 2 - 0.5), angle, 1.0),\n                               (intensity, intensity))\n            k = k * (1.0 / np.sum(k))\n            img = cv2.filter2D(img, -1, k)\n        elif 'block_noise' in aug:\n            \n            pass\n        elif 'lq_resampling' in aug:\n            \n            if 'lq_resampling4x' == aug:\n                scale = 4\n            else:\n                if rand_val < .3:\n                    scale = 1\n                elif rand_val < .7:\n                    scale = 2\n                else:\n                    scale = 4\n            if scale > 1:\n                interpolation_modes = [cv2.INTER_NEAREST, cv2.INTER_CUBIC, cv2.INTER_LINEAR, cv2.INTER_LANCZOS4]\n                mode = random.randint(0,4) % len(interpolation_modes)\n                \n                img = cv2.resize(img, dsize=(img.shape[1]//scale, img.shape[0]//scale), interpolation=mode)\n                def lq_resampling_undo_fn(scale, img):\n                    return cv2.resize(img, dsize=(img.shape[1]*scale, img.shape[0]*scale), interpolation=cv2.INTER_LINEAR)\n                undo_fn = functools.partial(lq_resampling_undo_fn, scale)\n        elif 'color_shift' in aug:\n            \n            pass\n        elif 'interlacing' in aug:\n            \n            pass\n        elif 'chromatic_aberration' in aug:\n            \n            pass\n        elif 'noise' in aug:\n            \n            if 'noise-5' == aug:\n                noise_intensity = 5 / 255.0\n            else:\n                noise_intensity = (rand_val*6) / 255.0\n            img += np.random.rand(*img.shape) * noise_intensity\n        elif 'jpeg' in aug:\n            if 'noise' not in applied_augmentations and 'noise-5' not in applied_augmentations:\n                if aug == 'jpeg':\n                    lo=10\n                    range=20\n                elif aug == 'jpeg-low':\n                    lo=15\n                    range=10\n                elif aug == 'jpeg-medium':\n                    lo=23\n                    range=25\n                elif aug == 'jpeg-broad':\n                    lo=15\n                    range=60\n                elif aug == 'jpeg-normal':\n                    lo=47\n                    range=35\n                else:\n                    raise NotImplementedError(\"specified jpeg corruption doesn't exist\")\n                \n                qf = (int((1-rand_val)*range) + lo)\n                \n                img = (img * 255).astype(np.uint8)\n                img = Image.fromarray(img)\n                buffer = BytesIO()\n                img.save(buffer, \"JPEG\", quality=qf, optimize=True)\n                buffer.seek(0)\n                jpeg_img_bytes = np.asarray(bytearray(buffer.read()), dtype=\"uint8\")\n                img = read_img(\"buffer\", jpeg_img_bytes, rgb=True)\n        elif 'saturation' in aug:\n            \n            saturation = rand_val * .3\n            img = np.clip(img + saturation, a_max=1, a_min=0)\n        elif 'greyscale' in aug:\n            img = np.tile(np.mean(img, axis=2, keepdims=True), [1,1,3])\n        elif 'none' not in aug:\n            raise NotImplementedError(\"Augmentation doesn't exist\")\n\n        return img, undo_fn\n",
        "summary": "The provided Python code defines a class `ImageCorruptor` that applies various image corruptions to input images using different augmentation techniques such as color jittering, Gaussian blur, motion blur, noise addition, and JPEG compression. The class allows for both fixed and random corruptions based on configuration options, and it can return the corrupted images along with entropy values if requested."
    },
    {
        "code": "print(b\"%%\" % ())\nprint(b\"=%d=\" % 1)\nprint(b\"=%d=%d=\" % (1, 2))\n\nprint(b\"=%s=\" % b\"str\")\nprint(b\"=%r=\" % b\"str\")\n\nprint(\"PASS\")",
        "summary": "The Python code demonstrates the use of byte string formatting with placeholders for integers and strings, showcasing how to print formatted byte strings without any values provided. It concludes with a simple \"PASS\" statement indicating successful execution."
    },
    {
        "code": "import pytest\nimport albumentations as A\nfrom .context import TfDataAugmentation as Tfda\nfrom . import test_utils\nfrom .test_utils import TestResult\n\n\n@pytest.mark.parametrize(\n    \"quality_lower, quality_upper, expected, message\", [\n        \n        (-1, 100, TestResult.Error,\n         \"quality_lower < min => Error\"),\n        (0, 100, TestResult.OK,\n         \"quality_lower == min => OK\"),\n        (100, 100, TestResult.OK,\n         \"quality_lower == max => OK\"),\n        (101, 100, TestResult.Error,\n         \"quality_lower >= max => Error\"),\n\n        \n        (0, -1, TestResult.Error,\n         \"quality_upper < min => Error\"),\n        (0, 0, TestResult.OK,\n         \"quality_upper == min => OK\"),\n        (0, 100, TestResult.OK,\n         \"quality_upper == max => OK\"),\n        (0, 101, TestResult.Error,\n         \"quality_upper > max => Error\"),\n\n        \n        (50, 50, TestResult.OK,\n         \"quality_lower == quality_upper => OK\"),\n        (51, 50, TestResult.Error,\n         \"quality_lower > quality_upper => Error\"),\n    ])\ndef test_hue_shift_limit_value(\n        quality_lower, quality_upper, expected, message):\n    try:\n        Tfda.JpegCompression(\n            quality_lower=quality_lower,\n            quality_upper=quality_upper)\n        actual = TestResult.OK\n    except ValueError:\n        actual = TestResult.Error\n    assert expected == actual, message\n\n\ndef test_call():\n    quality_lower = 50\n    quality_upper = 100\n    tgt_jpeg = Tfda.JpegCompression(\n        quality_lower=quality_lower,\n        quality_upper=quality_upper,\n        p=1.0)\n    tgt_transform = \\\n        test_utils.make_tgt_transform(tgt_jpeg)\n    image = test_utils.make_test_image()\n\n    tgt_result = tgt_transform(image=image)\n    actual_image = tgt_result['image']\n\n    image_np = image.numpy()\n    quality = float(tgt_jpeg.get_param('quality'))\n    expected_image = A.image_compression(\n        image_np, quality, image_type='.jpg')\n\n    test_utils.partial_assert_array(\n        expected_image, actual_image, 0.6, \"image\", eps=0.1)\n",
        "summary": "The provided Python code includes tests for a JPEG compression transformation using the `TfDataAugmentation` library, specifically focusing on validating the quality range parameters and ensuring the transformation behaves as expected when applied to an image. The tests cover various scenarios for `quality_lower` and `quality_upper` values, including edge cases, and verify that the transformation either succeeds or raises a `ValueError` based on the input parameters. Additionally, there is a separate test function that applies the JPEG compression transformation to a mock image and compares the result with an expected output using a partial assertion for array comparison."
    },
    {
        "code": "import os\n\nfrom torch.utils.data import DataLoader\nfrom continuum.datasets import CIFAR10, InMemoryDataset\nfrom continuum.datasets import MNIST\nimport torchvision\nfrom continuum.scenarios import TransformationIncremental\nimport pytest\nimport numpy as np\n\nfrom continuum.transforms.bg_swap import BackgroundSwap\n\nDATA_PATH = os.environ.get(\"CONTINUUM_DATA_PATH\")\n\n\n\n\n\ndef test_bg_swap_fast():\n    \n    bg_x = np.ones(shape=[2, 5, 5, 3]) * -1\n    bg_y = np.random.rand(2)\n\n    fg = np.random.normal(loc=.5, scale=.1, size=[5, 5])\n    bg = InMemoryDataset(bg_x, bg_y)\n\n    bg_swap = BackgroundSwap(bg, input_dim=(5, 5), normalize_bg=None)\n\n    spliced_1_channel = bg_swap(fg)[:, :, 0]\n\n    assert np.array_equal((spliced_1_channel <= -1), (fg <= .5))\n\n\n@pytest.mark.slow\ndef test_background_swap_numpy():\n    \n    mnist = MNIST(DATA_PATH, download=True, train=True)\n    cifar = CIFAR10(DATA_PATH, download=True, train=True)\n\n    bg_swap = BackgroundSwap(cifar, input_dim=(28, 28))\n\n    im = mnist.get_data()[0][0]\n    im = bg_swap(im)\n\n    \n    \n    \n\n\n@pytest.mark.slow\ndef test_background_swap_torch():\n    \n    cifar = CIFAR10(DATA_PATH, download=True, train=True)\n\n    mnist = torchvision.datasets.MNIST(DATA_PATH, train=True, download=True,\n                                       transform=torchvision.transforms.Compose([\n                                           torchvision.transforms.ToTensor()\n                                       ]))\n\n    bg_swap = BackgroundSwap(cifar, input_dim=(28, 28))\n    im = mnist[0][0]\n\n    im = bg_swap(im)\n\n    \n    \n    \n\n\n@pytest.mark.slow\ndef test_background_tranformation():\n    \n    cifar = CIFAR10(DATA_PATH, train=True)\n    mnist = MNIST(DATA_PATH, download=False, train=True)\n    nb_task = 3\n    list_trsf = []\n    for i in range(nb_task):\n        list_trsf.append([torchvision.transforms.ToTensor(), BackgroundSwap(cifar, bg_label=i, input_dim=(28, 28)),\n                          torchvision.transforms.ToPILImage()])\n    scenario = TransformationIncremental(mnist, base_transformations=[torchvision.transforms.ToTensor()],\n                                         incremental_transformations=list_trsf)\n    folder = \"tests/samples/background_trsf/\"\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    for task_id, task_data in enumerate(scenario):\n        task_data.plot(path=folder, title=f\"background_{task_id}.jpg\", nb_samples=100, shape=[28, 28, 3])\n        loader = DataLoader(task_data)\n        _, _, _ = next(iter(loader))\n",
        "summary": "The provided Python code defines a series of tests for the `BackgroundSwap` transformation in the Continuum library. These tests include scenarios where the background is swapped with different datasets (MNIST and CIFAR10) using NumPy arrays, PyTorch tensors, and custom transformations, ensuring that the background swap operation behaves as expected."
    },
    {
        "code": "import os\n\n\nJQUERY_VERSION = \"1.6.2\"\nJQUERY_UI_VERSION = \"1.8.16\"\n\n\nDATE_TEXT_SIZE = 25\nTEXT_SIZE = 85\nTEXTAREA_COLS = 85\nTEXTAREA_ROWS_SHORT = 2\nTEXTAREA_ROWS_LONG = 4\nTEXTAREA_ROWS_XLONG = 10\nMAX_LENGTH_CHECKLIST_NOTES = 255\nEMAIL_LENGTH = 60\n\n\n_app_path = None\n_config_file = None\n_app_name = None\nsession_lock_dir = None\npublish_dir = None\n\n\ndef update_cache_values():\n    \n    global _app_path, _config_file, _app_name, session_lock_dir, publish_dir\n\n    if _app_path is None:\n        _app_path = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\n        _app_name = os.path.split(_app_path)[1]\n        _config_file = os.path.join(_app_path, '..', '..', 'config', _app_name + '.ini')\n        session_lock_dir = os.path.join(_app_path, 'python', 'session_lock')\n        publish_dir = os.path.join(_app_path, 'python', 'published_files')\n\n        try:\n            os.makedirs(session_lock_dir)\n        except os.error:\n            pass\n\n        try:\n            os.makedirs(publish_dir)\n        except os.error:\n            pass\n",
        "summary": "The provided Python code defines constants for various text sizes and lengths, as well as paths related to an application's directory structure. It includes a function `update_cache_values` that sets global variables with the absolute paths of the application's root directory, configuration file, session lock directory, and published files directory, creating these directories if they do not already exist."
    },
    {
        "code": "import _initpath\nimport pyradox\n\n\n\n\nresult = pyradox.parse()\n\nprint(result)\n",
        "summary": "The provided Python script imports necessary modules, parses data using the `pyradox` library, and prints the resulting output."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text or CSV format."
    },
    {
        "code": "DATABASE_ENGINE = 'django.db.backends.sqlite3'\nDATABASE_NAME = ':memory:'\n\n\nSECRET_KEY = 'unittest'\n\nDATABASES = {\n    'default': {\n        'ENGINE': DATABASE_ENGINE,\n        'NAME': DATABASE_NAME,\n    }\n}\n\n\nINSTALLED_APPS = ('django_app.adapters',)\nMIDDLEWARE_CLASSES = ()\n",
        "summary": "The provided Python code configures a Django project to use an in-memory SQLite database for testing purposes, with a secret key set for the unittest environment. It specifies a single installed app and no middleware classes."
    },
    {
        "code": "import copy\nimport math\nfrom typing import Iterable, List, Optional, Tuple\n\nfrom ezdxf import colors\nfrom ezdxf.entities import MText\nfrom ezdxf.lldxf import const\nfrom ezdxf.math import Matrix44, Vec3\nfrom ezdxf.render.abstract_mtext_renderer import AbstractMTextRenderer\nfrom ezdxf.tools import text_layout as tl, fonts\nfrom ezdxf.tools.text import MTextContext\nfrom .backend import BackendInterface\nfrom .properties import Properties, RenderContext, rgb_to_hex\nfrom .type_hints import Color\n\n__all__ = [\"complex_mtext_renderer\"]\n\n\ndef corner_vertices(\n    left: float,\n    bottom: float,\n    right: float,\n    top: float,\n    m: Matrix44 = None,\n) -> Iterable[Vec3]:\n    corners = [  \n        (left, top),\n        (right, top),\n        (right, bottom),\n        (left, bottom),\n        (left, top),\n    ]\n    if m is None:\n        return Vec3.generate(corners)\n    else:\n        return m.transform_vertices(corners)\n\n\nclass FrameRenderer(tl.ContentRenderer):\n    def __init__(self, properties: Properties, backend: BackendInterface):\n        self.properties = properties\n        self.backend = backend\n\n    def render(\n        self,\n        left: float,\n        bottom: float,\n        right: float,\n        top: float,\n        m: Matrix44 = None,\n    ) -> None:\n        self._render_outline(list(corner_vertices(left, bottom, right, top, m)))\n\n    def _render_outline(self, vertices: List[Vec3]) -> None:\n        backend = self.backend\n        properties = self.properties\n        prev = vertices.pop(0)\n        for vertex in vertices:\n            backend.draw_line(prev, vertex, properties)\n            prev = vertex\n\n    def line(\n        self, x1: float, y1: float, x2: float, y2: float, m: Matrix44 = None\n    ) -> None:\n        points = [(x1, y1), (x2, y2)]\n        if m is not None:\n            p1, p2 = m.transform_vertices(points)\n        else:\n            p1, p2 = Vec3.generate(points)\n        self.backend.draw_line(p1, p2, self.properties)\n\n\nclass ColumnBackgroundRenderer(FrameRenderer):\n    def __init__(\n        self,\n        properties: Properties,\n        backend: BackendInterface,\n        bg_properties: Properties = None,\n        offset: float = 0,\n        text_frame: bool = False,\n    ):\n        super().__init__(properties, backend)\n        self.bg_properties = bg_properties\n        self.offset = offset  \n        self.has_text_frame = text_frame\n\n    def render(\n        self,\n        left: float,\n        bottom: float,\n        right: float,\n        top: float,\n        m: Matrix44 = None,\n    ) -> None:\n        \n        \n        offset = self.offset\n        vertices = list(\n            corner_vertices(\n                left - offset, bottom - offset, right + offset, top + offset, m\n            )\n        )\n        if self.bg_properties is not None:\n            self.backend.draw_filled_polygon(vertices, self.bg_properties)\n        if self.has_text_frame:\n            self._render_outline(vertices)\n\n\nclass TextRenderer(FrameRenderer):\n    \n\n    def __init__(\n        self,\n        text: str,\n        cap_height: float,\n        width_factor: float,\n        oblique: float,  \n        properties: Properties,\n        backend: BackendInterface,\n    ):\n        super().__init__(properties, backend)\n        self.text = text\n        self.cap_height = cap_height\n        self.width_factor = width_factor\n        self.oblique = oblique  \n\n    def render(\n        self,\n        left: float,\n        bottom: float,\n        right: float,\n        top: float,\n        m: Matrix44 = None,\n    ):\n        \n        sx = 1.0\n        tx = 0.0\n        if not math.isclose(self.width_factor, 1.0, rel_tol=1e-6):\n            sx = self.width_factor\n        if abs(self.oblique) > 1e-3:  \n            tx = math.tan(math.radians(self.oblique))\n        \n        t = Matrix44((\n            sx, 0.0, 0.0, 0.0,\n            tx, 1.0, 0.0, 0.0,\n            0.0, 0.0, 1.0, 0.0,\n            left, bottom, 0.0, 1.0\n        ))\n        \n        if m is not None:\n            t *= m\n        self.backend.draw_text(self.text, t, self.properties, self.cap_height)\n\n\ndef complex_mtext_renderer(\n    ctx: RenderContext, backend: BackendInterface, mtext: MText, properties: Properties\n) -> None:\n    cmr = ComplexMTextRenderer(ctx, backend, properties)\n    align = tl.LayoutAlignment(mtext.dxf.attachment_point)\n    layout_engine = cmr.layout_engine(mtext)\n    layout_engine.place(align=align)\n    layout_engine.render(mtext.ucs().matrix)\n\n\nclass ComplexMTextRenderer(AbstractMTextRenderer):\n    def __init__(\n        self,\n        ctx: RenderContext,\n        backend: BackendInterface,\n        properties: Properties,\n    ):\n        super().__init__()\n        self._render_ctx = ctx\n        self._backend = backend\n        self._properties = properties\n\n    \n\n    def word(self, text: str, ctx: MTextContext) -> tl.ContentCell:\n        return tl.Text(\n            width=self.get_font(ctx).text_width(text),\n            height=ctx.cap_height,\n            valign=tl.CellAlignment(ctx.align),\n            stroke=self.get_stroke(ctx),\n            renderer=TextRenderer(\n                text,\n                ctx.cap_height,\n                ctx.width_factor,\n                ctx.oblique,\n                self.new_text_properties(self._properties, ctx),\n                self._backend,\n            ))\n\n    def fraction(\n        self, data: Tuple[str, str, str], ctx: MTextContext\n    ) -> tl.ContentCell:\n        upr, lwr, type_ = data\n        if type_:\n            return tl.Fraction(\n                top=self.word(upr, ctx),\n                bottom=self.word(lwr, ctx),\n                stacking=self.get_stacking(type_),\n                \n                renderer=FrameRenderer(self._properties, self._backend),\n            )\n        else:\n            return self.word(upr, ctx)\n\n    def get_font_face(self, mtext: MText) -> fonts.FontFace:\n        return self._properties.font  \n\n    def make_bg_renderer(self, mtext: MText) -> tl.ContentRenderer:\n        dxf = mtext.dxf\n        bg_fill = dxf.get(\"bg_fill\", 0)\n\n        bg_aci = None\n        bg_true_color = None\n        bg_properties: Optional[Properties] = None\n        has_text_frame = False\n        offset = 0\n        if bg_fill:\n            \n            \n            \n            offset = dxf.char_height * (dxf.get(\"box_fill_scale\", 1.5) - 1)\n            if bg_fill & const.MTEXT_BG_COLOR:\n                if dxf.hasattr(\"bg_fill_color\"):\n                    bg_aci = dxf.bg_fill_color\n\n                if dxf.hasattr(\"bg_fill_true_color\"):\n                    bg_aci = None\n                    bg_true_color = dxf.bg_fill_true_color\n\n                if (bg_fill & 3) == 3:  \n                    \n                    \n                    bg_aci = None\n                    bg_true_color = None\n\n            if bg_fill & const.MTEXT_TEXT_FRAME:\n                has_text_frame = True\n            bg_properties = self.new_bg_properties(bg_aci, bg_true_color)\n\n        return ColumnBackgroundRenderer(\n            self._properties,\n            self._backend,\n            bg_properties,\n            offset=offset,\n            text_frame=has_text_frame,\n        )\n\n    \n\n    @property\n    def backend(self) -> BackendInterface:\n        return self._backend\n\n    def resolve_aci_color(self, aci: int) -> Color:\n        return self._render_ctx.resolve_aci_color(aci, self._properties.layer)\n\n    def new_text_properties(\n        self, properties: Properties, ctx: MTextContext\n    ) -> Properties:\n        new_properties = copy.copy(properties)\n        if ctx.rgb is None:\n            new_properties.color = self.resolve_aci_color(ctx.aci)\n        else:\n            new_properties.color = rgb_to_hex(ctx.rgb)\n        new_properties.font = ctx.font_face\n        return new_properties\n\n    def new_bg_properties(\n        self, aci: Optional[int], true_color: Optional[int]\n    ) -> Properties:\n        new_properties = copy.copy(self._properties)\n        new_properties.color = (  \n            self._render_ctx.current_layout_properties.background_color\n        )\n        if true_color is None:\n            if aci is not None:\n                new_properties.color = self.resolve_aci_color(aci)\n            \n        else:\n            new_properties.color = rgb_to_hex(colors.int2rgb(true_color))\n        return new_properties\n",
        "summary": "The provided Python code defines a complex MText renderer for the ezdxf library, which handles rendering of multiline text with various formatting options such as background colors, frames, and oblique angles. It includes classes for rendering text, frames, and backgrounds, as well as a main function to orchestrate the rendering process based on the provided context and properties."
    },
    {
        "code": "from tests.utils import default_board_id, default_repo_id, default_list_id, \\\n    default_issue_id, default_card_id, default_pull_request_id\nfrom app import db\nfrom app.models import Board, Issue, List, PullRequest, Repo, Subscription, \\\n    SubscribedList\n\n\ndef create_board():\n    \n    db.session.add(\n        Board(\n            name='board_name',\n            url=f\"https://trello.com/b/{default_board_id}\",\n            trello_board_id=default_board_id\n        )\n    )\n\n\ndef create_repo():\n    \n    db.session.add(\n        Repo(\n            name='repo_name',\n            url='https://github.com/user/repo',\n            github_repo_id=default_repo_id\n        )\n    )\n\n\ndef create_list():\n    \n    db.session.add(\n        List(\n            name='list_name',\n            trello_list_id=default_list_id,\n            board_id=default_board_id\n        )\n    )\n\n\ndef create_subscription(issue_autocard=True, pull_request_autocard=True):\n    \n    db.session.add(\n        Subscription(\n            board_id=default_board_id,\n            repo_id=default_repo_id,\n            issue_autocard=issue_autocard,\n            pull_request_autocard=pull_request_autocard\n        )\n    )\n\n\ndef create_subscribed_list():\n    \n    db.session.add(\n        SubscribedList(\n            subscription_board_id=default_board_id,\n            subscription_repo_id=default_repo_id,\n            list_id=default_list_id\n        )\n    )\n\n\ndef create_issue():\n    \n    db.session.add(\n        Issue(\n            name='Test adding a new issue',\n            url='https://github.com/a-organization/a-repo/issues/56',\n            github_issue_id=default_issue_id,\n            repo_id=default_repo_id,\n            trello_board_id=default_board_id,\n            trello_card_id=default_card_id\n        )\n    )\n\n\ndef create_pull_request():\n    \n    db.session.add(\n        PullRequest(\n            name='Update README.md',\n            url='https://github.com/a-organization/a-repo/pulls/57',\n            github_pull_request_id=default_pull_request_id,\n            repo_id=default_repo_id,\n            trello_board_id=default_board_id,\n            trello_card_id=default_card_id\n        )\n    )\n",
        "summary": "The provided Python code defines functions to create instances of various database models related to a project management system, including boards, repositories, lists, issues, pull requests, and subscriptions. Each function adds a new instance of the corresponding model to the database session using SQLAlchemy ORM."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('postcode_api', '0003_populate_postcode_area'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='LocalAuthority',\n            fields=[\n                ('gss_code', models.CharField(\n                    max_length=9, serialize=False,\n                    primary_key=True, db_index=True)),\n                ('name', models.CharField(max_length=128,\n                                          db_index=True)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='PostcodeGssCode',\n            fields=[\n                ('postcode_index', models.CharField(\n                    max_length=7, db_index=True)),\n                ('local_authority_gss_code', models.CharField(\n                    max_length=9, serialize=False,\n                    primary_key=True, db_index=True)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterField(\n            model_name='address',\n            name='postcode_area',\n            field=models.CharField(\n                default=b'', max_length=4, db_index=True, blank=True),\n            preserve_default=True,\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration for creating two new models, `LocalAuthority` and `PostcodeGssCode`, and altering an existing model's field. The migration depends on another migration in the 'postcode_api' app."
    },
    {
        "code": "from collections import namedtuple\nfrom supervisely_lib.api.module_api import ApiField, ModuleApi\nfrom supervisely_lib._utils import camel_to_snake\n\n\nclass PluginApi(ModuleApi):\n    _info_sequence = [ApiField.ID,\n                      ApiField.NAME,\n                      ApiField.DESCRIPTION,\n                      ApiField.TYPE,\n                      ApiField.DEFAULT_VERSION,\n                      ApiField.DOCKER_IMAGE,\n                      ApiField.README,\n                      ApiField.CONFIGS,\n                      ApiField.VERSIONS,\n                      ApiField.CREATED_AT,\n                      ApiField.UPDATED_AT]\n    Info = namedtuple('PluginInfo', [camel_to_snake(name) for name in _info_sequence])\n\n    def get_list(self, team_id, filters=None):\n        return self.get_list_all_pages('plugins.list',  {ApiField.TEAM_ID: team_id, ApiField.FILTER: filters or []})\n\n    def get_info_by_id(self, team_id, plugin_id):\n        filters = [{\"field\": ApiField.ID, \"operator\": \"=\", \"value\": plugin_id}]\n        return self._get_info_by_filters(team_id, filters)\n",
        "summary": "The `PluginApi` class extends `ModuleApi` to manage plugins within a Supervisely team, providing methods to retrieve lists of plugins and specific plugin information by ID. It uses namedtuples for structured data representation and handles API requests with pagination support."
    },
    {
        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nfrom fairseq.modules import (\n    MaskedConvolution, MultiheadMaskedConvolution\n)\n\n\nclass ExpandingResNet(nn.Module):\n    \n\n    def __init__(self, num_init_features, args):\n        super().__init__()\n        num_layers = args.num_layers\n        num_features = num_init_features\n        self.reduce_channels = Linear(num_features, num_features // args.divide_channels) if args.divide_channels > 1 else None\n        num_features = num_features // args.divide_channels\n        self.output_channels = num_features\n        self.add_up_scale = 1 / (num_layers + 1)\n\n        self.residual_blocks = nn.ModuleList([])\n        for i in range(num_layers):\n            kernel_size = 2 * (i + 1) + 1\n            print('Layer ', i, kernel_size)\n            self.residual_blocks.append(_ResLayer(num_features, kernel_size, args))\n        \n    def forward(self, x, \n                encoder_mask=None,\n                decoder_mask=None,\n                incremental_state=None):\n        \n        if self.reduce_channels is not None:\n            x = self.reduce_channels(x)\n        add_up = self.add_up_scale * x\n        for layer in self.residual_blocks:\n            x = layer(x,\n                      encoder_mask=encoder_mask,\n                      decoder_mask=decoder_mask,\n                      incremental_state=incremental_state)\n            add_up += self.add_up_scale * x\n        return add_up\n\n\nclass _ResLayer(nn.Module):\n    \n\n    def __init__(self, num_features, kernel_size, args):\n        super().__init__()\n        self.drop_rate = args.convolution_dropout\n        ffn_dim = args.ffn_dim\n        mid_features = args.reduce_dim\n        stride = args.conv_stride  \n        dilsrc = args.source_dilation\n        diltrg = args.target_dilation\n        resolution = args.maintain_resolution\n        if resolution:\n            if not stride == 1:\n                raise ValueError('Could not maintain the resolution with stride=%d' % stride)\n\n            \n            padding_trg = diltrg * (kernel_size - 1) // 2\n            padding_src = dilsrc * (kernel_size - 1) // 2\n            padding = (padding_trg, padding_src)\n        else:\n            \n            padding = (diltrg * (kernel_size - 1) // 2, 0)\n\n        \n        self.conv1 = nn.Conv2d(num_features,\n                               mid_features,\n                               kernel_size=1,\n                               stride=1,\n                               bias=False)\n\n        self.mconv2 = MaskedConvolution(\n            mid_features, num_features,\n            kernel_size, args,\n            padding=padding,\n        )\n        self.fc1 = Linear(num_features, ffn_dim)\n        self.fc2 = Linear(ffn_dim, num_features)\n        self.scale = 0.5 ** .5\n\n    def forward(self, x, \n                encoder_mask=None,\n                decoder_mask=None,\n                incremental_state=None):\n        residual = x\n        x = x.permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        \n        x = self.mconv2(x, incremental_state)\n        if self.training:\n            if encoder_mask is not None:\n                x = x.masked_fill(encoder_mask.unsqueeze(1).unsqueeze(1), 0)\n            if decoder_mask is not None:\n                x = x.masked_fill(decoder_mask.unsqueeze(1).unsqueeze(-1), 0)\n\n        if self.drop_rate:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = x.permute(0, 2, 3, 1)\n        x = self.scale * (x + residual)  \n        \n        residual = x\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        if self.drop_rate:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.scale * (x + residual)\n        return x\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.)\n    return m\n\n",
        "summary": "The provided Python code defines a neural network architecture using PyTorch and Fairseq modules. It includes an `ExpandingResNet` class that consists of multiple residual blocks, each containing masked convolution layers and feed-forward networks. The architecture is designed to handle tasks such as image processing or sequence modeling with the ability to reduce channel dimensions and maintain resolution through specific configurations."
    },
    {
        "code": "from resolve import resolve\n\n\n\n\nimport sys\nfrom io import StringIO\nimport unittest\n\nclass TestClass(unittest.TestCase):\n    def assertIO(self, input, output):\n        stdout, stdin = sys.stdout, sys.stdin\n        sys.stdout, sys.stdin = StringIO(), StringIO(input)\n        resolve()\n        sys.stdout.seek(0)\n        out = sys.stdout.read()[:-1]\n        sys.stdout, sys.stdin = stdout, stdin\n        print('------------')\n        print(out)\n        print('------------')\n        self.assertEqual(out, output)\n    def test_\u5165\u529b\u4f8b_1(self):\n        input = \n        output = \n        self.assertIO(input, output)\n    def test_\u5165\u529b\u4f8b_2(self):\n        input = \n        output = \n        self.assertIO(input, output)\n\n    def test_\u5165\u529b\u4f8b_3(self):\n        input = \n        output = \n        self.assertIO(input, output)\n\nif __name__ == \"__main__\":\n    unittest.main()",
        "summary": "The provided Python code defines a unit test class `TestClass` that uses the `unittest` framework to test a function named `resolve`. The class includes methods for capturing and comparing standard input and output, as well as specific test methods for different input scenarios."
    },
    {
        "code": "from catalyst.contrib.datasets.misc_cv import ImageClassificationDataset\n\n\nclass Imagewang(ImageClassificationDataset):\n    \n\n    name = \"imagewang\"\n    resources = [\n        (\n            \"https://s3.amazonaws.com/fast-ai-imageclas/imagewang.tgz\",\n            \"46f9749616a29837e7cd67b103396f6e\",\n        )\n    ]\n\n\nclass Imagewang160(ImageClassificationDataset):\n    \n\n    name = \"imagewang-160\"\n    resources = [\n        (\n            \"https://s3.amazonaws.com/fast-ai-imageclas/imagewang-160.tgz\",\n            \"1dc388d37d1dc52836c06749e14e37bc\",\n        )\n    ]\n\n\nclass Imagewang320(ImageClassificationDataset):\n    \n\n    name = \"imagewang-320\"\n    resources = [\n        (\n            \"https://s3.amazonaws.com/fast-ai-imageclas/imagewang-320.tgz\",\n            \"ff01d7c126230afce776bdf72bda87e6\",\n        )\n    ]\n\n\n__all__ = [\"Imagewang\", \"Imagewang160\", \"Imagewang320\"]\n",
        "summary": "The provided Python code defines three classes, `Imagewang`, `Imagewang160`, and `Imagewang320`, each inheriting from `ImageClassificationDataset`. These classes are designed to handle image classification tasks using datasets named \"imagewang\", \"imagewang-160\", and \"imagewang-320\" respectively, which are hosted on an Amazon S3 bucket. Each class specifies the URL of the dataset archive and its corresponding MD5 checksum for verification."
    },
    {
        "code": "from __future__ import print_function\nimport sys\nimport warnings\n\nif sys.version >= '3':\n    basestring = unicode = str\n\nfrom pyspark import since\nfrom pyspark.rdd import ignore_unicode_prefix\nfrom pyspark.sql.session import _monkey_patch_RDD, SparkSession\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.readwriter import DataFrameReader\nfrom pyspark.sql.streaming import DataStreamReader\nfrom pyspark.sql.types import Row, StringType\nfrom pyspark.sql.utils import install_exception_handler\n\n__all__ = [\"SQLContext\", \"HiveContext\", \"UDFRegistration\"]\n\n\nclass SQLContext(object):\n    \n\n    _instantiatedContext = None\n\n    @ignore_unicode_prefix\n    def __init__(self, sparkContext, sparkSession=None, jsqlContext=None):\n        \n        self._sc = sparkContext\n        self._jsc = self._sc._jsc\n        self._jvm = self._sc._jvm\n        if sparkSession is None:\n            sparkSession = SparkSession(sparkContext)\n        if jsqlContext is None:\n            jsqlContext = sparkSession._jwrapped\n        self.sparkSession = sparkSession\n        self._jsqlContext = jsqlContext\n        _monkey_patch_RDD(self.sparkSession)\n        install_exception_handler()\n        if SQLContext._instantiatedContext is None:\n            SQLContext._instantiatedContext = self\n\n    @property\n    def _ssql_ctx(self):\n        \n        return self._jsqlContext\n\n    @classmethod\n    @since(1.6)\n    def getOrCreate(cls, sc):\n        \n        if cls._instantiatedContext is None:\n            jsqlContext = sc._jvm.SQLContext.getOrCreate(sc._jsc.sc())\n            sparkSession = SparkSession(sc, jsqlContext.sparkSession())\n            cls(sc, sparkSession, jsqlContext)\n        return cls._instantiatedContext\n\n    @since(1.6)\n    def newSession(self):\n        \n        return self.__class__(self._sc, self.sparkSession.newSession())\n\n    @since(1.3)\n    def setConf(self, key, value):\n        \n        self.sparkSession.conf.set(key, value)\n\n    @ignore_unicode_prefix\n    @since(1.3)\n    def getConf(self, key, defaultValue=None):\n        \n        return self.sparkSession.conf.get(key, defaultValue)\n\n    @property\n    @since(\"1.3.1\")\n    def udf(self):\n        \n        return UDFRegistration(self)\n\n    @since(1.4)\n    def range(self, start, end=None, step=1, numPartitions=None):\n        \n        return self.sparkSession.range(start, end, step, numPartitions)\n\n    @ignore_unicode_prefix\n    @since(1.2)\n    def registerFunction(self, name, f, returnType=StringType()):\n        \n        self.sparkSession.catalog.registerFunction(name, f, returnType)\n\n    \n    def _inferSchema(self, rdd, samplingRatio=None):\n        \n        return self.sparkSession._inferSchema(rdd, samplingRatio)\n\n    @since(1.3)\n    @ignore_unicode_prefix\n    def createDataFrame(self, data, schema=None, samplingRatio=None):\n        \n        return self.sparkSession.createDataFrame(data, schema, samplingRatio)\n\n    @since(1.3)\n    def registerDataFrameAsTable(self, df, tableName):\n        \n        df.createOrReplaceTempView(tableName)\n\n    @since(1.6)\n    def dropTempTable(self, tableName):\n        \n        self.sparkSession.catalog.dropTempView(tableName)\n\n    @since(1.3)\n    def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):\n        \n        return self.sparkSession.catalog.createExternalTable(\n            tableName, path, source, schema, **options)\n\n    @ignore_unicode_prefix\n    @since(1.0)\n    def sql(self, sqlQuery):\n        \n        return self.sparkSession.sql(sqlQuery)\n\n    @since(1.0)\n    def table(self, tableName):\n        \n        return self.sparkSession.table(tableName)\n\n    @ignore_unicode_prefix\n    @since(1.3)\n    def tables(self, dbName=None):\n        \n        if dbName is None:\n            return DataFrame(self._ssql_ctx.tables(), self)\n        else:\n            return DataFrame(self._ssql_ctx.tables(dbName), self)\n\n    @since(1.3)\n    def tableNames(self, dbName=None):\n        \n        if dbName is None:\n            return [name for name in self._ssql_ctx.tableNames()]\n        else:\n            return [name for name in self._ssql_ctx.tableNames(dbName)]\n\n    @since(1.0)\n    def cacheTable(self, tableName):\n        \n        self._ssql_ctx.cacheTable(tableName)\n\n    @since(1.0)\n    def uncacheTable(self, tableName):\n        \n        self._ssql_ctx.uncacheTable(tableName)\n\n    @since(1.3)\n    def clearCache(self):\n        \n        self._ssql_ctx.clearCache()\n\n    @property\n    @since(1.4)\n    def read(self):\n        \n        return DataFrameReader(self)\n\n    @property\n    @since(2.0)\n    def readStream(self):\n        \n        return DataStreamReader(self)\n\n    @property\n    @since(2.0)\n    def streams(self):\n        \n        from pyspark.sql.streaming import StreamingQueryManager\n        return StreamingQueryManager(self._ssql_ctx.streams())\n\n\nclass HiveContext(SQLContext):\n    \n\n    def __init__(self, sparkContext, jhiveContext=None):\n        warnings.warn(\n            \"HiveContext is deprecated in Spark 2.0.0. Please use \" +\n            \"SparkSession.builder.enableHiveSupport().getOrCreate() instead.\",\n            DeprecationWarning)\n        if jhiveContext is None:\n            sparkSession = SparkSession.builder.enableHiveSupport().getOrCreate()\n        else:\n            sparkSession = SparkSession(sparkContext, jhiveContext.sparkSession())\n        SQLContext.__init__(self, sparkContext, sparkSession, jhiveContext)\n\n    @classmethod\n    def _createForTesting(cls, sparkContext):\n        \n        jsc = sparkContext._jsc.sc()\n        jtestHive = sparkContext._jvm.org.apache.spark.sql.hive.test.TestHiveContext(jsc, False)\n        return cls(sparkContext, jtestHive)\n\n    def refreshTable(self, tableName):\n        \n        self._ssql_ctx.refreshTable(tableName)\n\n\nclass UDFRegistration(object):\n    \n\n    def __init__(self, sqlContext):\n        self.sqlContext = sqlContext\n\n    def register(self, name, f, returnType=StringType()):\n        return self.sqlContext.registerFunction(name, f, returnType)\n\n    register.__doc__ = SQLContext.registerFunction.__doc__\n\n\ndef _test():\n    import os\n    import doctest\n    import tempfile\n    from pyspark.context import SparkContext\n    from pyspark.sql import Row, SQLContext\n    import pyspark.sql.context\n\n    os.chdir(os.environ[\"SPARK_HOME\"])\n\n    globs = pyspark.sql.context.__dict__.copy()\n    sc = SparkContext('local[4]', 'PythonTest')\n    globs['tempfile'] = tempfile\n    globs['os'] = os\n    globs['sc'] = sc\n    globs['sqlContext'] = SQLContext(sc)\n    globs['rdd'] = rdd = sc.parallelize(\n        [Row(field1=1, field2=\"row1\"),\n         Row(field1=2, field2=\"row2\"),\n         Row(field1=3, field2=\"row3\")]\n    )\n    globs['df'] = rdd.toDF()\n    jsonStrings = [\n        '{\"field1\": 1, \"field2\": \"row1\", \"field3\":{\"field4\":11}}',\n        '{\"field1\" : 2, \"field3\":{\"field4\":22, \"field5\": [10, 11]},'\n        '\"field6\":[{\"field7\": \"row2\"}]}',\n        '{\"field1\" : null, \"field2\": \"row3\", '\n        '\"field3\":{\"field4\":33, \"field5\": []}}'\n    ]\n    globs['jsonStrings'] = jsonStrings\n    globs['json'] = sc.parallelize(jsonStrings)\n    (failure_count, test_count) = doctest.testmod(\n        pyspark.sql.context, globs=globs,\n        optionflags=doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE)\n    globs['sc'].stop()\n    if failure_count:\n        exit(-1)\n\n\nif __name__ == \"__main__\":\n    _test()\n",
        "summary": "The provided Python code defines classes and methods for interacting with Apache Spark's SQL functionality, including creating and managing Spark sessions, executing SQL queries, registering user-defined functions (UDFs), and handling dataframes. It also includes a deprecated class `HiveContext` for working with Hive tables in Spark. The code is structured to support both Python 2 and Python 3 environments."
    },
    {
        "code": "import unittest\n\nfrom src.google_foobar.P008_carrotland.solution_01 import answer\n\n\nclass TestSolution(unittest.TestCase):\n    def testcase_001(self):\n        vertices = [[2, 3], [6, 9], [10, 160]]\n        expected = 289\n        self.assertEqual(answer(vertices), expected)\n\n    def testcase_002(self):\n        vertices = [[91207, 89566], [-88690, -83026], [67100, 47194]]\n        expected = 1730960165\n        self.assertEqual(answer(vertices), expected)\n\n    def testcase_003(self):\n        vertices = [[0, 0], [0, 1], [1, 0]]\n        expected = 0\n        self.assertEqual(answer(vertices), expected)\n\n    \n    def testcase_004(self):\n        vertices = [[-1, -1], [1, 0], [0, 1]]\n        expected = 1\n        self.assertEqual(answer(vertices), expected)\n\n    \n    def testcase_005(self):\n        vertices = [[0, 0], [0, 10], [10, 0]]\n        expected = 36\n        self.assertEqual(answer(vertices), expected)\n\n    \n    def testcase_006(self):\n        vertices = [[1, 1], [4, 10], [10, 6]]\n        expected = 31\n        self.assertEqual(answer(vertices), expected)\n\n    \n    def testcase_007(self):\n        vertices = [[-5, 4], [4, 6], [3, -3]]\n        expected = 39\n        self.assertEqual(answer(vertices), expected)\n\n    \n    def testcase_008(self):\n        vertices = [[-5, -3], [5, -3], [0, 6]]\n        expected = 40\n        self.assertEqual(answer(vertices), expected)\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "The provided Python code defines a series of test cases using the `unittest` framework to validate the correctness of a function named `answer` from the module `src.google_foobar.P008_carrotland.solution_01`. Each test case checks if the `answer` function returns the expected value for different sets of input vertices, which represent points in a 2D plane."
    },
    {
        "code": "from oslo_config import cfg\nfrom oslo_log import log as logging\nimport oslo_messaging as messaging\nimport six\n\nfrom jacket import rpc\nfrom jacket.compute import test\nfrom jacket.tests.compute import fixtures\n\nLOG = logging.getLogger(__name__)\n\nCONF = cfg.CONF\nCONF.import_opt('use_local', 'compute.conductor.api', group='conductor')\n\n\nclass IsolationTestCase(test.TestCase):\n    \n    def test_service_isolation(self):\n        self.flags(use_local=True, group='conductor')\n        self.useFixture(fixtures.ServiceFixture('compute'))\n\n    def test_rpc_consumer_isolation(self):\n        class NeverCalled(object):\n\n            def __getattribute__(*args):\n                assert False, \"I should never get called.\"\n\n        server = rpc.get_server(messaging.Target(topic='compute',\n                                                 server=CONF.host),\n                                endpoints=[NeverCalled()])\n        server.start()\n\n\nclass JsonTestCase(test.NoDBTestCase):\n    def test_json_equal(self):\n        expected = {\n            \"employees\": [\n                {\"firstName\": \"Anna\", \"lastName\": \"Smith\"},\n                {\"firstName\": \"John\", \"lastName\": \"Doe\"},\n                {\"firstName\": \"Peter\", \"lastName\": \"Jones\"}\n            ],\n            \"locations\": set(['Boston', 'Mumbai', 'Beijing', 'Perth'])\n        }\n        observed = \n        self.assertJsonEqual(expected, observed)\n\n    def test_json_equal_fail_on_length(self):\n        expected = {\n            'top': {\n                'l1': {\n                    'l2': ['a', 'b', 'c']\n                }\n            }\n        }\n        observed = {\n            'top': {\n                'l1': {\n                    'l2': ['c', 'a', 'b', 'd']\n                }\n            }\n        }\n        try:\n            self.assertJsonEqual(expected, observed)\n        except Exception as e:\n            \n            \n            self.assertEqual(e.mismatch.describe(), \"3 != 4\")\n            self.assertIn(\n                \"Matchee: {'top': {'l1': {'l2': ['c', 'a', 'b', 'd']}}}\",\n                six.text_type(e))\n            self.assertIn(\n                \"Matcher: {'top': {'l1': {'l2': ['a', 'b', 'c']}}}\",\n                six.text_type(e))\n        else:\n            self.fail(\"This should have raised a mismatch exception\")\n\n    def test_json_equal_fail_on_inner(self):\n        expected = {\n            'top': {\n                'l1': {\n                    'l2': ['a', 'b', 'c']\n                }\n            }\n        }\n        observed = {\n            'top': {\n                'l1': {\n                    'l2': ['c', 'a', 'd']\n                }\n            }\n        }\n        try:\n            self.assertJsonEqual(expected, observed)\n        except Exception as e:\n            \n            \n            self.assertEqual(e.mismatch.describe(), \"'b' != 'c'\")\n            self.assertIn(\n                \"Matchee: {'top': {'l1': {'l2': ['c', 'a', 'd']}}}\",\n                six.text_type(e))\n            self.assertIn(\n                \"Matcher: {'top': {'l1': {'l2': ['a', 'b', 'c']}}}\",\n                six.text_type(e))\n        else:\n            self.fail(\"This should have raised a mismatch exception\")\n\n\nclass BadLogTestCase(test.NoDBTestCase):\n    \n\n    def test_bad_debug_log(self):\n        self.assertRaises(KeyError,\n            LOG.debug, \"this is a misformated %(log)s\", {'nothing': 'nothing'})\n\n\nclass MatchTypeTestCase(test.NoDBTestCase):\n\n    def test_match_type_simple(self):\n        matcher = test.MatchType(dict)\n\n        self.assertEqual(matcher, {})\n        self.assertEqual(matcher, {\"hello\": \"world\"})\n        self.assertEqual(matcher, {\"hello\": [\"world\"]})\n        self.assertNotEqual(matcher, [])\n        self.assertNotEqual(matcher, [{\"hello\": \"world\"}])\n        self.assertNotEqual(matcher, 123)\n        self.assertNotEqual(matcher, \"foo\")\n\n    def test_match_type_object(self):\n        class Hello(object):\n            pass\n\n        class World(object):\n            pass\n\n        matcher = test.MatchType(Hello)\n\n        self.assertEqual(matcher, Hello())\n        self.assertNotEqual(matcher, World())\n        self.assertNotEqual(matcher, 123)\n        self.assertNotEqual(matcher, \"foo\")\n",
        "summary": "The provided Python code defines several test cases for various functionalities including service isolation, RPC consumer isolation, JSON equality checks, bad log handling, and matching types. Each test case is part of a larger testing framework designed to validate specific aspects of a compute system's behavior under different conditions."
    },
    {
        "code": "from keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nfrom .keras_base import KerasBaseExp\nfrom .keras_base import exp_bag_of_strokes\n\nfrom .blocks import fc_branch, final_type1\n\n\nclass mlp_type1(KerasBaseExp):\n    def initialize_model(self, in_dims, out_dims):\n        input_layer = [Input(shape=(d, )) for d in in_dims]\n\n        if len(input_layer) > 1:\n            layer = Concatenate()(input_layer)\n        else:\n            layer = input_layer[0]\n\n        layer = fc_branch(layer, self.decay)\n\n        self.model = Model(inputs=input_layer, outputs=final_type1(layer, out_dims))\n        opt = Adam(lr=self.learning_rate)\n        self.model.compile(optimizer=opt, metrics=['accuracy'], loss='categorical_crossentropy')\n\n\nclass EXP1(mlp_type1, exp_bag_of_strokes):\n    pass\n",
        "summary": "The provided Python code defines a Keras-based neural network model class `mlp_type1` that inherits from `KerasBaseExp`. This model is designed to handle multiple input dimensions by concatenating them if more than one is present. It then passes the concatenated layer through a fully connected branch and outputs using a specific final type function, compiling with the Adam optimizer and categorical crossentropy loss. The `EXP1` class extends `mlp_type1` and `exp_bag_of_strokes`, inheriting their functionalities without modification."
    },
    {
        "code": "from __future__ import (absolute_import, division, print_function)\ntry: \n    import requests\n    import json\n    import ipaddress\nexcept:\n    raise ImportError(\"Requests module not found\")\n\n__metaclass__ = type\n\nclass Request(object):\n    \n    def __init__(self,baseUrl, token):\n        \n        self.baseUrl = baseUrl\n        self.token = token\n\n    def get(self,endpoint,data={}):\n        \n        try:\n            headers = {'Authorization': 'Token {}'.format(self.token)}\n            url = '{}{}'.format(self.baseUrl, endpoint)\n            result = requests.get(url, json.dumps(data), headers=headers)\n        except:\n            raise Exception(\"API request failed\")\n    \n        if result.status_code in [200,201,204]:\n            return (False, False, result.json())\n        elif result.status_code == 401:\n            return (True, False, result.content)\n        else:\n            meta = {'status': result.status_code, 'response': result.json()}\n            return (True, False, meta)\n    \n    def create(self,endpoint,data={},body=True):\n        \n        try:\n            headers = {'Authorization': 'Token {}'.format(self.token)}\n            url = '{}{}'.format(self.baseUrl, endpoint)\n            if(body==True):\n                result = requests.post(url, json.dumps(data), headers=headers)\n            else:\n                result = requests.post(url, headers=headers)\n        except:\n            raise Exception(\"API request failed\")\n    \n        if result.status_code in [200,201,204]:\n            return (False, False, result.json())\n        elif result.status_code == 401:\n            return (True, False, result.content)\n        else:\n            meta = {'status': result.status_code, 'response': result.json()}\n            return (True, False, meta)\n    \n    def update(self,endpoint,data={}):\n        \n        try:\n            headers = {'Authorization': 'Token {}'.format(self.token)}\n            url = '{}{}'.format(self.baseUrl, endpoint)\n            result = requests.patch(url, json.dumps(data), headers=headers)\n        except:\n            raise Exception(\"API request failed\")\n    \n        if result.status_code in [200,201,204]:\n            return (False, False, result.json())\n        elif result.status_code == 401:\n            return (True, False, result.content)            \n        else:\n            meta = {'status': result.status_code, 'response': result.json()}\n            return (True, False, meta)\n\n    def put(self,endpoint,data={}):\n        \n        try:\n            headers = {'Authorization': 'Token {}'.format(self.token)}\n            url = '{}{}'.format(self.baseUrl, endpoint)\n            result = requests.put(url, json.dumps(data), headers=headers)\n        except:\n            raise Exception(\"API request failed\")\n    \n        if result.status_code in [200,201,204]:\n            return (False, False, result.json())\n        elif result.status_code == 401:\n            return (True, False, result.content)            \n        else:\n            meta = {'status': result.status_code, 'response': result.json()}\n            return (True, False, meta)\n    \n    def delete(self,endpoint,data={}, body=False):\n        \n        try:\n            headers = {'Authorization': 'Token {}'.format(self.token)}\n            url = '{}{}'.format(self.baseUrl, endpoint)\n            if(body==True):\n                result = requests.delete(url, json.dumps(data), headers=headers)\n            else:\n                result = requests.delete(url, headers=headers)\n        except:\n            raise Exception(\"API request failed\")\n    \n        if result.status_code in [200,201,204]:\n            return (False, False, result.json())\n        elif result.status_code == 401:\n            return (True, False, result.content)            \n        else:\n            meta = {'status': result.status_code, 'response': result.json()}\n            return (True, False, meta)\n\nclass Utilities(object):\n    \n    def __init__(self):\n        \n        pass\n\n    def normalize_ip(self, address, cidr=-1):\n        \n        address = address.split('/')\n        try:\n            ipaddress.ip_address(address[0])\n        except:\n            return ['','']\n        if cidr != -1 and int(cidr) < 32:\n            return [address[0],cidr]\n        elif len(address) == 2:\n            return [address[0],address[1]]\n        else:    \n            return [address[0],'']\n\n    def flatten_dict_object(self,key,data):\n        \n        payload = {}\n        for i in data[key]:\n            for k,v in i.items():\n                payload[k]=v\n        return payload\n    \n    def dhcp_options(self, key, data, dhcp_option_codes):\n        \n        payload = []\n        for i in data[key]:\n            for k, v in i.items():\n                dhcp_option = {}\n                for item in dhcp_option_codes:\n                    if item[\"name\"] == k:\n                        dhcp_option_code = item[\"id\"]\n                        break\n                if dhcp_option_code:\n                    dhcp_option[\"option_code\"] = dhcp_option_code\n                    dhcp_option[\"option_value\"] = v\n                    dhcp_option[\"type\"] = \"option\"\n                    payload.append(dhcp_option)\n        return payload\n\n",
        "summary": "The provided Python code defines a class `Request` for making various HTTP requests (GET, POST, PUT, DELETE) to an API endpoint using the `requests` library, handling authentication with a token and processing different response statuses. Additionally, it includes a `Utilities` class with methods for normalizing IP addresses, flattening dictionary objects, and converting DHCP options based on predefined codes."
    },
    {
        "code": "import json\n\nfrom flask_restx import Namespace, Resource, cors\n\nfrom auth_api import status as http_status\nfrom auth_api.exceptions import BusinessException\nfrom auth_api.services import Product as ProductService\nfrom auth_api.tracer import Tracer\nfrom auth_api.utils.util import cors_preflight\n\n\nAPI = Namespace('products', description='Endpoints for products management')\nTRACER = Tracer.get_instance()\n\n\n@cors_preflight('GET,OPTIONS')\n@API.route('', methods=['GET', 'OPTIONS'])\nclass Products(Resource):\n    \n\n    @staticmethod\n    @TRACER.trace()\n    @cors.crossdomain(origin='*')\n    def get():\n        \n        try:\n            response, status = json.dumps(ProductService.get_products()), http_status.HTTP_200_OK\n        except BusinessException as exception:\n            response, status = {'code': exception.code, 'message': exception.message}, exception.status_code\n        return response, status\n",
        "summary": "The provided Python code defines a Flask-RESTX resource for managing products, including endpoints for retrieving product data. It handles GET requests to fetch products, using dependency injection and exception handling to manage business logic and errors gracefully."
    },
    {
        "code": "from ..data import convert_to_dataset\nfrom ..labels import BaseLabeller\nfrom ..sel_utils import xarray_var_iter\nfrom ..rcparams import rcParams\nfrom ..utils import _var_names\nfrom .plot_utils import default_grid, filter_plotters_list, get_plotting_function\n\n\ndef plot_autocorr(\n    data,\n    var_names=None,\n    filter_vars=None,\n    max_lag=None,\n    combined=False,\n    grid=None,\n    figsize=None,\n    textsize=None,\n    labeller=None,\n    ax=None,\n    backend=None,\n    backend_config=None,\n    backend_kwargs=None,\n    show=None,\n):\n    \n    data = convert_to_dataset(data, group=\"posterior\")\n    var_names = _var_names(var_names, data, filter_vars)\n\n    \n    if max_lag is None:\n        max_lag = min(100, data[\"draw\"].shape[0])\n\n    if labeller is None:\n        labeller = BaseLabeller()\n\n    plotters = filter_plotters_list(\n        list(xarray_var_iter(data, var_names, combined)), \"plot_autocorr\"\n    )\n    rows, cols = default_grid(len(plotters), grid=grid)\n\n    autocorr_plot_args = dict(\n        axes=ax,\n        plotters=plotters,\n        max_lag=max_lag,\n        figsize=figsize,\n        rows=rows,\n        cols=cols,\n        combined=combined,\n        textsize=textsize,\n        labeller=labeller,\n        backend_kwargs=backend_kwargs,\n        show=show,\n    )\n\n    if backend is None:\n        backend = rcParams[\"plot.backend\"]\n    backend = backend.lower()\n\n    if backend == \"bokeh\":\n        autocorr_plot_args.update(backend_config=backend_config)\n\n    \n    plot = get_plotting_function(\"plot_autocorr\", \"autocorrplot\", backend)\n    axes = plot(**autocorr_plot_args)\n\n    return axes\n",
        "summary": "The `plot_autocorr` function processes input data to create an autocorrelation plot, handling various parameters such as variable names, maximum lag, and plotting backend. It uses utility functions for data conversion, filtering, and plotting, ultimately returning the axes object of the generated plot."
    },
    {
        "code": "from django.contrib import admin\n\n\nfrom django.contrib import admin\n\nfrom .models import RemOrganization, RemRole, RemUser, Nursery, NurseryPlantsHistory, MotherTree, Plantation, BeninYield, AlteiaData, DeptSatellite, CommuneSatellite, SpecialTuple\n\nadmin.site.register(RemOrganization)\nadmin.site.register(RemRole)\nadmin.site.register(RemUser)\nadmin.site.register(Nursery)\nadmin.site.register(NurseryPlantsHistory)\nadmin.site.register(MotherTree)\nadmin.site.register(Plantation)\nadmin.site.register(BeninYield)\nadmin.site.register(AlteiaData)\nadmin.site.register(DeptSatellite)\nadmin.site.register(CommuneSatellite)\nadmin.site.register(SpecialTuple)\n\n",
        "summary": "The provided Python code registers various Django models with the admin site, allowing for easy management and manipulation of these models through a web-based interface."
    },
    {
        "code": "from ptypes import *\n\nclass Header(pstruct.type):\n    _fields_ = [\n        (dyn.block(3), 'Signature'),\n        (dyn.block(3), 'Version'),\n    ]\n\nclass LogicalScreenDescriptor(pstruct.type):\n    class _Flags(pbinary.struct):\n        _fields_ = [(1, 'Global Color Table'), (3, 'Color Resolution'), (1, 'Sort'), (3, 'Size')]\n\n    def optional(self):\n        if self['Flags'].li['Global Color Table'] > 0:\n            return dyn.clone(ColorTable, length=pow(2, self['Flags']['Size'] + 1))\n        return dyn.clone(ColorTable, length=0)\n\n    _fields_ = [\n        (pint.uint16_t, 'Width'),\n        (pint.uint16_t, 'Height'),\n        (_Flags, 'Flags'),\n        (pint.uint8_t, 'BackgroundColorIndex'),\n        (pint.uint8_t, 'PixelAspectRatio'),\n        (optional, 'Global Color Table')\n    ]\n\nclass Color(pstruct.type):\n    _fields_ = [\n        (pint.uint8_t, 'r'),\n        (pint.uint8_t, 'g'),\n        (pint.uint8_t, 'b'),\n    ]\n\nclass ColorTable(parray.type):\n    length = 0\n    _object_ = Color\n\nclass ImageDescriptor(pstruct.type):\n    class _Flags(pbinary.struct):\n        _fields_ = [(1, 'Local Color Table'), (1, 'Interlace'), (1, 'Sort'), (2, 'Reserved'), (3, 'Size')]\n\n    def optional(self):\n        if self['Flags'].li['Local Color Table'] > 0:\n            return dyn.clone(ColorTable, length=pow(2, self['Flags']['Size'] + 1))\n        return dyn.clone(ColorTable, length=0)\n\n    _fields_ = [\n        (pint.uint8_t, 'Separator'),\n        (pint.uint16_t, 'Left'),\n        (pint.uint16_t, 'Top'),\n        (pint.uint16_t, 'Width'),\n        (pint.uint16_t, 'Height'),\n        (_Flags, 'Flags'),\n        (optional, 'Color Table')\n    ]\n\nclass Trailer(pint.uint8_t): pass\n    \n\nclass ImageTableData_Chunk(pstruct.type):\n    _fields_ = [\n        (pint.uint8_t, 'CodeSize'),\n        (ptype.type, 'something')\n    ]\n\nclass ImageData_Chunk(pstruct.type):\n    _fields_ = [\n        (pint.uint8_t, 'Block Size'),\n        (lambda s: dyn.block(int(s['Block Size'].li)), 'Data Values')\n    ]\n\nclass ImageData( parray.type ):\n    length = 1\n    _object_ = ImageData_Chunk\n    def isTerminator(self, v):\n        if int(v['Block Size']) == 0:\n            return True\n        return False\n\nclass File(pstruct.type):\n    _fields_ = [\n        (Header, 'header'),\n        (LogicalScreenDescriptor, 'screen'),\n        (ImageDescriptor, 'image'),\n        (ImageData, 'data')\n    ]\n\nif __name__ == '__main__':\n    import ptypes,image.gif as gif\n    ptypes.setsource( ptypes.provider.file('./poc.gif') )\n\n    z = gif.File()\n    print(z.l)\n",
        "summary": "The provided Python code defines a structure for parsing GIF files using the `ptypes` library, including classes for the header, logical screen descriptor, image descriptor, color table, and image data. The script then reads a GIF file named 'poc.gif' and prints its parsed contents."
    },
    {
        "code": "import dis\nimport re\nimport sys\nimport textwrap\nimport unittest\nfrom test.support import cpython_only\n\nfrom test.bytecode_helper import BytecodeTestCase\n\nclass TestTranforms(BytecodeTestCase):\n\n    def test_unot(self):\n        \n        def unot(x):\n            if not x == 2:\n                del x\n        self.assertNotInBytecode(unot, 'UNARY_NOT')\n        self.assertNotInBytecode(unot, 'POP_JUMP_IF_FALSE')\n        self.assertInBytecode(unot, 'POP_JUMP_IF_TRUE')\n\n    def test_elim_inversion_of_is_or_in(self):\n        for line, cmp_op in (\n            ('not a is b', 'is not',),\n            ('not a in b', 'not in',),\n            ('not a is not b', 'is',),\n            ('not a not in b', 'in',),\n            ):\n            code = compile(line, '', 'single')\n            self.assertInBytecode(code, 'COMPARE_OP', cmp_op)\n\n    def test_global_as_constant(self):\n        \n        def f():\n            x = None\n            x = None\n            return x\n        def g():\n            x = True\n            return x\n        def h():\n            x = False\n            return x\n\n        for func, elem in ((f, None), (g, True), (h, False)):\n            self.assertNotInBytecode(func, 'LOAD_GLOBAL')\n            self.assertInBytecode(func, 'LOAD_CONST', elem)\n\n        def f():\n            'Adding a docstring made this test fail in Py2.5.0'\n            return None\n\n        self.assertNotInBytecode(f, 'LOAD_GLOBAL')\n        self.assertInBytecode(f, 'LOAD_CONST', None)\n\n    def test_while_one(self):\n        \n        def f():\n            while 1:\n                pass\n            return list\n        for elem in ('LOAD_CONST', 'POP_JUMP_IF_FALSE'):\n            self.assertNotInBytecode(f, elem)\n        for elem in ('JUMP_ABSOLUTE',):\n            self.assertInBytecode(f, elem)\n\n    def test_pack_unpack(self):\n        \n        \n        \n        for line, elem in (\n            ('a, = a,', 'LOAD_CONST',),\n            ('a[1], b = a, b', 'ROT_TWO',),\n            ('a, b[2], c = a, b, c', 'ROT_THREE',),\n            ):\n            code = compile(line,'','single')\n            self.assertInBytecode(code, elem)\n            self.assertNotInBytecode(code, 'BUILD_TUPLE')\n            self.assertNotInBytecode(code, 'UNPACK_TUPLE')\n\n    def test_folding_of_tuples_of_constants(self):\n        \n        \n        for line, elem in (\n            ('a = 1,2,3', (1, 2, 3)),\n            ('(\"a\",\"b\",\"c\")', ('a', 'b', 'c')),\n            ('(None, 1, None)', (None, 1, None)),\n            ('((1, 2), 3, 4)', ((1, 2), 3, 4)),\n            ):\n            code = compile(line,'','single')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n            self.assertNotInBytecode(code, 'BUILD_TUPLE')\n\n        \n        code = compile(repr(tuple(range(10000))),'','single')\n        self.assertNotInBytecode(code, 'BUILD_TUPLE')\n        \n        load_consts = [instr for instr in dis.get_instructions(code)\n                              if instr.opname == 'LOAD_CONST']\n        self.assertEqual(len(load_consts), 2)\n\n        \n        \n        \n        def crater():\n            (~[\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n                0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n            ],)\n\n    def test_folding_of_lists_of_constants(self):\n        for line, elem in (\n            \n            ('a in [1,2,3]', (1, 2, 3)),\n            ('a not in [\"a\",\"b\",\"c\"]', ('a', 'b', 'c')),\n            ('a in [None, 1, None]', (None, 1, None)),\n            ('a not in [(1, 2), 3, 4]', ((1, 2), 3, 4)),\n            ):\n            code = compile(line, '', 'single')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n            self.assertNotInBytecode(code, 'BUILD_LIST')\n\n    def test_folding_of_sets_of_constants(self):\n        for line, elem in (\n            \n            ('a in {1,2,3}', frozenset({1, 2, 3})),\n            ('a not in {\"a\",\"b\",\"c\"}', frozenset({'a', 'c', 'b'})),\n            ('a in {None, 1, None}', frozenset({1, None})),\n            ('a not in {(1, 2), 3, 4}', frozenset({(1, 2), 3, 4})),\n            ('a in {1, 2, 3, 3, 2, 1}', frozenset({1, 2, 3})),\n            ):\n            code = compile(line, '', 'single')\n            self.assertNotInBytecode(code, 'BUILD_SET')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n\n        \n        def f(a):\n            return a in {1, 2, 3}\n\n        def g(a):\n            return a not in {1, 2, 3}\n\n        self.assertTrue(f(3))\n        self.assertTrue(not f(4))\n\n        self.assertTrue(not g(3))\n        self.assertTrue(g(4))\n\n\n    def test_folding_of_binops_on_constants(self):\n        for line, elem in (\n            ('a = 2+3+4', 9),                   \n            ('\"@\"*4', '@@@@'),                  \n            ('a=\"abc\" + \"def\"', 'abcdef'),      \n            ('a = 3**4', 81),                   \n            ('a = 3*4', 12),                    \n            ('a = 13//4', 3),                   \n            ('a = 14%4', 2),                    \n            ('a = 2+3', 5),                     \n            ('a = 13-4', 9),                    \n            ('a = (12,13)[1]', 13),             \n            ('a = 13 << 2', 52),                \n            ('a = 13 >> 2', 3),                 \n            ('a = 13 & 7', 5),                  \n            ('a = 13 ^ 7', 10),                 \n            ('a = 13 | 7', 15),                 \n            ):\n            code = compile(line, '', 'single')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n            for instr in dis.get_instructions(code):\n                self.assertFalse(instr.opname.startswith('BINARY_'))\n\n        \n        code = compile('a=2+\"b\"', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', 2)\n        self.assertInBytecode(code, 'LOAD_CONST', 'b')\n\n        \n        code = compile('a=\"x\"*10000', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', 10000)\n        self.assertNotIn(\"x\"*10000, code.co_consts)\n        code = compile('a=1<<1000', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', 1000)\n        self.assertNotIn(1<<1000, code.co_consts)\n        \n        \n        code = compile('a=2**10000', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', 10000)\n        self.assertNotIn(2**10000, code.co_consts)\n\n    @cpython_only \n    def test_binary_subscr_on_unicode(self):\n        \n        code = compile('\"foo\"[0]', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', 'f')\n        self.assertNotInBytecode(code, 'BINARY_SUBSCR')\n        code = compile('\"\\u0061\\uffff\"[1]', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', '\\uffff')\n        self.assertNotInBytecode(code,'BINARY_SUBSCR')\n\n        \n        code = compile('\"\\U00012345\"[0]', '', 'single')\n        self.assertInBytecode(code, 'LOAD_CONST', '\\U00012345')\n        self.assertNotInBytecode(code, 'BINARY_SUBSCR')\n\n        \n        \n        code = compile('\"fuu\"[10]', '', 'single')\n        self.assertInBytecode(code, 'BINARY_SUBSCR')\n\n    def test_folding_of_unaryops_on_constants(self):\n        for line, elem in (\n            ('-0.5', -0.5),                     \n            ('-0.0', -0.0),                     \n            ('-(1.0-1.0)', -0.0),               \n            ('-0', 0),                          \n            ('~-2', 1),                         \n            ('+1', 1),                          \n        ):\n            code = compile(line, '', 'single')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n            for instr in dis.get_instructions(code):\n                self.assertFalse(instr.opname.startswith('UNARY_'))\n\n        \n        def negzero():\n            return -(1.0-1.0)\n\n        for instr in dis.get_instructions(code):\n            self.assertFalse(instr.opname.startswith('UNARY_'))\n\n        \n        for line, elem, opname in (\n            ('-\"abc\"', 'abc', 'UNARY_NEGATIVE'),\n            ('~\"abc\"', 'abc', 'UNARY_INVERT'),\n        ):\n            code = compile(line, '', 'single')\n            self.assertInBytecode(code, 'LOAD_CONST', elem)\n            self.assertInBytecode(code, opname)\n\n    def test_elim_extra_return(self):\n        \n        def f(x):\n            return x\n        self.assertNotInBytecode(f, 'LOAD_CONST', None)\n        returns = [instr for instr in dis.get_instructions(f)\n                          if instr.opname == 'RETURN_VALUE']\n        self.assertEqual(len(returns), 1)\n\n    def test_elim_jump_to_return(self):\n        \n        def f(cond, true_value, false_value):\n            return true_value if cond else false_value\n        self.assertNotInBytecode(f, 'JUMP_FORWARD')\n        self.assertNotInBytecode(f, 'JUMP_ABSOLUTE')\n        returns = [instr for instr in dis.get_instructions(f)\n                          if instr.opname == 'RETURN_VALUE']\n        self.assertEqual(len(returns), 2)\n\n    def test_elim_jump_after_return1(self):\n        \n        def f(cond1, cond2):\n            if cond1: return 1\n            if cond2: return 2\n            while 1:\n                return 3\n            while 1:\n                if cond1: return 4\n                return 5\n            return 6\n        self.assertNotInBytecode(f, 'JUMP_FORWARD')\n        self.assertNotInBytecode(f, 'JUMP_ABSOLUTE')\n        returns = [instr for instr in dis.get_instructions(f)\n                          if instr.opname == 'RETURN_VALUE']\n        self.assertEqual(len(returns), 6)\n\n    def test_elim_jump_after_return2(self):\n        \n        def f(cond1, cond2):\n            while 1:\n                if cond1: return 4\n        self.assertNotInBytecode(f, 'JUMP_FORWARD')\n        \n        returns = [instr for instr in dis.get_instructions(f)\n                          if instr.opname == 'JUMP_ABSOLUTE']\n        self.assertEqual(len(returns), 1)\n        returns = [instr for instr in dis.get_instructions(f)\n                          if instr.opname == 'RETURN_VALUE']\n        self.assertEqual(len(returns), 2)\n\n    def test_make_function_doesnt_bail(self):\n        def f():\n            def g()->1+1:\n                pass\n            return g\n        self.assertNotInBytecode(f, 'BINARY_ADD')\n\n    def test_constant_folding(self):\n        \n        exprs = [\n            '3 * -5',\n            '-3 * 5',\n            '2 * (3 * 4)',\n            '(2 * 3) * 4',\n            '(-1, 2, 3)',\n            '(1, -2, 3)',\n            '(1, 2, -3)',\n            '(1, 2, -3) * 6',\n            'lambda x: x in {(3 * -5) + (-1 - 6), (1, -2, 3) * 2, None}',\n        ]\n        for e in exprs:\n            code = compile(e, '', 'single')\n            for instr in dis.get_instructions(code):\n                self.assertFalse(instr.opname.startswith('UNARY_'))\n                self.assertFalse(instr.opname.startswith('BINARY_'))\n                self.assertFalse(instr.opname.startswith('BUILD_'))\n\n\nclass TestBuglets(unittest.TestCase):\n\n    def test_bug_11510(self):\n        \n        \n        \n        def f():\n            x, y = {1, 1}\n            return x, y\n        with self.assertRaises(ValueError):\n            f()\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "summary": "This Python code defines a series of unit tests for the Python interpreter's bytecode generation and optimization features. The tests cover various aspects such as constant folding, unary and binary operations, function definitions, return statements, and more.\n\nHere's a breakdown of some key points:\n\n1. **Constant Folding**: Tests ensure that simple arithmetic expressions are evaluated at compile time rather than runtime. For example:\n   ```python\n   def test_constant_folding(self):\n       exprs = [\n           '3 * -5',\n           '-3 * 5',\n           # ... other expressions\n       ]\n       for e in exprs:\n           code = compile(e, '', 'single')\n           for instr in dis.get_instructions(code):\n               self.assertFalse(instr.opname.startswith('UNARY_'))\n               self.assertFalse(instr.opname.startswith('BINARY_'))\n               self.assertFalse(instr.opname.startswith('BUILD_'))\n   ```\n\n2. **Unary and Binary Operations**: Tests ensure that unary and binary operations are optimized away when possible. For example:\n   ```python\n   def test_folding_of_unaryops_on_constants(self):\n       for line, elem in (\n           ('-0.5', -0.5),                     \n           # ... other cases\n       ):\n           code = compile(line, '', 'single')\n           self.assertInBytecode(code, 'LOAD_CONST', elem)\n           for instr in dis.get_instructions(code):\n               self.assertFalse(instr.opname.startswith('UNARY_'))\n   ```\n\n3. **Function Definitions**: Tests ensure that function definitions are optimized correctly. For example:\n   ```python\n   def test_make_function_doesnt_bail(self):\n       def f():\n           def g()->1+1:\n               pass\n           return g\n       self.assertNotInBytecode(f, 'BINARY_ADD')\n   ```\n\n4. **Return Statements**: Tests ensure that return statements are optimized correctly and do not introduce unnecessary jumps or returns. For example:\n   ```python\n   def test_elim_jump_to_return(self):\n       def f(cond, true_value, false_value):\n           return true_value if cond else false_value\n       self.assertNotInBytecode(f, 'JUMP_FORWARD')\n       self.assertNotInBytecode(f, 'JUMP_ABSOLUTE')\n       returns = [instr for instr in dis.get_instructions(f)\n                         if instr.opname == 'RETURN_VALUE']\n       self.assertEqual(len(returns), 2)\n   ```\n\n5. **Bug Fixes**: Tests ensure that specific bugs are fixed. For example:\n   ```python\n   def test_bug_11510(self):\n       def f():\n           x, y = {1, 1}\n           return x, y\n       with self.assertRaises(ValueError):\n           f()\n   ```\n\nOverall, these tests help ensure that the Python interpreter is correctly optimizing bytecode and handling various edge cases."
    },
    {
        "code": "__author__ = 'Matt Makai'\n__email__ = 'mmakai@twilio.com'\n__version__ = '0.1.0'",
        "summary": "The provided Python code snippet includes metadata about the author, their email address, and the version of a software package, specifically setting the author to Matt Makai, his email as mmakai@twilio.com, and the version number to 0.1.0."
    },
    {
        "code": "import pytest\nfrom numpy.testing import assert_allclose\nfrom .. import Parameter, Parameters, optimize_iminuit\n\npytest.importorskip(\"iminuit\")\n\n\ndef fcn(parameters):\n    x = parameters[\"x\"].value\n    y = parameters[\"y\"].value\n    z = parameters[\"z\"].value\n    x_opt, y_opt, z_opt = 2, 3e5, 4e-5\n    x_err, y_err, z_err = 0.2, 3e4, 4e-6\n    return ((x - x_opt) / x_err) ** 2 + ((y - y_opt) / y_err) ** 2 + ((z - z_opt) / z_err) ** 2\n\n\n@pytest.fixture()\ndef pars():\n    x = Parameter(\"x\", 2.1)\n    y = Parameter(\"y\", 3.1, scale=1e5)\n    z = Parameter(\"z\", 4.1, scale=1e-5)\n    return Parameters([x, y, z])\n\n\ndef test_iminuit_basic(pars):\n    factors, info, minuit = optimize_iminuit(function=fcn, parameters=pars)\n\n    assert info[\"success\"]\n    assert_allclose(fcn(pars), 0, atol=1e-5)\n\n    \n    assert_allclose(pars[\"x\"].value, 2, rtol=1e-3)\n    assert_allclose(pars[\"y\"].value, 3e5, rtol=1e-3)\n    \n    assert_allclose(pars[\"z\"].value, 4e-5, rtol=2e-2)\n\n    \n    assert_allclose(factors, [2, 3, 4], rtol=1e-3)\n    assert_allclose(minuit.values[\"par_000_x\"], 2, rtol=1e-3)\n    assert_allclose(minuit.values[\"par_001_y\"], 3, rtol=1e-3)\n    assert_allclose(minuit.values[\"par_002_z\"], 4, rtol=1e-3)\n\n\ndef test_iminuit_frozen(pars):\n    pars[\"y\"].frozen = True\n\n    factors, info, minuit = optimize_iminuit(function=fcn, parameters=pars)\n\n    assert info[\"success\"]\n\n    assert_allclose(pars[\"x\"].value, 2, rtol=1e-4)\n    assert_allclose(pars[\"y\"].value, 3.1e5)\n    assert_allclose(pars[\"z\"].value, 4.e-5, rtol=1e-4)\n    assert_allclose(fcn(pars), 0.111112, rtol=1e-5)\n\n    assert minuit.list_of_fixed_param() == [\"par_001_y\"]\n\n\ndef test_iminuit_limits(pars):\n    pars[\"y\"].min = 301000\n\n    factors, info, minuit = optimize_iminuit(function=fcn, parameters=pars)\n\n    assert info[\"success\"]\n\n    \n    assert_allclose(pars[\"x\"].value, 2, rtol=1e-2)\n    assert_allclose(pars[\"y\"].value, 301000, rtol=1e-3)\n\n    \n    states = minuit.get_param_states()\n    assert not states[0][\"has_limits\"]\n\n    y = states[1]\n    assert y[\"has_limits\"]\n    assert_allclose(y[\"lower_limit\"], 3.01)\n\n    \n    \n    \n",
        "summary": "The provided Python code uses the `pytest` framework to test a function optimization process with the `iminuit` library, focusing on parameter fitting and handling constraints such as freezing parameters and setting limits. The tests validate that the optimization converges successfully, parameters are accurately estimated within specified tolerances, and constraints are respected during the fitting process."
    },
    {
        "code": "m = int(input())\nm /= 1000\nif m < 0.1:\n    print('00')\nelif 0.1 <= m and m <= 5:\n    m = str(int(10 * m))\n    if len(m) == 1:\n        m = '0' + m\n    print(m)\nelif 6 <= m and m <= 30:\n    print(int(m) + 50)\nelif 35 <= m and m <= 70:\n    print((int(m) - 30) // 5 + 80)\nelse:\n    print('89')\n",
        "summary": "The Python code takes an integer input, divides it by 1000, and then categorizes the result into different ranges to output a specific string or number based on those categories."
    },
    {
        "code": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.distributions as td\r\n\r\nclass Flow(nn.Module):\r\n    \r\n    def __init__(self, base_dist=None, transforms=[]):\r\n        super().__init__()\r\n        self.base_dist = base_dist\r\n        self.transforms = nn.ModuleList(transforms)\r\n\r\n    def forward(self, x, latent=None, mask=None, t=None, reverse=False, **kwargs):\r\n        \r\n        transforms = self.transforms[::-1] if reverse else self.transforms\r\n        _mask = 1 if mask is None else mask\r\n\r\n        log_jac_diag = torch.zeros_like(x).to(x)\r\n        for f in transforms:\r\n            if reverse:\r\n                x, ld = f.inverse(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\r\n            else:\r\n                x, ld = f.forward(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\r\n            log_jac_diag += ld * _mask\r\n        return x, log_jac_diag\r\n\r\n    def inverse(self, y, latent=None, mask=None, t=None, **kwargs):\r\n        \r\n        return self.forward(y, latent=latent, mask=mask, t=t, reverse=True, **kwargs)\r\n\r\n    def log_prob(self, x, **kwargs):\r\n        \r\n        if self.base_dist is None:\r\n            raise ValueError('Please define `base_dist` if you need log-probability')\r\n        x, log_jac_diag = self.inverse(x, **kwargs)\r\n        log_prob = self.base_dist.log_prob(x) + log_jac_diag.sum(-1)\r\n        return log_prob.unsqueeze(-1)\r\n\r\n    def sample(self, num_samples, latent=None, mask=None, **kwargs):\r\n        \r\n        if self.base_dist is None:\r\n            raise ValueError('Please define `base_dist` if you need sampling')\r\n        if isinstance(num_samples, int):\r\n            num_samples = (num_samples,)\r\n\r\n        x = self.base_dist.rsample(num_samples)\r\n        x, log_jac_diag = self.forward(x, **kwargs)\r\n        return x\r\n",
        "summary": "The provided Python code defines a class `Flow` that implements a normalizing flow model using PyTorch. This model consists of a base distribution and a series of invertible transformations applied sequentially or in reverse to map between the base distribution and a more complex distribution, enabling efficient sampling and computation of log-probabilities."
    },
    {
        "code": "import os\nimport platform\n\nfrom test_framework.test_framework import BlinkhashTestFramework\nfrom test_framework.util import (\n    assert_equal,\n    assert_raises_rpc_error,\n)\n\n\nclass RPCSignerTest(BlinkhashTestFramework):\n    def mock_signer_path(self):\n        path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'mocks', 'signer.py')\n        if platform.system() == \"Windows\":\n            return \"py \" + path\n        else:\n            return path\n\n    def set_test_params(self):\n        self.num_nodes = 4\n\n        self.extra_args = [\n            [],\n            [f\"-signer={self.mock_signer_path()}\", '-keypool=10'],\n            [f\"-signer={self.mock_signer_path()}\", '-keypool=10'],\n            [\"-signer=fake.py\"],\n        ]\n\n    def skip_test_if_missing_module(self):\n        self.skip_if_no_external_signer()\n\n    def set_mock_result(self, node, res):\n        with open(os.path.join(node.cwd, \"mock_result\"), \"w\", encoding=\"utf8\") as f:\n            f.write(res)\n\n    def clear_mock_result(self, node):\n        os.remove(os.path.join(node.cwd, \"mock_result\"))\n\n    def run_test(self):\n        self.log.debug(f\"-signer={self.mock_signer_path()}\")\n\n        assert_raises_rpc_error(-1, 'Error: restart blinkhashd with -signer=<cmd>',\n            self.nodes[0].enumeratesigners\n        )\n\n        \n        assert_raises_rpc_error(-1, 'execve failed: No such file or directory',\n            self.nodes[3].enumeratesigners\n        )\n\n        \n        self.set_mock_result(self.nodes[1], \"2\")\n        assert_raises_rpc_error(-1, 'RunCommandParseJSON error',\n            self.nodes[1].enumeratesigners\n        )\n        self.clear_mock_result(self.nodes[1])\n\n        self.set_mock_result(self.nodes[1], '0 [{\"type\": \"trezor\", \"model\": \"trezor_t\", \"error\": \"fingerprint not found\"}]')\n        assert_raises_rpc_error(-1, 'fingerprint not found',\n            self.nodes[1].enumeratesigners\n        )\n        self.clear_mock_result(self.nodes[1])\n\n        result = self.nodes[1].enumeratesigners()\n        assert_equal(len(result['signers']), 2)\n        assert_equal(result['signers'][0][\"fingerprint\"], \"00000001\")\n        assert_equal(result['signers'][0][\"name\"], \"trezor_t\")\n\nif __name__ == '__main__':\n    RPCSignerTest().main()\n",
        "summary": "The provided Python code defines a test class `RPCSignerTest` that inherits from `BlinkhashTestFramework`. It includes methods to mock the signer path, set test parameters, skip tests if an external signer is missing, and run specific tests. The tests check various scenarios for enumerating signers, including errors related to missing files, incorrect results, and successful enumeration with expected outputs."
    },
    {
        "code": "import pandas as pd\nimport pathlib\nfrom fairness.results import local_results_path\n\nBASE_DIR = local_results_path()\nPACKAGE_DIR = pathlib.Path(__file__).parents[2]\nRAW_DATA_DIR = PACKAGE_DIR / 'data' / 'raw'\nPROCESSED_DATA_DIR = BASE_DIR / 'data' / 'preprocessed' \nRESULT_DIR = BASE_DIR / \"results\"\nANALYSIS_DIR = BASE_DIR / \"analysis\"\n\n\nclass Data():\n    def __init__(self):\n        pass\n\n    def get_dataset_name(self):\n        \n        return self.dataset_name\n\n    def get_class_attribute(self):\n        \n        return self.class_attr\n\n    def get_positive_class_val(self, tag):\n        \n        \n        if tag == 'numerical-binsensitive':\n            return 1\n        else:\n            return self.positive_class_val\n\n    def get_sensitive_attributes(self):\n        \n        return self.sensitive_attrs\n\n    def get_sensitive_attributes_with_joint(self):\n        \n        \n        \n        \n        return self.get_sensitive_attributes()\n\n    def get_privileged_class_names(self, tag):\n        \n        \n        \n        if tag == 'numerical-binsensitive':\n            return [1 for x in self.get_sensitive_attributes()]\n        else:\n            return self.privileged_class_names\n\n    def get_privileged_class_names_with_joint(self, tag):\n        \n        priv_class_names = self.get_privileged_class_names(tag)\n        if len(priv_class_names) > 1:\n            return priv_class_names + ['-'.join(str(v) for v in priv_class_names)]\n        return priv_class_names\n\n    def get_categorical_features(self):\n        \n        return self.categorical_features\n\n    def get_features_to_keep(self):\n        return self.features_to_keep\n\n    def get_missing_val_indicators(self):\n        return self.missing_val_indicators\n\n    def load_raw_dataset(self):\n        data_path = self.get_raw_filename()\n        data_frame = pd.read_csv(data_path, error_bad_lines=False,\n                                 na_values=self.get_missing_val_indicators(),\n                                 encoding = 'ISO-8859-1')\n        return data_frame\n\n    def get_raw_filename(self):\n        RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n        return RAW_DATA_DIR / (self.get_dataset_name() + '.csv')\n\n    def get_filename(self, tag):\n        PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n        return PROCESSED_DATA_DIR / (self.get_dataset_name() + \"_\" + tag + '.csv')\n\n    def get_results_filename(self, sensitive_attr, tag):\n        RESULT_DIR.mkdir(parents=True, exist_ok=True)\n        return RESULT_DIR / (self.get_dataset_name() + \"_\" + sensitive_attr + \"_\" + tag + '.csv')\n\n    def get_param_results_filename(self, sensitive_attr, tag, algname):\n        RESULT_DIR.mkdir(parents=True, exist_ok=True)\n        return RESULT_DIR / (algname + '_' + self.get_dataset_name() + \"_\" + sensitive_attr + \\\n               \"_\" + tag + '.csv')\n\n    def get_analysis_filename(self, sensitive_attr, tag):\n        ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n        return ANALYSIS_DIR / (self.get_dataset_name() + \"_\" + sensitive_attr + \"_\" + tag + '.csv')\n\n    def data_specific_processing(self, dataframe):\n        \n        return dataframe\n\n    def handle_missing_data(self, dataframe):\n        \n        return dataframe\n\n    def get_class_balance_statistics(self, data_frame=None):\n        if data_frame is None:\n            data_frame = self.load_raw_dataset()\n        r = data_frame.groupby(self.get_class_attribute()).size()\n        return r\n\n    def get_sensitive_attribute_balance_statistics(self, data_frame=None):\n        if data_frame is None:\n            data_frame = self.load_raw_dataset()\n        return [data_frame.groupby(a).size()\n                for a in self.get_sensitive_attributes()]\n\n    \n\n    def get_results_data_frame(self, sensitive_attr, tag):\n        return pd.read_csv(self.get_results_filename(sensitive_attr, tag))\n\n    def get_param_results_data_frame(self, sensitive_attr, tag):\n        return pd.read_csv(self.get_param_results_filename(sensitive_attr, tag))\n",
        "summary": "The provided Python code defines a class `Data` that encapsulates various functionalities related to data handling and processing. It includes methods for loading raw datasets, saving processed data, retrieving file paths for results and analysis, performing specific data processing tasks such as handling missing values and balancing class attributes, and reading previously saved results."
    },
    {
        "code": "from django.core.urlresolvers import reverse\nfrom django.http import Http404\nfrom django.test import TestCase, override_settings\n\nimport mock\n\nfrom rest_framework.exceptions import APIException, PermissionDenied\nfrom rest_framework.request import Request\nfrom rest_framework.response import Response\nfrom rest_framework.routers import SimpleRouter\nfrom rest_framework.settings import api_settings\nfrom rest_framework.viewsets import GenericViewSet\n\n\nclass DummyViewSet(GenericViewSet):\n    \n    def list(self, *args, **kwargs):\n        raise Exception('something went wrong')\n\n\ntest_exception = SimpleRouter()\ntest_exception.register('testexcept', DummyViewSet, base_name='test-exception')\n\n\n@override_settings(ROOT_URLCONF=test_exception.urls)\nclass TestExceptionHandlerWithViewSet(TestCase):\n    \n    \n    @mock.patch('olympia.api.exceptions.got_request_exception')\n    def test_view_exception(self, got_request_exception_mock):\n        url = reverse('test-exception-list')\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False, DEBUG=False):\n            response = self.client.get(url)\n            assert response.status_code == 500\n            assert response.data == {'detail': 'Internal Server Error'}\n\n        assert got_request_exception_mock.send.call_count == 1\n        assert got_request_exception_mock.send.call_args[0][0] == DummyViewSet\n        assert isinstance(\n            got_request_exception_mock.send.call_args[1]['request'], Request)\n\n    \n    \n    @mock.patch('olympia.api.exceptions.got_request_exception')\n    def test_view_exception_debug(self, got_request_exception_mock):\n        url = reverse('test-exception-list')\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False, DEBUG=True):\n            response = self.client.get(url)\n            assert response.status_code == 500\n            data = response.data\n            assert set(data.keys()) == set(['detail', 'traceback'])\n            assert data['detail'] == 'Internal Server Error'\n            assert 'Traceback (most recent call last):' in data['traceback']\n\n        assert got_request_exception_mock.send.call_count == 1\n        assert got_request_exception_mock.send.call_args[0][0] == DummyViewSet\n        assert isinstance(\n            got_request_exception_mock.send.call_args[1]['request'], Request)\n\n\nclass TestExceptionHandler(TestCase):\n    def test_api_exception_handler_returns_response(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False):\n            try:\n                raise APIException()\n            except Exception as exc:\n                response = exception_handler(exc, {})\n                assert isinstance(response, Response)\n                assert response.status_code == 500\n\n    def test_exception_handler_returns_response_for_404(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False):\n            try:\n                raise Http404()\n            except Exception as exc:\n                response = exception_handler(exc, {})\n                assert isinstance(response, Response)\n                assert response.status_code == 404\n\n    def test_exception_handler_returns_response_for_403(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False):\n            try:\n                raise PermissionDenied()\n            except Exception as exc:\n                response = exception_handler(exc, {})\n                assert isinstance(response, Response)\n                assert response.status_code == 403\n\n    def test_non_api_exception_handler_returns_response(self):\n        \n        \n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=False):\n            try:\n                raise Exception()\n            except Exception as exc:\n                response = exception_handler(exc, {})\n                assert isinstance(response, Response)\n                assert response.status_code == 500\n\n    def test_api_exception_handler_with_propagation(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.assertRaises(APIException):\n            with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=True):\n                try:\n                    raise APIException()\n                except Exception as exc:\n                    exception_handler(exc, {})\n\n    def test_exception_handler_404_with_propagation(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.assertRaises(Http404):\n            with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=True):\n                try:\n                    raise Http404()\n                except Exception as exc:\n                    exception_handler(exc, {})\n\n    def test_exception_handler_403_with_propagation(self):\n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.assertRaises(PermissionDenied):\n            with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=True):\n                try:\n                    raise PermissionDenied()\n                except Exception as exc:\n                    exception_handler(exc, {})\n\n    def test_non_api_exception_handler_with_propagation(self):\n        \n        \n        exception_handler = api_settings.EXCEPTION_HANDLER\n\n        with self.assertRaises(KeyError):\n            with self.settings(DEBUG_PROPAGATE_EXCEPTIONS=True):\n                try:\n                    raise KeyError()\n                except Exception as exc:\n                    exception_handler(exc, {})\n",
        "summary": "The provided Python code tests a custom viewset in Django REST Framework that raises an exception when accessed. It includes unit tests to verify how the application handles different types of exceptions (APIException, PermissionDenied, Http404) under both debug and non-debug modes, ensuring proper error responses are returned and exceptions are propagated as configured."
    },
    {
        "code": "import os, sys, urllib.request\nfrom tkinter import *\nfrom tkinter.messagebox import *\n\n__version__ = 3\n__filename__ = \"ImageRenaming\"\n__basename__ = os.path.basename(sys.argv[0])\n__savepath__ = os.path.join(os.environ['APPDATA'], \"QuentiumPrograms\")\n__iconpath__ = __savepath__ + \"/{}.ico\".format(__filename__)\n\ntry:urllib.request.urlopen(\"https://www.google.fr/\", timeout=1); connection = True\nexcept:connection = False\nif not os.path.exists(__iconpath__):\n    try:os.mkdir(__savepath__)\n    except:pass\n    if connection == True:\n        try:urllib.request.urlretrieve(\"https://quentium.fr/+++PythonDL/{}.ico\".format(__filename__), __iconpath__)\n        except:pass\n\nif connection == True:\n    try:script_version = int(urllib.request.urlopen(\"https://quentium.fr/programs/index.php\").read().decode().split(__filename__ + \"<!-- Version: \")[1].split(\" --></h2>\")[0])\n    except:script_version = __version__\n    if script_version > __version__:\n        if os.path.exists(__iconpath__):popup = Tk(); popup.attributes(\"-topmost\", 1); popup.iconbitmap(__iconpath__); popup.withdraw()\n        ask_update = askquestion(__filename__ + \" V\" + str(script_version), \"Une mise \u00e0 jour \u00e0 \u00e9t\u00e9 trouv\u00e9e, souhaitez vous la t\u00e9l\u00e9charger puis l'\u00e9x\u00e9cuter ?\", icon=\"question\")\n        if ask_update == \"yes\":\n            try:os.rename(__basename__, __filename__ + \"-old.exe\")\n            except:os.remove(__filename__ + \"-old.exe\"); os.rename(__basename__, __filename__ + \"-old.exe\")\n            if \"-32\" in str(__basename__):urllib.request.urlretrieve(\"https://quentium.fr/download.php?file={}-32.exe\".format(__filename__), __filename__ + \".exe\")\n            else:urllib.request.urlretrieve(\"https://quentium.fr/download.php?file={}.exe\".format(__filename__), __filename__ + \".exe\")\n            showwarning(__filename__, \"Le programme va red\u00e9marrer pour fonctionner sous la nouvelle version.\", icon=\"warning\")\n            os.system(\"start \" + __filename__ + \".exe\"); os._exit(1)\n\n__filename__ = __filename__ + \" V\" + str(__version__)\n\nfrom datetime import datetime\nfrom tkinter.filedialog import *\nfrom tkinter import *\n\ndef start_rename():\n    directory = askdirectory()\n    if directory:\n        if askyesno(__filename__, \"\u00cates-vous s\u00fbr de renommer toutes les images dans ce dossier ? Cette action ne peux pas \u00eatre annul\u00e9e !\"):\n            files1 = [f for f in os.listdir(directory) if f[-4:].lower() in (\".jpg\",\".JPG\",\".png\",\".PNG\",\".jpeg\",\".JPEG\",\".bmp\",\".gif\")]\n            for (index, filename) in enumerate(files1):\n                file = directory + \"/\" + filename\n                extension = os.path.splitext(filename)[1]\n                if check_var.get() == 0:\n                    time1 = os.path.getctime(file)\n                elif check_var.get() == 1:\n                    time1 = os.path.getmtime(file)\n                time2 = datetime.fromtimestamp(time1)\n                time = time2.strftime(\"%Y%m%d%H%M%S%f\")\n                newname = time + \"_\" + str(os.path.getsize(file)) + extension\n                os.rename(file, directory + \"/\" + newname)\n\n            files2 = [f for f in os.listdir(directory) if f[-4:].lower() in (\".jpg\",\".JPG\",\".png\",\".PNG\",\".jpeg\",\".JPEG\",\".bmp\",\".gif\")]\n            for (index, filename) in enumerate(files2):\n                file = directory + \"/\" + filename\n                extension = os.path.splitext(filename)[1]\n                newname = \"Image-%05d%s\" % (index + 1, extension)\n                if os.path.exists(newname):\n                    continue\n                if True:\n                    os.rename(file, directory + \"/\" + newname)\n            imagerenaming.destroy()\n        os._exit(0)\n    else:\n        showwarning(__filename__, \"Erreur : Aucun dossier n'a \u00e9t\u00e9 s\u00e9lectionn\u00e9 !\")\n\nimagerenaming = Tk()\nwidth = 800\nheight = 500\nimagerenaming.update_idletasks()\nx = (imagerenaming.winfo_screenwidth() - width) // 2\ny = (imagerenaming.winfo_screenheight() - height) // 2\nimagerenaming.geometry(\"{}x{}+{}+{}\".format(width , height, int(x), int(y)))\nimagerenaming.resizable(width=False, height=False)\nimagerenaming.configure(bg = \"lightgray\")\nif os.path.exists(__iconpath__):\n    imagerenaming.iconbitmap(__iconpath__)\nimagerenaming.title(__filename__)\nLabel(imagerenaming, text=\"Bienvenue dans le programme de renommage !\", font=\"impact 30\", fg=\"red\", bg=\"lightgray\").pack(pady=60)\ncheck_var = IntVar()\ncheck_var.set(0)\nRadiobutton(imagerenaming, text=\"Date de cr\u00e9ation\", variable=check_var, value=0, font=\"impact 20\", bg=\"lightgray\").pack(pady=10)\nRadiobutton(imagerenaming, text=\"Date de modification\", variable=check_var, value=1, font=\"impact 20\", bg=\"lightgray\").pack()\nButton(imagerenaming, text=\"Renommer des images\", command=start_rename, relief=GROOVE, width=25, font=\"impact 20\", fg=\"black\").pack(pady=50)\nimagerenaming.mainloop()\n",
        "summary": "This Python script is a GUI application for renaming image files in a selected directory. It allows users to choose between renaming based on file creation or modification dates and provides an option to rename all images sequentially. The script also includes functionality to check for updates, download them if available, and restart the program with the new version."
    },
    {
        "code": "from django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('exam', '0006_exam_duration'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='exam',\n            name='duration',\n            field=models.CharField(default=0, max_length=4, verbose_name='Durasi Ujian'),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that alters the 'duration' field of the 'Exam' model in the 'exam' app. The change sets the default value to 0, limits the maximum length to 4 characters, and updates the verbose name to \"Durasi Ujian\"."
    },
    {
        "code": "import asyncio\nimport inspect\nimport json\nimport os\nimport random\nimport unittest\nfrom unittest.mock import Mock\n\nimport aiohttp\nimport aiohttp.web\nfrom aiohttp.test_utils import unittest_run_loop, setup_test_loop, teardown_test_loop\nimport pep8\n\nimport jsonrpc_base\nimport jsonrpc_websocket.jsonrpc\nfrom jsonrpc_websocket import Server, ProtocolError, TransportError\n\n\nclass JsonTestClient():\n    def __init__(self, loop=None):\n        self.test_server = None\n        self.loop = loop\n        self.connect_side_effect = None\n\n    async def ws_connect(self, *args, **kwargs):\n        if self.connect_side_effect:\n            self.connect_side_effect()\n        self.test_server = JsonTestServer(self.loop)\n        return self.test_server\n\nclass JsonTestServer():\n    def __init__(self, loop=None):\n        self.loop = loop\n        self.send_handler = None\n        self.receive_queue = asyncio.Queue(loop=loop)\n        self._closed = False\n        self.receive_side_effect = None\n\n    async def send_str(self, data):\n        self.send_handler(self, data)\n\n    def test_receive(self, data):\n        self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.TEXT, data, ''))\n\n    def test_binary(self, data=bytes()):\n        self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.BINARY, data, ''))\n\n    def test_error(self):\n        self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.ERROR, 0, ''))\n\n    def test_close(self):\n        self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.CLOSED, 0, ''))\n\n    def test_ping(self):\n        self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.PING, 0, ''))\n\n    async def receive(self):\n        value = await self.receive_queue.get()\n        if self.receive_side_effect:\n            self.receive_side_effect()\n        return (value)\n\n    async def close(self):\n        if not self._closed:\n            self._closed = True\n            self.receive_queue.put_nowait(aiohttp.WSMessage(aiohttp.WSMsgType.CLOSED, 0, ''))\n\nclass TestCase(unittest.TestCase):\n    def assertSameJSON(self, json1, json2):\n        \n        return self.assertDictEqual(json.loads(json1), json.loads(json2))\n\n    def assertRaisesRegex(self, *args, **kwargs):\n        return super(TestCase, self).assertRaisesRegex(*args, **kwargs)\n\n\nclass TestJSONRPCClient(TestCase):\n\n    def setUp(self):\n        self.loop = setup_test_loop()\n        self.client = JsonTestClient(self.loop)\n        random.randint = Mock(return_value=1)\n        self.server = Server('/xmlrpc', session=self.client, timeout=0.2)\n        self.ws_loop_future = self.loop.run_until_complete(self.server.ws_connect())\n\n    def tearDown(self):\n        if self.server.connected:\n            self.client.test_server.test_close()\n            self.loop.run_until_complete(self.ws_loop_future)\n        teardown_test_loop(self.loop)\n\n    @property\n    def handler(self):\n        return self.client.test_server.send_handler\n\n    @handler.setter\n    def handler(self, value):\n        self.client.test_server.send_handler = value\n\n    def receive(self, data):\n        self.client.test_server.test_receive(data)\n\n    def receive_binary(self, data):\n        self.client.test_server.test_binary(data)\n\n    def test_pep8_conformance(self):\n        \n\n        source_files = []\n        project_dir = os.path.dirname(os.path.abspath(__file__))\n        package_dir = os.path.join(project_dir, 'jsonrpc_async')\n        for root, directories, filenames in os.walk(package_dir):\n            source_files.extend([os.path.join(root, f) for f in filenames if f.endswith('.py')])\n\n        pep8style = pep8.StyleGuide(quiet=False, max_line_length=120)\n        result = pep8style.check_files(source_files)\n        self.assertEqual(result.total_errors, 0, \"Found code style errors (and warnings).\")\n\n    def test_pending_message_response(self):\n        pending_message = jsonrpc_websocket.jsonrpc.PendingMessage(loop=self.loop)\n        pending_message.response = 10\n        self.assertEqual(pending_message.response, 10)\n\n    @unittest_run_loop\n    async def test_send_message(self):\n        \n        with self.assertRaises(TransportError) as transport_error:\n            def handler(server, data):\n                try:\n                    asyncio.wait(asyncio.sleep(10, loop=self.loop))\n                except asyncio.CancelledError:\n                    \n                    pass\n\n            self.handler = handler\n            await self.server.send_message(jsonrpc_base.Request('my_method', params=None, msg_id=1))\n\n        self.assertIsInstance(transport_error.exception.args[1], asyncio.TimeoutError)\n\n    @unittest_run_loop\n    async def test_client_closed(self):\n        await self.server.close()\n        with self.assertRaisesRegex(TransportError, 'Client is not connected.'):\n            def handler(server, data):\n                pass\n            self.handler = handler\n            await self.server.send_message(jsonrpc_base.Request('my_method', params=None, msg_id=1))\n\n    @unittest_run_loop\n    async def test_double_connect(self):\n        with self.assertRaisesRegex(TransportError, 'Connection already open.'):\n            await self.server.ws_connect()\n\n    @unittest_run_loop\n    async def test_ws_error(self):\n        self.client.test_server.test_error()\n        with self.assertRaisesRegex(TransportError, 'Websocket error detected. Connection closed.'):\n            await self.ws_loop_future\n\n    @unittest_run_loop\n    async def test_binary(self):\n        self.client.test_server.test_binary()\n\n    @unittest_run_loop\n    async def test_message_not_json(self):\n        with self.assertRaises(TransportError) as transport_error:\n            self.receive('not json')\n            await self.ws_loop_future\n        self.assertIsInstance(transport_error.exception.args[1], ValueError)\n\n    @unittest_run_loop\n    async def test_message_binary_not_utf8(self):\n        \n        \n        \n        self.receive_binary(bytes((0xE0, 0x80, 0x80)))\n        self.client.test_server.test_close()\n        await self.ws_loop_future\n\n    @unittest_run_loop\n    async def test_message_binary_not_json(self):\n        \n        \n        \n        self.receive_binary('not json'.encode())\n        self.client.test_server.test_close()\n        await self.ws_loop_future\n\n    @unittest_run_loop\n    async def test_message_ping_ignored(self):\n        self.client.test_server.test_ping()\n        self.client.test_server.test_close()\n        await self.ws_loop_future\n\n    @unittest_run_loop\n    async def test_connection_timeout(self):\n        def bad_connect():\n            raise aiohttp.ClientError(\"Test Error\")\n        self.client.connect_side_effect = bad_connect\n        await self.server.close()\n        with self.assertRaises(TransportError) as transport_error:\n            await self.server.ws_connect()\n        self.assertIsInstance(transport_error.exception.args[1], aiohttp.ClientError)\n\n    @unittest_run_loop\n    async def test_server_request(self):\n        def test_method():\n            return 1\n        self.server.test_method = test_method\n\n        def handler(server, data):\n            response = json.loads(data)\n            self.assertEqual(response[\"result\"], 1)\n        self.handler = handler\n\n        self.receive('{\"jsonrpc\": \"2.0\", \"method\": \"test_method\", \"id\": 1}')\n\n    @unittest_run_loop\n    async def test_server_request_binary(self):\n        \n        \n        def test_method():\n            return 1\n        self.server.test_method = test_method\n\n        def handler(server, data):\n            response = json.loads(data)\n            self.assertEqual(response[\"result\"], 1)\n\n        self.handler = handler\n\n        self.receive_binary('{\"jsonrpc\": \"2.0\", \"method\": \"test_method\", \"id\": 1}'.encode())\n\n    @unittest_run_loop\n    async def test_server_notification(self):\n        def test_method():\n            pass\n        self.server.test_method = test_method\n        self.receive('{\"jsonrpc\": \"2.0\", \"method\": \"test_method\"}')\n\n    @unittest_run_loop\n    async def test_server_response_error(self):\n        def test_method():\n            return 1\n        self.server.test_method = test_method\n\n        def receive_side_effect():\n            raise aiohttp.ClientError(\"Test Error\")\n        self.client.test_server.receive_side_effect = receive_side_effect\n        self.receive('{\"jsonrpc\": \"2.0\", \"method\": \"test_method\", \"id\": 1}')\n\n        with self.assertRaises(TransportError) as transport_error:\n            await self.ws_loop_future\n        self.assertIsInstance(transport_error.exception.args[1], aiohttp.ClientError)\n\n    @unittest_run_loop\n    async def test_calls(self):\n        \n        def handler1(server, data):\n            request = json.loads(data)\n            self.assertEqual(request[\"params\"], [42, 23])\n            server.test_receive('{\"jsonrpc\": \"2.0\", \"result\": 19, \"id\": 1}')\n\n        self.handler = handler1\n        self.assertEqual((await self.server.subtract(42, 23)), 19)\n\n        def handler2(server, data):\n            request = json.loads(data)\n            self.assertEqual(request[\"params\"], {'y': 23, 'x': 42})\n            server.test_receive('{\"jsonrpc\": \"2.0\", \"result\": 19, \"id\": 1}')\n\n        self.handler = handler2\n        self.assertEqual((await self.server.subtract(x=42, y=23)), 19)\n\n        def handler3(server, data):\n            request = json.loads(data)\n            self.assertEqual(request[\"params\"], {'foo': 'bar'})\n\n        self.handler = handler3\n        await self.server.foobar({'foo': 'bar'}, _notification=True)\n\n    @unittest_run_loop\n    async def test_simultaneous_calls(self):\n        \n        \n        def handler(server, data):\n            pass\n\n        self.handler = handler\n\n        random.randint = Mock(return_value=1)\n        task1 = self.loop.create_task(self.server.call1())\n        random.randint = Mock(return_value=2)\n        task2 = self.loop.create_task(self.server.call2())\n\n        self.assertFalse(task1.done())\n        self.assertFalse(task2.done())\n\n        self.receive('{\"jsonrpc\": \"2.0\", \"result\": 2, \"id\": 2}')\n        await task2\n\n        self.assertFalse(task1.done())\n        self.assertTrue(task2.done())\n\n        self.receive('{\"jsonrpc\": \"2.0\", \"result\": 1, \"id\": 1}')\n        await task1\n\n        self.assertTrue(task1.done())\n        self.assertTrue(task2.done())\n\n        self.assertEqual(1, task1.result())\n        self.assertEqual(2, task2.result())\n\n    @unittest_run_loop\n    async def test_notification(self):\n        \n        def handler(server, data):\n            pass\n        self.handler = handler\n        self.assertIsNone((await self.server.subtract(42, 23, _notification=True)))\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This code is a unit test suite for a JSON-RPC server implementation. The `JsonRpcServerTest` class contains several methods that simulate different scenarios and test the behavior of the server.\n\nHere's a breakdown of some key parts:\n\n1. **Setup and Teardown**: The `setUp` method initializes the necessary objects (like the server instance) before each test, while `tearDown` cleans up after tests.\n\n2. **Mocking Functions**: Various functions are mocked using `unittest.mock.patch` to simulate external behavior or control flow during testing.\n\n3. **Test Methods**:\n   - `test_server_request`: Tests handling of JSON-RPC requests.\n   - `test_server_notification`: Tests handling of JSON-RPC notifications (requests without an ID).\n   - `test_calls`: Tests making synchronous and asynchronous calls to the server methods.\n   - `test_simultaneous_calls`: Tests handling multiple simultaneous calls.\n\n4. **Assertions**: The tests use assertions like `self.assertEqual`, `self.assertTrue`, etc., to verify that the server's behavior matches expectations.\n\n5. **Error Handling**: Some tests include error handling scenarios, such as simulating client errors or exceptions during request processing.\n\n6. **Asynchronous Testing**: Many tests are marked with `@unittest_run_loop` and use asyncio for asynchronous operations.\n\n7. **Side Effects**: Methods like `test_receive` and `receive_side_effect` allow controlling the server's response to incoming requests.\n\nOverall, this test suite provides comprehensive coverage of the JSON-RPC server's functionality, ensuring that it correctly handles various types of requests, notifications, and error conditions."
    },
    {
        "code": "import sys\n\n\nimport pytest\n\n\nfrom ddtrace.compat import to_unicode, PY2, reraise, get_connection_response\n\n\n\n\nif PY2:\n    class TestCompatPY2(object):\n\n        def test_to_unicode_string(self):\n            \n            res = to_unicode('test')\n            assert type(res) == unicode\n            assert res == 'test'\n\n        def test_to_unicode_unicode_encoded(self):\n            \n            res = to_unicode('\\xc3\\xbf')\n            assert type(res) == unicode\n            assert res == u'\u00ff'\n\n        def test_to_unicode_unicode_double_decode(self):\n            \n            \n            \n            res = to_unicode('\\xc3\\xbf'.decode('utf-8'))\n            assert type(res) == unicode\n            assert res == u'\u00ff'\n\n        def test_to_unicode_unicode_string(self):\n            \n            res = to_unicode(u'\u00ff')\n            assert type(res) == unicode\n            assert res == u'\u00ff'\n\n        def test_to_unicode_bytearray(self):\n            \n            res = to_unicode(bytearray('\\xc3\\xbf'))\n            assert type(res) == unicode\n            assert res == u'\u00ff'\n\n        def test_to_unicode_bytearray_double_decode(self):\n            \n            \n            \n            res = to_unicode(bytearray('\\xc3\\xbf').decode('utf-8'))\n            assert type(res) == unicode\n            assert res == u'\u00ff'\n\n        def test_to_unicode_non_string(self):\n            \n            assert to_unicode(1) == u'1'\n            assert to_unicode(True) == u'True'\n            assert to_unicode(None) == u'None'\n            assert to_unicode(dict(key='value')) == u'{\\'key\\': \\'value\\'}'\n\n        def test_get_connection_response(self):\n            \n\n            class MockConn(object):\n                def getresponse(self, *args, **kwargs):\n                    assert 'buffering' in kwargs\n\n            mock = MockConn()\n            get_connection_response(mock)\n\nelse:\n    class TestCompatPY3(object):\n        def test_to_unicode_string(self):\n            \n            res = to_unicode('test')\n            assert type(res) == str\n            assert res == 'test'\n\n        def test_to_unicode_unicode_encoded(self):\n            \n            res = to_unicode('\\xff')\n            assert type(res) == str\n            assert res == '\u00ff'\n\n        def test_to_unicode_unicode_string(self):\n            \n            res = to_unicode('\u00ff')\n            assert type(res) == str\n            assert res == '\u00ff'\n\n        def test_to_unicode_bytearray(self):\n            \n    Python 3 compatible.\n    \"\"\"\n    def test_reraise(self):\n        \n        with pytest.raises(Exception) as ex:\n            try:\n                raise Exception('Ouch!')\n            except Exception:\n                \n                (typ, val, tb) = sys.exc_info()\n                try:\n                    \n                    \n                    raise Exception('Obfuscate!')\n                except Exception:\n                    pass\n                \n                raise reraise(typ, val, tb)\n        assert ex.value.args[0] == 'Ouch!'\n",
        "summary": "The provided Python code defines test cases for compatibility between Python 2 and Python 3 using the `ddtrace.compat` module. It includes tests for converting various types to Unicode or strings, handling exceptions, and ensuring proper behavior in different Python versions."
    },
    {
        "code": "from hummingbot.client.config.config_var import ConfigVar\nfrom hummingbot.client.config.config_validators import (\n    validate_exchange,\n    validate_market_trading_pair,\n)\nfrom hummingbot.client.settings import (\n    required_exchanges,\n    EXAMPLE_PAIRS,\n)\nfrom typing import Optional\n\n\ndef symbol_prompt():\n    exchange = dev_5_vwap_config_map.get(\"exchange\").value\n    example = EXAMPLE_PAIRS.get(exchange)\n    return \"Enter the trading pair you would like to trade on %s%s >>> \" \\\n           % (exchange, f\" (e.g. {example})\" if example else \"\")\n\n\ndef str2bool(value: str):\n    return str(value).lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n\n\ndef validate_market_trading_pair_tuple(value: str) -> Optional[str]:\n    market = dev_5_vwap_config_map.get(\"exchange\").value\n    return validate_market_trading_pair(market, value)\n\n\ndef order_percent_of_volume_prompt():\n    percent_slippage = dev_5_vwap_config_map.get(\"percent_slippage\").value\n    return (\"What percent of open order volume up to %s percent slippage do you want\" % percent_slippage\n            + \"each order to be? (default is 100 percent)? >>> \")\n\n\ndev_5_vwap_config_map = {\n    \"strategy\":\n        ConfigVar(key=\"strategy\",\n                  prompt=\"\",\n                  default=\"dev_5_vwap\"),\n    \"exchange\":\n        ConfigVar(key=\"exchange\",\n                  prompt=\"Enter the name of the exchange >>> \",\n                  validator=validate_exchange,\n                  on_validated=lambda value: required_exchanges.append(value),\n                  prompt_on_new=True),\n    \"market\":\n        ConfigVar(key=\"market\",\n                  prompt=symbol_prompt,\n                  validator=validate_market_trading_pair_tuple,\n                  prompt_on_new=True),\n    \"order_type\":\n        ConfigVar(key=\"order_type\",\n                  prompt=\"Enter type of order (limit/market) default is market >>> \",\n                  type_str=\"str\",\n                  validator=lambda v: None if v in {\"limit\", \"market\", \"\"} else \"Invalid order type.\",\n                  default=\"market\"),\n    \"order_amount\":\n        ConfigVar(key=\"order_amount\",\n                  prompt=\"What is your preferred quantity (denominated in the base asset, default is 1)? \"\n                         \">>> \",\n                  default=1.0,\n                  type_str=\"float\"),\n    \"is_buy\":\n        ConfigVar(key=\"is_buy\",\n                  prompt=\"Enter True for Buy order and False for Sell order (default is Buy Order) >>> \",\n                  type_str=\"bool\",\n                  default=True),\n    \"is_vwap\":\n        ConfigVar(key=\"is_vwap\",\n                  prompt=\"Would you like to use VWAP or TWAP? (default is VWAP) >>> \",\n                  type_str=\"bool\",\n                  default=True),\n    \"num_individual_orders\":\n        ConfigVar(key=\"num_individual_orders\",\n                  prompt=\"Into how many individual orders do you want to split this order? (Enter 10 to indicate 10 individual orders. \"\n                         \"Default is 1)? >>> \",\n                  required_if=lambda: dev_5_vwap_config_map.get(\"is_vwap\").value is False,\n                  type_str=\"float\",\n                  default=1),\n    \"percent_slippage\":\n        ConfigVar(key=\"percent_slippage\",\n                  prompt=\"What percent of price do you want to calculate open order volume? (default is 0 percent slippage) >>> \",\n                  required_if=lambda: dev_5_vwap_config_map.get(\"is_vwap\").value is True,\n                  type_str=\"float\",\n                  default=0.1),\n    \"order_percent_of_volume\":\n        ConfigVar(key=\"order_percent_of_volume\",\n                  prompt=order_percent_of_volume_prompt,\n                  required_if=lambda: dev_5_vwap_config_map.get(\"is_vwap\").value is True,\n                  type_str=\"float\",\n                  default=0.01),\n    \"time_delay\":\n        ConfigVar(key=\"time_delay\",\n                  prompt=\"How many seconds do you want to wait between each individual order? (Enter 10 to indicate 10 seconds. \"\n                         \"Default is 10)? >>> \",\n                  type_str=\"float\",\n                  default=10),\n    \"order_price\":\n        ConfigVar(key=\"order_price\",\n                  prompt=\"What is the price of the limit order ? >>> \",\n                  required_if=lambda: dev_5_vwap_config_map.get(\"order_type\").value == \"limit\",\n                  type_str=\"float\"),\n    \"cancel_order_wait_time\":\n        ConfigVar(key=\"cancel_order_wait_time\",\n                  prompt=\"How long do you want to wait before cancelling your limit order (in seconds). \"\n                         \"(Default is 60 seconds) ? >>> \",\n                  required_if=lambda: dev_5_vwap_config_map.get(\"order_type\").value == \"limit\",\n                  type_str=\"float\",\n                  default=60),\n\n}\n",
        "summary": "The provided Python code defines a configuration map for a trading strategy using the Hummingbot client, including prompts and validators for various parameters such as exchange, market trading pair, order type, and slippage settings. It also includes utility functions like `symbol_prompt` to generate prompts based on the current configuration and `str2bool` to convert string inputs to boolean values."
    },
    {
        "code": "from django.views.generic import View\nfrom django.http import HttpResponse\nfrom django.conf import settings\nimport os\n\n\nclass ReactAppView(View):\n\n    def get(self, request):\n        try:\n            with open(os.path.join(str(settings.ROOT_DIR), 'frontend', 'build', 'index.html')) as file:\n                return HttpResponse(file.read())\n\n        except:\n            return HttpResponse(\n                ,\n                status=501,\n            )\n",
        "summary": "The `ReactAppView` class in Django serves as a view to handle GET requests by reading and returning the contents of an `index.html` file located in the frontend build directory, ensuring that the React application is properly served. If the file cannot be found or accessed, it returns a 501 Not Implemented status."
    },
    {
        "code": "from scrapy.spiders import Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nXPATH = {\n    'name' : \"//div[@class='ProductMain']/div[@class='product-title']/h1\",\n    'price' : \"//div[@class='Row Price']/div[@class='ProductPrice VariationProductPrice']\",\n    'category' : \"//div[@id='Breadcrumb']/ul/li/a\",\n    'description' : \"//div[@id='ProductDescription']/div[@class='ProductDescriptionContainer']\",\n    'images' : \"//div[@class='ProductThumbImage']/a/@href\",\n    'canonical' : \"//link[@rel='canonical']/@href\",\n    'base_url' : \"\",\n    'brand' : \"\"\n}\nname = 'dcmobile.vn'\nallowed_domains = ['dcmobile.vn']\nstart_urls = ['http://dcmobile.vn/']\ntracking_url = ''\nsitemap_urls = ['']\nsitemap_rules = [('', 'parse_item')]\nsitemap_follow = []\nrules = [\n    Rule(LinkExtractor(allow = ['/[a-zA-Z0-9-]+-\\d+\\.html$']), 'parse_item'),\n    Rule(LinkExtractor(deny = ['/ban-tin'], allow = ['/[a-zA-Z0-9-]+-b+\\d+\\.html']), 'parse'),\n    \n]\n",
        "summary": "This Python code defines a Scrapy spider for scraping data from the website dcmobile.vn, including product details such as name, price, category, and images. It uses XPath expressions to extract information and follows specific rules for navigating through the site's pages."
    },
    {
        "code": "from test_framework.test_framework import BayemcoinTestFramework\nfrom test_framework.util import (\n    assert_equal,\n    assert_raises_rpc_error,\n)\n\nclass NamedArgumentTest(BayemcoinTestFramework):\n    def set_test_params(self):\n        self.num_nodes = 1\n\n    def run_test(self):\n        node = self.nodes[0]\n        h = node.help(command='getinfo')\n        assert(h.startswith('getinfo\\n'))\n\n        assert_raises_jsonrpc(-8, 'Unknown named parameter', node.help, random='getinfo')\n\n        h = node.getblockhash(height=0)\n        node.getblock(blockhash=h)\n\n        assert_equal(node.echo(), [])\n        assert_equal(node.echo(arg0=0,arg9=9), [0] + [None]*8 + [9])\n        assert_equal(node.echo(arg1=1), [None, 1])\n        assert_equal(node.echo(arg9=None), [None]*10)\n        assert_equal(node.echo(arg0=0,arg3=3,arg9=9), [0] + [None]*2 + [3] + [None]*5 + [9])\n\nif __name__ == '__main__':\n    NamedArgumentTest().main()\n",
        "summary": "The provided Python code defines a test class `NamedArgumentTest` that inherits from `BayemcoinTestFramework`. It includes methods to set test parameters and run tests, focusing on verifying the functionality of named arguments in RPC commands for a blockchain node. The tests check the behavior of help commands, block retrieval, and echo commands with various named arguments, ensuring they handle missing or incorrect parameters gracefully."
    },
    {
        "code": "import pandas as pd \nimport numpy as np \nimport cv2 \nfrom scipy.signal import wiener\n\nnp.random.seed(1207) \n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n\n\n\n\n\n\ndf_train = pd.read_json('./input/train.json') \n\n\n\n\n\n\n\ndef get_scaled_imgs(df):\n    imgs = []\n    \n    for i, row in df.iterrows():\n        band_1 = np.array(row['band_1'])\n        band_2 = np.array(row['band_2'])\n\n        \n        band_1 = band_1.reshape(75, 75)\n        band_2 = band_2.reshape(75, 75)\n        \n        \n        \n        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n        \n\n        imgs.append(np.dstack((a, b)))\n\n    return np.array(imgs)\n\n\n\n\ndef get_more_images(imgs):\n    \n    more_images = []\n    vert_flip_imgs = []\n    hori_flip_imgs = []\n      \n    for i in range(0,imgs.shape[0]):\n        a=imgs[i,:,:,0]\n        b=imgs[i,:,:,1]\n        \n        \n        av=cv2.flip(a,1)\n        ah=cv2.flip(a,0)\n        bv=cv2.flip(b,1)\n        bh=cv2.flip(b,0)\n        \n        \n        \n        \n        \n        vert_flip_imgs.append(np.dstack((av, bv)))\n        hori_flip_imgs.append(np.dstack((ah, bh)))\n      \n    v = np.array(vert_flip_imgs)\n    h = np.array(hori_flip_imgs)\n       \n    more_images = np.concatenate((imgs,v,h))\n    \n    return more_images\n\n\ndef getModel():\n    \n    \n    model=Sequential()\n    \n    \n    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 2)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    \n    \n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n\n    \n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n\n    \n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    \n    \n    model.add(Flatten())\n\n    \n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n\n    \n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.2))\n\n    \n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    optimizer = Adam(lr=0.0001, decay=0.0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    return model\n\n\n\nXtrain = get_scaled_imgs(df_train)\nYtrain = np.array(df_train['is_iceberg'])\ndf_train.inc_angle = df_train.inc_angle.replace('na',0)\nidx_tr = np.where(df_train.inc_angle>0)\n\nYtrain = Ytrain[idx_tr[0]]\nXtrain = Xtrain[idx_tr[0],...]\n\n\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(Xtrain, Ytrain, test_size=0.1)\n\nX_train_more = get_more_images(X_train)\ny_train_more = np.concatenate([y_train, y_train, y_train])\nX_valid_more = get_more_images(X_valid)\ny_valid_more = np.concatenate([y_valid, y_valid, y_valid])\n\n\nmodel = getModel()\nmodel.summary()\n\nbatch_size = 32\nmodel_file = '.mdl_2l2_wts.hdf5'\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint(model_file, save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-6, mode='min')\n\n\n\n\n\nmodel.fit(X_train_more, y_train_more, batch_size=32, epochs=60, verbose=1,\n                     callbacks=[mcp_save, reduce_lr_loss],\n                     validation_data=(X_valid, y_valid))\n\n\nmodel.load_weights(filepath = model_file)\n\nscore = model.evaluate(Xtrain, Ytrain, verbose=1)\nprint('Train score:', score[0])\nprint('Train accuracy:', score[1])\n\n\ndf_test = pd.read_json('./input/test.json')\ndf_test.inc_angle = df_test.inc_angle.replace('na',0)\nXtest = (get_scaled_imgs(df_test))\npred_test = model.predict(Xtest)\n\nsubmission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\nprint(submission.head(10))\n\nsubmission.to_csv('sub-2bands-nodrop-aug.csv', index=False)\n\n",
        "summary": "The provided Python code is a machine learning pipeline for image classification, specifically designed to identify icebergs from satellite imagery. It uses convolutional neural networks (CNNs) with Keras, processes the input images through scaling and augmentation techniques, trains the model on labeled data, and finally evaluates its performance on test data, outputting predictions in a submission file."
    },
    {
        "code": "from concurrent import futures\n\ndef naehere_pi_an(n):\n    pi_halbe = 1\n    zaehler, nenner = 2.0, 1.0\n    \n    for i in range(n):\n        pi_halbe *= zaehler / nenner\n        if i % 2:\n            zaehler += 2\n        else:\n            nenner += 2\n            \n    return 2*pi_halbe\n\n\nN = (\n    12345678,\n    1234567,\n    123456,\n    12345,\n    1234\n)\n\nwith futures.ThreadPoolExecutor(max_workers=4) as ex:       \n    print(list(ex.map(naehere_pi_an, N)))\n    ",
        "summary": "The provided Python code defines a function `naehere_pi_an` that calculates an approximation of \u03c0 using the Leibniz formula for \u03c0. It then uses the `concurrent.futures.ThreadPoolExecutor` to parallelize the execution of this function with multiple input values stored in the tuple `N`, printing the results of these calculations."
    },
    {
        "code": "__all__ = ['nicovideo_download']\n\nfrom ..common import *\n\ndef nicovideo_login(user, password):\n    data = \"current_form=login&mail=\" + user +\"&password=\" + password + \"&login_submit=Log+In\"\n    response = request.urlopen(request.Request(\"https://secure.nicovideo.jp/secure/login?site=niconico\", headers=fake_headers, data=data.encode('utf-8')))\n    return response.headers\n\ndef nicovideo_download(url, output_dir='.', merge=True, info_only=False):\n    import ssl\n    ssl_context = request.HTTPSHandler(\ncontext=ssl.SSLContext(ssl.PROTOCOL_TLSv1))\n    cookie_handler = request.HTTPCookieProcessor()\n    opener = request.build_opener(ssl_context, cookie_handler)\n    request.install_opener(opener)\n\n    import netrc, getpass\n    try:\n        info = netrc.netrc().authenticators('nicovideo')\n    except FileNotFoundError:\n        info = None\n    if info is None:\n        user = input(\"User:     \")\n        password = getpass.getpass(\"Password: \")\n    else:\n        user, password = info[0], info[2]\n    print(\"Logging in...\")\n    nicovideo_login(user, password)\n\n    html = get_html(url) \n    title = unicodize(r1(r'<span class=\"videoHeaderTitle\"[^>]*>([^<]+)</span>', html))\n\n    vid = url.split('/')[-1].split('?')[0]\n    api_html = get_html('http://www.nicovideo.jp/api/getflv?v=%s' % vid)\n    real_url = parse.unquote(r1(r'url=([^&]+)&', api_html))\n\n    type, ext, size = url_info(real_url)\n\n    print_info(site_info, title, type, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir, merge = merge)\n\nsite_info = \"Nicovideo.jp\"\ndownload = nicovideo_download\ndownload_playlist = playlist_not_supported('nicovideo')\n",
        "summary": "The provided Python code defines functions for logging into Nicovideo and downloading videos from the platform. It includes a login function that handles user credentials and a download function that retrieves video URLs, checks their details, and downloads them to a specified directory."
    },
    {
        "code": "import pytest\n\nfrom .base import TestBaseClass\n\n\nclass TestClassOelintVarsValueQuoted(TestBaseClass):\n\n    @pytest.mark.parametrize('id', ['oelint.vars.valuequoted'])\n    @pytest.mark.parametrize('occurrence', [2])\n    @pytest.mark.parametrize('input',\n                             [\n                                 {\n                                     'oelint_adv_test.bb':\n                                     ,\n                                 },\n                             ],\n                             )\n    def test_bad(self, input, id, occurrence):\n        self.check_for_id(self._create_args(input), id, occurrence)\n\n    @pytest.mark.parametrize('id', ['oelint.vars.valuequoted'])\n    @pytest.mark.parametrize('occurrence', [0])\n    @pytest.mark.parametrize('input',\n                             [\n                                 {\n                                     'oelint_adv_test.bb':\n                                     'A = \"a\"',\n                                 },\n                                 {\n                                     'oelint_adv_test.bb':\n                                     'A += \"b\"',\n                                 },\n                                 {\n                                     'oelint_adv_test.bb':\n                                     'PACKAGECONFIG[foo] = \"-DFOO=ON,-DFOO=OFF,\"',\n                                 },\n                                 {\n                                     'oelint_adv_test.bb':\n                                     'EXTRA_OEMAKE = \\'CROSS_COMPILE=${TARGET_PREFIX} CC=\"${TARGET_PREFIX}gcc ${TOOLCHAIN_OPTIONS}\" V=1\\'',\n                                 },\n                                 {\n                                     'oelint_adv_test.bb':\n                                     ,\n                                 },\n                                 {\n                                     'oelint_adv_test.bb':\n                                     ,\n                                 },\n                             ],\n                             )\n    def test_good(self, input, id, occurrence):\n        self.check_for_id(self._create_args(input), id, occurrence)\n",
        "summary": "The provided Python code defines a test class `TestClassOelintVarsValueQuoted` that inherits from `TestBaseClass`. It uses the `pytest` framework to parameterize tests for both bad and good cases related to quoted values in BitBake configuration files. The `test_bad` method checks for occurrences of unquoted values, while the `test_good` method verifies that quoted values are correctly formatted according to specified rules."
    },
    {
        "code": "__version__ = '0.9.0'\n",
        "summary": "The provided Python code snippet sets the version of a module to '0.9.0'."
    },
    {
        "code": "def solution(s):\n    \n    if( s==''.join(reversed(s))):\n        return  bool(True)\n    return bool(False)\n\nif __name__ == \"__main__\":\n    print('----------start------------')\n    s = \"zork\"\n    print(solution( s ))\n    print('------------end------------')\n",
        "summary": "The Python code defines a function `solution` that checks if a given string `s` is a palindrome by comparing it to its reverse. If they are the same, it returns `True`; otherwise, it returns `False`. The main block tests this function with the string \"zork\", which is not a palindrome, so it prints `False`."
    },
    {
        "code": "import ctypes\nfrom . import vxlapy\n\ndef stringify(cobj, indent=2):\n    s = \"%s\\n\" % type(cobj)\n    if issubclass(type(cobj), ctypes.Union):\n        cobj = getattr(cobj, cobj._fields_[0][0])\n    if issubclass(type(cobj), ctypes.Structure):\n        for field in cobj._fields_:\n            s += \"%s%s=%s\\n\" % (indent * ' ', field[0], stringify(getattr(cobj, field[0]), indent + 2))\n        return s\n    try:\n        return bytearray(cobj[:])\n    except TypeError:\n        return \"%d (0x%x)\" % (cobj, cobj)\n\n\ndef debugwrap(func):\n    def caller(*args, **kwargs):\n        if hasattr(args[0], 'debug') and args[0].debug:\n            print(args[0].__class__.__name__, repr(func), repr(args), repr(kwargs))\n        return func(*args, **kwargs)\n    return caller\n\nclass VxlBaseException(Exception):\n    pass\n\n\nclass VxlBaseEvent(object):\n\n    def __str__(self):\n        return stringify(getattr(self.event.tagData, self.tagDataAttr))\n\n\nclass VxlBase(object):\n\n    def __init__(self, debug=False, debugapi=False):\n        self.api = vxlapy.vxlapy(trace=debugapi)\n        \n        self.debug = debug\n        self.initAccess = False\n        self.portIsOpen = False\n        self.portHandle = vxlapy.XLportHandle(vxlapy.XL_INVALID_PORTHANDLE)\n        self.accessMask = vxlapy.XLaccess(0)\n        self.permissionMask = vxlapy.XLaccess(0)\n\n        self.api.xlOpenDriver()\n\n    @debugwrap\n    def __del__(self):\n        self.api.xlCloseDriver()\n\n    @debugwrap\n    def getchannelIdx(self, channel=0, app_name=None, busType=vxlapy.XL_INTERFACE_VERSION):\n        if app_name is not None:\n            hw_type = ctypes.c_uint(0)\n            hw_index = ctypes.c_uint(0)\n            hw_channel = ctypes.c_uint(0)\n            self.api.xlGetApplConfig(\n                self._app_name, channel, hw_type, hw_index, hw_channel,busType)\n            channelIdx = self.api.xlGetChannelIndex(hw_type.value, hw_index.value,\n                                                       hw_channel.value)\n            if self.debug:\n                print('Channel %d idex %d found'%(channel,channelIdx))\n            if channelIdx < 0:\n                raise VxlBaseException(\"No HW port available\")\n        else:\n            channelIdx = channel\n        return channelIdx\n\n    @debugwrap\n    def getChannelMask(self, busType, channelIdx=1, xlInterfaceVersion=vxlapy.XL_INTERFACE_VERSION):\n        driverConfig = vxlapy.XLdriverConfig()\n        self.api.xlGetDriverConfig(ctypes.byref(driverConfig))\n        for i in range(driverConfig.channelCount):\n            if self.debug:\n                print(\"Channel %d cap 0x%x ifver %d\" % (i, driverConfig.channel[i].channelBusCapabilities, driverConfig.channel[i].interfaceVersion))\n            if (driverConfig.channel[i].channelBusCapabilities & busType and  \n                    driverConfig.channel[i].interfaceVersion >= xlInterfaceVersion):  \n                \n                if self.accessMask.value == 0 and channelIdx == i:\n                    if self.debug:\n                        print(\"Using %s, (sn=%06d, mask=0x%04x)\" % (stringify(driverConfig.channel[i].name), driverConfig.channel[i].serialNumber,\n                                                                    driverConfig.channel[i].channelMask))\n                    self.accessMask.value |= driverConfig.channel[i].channelMask\n                    return True\n                \n        return False\n\n    @debugwrap\n    def openPort(self, busType, userName='vxlapy', accessMask=None, permissionMask=None, rxQueueSize=32768, xlInterfaceVersion=vxlapy.XL_INTERFACE_VERSION_V4):\n        if accessMask is not None:\n            self.accessMask.value = accessMask\n        if permissionMask is not None:\n            self.permissionMask.value = permissionMask\n        if permissionMask is None and self.accessMask.value != 0:\n            self.permissionMask.value = self.accessMask.value\n            self.api.xlOpenPort(ctypes.byref(self.portHandle), userName, self.accessMask.value, ctypes.byref(self.permissionMask), rxQueueSize, xlInterfaceVersion, busType)\n            self.portIsOpen = True\n\n            self.initAccess = self.permissionMask.value == self.accessMask.value and self.accessMask.value != 0\n        else:\n            raise VxlBaseException(\"No HW port available\")\n\n    @debugwrap\n    def activateChannel(self, bustype):\n        return self.api.xlActivateChannel(self.portHandle, self.accessMask, bustype, 0)\n\n    @debugwrap\n    def deactivateChannel(self):\n        return self.api.xlDeactivateChannel(self.portHandle, self.accessMask)\n\n    @debugwrap\n    def flush_rx_buffer(self):\n        self.api.xlFlushReceiveQueue(self.portHandle)\n\n    @debugwrap\n    def flush_tx_buffer(self):\n        self.api.xlCanFlushTransmitQueue(self.portHandle, self.accessMask)\n\n    @debugwrap\n    def closePort(self):\n        if self.portIsOpen:\n            self.api.xlDeactivateChannel(self.portHandle, self.accessMask)\n            self.api.xlClosePort(self.portHandle)\n            self.api.xlCloseDriver()\n            self.portIsOpen = False\n\n    @debugwrap\n    def receive(self):\n        raise NotImplemented\n\n\nif __name__ == \"__main__\":\n    b = VxlBase()\n",
        "summary": "The provided Python code defines a base class `VxlBase` for interacting with hardware ports using the `vxlapy` library, which is wrapped in C types through `ctypes`. It includes methods for opening and closing ports, activating and deactivating channels, flushing buffers, and handling exceptions. The `debugwrap` decorator allows for debugging output when enabled."
    },
    {
        "code": "import pandas as pd\nimport numpy as np\nfrom time import time\nimport sys\n\nclass StateBasedBucketer(object):\n    \n    def __init__(self, encoder):\n        self.encoder = encoder\n        \n        self.dt_states = None\n        self.n_states = 0\n        \n    \n    def fit(self, X, y=None):\n        \n        dt_encoded = self.encoder.fit_transform(X)\n        \n        self.dt_states = dt_encoded.drop_duplicates()\n        self.dt_states = self.dt_states.assign(state = range(len(self.dt_states)))\n        \n        self.n_states = len(self.dt_states)\n        \n        return self\n    \n    \n    def predict(self, X, y=None):\n        \n        dt_encoded = self.encoder.transform(X)\n        \n        dt_transformed = pd.merge(dt_encoded, self.dt_states, how='left')\n        dt_transformed.fillna(-1, inplace=True)\n        \n        return dt_transformed[\"state\"].astype(int).as_matrix()\n    \n    \n    def fit_predict(self, X, y=None):\n        \n        self.fit(X)\n        return self.predict(X)",
        "summary": "The `StateBasedBucketer` class in Python uses an encoder to transform input data into a state-based format. It then fits the model by identifying unique states and assigning them sequential identifiers. The predict method transforms new data using the same encoding and assigns it to the nearest existing state, filling any missing values with -1."
    },
    {
        "code": "import json\n\nfrom alipay.aop.api.constant.ParamConstants import *\n\n\nclass BillDingBizOrderSum(object):\n\n    def __init__(self):\n        self._biz_date = None\n        self._expenses = None\n        self._income = None\n\n    @property\n    def biz_date(self):\n        return self._biz_date\n\n    @biz_date.setter\n    def biz_date(self, value):\n        self._biz_date = value\n    @property\n    def expenses(self):\n        return self._expenses\n\n    @expenses.setter\n    def expenses(self, value):\n        self._expenses = value\n    @property\n    def income(self):\n        return self._income\n\n    @income.setter\n    def income(self, value):\n        self._income = value\n\n\n    def to_alipay_dict(self):\n        params = dict()\n        if self.biz_date:\n            if hasattr(self.biz_date, 'to_alipay_dict'):\n                params['biz_date'] = self.biz_date.to_alipay_dict()\n            else:\n                params['biz_date'] = self.biz_date\n        if self.expenses:\n            if hasattr(self.expenses, 'to_alipay_dict'):\n                params['expenses'] = self.expenses.to_alipay_dict()\n            else:\n                params['expenses'] = self.expenses\n        if self.income:\n            if hasattr(self.income, 'to_alipay_dict'):\n                params['income'] = self.income.to_alipay_dict()\n            else:\n                params['income'] = self.income\n        return params\n\n    @staticmethod\n    def from_alipay_dict(d):\n        if not d:\n            return None\n        o = BillDingBizOrderSum()\n        if 'biz_date' in d:\n            o.biz_date = d['biz_date']\n        if 'expenses' in d:\n            o.expenses = d['expenses']\n        if 'income' in d:\n            o.income = d['income']\n        return o\n\n\n",
        "summary": "The `BillDingBizOrderSum` class is a Python representation of an object that encapsulates business order details such as date, expenses, and income. It provides methods to convert the object to and from a dictionary format suitable for serialization or deserialization, particularly useful for interacting with APIs like Alipay's."
    },
    {
        "code": "class Calculator:\n    def __init__(self):\n        pass\n\n    def add(self, a, b):\n        return a + b\n\n    def divide(self, a, b):\n        return b / a\n\n    \n\n    \n    \n\n\ndef greetings(name):\n    print('Hello ' + name + '!')\n\n\ndef goodbye():\n    print('Goodbye!')\n\n\nmyCalculator = Calculator\nmyCalculator.subtract()\n\n\n\n\n",
        "summary": "The provided Python code defines a `Calculator` class with methods for addition and division, as well as functions for greeting and saying goodbye. However, the subtraction method is not implemented in the `Calculator` class, and an attempt to call it results in an error since it does not exist. Additionally, there are syntax errors in the code, such as missing parentheses when calling the `subtract()` method on the `myCalculator` instance."
    },
    {
        "code": "image_folder = './images'\n\n\nextracted_folder = './extracted_images'\n\n\nmodels = './models'\n\n\ngraphs = './graphs'\n\n\n\nimage_size_vertical = 100\nimage_size_horizontal = 100\n\n\nepoch = 100\n\n\nbatch_size = 64\n\n\ntrain_ratio = 0.6\ntest_ratio = 0.2\nvalidation_ratio = 0.2\n\n\n\n\nx_shape = ()\n\n\nGREY = 1\nRGB = 3\n\n\n\nvertical = \"VERTICAL\"\nhorizontal = \"HORIZONTAL\"\n\n\n\nnum_classes = 0\n\n\n\nperson_label = {}",
        "summary": "The provided Python code sets up variables and constants for a machine learning project, including paths to folders for images, models, and graphs, dimensions for image resizing, training parameters such as epochs and batch size, ratios for data splitting, and color configurations. It also initializes variables for storing the number of classes and person labels."
    },
    {
        "code": "import numpy as np\nimport theano\nimport theano.tensor as TT\n\nfrom rllab.core.serializable import Serializable\nfrom rllab.misc import ext\nfrom rllab.misc import krylov\nfrom rllab.misc import logger\nfrom rllab.misc.ext import sliced_fun\n\n\nclass PerlmutterHvp(Serializable):\n\n    def __init__(self, num_slices=1):\n        Serializable.quick_init(self, locals())\n        self.target = None\n        self.reg_coeff = None\n        self.opt_fun = None\n        self._num_slices = num_slices\n\n    def update_opt(self, f, target, inputs, reg_coeff):\n        self.target = target\n        self.reg_coeff = reg_coeff\n        params = target.get_params(trainable=True)\n\n        constraint_grads = theano.grad(\n            f, wrt=params, disconnected_inputs='warn')\n        xs = tuple([ext.new_tensor_like(\"%s x\" % p.name, p) for p in params])\n\n        def Hx_plain():\n            Hx_plain_splits = TT.grad(\n                TT.sum([TT.sum(g * x)\n                        for g, x in zip(constraint_grads, xs)]),\n                wrt=params,\n                disconnected_inputs='warn'\n            )\n            return TT.concatenate([TT.flatten(s) for s in Hx_plain_splits])\n\n        self.opt_fun = ext.lazydict(\n            f_Hx_plain=lambda: ext.compile_function(\n                inputs=inputs + xs,\n                outputs=Hx_plain(),\n                log_name=\"f_Hx_plain\",\n            ),\n        )\n\n    def build_eval(self, inputs):\n        def eval(x):\n            xs = tuple(self.target.flat_to_params(x, trainable=True))\n            ret = sliced_fun(self.opt_fun[\"f_Hx_plain\"], self._num_slices)(\n                inputs, xs) + self.reg_coeff * x\n            return ret\n\n        return eval\n\n\nclass FiniteDifferenceHvp(Serializable):\n\n    def __init__(self, base_eps=1e-8, symmetric=True, grad_clip=None, num_slices=1):\n        Serializable.quick_init(self, locals())\n        self.base_eps = base_eps\n        self.symmetric = symmetric\n        self.grad_clip = grad_clip\n        self._num_slices = num_slices\n\n    def update_opt(self, f, target, inputs, reg_coeff):\n        self.target = target\n        self.reg_coeff = reg_coeff\n\n        params = target.get_params(trainable=True)\n\n        constraint_grads = theano.grad(\n            f, wrt=params, disconnected_inputs='warn')\n        flat_grad = ext.flatten_tensor_variables(constraint_grads)\n\n        def f_Hx_plain(*args):\n            inputs_ = args[:len(inputs)]\n            xs = args[len(inputs):]\n            flat_xs = np.concatenate([np.reshape(x, (-1,)) for x in xs])\n            param_val = self.target.get_param_values(trainable=True)\n            eps = np.cast['float32'](\n                self.base_eps / (np.linalg.norm(param_val) + 1e-8))\n            self.target.set_param_values(\n                param_val + eps * flat_xs, trainable=True)\n            flat_grad_dvplus = self.opt_fun[\"f_grad\"](*inputs_)\n            if self.symmetric:\n                self.target.set_param_values(\n                    param_val - eps * flat_xs, trainable=True)\n                flat_grad_dvminus = self.opt_fun[\"f_grad\"](*inputs_)\n                hx = (flat_grad_dvplus - flat_grad_dvminus) / (2 * eps)\n                self.target.set_param_values(param_val, trainable=True)\n            else:\n                self.target.set_param_values(param_val, trainable=True)\n                flat_grad = self.opt_fun[\"f_grad\"](*inputs_)\n                hx = (flat_grad_dvplus - flat_grad) / eps\n            return hx\n\n        self.opt_fun = ext.lazydict(\n            f_grad=lambda: ext.compile_function(\n                inputs=inputs,\n                outputs=flat_grad,\n                log_name=\"f_grad\",\n            ),\n            f_Hx_plain=lambda: f_Hx_plain,\n        )\n\n    def build_eval(self, inputs):\n        def eval(x):\n            xs = tuple(self.target.flat_to_params(x, trainable=True))\n            ret = sliced_fun(self.opt_fun[\"f_Hx_plain\"], self._num_slices)(\n                inputs, xs) + self.reg_coeff * x\n            return ret\n\n        return eval\n\n\nclass ConjugateGradientOptimizer(Serializable):\n    \n\n    def __init__(\n            self,\n            cg_iters=10,\n            reg_coeff=1e-5,\n            subsample_factor=1.,\n            backtrack_ratio=0.8,\n            max_backtracks=15,\n            accept_violation=False,\n            hvp_approach=None,\n            num_slices=1):\n        \n        Serializable.quick_init(self, locals())\n        self._cg_iters = cg_iters\n        self._reg_coeff = reg_coeff\n        self._subsample_factor = subsample_factor\n        self._backtrack_ratio = backtrack_ratio\n        self._max_backtracks = max_backtracks\n        self._num_slices = num_slices\n\n        self._opt_fun = None\n        self._target = None\n        self._max_constraint_val = None\n        self._constraint_name = None\n        self._accept_violation = accept_violation\n        if hvp_approach is None:\n            hvp_approach = PerlmutterHvp(num_slices)\n        self._hvp_approach = hvp_approach\n\n    def update_opt(self, loss, target, leq_constraint, inputs, extra_inputs=None, constraint_name=\"constraint\", *args,\n                   **kwargs):\n        \n\n        inputs = tuple(inputs)\n        if extra_inputs is None:\n            extra_inputs = tuple()\n        else:\n            extra_inputs = tuple(extra_inputs)\n\n        constraint_term, constraint_value = leq_constraint\n\n        params = target.get_params(trainable=True)\n        grads = theano.grad(loss, wrt=params, disconnected_inputs='warn')\n        flat_grad = ext.flatten_tensor_variables(grads)\n\n        self._hvp_approach.update_opt(f=constraint_term, target=target, inputs=inputs + extra_inputs,\n                                      reg_coeff=self._reg_coeff)\n\n        self._target = target\n        self._max_constraint_val = constraint_value\n        self._constraint_name = constraint_name\n\n        self._opt_fun = ext.lazydict(\n            f_loss=lambda: ext.compile_function(\n                inputs=inputs + extra_inputs,\n                outputs=loss,\n                log_name=\"f_loss\",\n            ),\n            f_grad=lambda: ext.compile_function(\n                inputs=inputs + extra_inputs,\n                outputs=flat_grad,\n                log_name=\"f_grad\",\n            ),\n            f_constraint=lambda: ext.compile_function(\n                inputs=inputs + extra_inputs,\n                outputs=constraint_term,\n                log_name=\"constraint\",\n            ),\n            f_loss_constraint=lambda: ext.compile_function(\n                inputs=inputs + extra_inputs,\n                outputs=[loss, constraint_term],\n                log_name=\"f_loss_constraint\",\n            ),\n        )\n\n    def loss(self, inputs, extra_inputs=None):\n        inputs = tuple(inputs)\n        if extra_inputs is None:\n            extra_inputs = tuple()\n        return sliced_fun(self._opt_fun[\"f_loss\"], self._num_slices)(inputs, extra_inputs)\n\n    def constraint_val(self, inputs, extra_inputs=None):\n        inputs = tuple(inputs)\n        if extra_inputs is None:\n            extra_inputs = tuple()\n        return sliced_fun(self._opt_fun[\"f_constraint\"], self._num_slices)(inputs, extra_inputs)\n\n    def optimize(self, inputs, extra_inputs=None, subsample_grouped_inputs=None):\n\n        inputs = tuple(inputs)\n        if extra_inputs is None:\n            extra_inputs = tuple()\n\n        if self._subsample_factor < 1:\n            if subsample_grouped_inputs is None:\n                subsample_grouped_inputs = [inputs]\n            subsample_inputs = tuple()\n            for inputs_grouped in subsample_grouped_inputs:\n                n_samples = len(inputs_grouped[0])\n                inds = np.random.choice(\n                    n_samples, int(n_samples * self._subsample_factor), replace=False)\n                subsample_inputs += tuple([x[inds] for x in inputs_grouped])\n        else:\n            subsample_inputs = inputs\n\n        logger.log(\"computing loss before\")\n        loss_before = sliced_fun(self._opt_fun[\"f_loss\"], self._num_slices)(\n            inputs, extra_inputs)\n        logger.log(\"performing update\")\n        logger.log(\"computing descent direction\")\n\n        flat_g = sliced_fun(self._opt_fun[\"f_grad\"], self._num_slices)(\n            inputs, extra_inputs)\n\n        Hx = self._hvp_approach.build_eval(subsample_inputs + extra_inputs)\n\n        descent_direction = krylov.cg(Hx, flat_g, cg_iters=self._cg_iters)\n\n        initial_step_size = np.sqrt(\n            2.0 * self._max_constraint_val *\n            (1. / (descent_direction.dot(Hx(descent_direction)) + 1e-8))\n        )\n        if np.isnan(initial_step_size):\n            initial_step_size = 1.\n        flat_descent_step = initial_step_size * descent_direction\n\n        logger.log(\"descent direction computed\")\n\n        prev_param = np.copy(self._target.get_param_values(trainable=True))\n        n_iter = 0\n        for n_iter, ratio in enumerate(self._backtrack_ratio ** np.arange(self._max_backtracks)):\n            cur_step = ratio * flat_descent_step\n            cur_param = prev_param - cur_step\n            self._target.set_param_values(cur_param, trainable=True)\n            loss, constraint_val = sliced_fun(\n                self._opt_fun[\"f_loss_constraint\"], self._num_slices)(inputs, extra_inputs)\n            if loss < loss_before and constraint_val <= self._max_constraint_val:\n                break\n        if (np.isnan(loss) or np.isnan(constraint_val) or loss >= loss_before or constraint_val >=\n                self._max_constraint_val) and not self._accept_violation:\n            logger.log(\"Line search condition violated. Rejecting the step!\")\n            if np.isnan(loss):\n                logger.log(\"Violated because loss is NaN\")\n            if np.isnan(constraint_val):\n                logger.log(\"Violated because constraint %s is NaN\" %\n                           self._constraint_name)\n            if loss >= loss_before:\n                logger.log(\"Violated because loss not improving\")\n            if constraint_val >= self._max_constraint_val:\n                logger.log(\n                    \"Violated because constraint %s is violated\" % self._constraint_name)\n            self._target.set_param_values(prev_param, trainable=True)\n        logger.log(\"backtrack iters: %d\" % n_iter)\n        logger.log(\"computing loss after\")\n        logger.log(\"optimization finished\")\n",
        "summary": "This code defines a class `ConstrainedOptimizer` that implements an optimization algorithm with constraints. The key features include:\n\n1. **Constraint Handling**: It can handle inequality constraints using the method of Lagrange multipliers.\n\n2. **Gradient and Hessian Approximation**: It computes gradients and approximates Hessians using automatic differentiation (Theano) and a Hessian-vector product approach.\n\n3. **Line Search**: It uses a line search to find an appropriate step size that satisfies the Wolfe conditions for sufficient decrease in function value and curvature condition.\n\n4. **Krylov Subspace Method**: It uses the Conjugate Gradient method from the Krylov subspace to solve the linear system Hx = g, where H is the Hessian approximation and g is the gradient.\n\n5. **Subsampling**: Optionally, it can subsample the data during optimization to reduce computational cost.\n\n6. **Logging**: It includes logging for debugging and monitoring the optimization process.\n\nHere's a breakdown of some key methods:\n\n- `update_opt`: Initializes the optimizer with the objective function, constraints, and parameters.\n  \n- `optimize`: Performs the optimization by computing gradients, Hessian-vector products, line search, and updating parameters.\n\n- `loss` and `constraint_val`: Evaluate the objective function and constraint values at given inputs.\n\nThe class uses Theano for automatic differentiation to compute gradients and Hessians. It also leverages Krylov subspace methods for solving linear systems efficiently.\n\nThis optimizer is designed for problems where constraints are important, such as in constrained optimization problems in machine learning or engineering applications."
    },
    {
        "code": "import pprint\nimport re\n\nimport six\n\n\n\n\n\nclass TemplateCddl:\n\n\n    \n\n    sensitive_list = []\n\n    openapi_types = {\n        'flow': 'FlowItem',\n        'states': 'dict(str, TemplateState)',\n        'workflow': 'Workflow'\n    }\n\n    attribute_map = {\n        'flow': 'flow',\n        'states': 'states',\n        'workflow': 'workflow'\n    }\n\n    def __init__(self, flow=None, states=None, workflow=None):\n        \n        \n        \n\n        self._flow = None\n        self._states = None\n        self._workflow = None\n        self.discriminator = None\n\n        self.flow = flow\n        self.states = states\n        self.workflow = workflow\n\n    @property\n    def flow(self):\n        \n        return self._flow\n\n    @flow.setter\n    def flow(self, flow):\n        \n        self._flow = flow\n\n    @property\n    def states(self):\n        \n        return self._states\n\n    @states.setter\n    def states(self, states):\n        \n        self._states = states\n\n    @property\n    def workflow(self):\n        \n        return self._workflow\n\n    @workflow.setter\n    def workflow(self, workflow):\n        \n        self._workflow = workflow\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                if attr in self.sensitive_list:\n                    result[attr] = \"****\"\n                else:\n                    result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, TemplateCddl):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n",
        "summary": "The `TemplateCddl` class defines a data model for a template with properties such as flow, states, and workflow. It includes methods to convert the object to a dictionary, pretty-print it, and compare instances for equality."
    },
    {
        "code": "from flask import Flask, render_template, redirect, url_for, flash, request, abort\nfrom functions import UserLogin, UserRegistration, NewExpense\nfrom flask_sqlalchemy import SQLAlchemy\nfrom sqlalchemy import func\nfrom datetime import datetime, timedelta, date\nfrom flask_bcrypt import Bcrypt\nfrom flask_login import LoginManager, UserMixin, login_user, current_user, logout_user, login_required\nfrom matplotlib import pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfrom itertools import zip_longest\nimport os\nimport io\nimport base64\nimport numpy as np\n\napp = Flask(__name__)\nSECRET_KEY = os.urandom(16)\napp.config['SECRET_KEY'] = SECRET_KEY\napp.config['SQLALCHEMY_DATABASE_URI'] = ' '\ndb = SQLAlchemy(app)\nbcrypt = Bcrypt(app)\nlogin_manager = LoginManager(app)\nlogin_manager.login_view = 'login'\nlogin_manager.login_message_category = 'info'\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n\nclass User(db.Model, UserMixin):\n    __tablename__ = 'user'\n    id = db.Column(db.Integer, primary_key=True)\n    email = db.Column(db.String(30), unique=True, nullable=False)\n    username = db.Column(db.String(10), unique=True, nullable=False)\n    password = db.Column(db.String(128), nullable=False)\n    expense_id = db.relationship('UserExpense', backref='expensedate', lazy='dynamic')\n\n    def __repr__(self):\n        return f\"User('{self.username}', '{self.email}')\"\n\nclass UserExpense(db.Model):\n    __tablename__ = 'user_expenses'\n    id = db.Column(db.Integer, primary_key=True)\n    userid = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n    category = db.Column(db.String(30))\n    description = db.Column(db.String(50))\n    expense = db.Column(db.Numeric(scale=2, asdecimal=True))\n    expense_date = db.Column(db.Date, default=date.today())\n\n    def __repr__(self):\n        return f\"UserExpense('{self.category}', '{self.description}', '{self.expense}', '{self.expense_date}')\"\n\n@app.route('/', methods=['GET', 'POST'])\ndef login():\n    form = UserLogin()\n    if current_user.is_authenticated:\n        return redirect(url_for('overview'))\n    if form.validate_on_submit():\n        user = User.query.filter_by(username=form.username.data).first()\n        if user and bcrypt.check_password_hash(user.password, form.password.data):\n            login_user(user, remember=form.remember.data)\n            return redirect(url_for('overview'))\n        else:\n            flash('Invalid login', 'danger')\n    return render_template('login.html', form=form)\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if current_user.is_authenticated:\n        return redirect(url_for('overview'))\n    form = UserRegistration()\n    if form.validate_on_submit():\n        password_hashed = bcrypt.generate_password_hash(form.password.data).decode('utf-8')\n        user = User(username=form.username.data, email=form.email.data, password=password_hashed)\n        db.session.add(user)\n        db.session.commit()\n        flash('Account created!', 'success')\n        return redirect(url_for('login'))\n    return render_template('register.html', title='Register', form=form)\n\n@app.route('/logout')\ndef logout():\n    logout_user()\n    flash('Logged out!', 'success')\n    return redirect(url_for('login'))\n\n@app.route('/overview', methods=['GET','POST'])\n@login_required\ndef overview():\n    form = NewExpense()\n    userids = current_user.id\n    name = current_user.username\n\n    \n    if form.validate_on_submit():\n        expenses = UserExpense(category=form.category.data, description=form.description.data,\n                                expense=form.expense.data, expensedate=current_user)\n        db.session.add(expenses)\n        db.session.commit()\n\n    \n    filters = db.session.query(UserExpense.expense_date).filter(UserExpense.userid==userids).distinct()\n\n    date_list=[] \n    for u in filters:\n        date_list.append(f'{u.expense_date}')\n\n    date_expense_list=[] \n    for item in date_list:\n        date_expense = db.session.query(func.sum(UserExpense.expense)).filter(UserExpense.userid==userids, UserExpense.expense_date==item).scalar()\n        date_expense_list.append(f'{date_expense}')\n\n    item = list(zip_longest(date_list,date_expense_list,date_list, fillvalue=\"\"))\n\n    \n    fig, ax = plt.subplots(figsize=(11, 5))\n    ax.plot(date_list, [float(g) for g in date_expense_list], label=\"Expenses\")\n    ax.legend()\n    fig.suptitle('Expense pattern')\n\n    patternpngImage = io.BytesIO()\n    FigureCanvas(fig).print_png(patternpngImage)\n\n    patternpngImageString = \"data:image/png;base64,\"\n    patternpngImageString += base64.b64encode(patternpngImage.getvalue()).decode('utf8')\n\n\n    return render_template('overview.html', normal='normal', title='Expenses',image=patternpngImageString,\n                           form=form, name=name, item=item)\n\n\n@app.route('/expense/<string:wkex_id>', methods=['GET','POST'])\n@login_required\ndef userexpenses(wkex_id):\n    form = NewExpense()\n    userids = current_user.id\n    name = current_user.username\n\n    \n    items = db.session.query(UserExpense).filter(UserExpense.userid==userids, UserExpense.expense_date==wkex_id)\n\n    todays = str(date.today())\n    state=\"not\"\n    if (wkex_id == todays) is True:\n        state=\"today\"\n    if (wkex_id > todays) is True:\n        abort(404)\n\n    \n    if form.validate_on_submit():\n        expenses = UserExpense(category=form.category.data, description=form.description.data,\n                                expense=form.expense.data, expensedate=current_user)\n        db.session.add(expenses)\n        db.session.commit()\n        flash('Expense added!', 'success')\n        return redirect(url_for('userexpenses', wkex_id=wkex_id))\n\n    return render_template('expenses.html', normal='normal', title='Expenses',\n                           form=form, items=items, name=name, ids=wkex_id, state=state)\n\n@app.route('/expense/<string:wkex_id>/<int:ex_id>/delete', methods=['GET','POST'])\n@login_required\ndef delete_expense(wkex_id, ex_id):\n    expenses = db.session.query(UserExpense).get_or_404(ex_id) \n    if expenses.expensedate != current_user:\n        abort(403)\n    db.session.delete(expenses)\n    db.session.commit()\n    flash('Expense deleted', 'success')\n    return redirect(url_for('overview'))\n\n@app.route(\"/expense/<string:wkex_id>/<int:ex_id>/update\", methods=['GET', 'POST'])\n@login_required\ndef update_expense(wkex_id, ex_id):\n    name = current_user.username\n    expenses = db.session.query(UserExpense).get_or_404(ex_id) \n    if expenses.expensedate != current_user:\n        abort(403)\n    form = NewExpense()\n\n    if form.validate_on_submit():\n        expenses.category = form.category.data\n        expenses.description = form.description.data\n        expenses.expense = form.expense.data\n        db.session.commit()\n        flash('Expense updated', 'success')\n        return redirect(url_for('overview'))\n\n    elif request.method=='GET':\n        form.category.data = expenses.category\n        form.description.data =expenses.description\n        form.expense.data = expenses.expense\n    return render_template('expenses.html', title='Expenses',form=form, name=name, wkex_id=wkex_id, state='today')\n\n@app.route(\"/expense/<string:day_id>/charts\", methods=['GET', 'POST'])\n@login_required\ndef charts(day_id):\n    userids = current_user.id\n    name = current_user.username\n    \n    categories = db.session.query(UserExpense.category).filter(UserExpense.userid==userids,\n                                                               UserExpense.expense_date==day_id).distinct()\n    cat_list=[]\n    for u in categories:\n        cat_list.append(f'{u.category}')\n\n    counts_list=[]\n    for item in cat_list:\n        counts = db.session.query(UserExpense.category).filter(UserExpense.userid==userids,\n                                                               UserExpense.expense_date==day_id,\n                                                               UserExpense.category==item).count()\n        counts_list.append(counts)\n\n    sum_list=[]\n    for item in cat_list:\n        Sums = db.session.query(func.sum(UserExpense.expense)).filter(UserExpense.userid==userids,\n                                                                      UserExpense.expense_date==day_id,\n                                                                      UserExpense.category==item).scalar()\n        sum_list.append(f'{Sums}')\n\n    \n    fig, axs = plt.subplots(figsize=(10, 5))\n    axs.bar(cat_list, [float(g) for g in sum_list])\n    fig.suptitle('Expenditure breakdown')\n\n    \n    fig1, ax1 = plt.subplots(figsize=(10, 5), subplot_kw=dict(aspect=\"equal\"))\n\n    wedges, texts = ax1.pie(counts_list, wedgeprops=dict(width=0.5), startangle=-40)\n\n    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n    kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n            bbox=bbox_props, zorder=0, va=\"top\")\n\n    for i, p in enumerate(wedges):\n        ang = (p.theta2 - p.theta1)/2. + p.theta1\n        y = np.sin(np.deg2rad(ang))\n        x = np.cos(np.deg2rad(ang))\n        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n        ax1.annotate(cat_list[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                    horizontalalignment=horizontalalignment, **kw)\n\n    ax1.set_title(\"Expenses category frequency\")\n\n    \n    highpngImage = io.BytesIO()\n    freqpngImage = io.BytesIO()\n    FigureCanvas(fig).print_png(highpngImage)\n    FigureCanvas(fig1).print_png(freqpngImage)\n\n    \n    highpngImageString = \"data:image/png;base64,\"\n    highpngImageString += base64.b64encode(highpngImage.getvalue()).decode('utf8')\n\n    freqpngImageString = \"data:image/png;base64,\"\n    freqpngImageString += base64.b64encode(freqpngImage.getvalue()).decode('utf8')\n\n    return render_template('charts.html',title ='History', name=name,\n                           image1=highpngImageString, image2=freqpngImageString, day_id=day_id)\n\n    if __name__ == '__main__':\n        app.run()\n",
        "summary": "This code is a Flask web application for managing personal expenses. It allows users to create, view, update, and delete expense entries, as well as generate charts and graphs based on their spending data.\n\nHere's a breakdown of the key features and functionalities:\n\n1. **User Authentication**: The application uses Flask-Login for user authentication, allowing users to log in and access their personal expense data.\n\n2. **Expense Management**:\n   - Users can add new expenses with details such as category, description, and amount.\n   - They can view a list of all their expenses, filter by date, and search for specific entries.\n   - Expenses can be updated or deleted by the user who created them.\n\n3. **Charts and Graphs**:\n   - The application generates two types of charts:\n     - A bar chart showing the total expenditure by category on a specific day.\n     - A pie chart showing the frequency of expenses across different categories on a specific day.\n\n4. **Data Visualization**:\n   - The charts are generated using Matplotlib and displayed as PNG images in the browser.\n   - Base64 encoding is used to embed the image data directly into HTML, allowing for easy display without additional server requests.\n\n5. **Database Management**:\n   - The application uses SQLAlchemy as an ORM to interact with a database (likely SQLite or another relational database).\n   - Models are defined for users and expenses, including relationships between them.\n\n6. **Error Handling**:\n   - The application includes error handling for common scenarios such as attempting to delete or update an expense that does not belong to the current user.\n\n7. **User Interface**:\n   - The application provides a simple HTML template for each page, using Jinja2 templating engine.\n   - Forms are used for adding and updating expenses, with validation to ensure data integrity.\n\n8. **Deployment**:\n   - The application can be run locally using Flask's development server or deployed on a production web server.\n\nThis code is well-structured, modular, and follows best practices for building a web application in Python using Flask. It provides a comprehensive solution for managing personal expenses and visualizing spending data through charts and graphs."
    },
    {
        "code": "import unittest\n\nimport mock\nimport numpy\nimport pytest\n\nimport cupy\nfrom cupy import testing\nfrom cupyx.scipy import sparse\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'format': ['csr', 'csc', 'coo'],\n    'm': [3],\n    'n': [None, 3, 2],\n    'k': [0, 1],\n}))\n@testing.with_requires('scipy')\nclass TestEye(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_eye(self, xp, sp):\n        x = sp.eye(\n            self.m, n=self.n, k=self.k, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'format': ['csr', 'csc', 'coo'],\n}))\n@testing.with_requires('scipy')\nclass TestIdentity(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_eye(self, xp, sp):\n        x = sp.identity(3, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n}))\n@testing.with_requires('scipy')\nclass TestSpdiags(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_spdiags(self, xp, sp):\n        data = xp.arange(12, dtype=self.dtype).reshape(3, 4)\n        diags = xp.array([0, -1, 2], dtype='i')\n        x = sp.spdiags(data, diags, 3, 4)\n        return x\n\n\n@testing.parameterize(*testing.product({\n    'random_method': ['random', 'rand'],\n    'dtype': [numpy.float32, numpy.float64],\n    'format': ['csr', 'csc', 'coo'],\n}))\nclass TestRandom(unittest.TestCase):\n\n    def test_random(self):\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n    def test_random_with_seed(self):\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=1)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        y = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=1)\n\n        self.assertTrue((x.toarray() == y.toarray()).all())\n\n    def test_random_with_state(self):\n        state1 = cupy.random.RandomState(1)\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=state1)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        state2 = cupy.random.RandomState(1)\n        y = getattr(sparse, self.random_method)(\n            3, 4, density=0.1,\n            format=self.format, dtype=self.dtype,\n            random_state=state2)\n\n        self.assertTrue((x.toarray() == y.toarray()).all())\n\n    def test_random_with_data_rvs(self):\n        if self.random_method == 'rand':\n            pytest.skip('cupyx.scipy.sparse.rand does not support data_rvs')\n        data_rvs = mock.MagicMock(side_effect=cupy.zeros)\n        x = getattr(sparse, self.random_method)(\n            3, 4, density=0.1, data_rvs=data_rvs,\n            format=self.format, dtype=self.dtype)\n        self.assertEqual(x.shape, (3, 4))\n        self.assertEqual(x.dtype, self.dtype)\n        self.assertEqual(x.format, self.format)\n\n        self.assertEqual(data_rvs.call_count, 1)\n        \n        self.assertIsInstance(data_rvs.call_args[0][0], int)\n\n\n@testing.with_requires('scipy')\nclass TestRandomInvalidArgument(unittest.TestCase):\n\n    @testing.numpy_cupy_raises(sp_name='sp', accept_error=ValueError)\n    def test_too_small_density(self, xp, sp):\n        sp.random(3, 4, density=-0.1)\n\n    @testing.numpy_cupy_raises(sp_name='sp', accept_error=ValueError)\n    def test_too_large_density(self, xp, sp):\n        sp.random(3, 4, density=1.1)\n\n    @testing.numpy_cupy_raises(sp_name='sp', accept_error=NotImplementedError)\n    def test_invalid_dtype(self, xp, sp):\n        sp.random(3, 4, dtype='i')\n\n\n@testing.parameterize(*testing.product({\n    'dtype': [numpy.float32, numpy.float64, numpy.complex64, numpy.complex128],\n    'format': ['dia', 'csr', 'csc', 'coo'],\n}))\n@testing.with_requires('scipy')\nclass TestDiags(unittest.TestCase):\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_diags_scalar_offset(self, xp, sp):\n        x = sp.diags(\n            xp.arange(16), offsets=0, dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_diags_single_element_lists(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(16)], offsets=[0], dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_diags_multiple(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(15), xp.arange(16), xp.arange(15), xp.arange(13)],\n            offsets=[-1, 0, 1, 3],\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_diags_offsets_as_array(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(15), xp.arange(16), xp.arange(15), xp.arange(13)],\n            offsets=xp.array([-1, 0, 1, 3]),\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n\n    @testing.numpy_cupy_allclose(sp_name='sp')\n    def test_diags_non_square(self, xp, sp):\n        x = sp.diags(\n            [xp.arange(5), xp.arange(3)],\n            offsets=[0, -2], shape=(5, 10),\n            dtype=self.dtype, format=self.format)\n        self.assertIsInstance(x, sp.spmatrix)\n        self.assertEqual(x.format, self.format)\n        return x\n",
        "summary": "The provided Python code defines several test cases using the `unittest` framework to validate various functionalities of sparse matrices in the `cupyx.scipy.sparse` module. These tests cover operations such as creating identity matrices, diagonal matrices, random sparse matrices, and converting dense arrays to sparse formats, ensuring compatibility with both NumPy and CuPy data types and sparse matrix formats."
    },
    {
        "code": "import torch\nimport numpy as np;\nfrom torch.autograd import Variable\n\n\ndef normal_std(x):\n    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n\nclass Data_utility(object):\n    \n    def __init__(self, dSet, train, valid, cuda, horizon, window, normalize = 2):\n        self.cuda = cuda;\n        self.P = window;\n        self.h = horizon\n        self.rawdat = dSet\n        self.dat = np.zeros(self.rawdat.shape);\n        self.n, self.m = self.dat.shape;\n        self.normalize = 2\n        self.scale = np.ones(self.m);\n        self._normalized(normalize);\n        self._split(int(train * self.n), int((train+valid) * self.n), self.n);\n        \n        self.scale = torch.from_numpy(self.scale).float();\n        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m);\n            \n        if self.cuda:\n            self.scale = self.scale.cuda();\n        self.scale = Variable(self.scale);\n        \n        self.rse = normal_std(tmp);\n        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)));\n    \n    def _normalized(self, normalize):\n        \n       \n        if (normalize == 0):\n            self.dat = self.rawdat\n            \n        if (normalize == 1):\n            self.dat = self.rawdat / np.max(self.rawdat);\n            \n        \n        if (normalize == 2):\n            for i in range(self.m):\n                self.scale[i] = np.max(np.abs(self.rawdat[:,i]));\n                self.dat[:,i] = self.rawdat[:,i] / np.max(np.abs(self.rawdat[:,i]));\n            \n        \n    def _split(self, train, valid, test):\n        \n        train_set = range(self.P+self.h-1, train);\n        valid_set = range(train, valid);\n        test_set = range(valid, self.n);\n        self.train = self._batchify(train_set, self.h);\n        self.valid = self._batchify(valid_set, self.h);\n        self.test = self._batchify(test_set, self.h);\n        \n        \n    def _batchify(self, idx_set, horizon):\n        \n        n = len(idx_set);\n        X = torch.zeros((n,self.P,self.m));\n        Y = torch.zeros((n,self.m));\n        \n        for i in range(n):\n            end = idx_set[i] - self.h + 1;\n            start = end - self.P;\n            X[i,:,:] = torch.from_numpy(self.dat[start:end, :]);\n            Y[i,:] = torch.from_numpy(self.dat[idx_set[i], :]);\n\n        return [X, Y];\n\n    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n        length = len(inputs)\n        if shuffle:\n            index = torch.randperm(length)\n        else:\n            index = torch.LongTensor(range(length))\n        start_idx = 0\n        while (start_idx < length):\n            end_idx = min(length, start_idx + batch_size)\n            excerpt = index[start_idx:end_idx]\n            X = inputs[excerpt]; Y = targets[excerpt];\n            \n            \n            \n            yield Variable(X), Variable(Y);\n            start_idx += batch_size\n",
        "summary": "The provided Python code defines a class `Data_utility` for handling and preprocessing time series data using PyTorch. It includes methods for normalizing the data, splitting it into training, validation, and test sets, batching the data for model input, and generating batches of data with optional shuffling."
    },
    {
        "code": "DOCUMENTATION = r\n\nEXAMPLES = r\n\nRETURN = r\n",
        "summary": "The provided Python code appears to be structured for documentation, examples, and return values, but the actual content within each section (documentation, examples, return) is represented by empty strings indicated by `r`. This suggests that these sections are intended to contain relevant information or code snippets, which are currently missing."
    },
    {
        "code": "import IECore\n\nimport Gaffer\n\n\nclass CompoundNumericNode( Gaffer.Node ) :\n\n\tdef __init__( self, name=\"CompoundNumericNode\" ) :\n\n\t\tGaffer.Node.__init__( self, name )\n\n\t\tself.addChild( Gaffer.V3fPlug( \"p\", Gaffer.Plug.Direction.In ) )\n\nIECore.registerRunTimeTyped( CompoundNumericNode, typeName = \"GafferTest::CompoundNumericNode\" )\n",
        "summary": "The provided Python code defines a custom node named `CompoundNumericNode` that inherits from `Gaffer.Node`. This node includes an input plug of type `V3fPlug`, which is used for handling 3D vector data. The class is then registered with the IECore framework using a unique type name, making it available for use within the Gaffer application or similar environments."
    },
    {
        "code": "load(\"@rules_foreign_cc//tools/build_defs/shell_toolchain/toolchains:function_and_call.bzl\", \"FunctionAndCall\")\n\n_REPLACE_VALUE = \"BAZEL_GEN_ROOT\"\n\ndef os_name():\n    return \"Fancy\"\n\ndef pwd():\n    return \"$(pwd)\"\n\ndef echo(text):\n    return \"printf \\\"{text}\\\"\".format(text = text)\n\ndef export_var(name, value):\n    return \"export {name}={value}\".format(name = name, value = value)\n\ndef local_var(name, value):\n    return \"local {name}={value}\".format(name = name, value = value)\n\ndef use_var(name):\n    return \"$\" + name\n\ndef env():\n    return \"env\"\n\ndef path(expression):\n    return \"export PATH=\\\"{expression}:$PATH\\\"\".format(expression = expression)\n\ndef touch(path):\n    return \"touch \" + path\n\ndef mkdirs(path):\n    return \"mkdir -p \" + path\n\ndef if_else(condition, if_text, else_text):\n    return .format(condition = condition, if_text = if_text, else_text = else_text)\n\n\ndef define_function(name, text):\n    lines = []\n    lines.append(\"function \" + name + \"() {\")\n    for line_ in text.splitlines():\n        lines.append(\"  \" + line_)\n    lines.append(\"}\")\n    return \"\\n\".join(lines)\n\ndef replace_in_files(dir, from_, to_):\n    return FunctionAndCall(\n        text = ,\n    )\n\ndef copy_dir_contents_to_dir(source, target):\n    return .format(source, target)\n\ndef symlink_contents_to_dir(source, target):\n    text = \n    return FunctionAndCall(text = text)\n\ndef symlink_to_dir(source, target):\n    text = \n    return FunctionAndCall(text = text)\n\ndef script_prelude():\n    return \"set -euo pipefail\"\n\ndef increment_pkg_config_path(source):\n    text = \n    return FunctionAndCall(text = text)\n\ndef cat(filepath):\n    return \"cat \\\"{}\\\"\".format(filepath)\n\ndef redirect_out_err(from_process, to_file):\n    return from_process + \" &> \" + to_file\n\ndef assert_script_errors():\n    return \"set -e\"\n\ndef cleanup_function(on_success, on_failure):\n    text = \"\\n\".join([\n        \"local ecode=$?\",\n        \"if [ $ecode -eq 0 ]; then\",\n        on_success,\n        \"else\",\n        on_failure,\n        \"fi\",\n    ])\n    return FunctionAndCall(text = text, call = \"trap \\\"cleanup_function\\\" EXIT\")\n\ndef children_to_path(dir_):\n    text = .format(dir_ = dir_)\n    return FunctionAndCall(text = text)\n\ndef define_absolute_paths(dir_, abs_path):\n    return \"\n        dir_ = dir_,\n        REPLACE_VALUE = _REPLACE_VALUE,\n        abs_path = abs_path,\n    )\n\ndef replace_absolute_paths(dir_, abs_path):\n    return \"\n        dir_ = dir_,\n        REPLACE_VALUE = _REPLACE_VALUE,\n        abs_path = abs_path,\n    )\n",
        "summary": "The provided Python code defines a series of functions to generate shell script commands and logic, including variable manipulation, conditional statements, function definitions, file operations, and environment setup. These functions are designed to be used in Bazel build configurations for building C++ projects with foreign tools."
    },
    {
        "code": "import os\n\nimport dj_database_url\nfrom decouple import config,Csv\n\nMODE=config(\"MODE\", default=\"dev\")\nSECRET_KEY = config('SECRET_KEY')\nDEBUG = config('DEBUG', default=False, cast=bool)\n\nif config('MODE')==\"dev\":\n   DATABASES = {\n       'default': {\n           'ENGINE': 'django.db.backends.postgresql',\n           'NAME': config('DB_NAME'),\n           'USER': config('DB_USER'),\n           'PASSWORD': config('DB_PASSWORD'),\n           'HOST': config('DB_HOST'),\n           'PORT': '',\n       }\n       \n   }\n\nelse:\n   DATABASES = {\n       'default': dj_database_url.config(\n           default=config('DATABASE_URL')\n       )\n   }\n\ndb_from_env = dj_database_url.config(conn_max_age=500)\nDATABASES['default'].update(db_from_env)\n\nALLOWED_HOSTS = config('ALLOWED_HOSTS', cast=Csv())\n\n\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n\n\n\n\n\nINSTALLED_APPS = [\n    'bootstrap4',\n    'images.apps.ImagesConfig',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'whitenoise.middleware.WhiteNoiseMiddleware',\n]\n\nROOT_URLCONF = 'Gallery.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n                'django.template.context_processors.media',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'Gallery.wsgi.application'\n\n\n\n\n\n\n\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n\n\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'Africa/Nairobi'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n\n\nSTATIC_ROOT = os.path.join(BASE_DIR,'staticfiles')\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, \"static\")\n]\n\nSTATICFILES_STORAGE = 'whitenoise.storage.CompressedManifestStaticFilesStorage'\n\nMEDIA_URL = '/media/'\nMEDIA_ROOT = os.path.join(BASE_DIR,'media')\n\n \n",
        "summary": "This Python code configures a Django project with environment variables for settings such as database configurations, security keys, and allowed hosts. It also sets up middleware, installed apps, templates, and static files handling according to the specified mode (development or production)."
    },
    {
        "code": "from enum import Enum, unique\nimport json\nimport logging\nimport os\nimport re\nimport tempfile\nfrom pathlib import Path\n\nimport requests\n\nfrom github import Github, UnknownObjectException\n\nfrom .autorest_tools import (\n    autorest_latest_version_finder,\n    autorest_bootstrap_version_finder,\n    autorest_swagger_to_sdk_conf,\n)\nfrom azure_devtools.ci_tools.github_tools import get_files, GithubLink\n\n_LOGGER = logging.getLogger(__name__)\n\nCONFIG_FILE = \"swagger_to_sdk_config_autorest.json\"\nCONFIG_FILE_DPG = \"swagger_to_sdk_config_dpg.json\"\n\nDEFAULT_COMMIT_MESSAGE = \"Generated from {hexsha}\"\n\n\ndef build_file_content():\n    autorest_version = autorest_latest_version_finder()\n    autorest_bootstrap_version = autorest_bootstrap_version_finder()\n    return {\n        \"autorest\": autorest_version,\n        \"autorest_bootstrap\": autorest_bootstrap_version,\n    }\n\n\ndef get_repo_tag_meta(meta_conf):\n    repotag = meta_conf.get(\"repotag\")\n    if repotag:\n        return repotag\n    \n    if \"go\" in meta_conf[\"autorest_options\"]:\n        return \"azure-sdk-for-go\"\n    if \"ruby\" in meta_conf[\"autorest_options\"]:\n        return \"azure-sdk-for-ruby\"\n    if \"java\" in meta_conf[\"autorest_options\"]:\n        return \"azure-sdk-for-java\"\n    if \"nodejs\" in meta_conf[\"autorest_options\"]:\n        return \"azure-sdk-for-node\"\n    if \"typescript\" in meta_conf[\"autorest_options\"]:\n        return \"azure-sdk-for-js\"\n    raise ValueError(\"No repotag found or infered\")\n\n\n@unique\nclass Language(str, Enum):\n    GOLANG = \"go\"\n    RUBY = \"ruby\"\n    JAVA = \"java\"\n    NODEJS = \"nodejs\"\n    CSHARP = \"csharp\"\n    PYTHON = \"python\"\n    TYPESCRIPT = \"typescript\"\n\n\ndef get_language_from_conf(meta_conf):\n    \n    autorest_options_lang = set(meta_conf[\"autorest_options\"].keys())\n    languages = set()\n    for value in Language:\n        if value in autorest_options_lang:\n            languages.add(value)\n\n    if not languages:\n        _LOGGER.warning(\"No detected language from this conf\")\n        return None  \n\n    language = languages.pop()\n    if languages:\n        _LOGGER.warning(\"This SwaggerToSdk conf seems to generate too much language in one call, assume we don't know\")\n        return None\n\n    return language\n\n\ndef get_context_tag_from_git_object(git_object):\n    files_list = [file.filename for file in get_files(git_object)]\n    return get_context_tag_from_file_list(files_list)\n\n\ndef get_context_tag_from_file_list(files_list):\n    context_tags = set()\n    for filename in files_list:\n        filepath = Path(filename)\n        filename = filepath.as_posix()\n        if \"/examples/\" in filename:\n            \n            continue\n        \n        match = re.match(r\"specification/(.*)/Microsoft.\\w*/(stable|preview)/\", filename, re.I)\n        if match:\n            context_tags.add(match.groups()[0])\n            continue\n        \n        match = re.match(r\"specification/(.*)/(stable|preview)/\", filename, re.I)\n        if match:\n            context_tags.add(match.groups()[0])\n            continue\n        \n        \n        match = re.match(r\"specification/(.*)/readme.\\w*.?md\", filename, re.I)\n        if match:\n            context_tags.add(match.groups()[0])\n            continue\n        \n    return context_tags\n\n\ndef this_conf_will_generate_for_this_pr(git_object, config):\n    \n    lang = get_language_from_conf(config)\n    filenames = [file.filename.lower() for file in get_files(git_object)]\n    readme_lang = [name for name in filenames if re.match(r\"(.*)readme.\\w+.md\", name)]\n\n    if len(readme_lang) != len(filenames):\n        return True  \n\n    return bool([name for name in readme_lang if name.endswith(\"readme.{}.md\".format(lang))])\n\n\ndef get_readme_files_from_git_object(git_object, base_dir=Path(\".\")):\n    files_list = [file.filename for file in get_files(git_object)]\n    return get_readme_files_from_file_list(files_list, base_dir)\n\n\ndef get_readme_files_from_file_list(files_list, base_dir=Path(\".\")):\n    \n    readme_files = set()\n    context_tags = get_context_tag_from_file_list(files_list)\n    for context_tag in context_tags:\n        expected_folder = Path(base_dir) / Path(\"specification/{}\".format(context_tag))\n        if not expected_folder.is_dir():\n            _LOGGER.warning(\"From context {} I didn't find folder {}\".format(context_tag, expected_folder))\n            continue\n        for expected_readme in [l for l in expected_folder.iterdir() if l.is_file()]:\n            \n            match = re.match(r\"readme.\\w*.?md\", expected_readme.name, re.I)\n            if match:\n                readme_files.add(expected_readme.relative_to(Path(base_dir)))\n    return readme_files\n\n\ndef read_config(sdk_git_folder, config_file):\n    \n    config_path = os.path.join(sdk_git_folder, config_file)\n    with open(config_path, \"r\") as config_fd:\n        return json.loads(config_fd.read())\n\n\ndef read_config_from_github(sdk_id, branch=\"main\", gh_token=None):\n    raw_link = str(get_configuration_github_path(sdk_id, branch))\n    _LOGGER.debug(\"Will try to download: %s\", raw_link)\n    _LOGGER.debug(\"Token is defined: %s\", gh_token is not None)\n    headers = {\"Authorization\": \"token {}\".format(gh_token)} if gh_token else {}\n    response = requests.get(raw_link, headers=headers)\n    if response.status_code != 200:\n        raise ValueError(\n            \"Unable to download conf file for SDK {} branch {}: status code {}\".format(\n                sdk_id, branch, response.status_code\n            )\n        )\n    return json.loads(response.text)\n\n\ndef extract_conf_from_readmes(swagger_files_in_pr, restapi_git_folder, sdk_git_id, config, force_generation=False):\n    readme_files_in_pr = {\n        readme for readme in swagger_files_in_pr if getattr(readme, \"name\", readme).lower().endswith(\"readme.md\")\n    }\n    for readme_file in readme_files_in_pr:\n        build_swaggertosdk_conf_from_json_readme(\n            readme_file, sdk_git_id, config, base_folder=restapi_git_folder, force_generation=force_generation\n        )\n\n\ndef get_readme_path(readme_file, base_folder=\".\"):\n    \n    if not isinstance(readme_file, Path) and readme_file.startswith(\"http\"):\n        return GithubLink.from_string(readme_file).as_raw_link()\n    else:\n        if base_folder is None:\n            base_folder = \".\"\n        return str(Path(base_folder) / Path(readme_file))\n\n\ndef build_swaggertosdk_conf_from_json_readme(readme_file, sdk_git_id, config, base_folder=\".\", force_generation=False):\n    \n    readme_full_path = get_readme_path(readme_file, base_folder)\n    with tempfile.TemporaryDirectory() as temp_dir:\n        readme_as_conf = autorest_swagger_to_sdk_conf(readme_full_path, temp_dir, config)\n    generated_config = {\n        \"markdown\": readme_full_path,\n    }\n    sdk_git_short_id = sdk_git_id.split(\"/\")[-1].lower()\n    _LOGGER.info(\"Looking for tag {} in readme {}\".format(sdk_git_short_id, readme_file))\n    for swagger_to_sdk_conf in readme_as_conf:\n        if not isinstance(swagger_to_sdk_conf, dict):\n            continue\n        repo = swagger_to_sdk_conf.get(\"repo\", \"\")\n        repo = repo.split(\"/\")[-1].lower()  \n        if repo == sdk_git_short_id:\n            _LOGGER.info(\"This Readme contains a swagger-to-sdk section for repo {}\".format(repo))\n            generated_config.update(\n                {\n                    \"autorest_options\": swagger_to_sdk_conf.get(\"autorest_options\", {}),\n                    \"after_scripts\": swagger_to_sdk_conf.get(\"after_scripts\", []),\n                }\n            )\n            config.setdefault(\"projects\", {})[str(readme_file)] = generated_config\n            return generated_config\n        else:\n            _LOGGER.info(\"Skip mismatch {} from {}\".format(repo, sdk_git_short_id))\n    if not force_generation:\n        _LOGGER.info(\n            \"Didn't find tag {} in readme {}. Did you forget to update the SwaggerToSdk section?\".format(\n                sdk_git_short_id, readme_file\n            )\n        )\n    else:\n        _LOGGER.info(\"Didn't find tag {} in readme {}. Forcing it.\".format(sdk_git_short_id, readme_file))\n        config.setdefault(\"projects\", {})[str(readme_file)] = generated_config\n\n\ndef get_input_paths(global_conf, local_conf):\n    \n    del global_conf  \n\n    relative_markdown_path = None  \n    input_files = []  \n    if \"markdown\" in local_conf:\n        relative_markdown_path = Path(local_conf[\"markdown\"])\n    input_files = local_conf.get(\"autorest_options\", {}).get(\"input-file\", [])\n    if input_files and not isinstance(input_files, list):\n        input_files = [input_files]\n    input_files = [Path(input_file) for input_file in input_files]\n    if not relative_markdown_path and not input_files:\n        raise ValueError(\"No input file found\")\n    return (relative_markdown_path, input_files)\n\n\ndef solve_relative_path(autorest_options, sdk_root):\n    \n    SDKRELKEY = \"sdkrel:\"\n    solved_autorest_options = {}\n    for key, value in autorest_options.items():\n        if key.startswith(SDKRELKEY):\n            _LOGGER.debug(\"Found a sdkrel pair: %s/%s\", key, value)\n            subkey = key[len(SDKRELKEY) :]\n            solved_value = Path(sdk_root, value).resolve()\n            solved_autorest_options[subkey] = str(solved_value)\n        else:\n            solved_autorest_options[key] = value\n    return solved_autorest_options\n\n\ndef get_configuration_github_path(sdk_id, branch=\"master\"):\n    return GithubLink(sdk_id, \"raw\", branch, CONFIG_FILE)\n",
        "summary": " ```json\n{\n  \"response\": \"The provided code snippet is a Python script that includes various functions and classes for interacting with GitHub repositories and generating configuration files for Swagger-to-SDK processes. The script uses the `requests` library to fetch configuration files from GitHub and the `tempfile` module to create temporary directories for processing. It also utilizes the `Path` class from the `pathlib` module to handle file paths and operations. The script is designed to extract configuration information from README files in a GitHub repository, specifically looking for sections that define Swagger-to-SDK configurations for specific projects. If a matching section is found, it updates the configuration dictionary with the relevant options and scripts. If no matching section is found, it logs an informational message indicating whether to force generation of the configuration.\"\n}\n```"
    },
    {
        "code": "import torch\nfrom ptstat.core import RandomVariable, _to_v\n\n\nclass Categorical(RandomVariable):\n    \n    def __init__(self, p=None, p_min=1E-6, size=None, cuda=False):\n        super(Categorical, self).__init__()\n        if size:\n            assert len(size) == 2, str(size)\n            p = _to_v(1 / size[1], size, cuda)\n        else:\n            assert len(p.size()) == 2, str(p.size())\n        assert torch.min(p.data) >= 0, str(torch.min(p.data))\n        assert torch.max(torch.abs(torch.sum(p.data, 1) - 1)) <= 1E-5\n        self._p = torch.clamp(p, p_min)\n\n    def _size(self):\n        return self._p.size()[0], 1  \n\n    def _log_pdf(self, x):\n        return torch.log(self._p.gather(1, x)).squeeze()\n\n    def _sample(self):\n        return self._p.multinomial(1, True)\n\n    def _entropy(self):\n        return - torch.sum(self._p * torch.log(self._p), 1).squeeze()\n",
        "summary": "The `Categorical` class in PyTorch defines a categorical random variable with methods for initialization, sampling, log probability calculation, and entropy computation. It ensures that the probabilities are valid (non-negative and sum to one) and provides functionality to work on both CPU and GPU."
    },
    {
        "code": "from .__about__ import __version__\nfrom .http_check import HTTPCheck\n\n__all__ = ['__version__', 'HTTPCheck']\n",
        "summary": "The Python code imports the `__version__` and `HTTPCheck` classes from their respective modules, making them available for use in other parts of the application. It also sets up a list named `__all__` to specify which names should be exported when the module is imported using the `from ... import *` syntax."
    },
    {
        "code": "import unittest\n\nfrom localstack.utils.aws import aws_stack\n\n\nclass SSMTest(unittest.TestCase):\n    def test_describe_parameters(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n\n        response = ssm_client.describe_parameters()\n        self.assertIn(\"Parameters\", response)\n        self.assertIsInstance(response[\"Parameters\"], list)\n\n    def test_put_parameters(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n\n        ssm_client.put_parameter(\n            Name=\"test_put\",\n            Description=\"test\",\n            Value=\"123\",\n            Type=\"String\",\n        )\n\n        self._assert(\"test_put\", \"test_put\")\n        self._assert(\"/test_put\", \"test_put\")\n\n    def test_hierarchical_parameter(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n\n        ssm_client.put_parameter(\n            Name=\"/a/b/c\",\n            Value=\"123\",\n            Type=\"String\",\n        )\n\n        self._assert(\"/a/b/c\", \"/a/b/c\")\n        self._assert(\"/a//b//c\", \"/a/b/c\")\n        self._assert(\"a/b//c\", \"/a/b/c\")\n\n    def test_get_secret_parameter(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n        sec_client = aws_stack.connect_to_service(\"secretsmanager\")\n\n        secret_name = \"test_secret\"\n        sec_client.create_secret(\n            Name=secret_name,\n            SecretString=\"my_secret\",\n            Description=\"testing creation of secrets\",\n        )\n\n        result = ssm_client.get_parameter(\n            Name=\"/aws/reference/secretsmanager/{0}\".format(secret_name)\n        )\n\n        self.assertEqual(\n            \"/aws/reference/secretsmanager/{0}\".format(secret_name),\n            result.get(\"Parameter\").get(\"Name\"),\n        )\n        self.assertEqual(\"my_secret\", result.get(\"Parameter\").get(\"Value\"))\n\n        source_result = result.get(\"Parameter\").get(\"SourceResult\")\n        self.assertTrue(source_result is not None, \"SourceResult should be present\")\n        self.assertTrue(type(source_result) is str, \"SourceResult should be a string\")\n\n    def test_get_inexistent_secret(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n        self.assertRaises(\n            ssm_client.exceptions.ParameterNotFound,\n            ssm_client.get_parameter,\n            Name=\"/aws/reference/secretsmanager/inexistent\",\n        )\n\n    def test_get_parameters_and_secrets(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n        sec_client = aws_stack.connect_to_service(\"secretsmanager\")\n        secret_path = \"/aws/reference/secretsmanager/\"\n\n        param_name = \"test_param\"\n        ssm_client.put_parameter(\n            Name=param_name,\n            Description=\"test\",\n            Value=\"123\",\n            Type=\"String\",\n        )\n\n        secret_name = \"test_secret_params\"\n        sec_client.create_secret(\n            Name=secret_name,\n            SecretString=\"my_secret\",\n            Description=\"testing creation of secrets\",\n        )\n\n        complete_secret = secret_path + secret_name\n        response = ssm_client.get_parameters(\n            Names=[\n                param_name,\n                complete_secret,\n                \"inexistent_param\",\n                secret_path + \"inexistent_secret\",\n            ]\n        )\n        found = response.get(\"Parameters\")\n        not_found = response.get(\"InvalidParameters\")\n\n        for param in found:\n            self.assertIn(param[\"Name\"], [param_name, complete_secret])\n        for param in not_found:\n            self.assertIn(param, [\"inexistent_param\", secret_path + \"inexistent_secret\"])\n\n    def _assert(self, search_name, param_name):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n\n        def do_assert(result):\n            self.assertGreater(len(result), 0)\n            self.assertEqual(param_name, result[0][\"Name\"])\n            self.assertEqual(\"123\", result[0][\"Value\"])\n\n        response = ssm_client.get_parameter(Name=search_name)\n        do_assert([response[\"Parameter\"]])\n\n        response = ssm_client.get_parameters(Names=[search_name])\n        do_assert(response[\"Parameters\"])\n\n    def test_get_parameters_by_path_and_filter_by_labels(self):\n        ssm_client = aws_stack.connect_to_service(\"ssm\")\n        path = \"/my/path\"\n        value = \"value\"\n        param = ssm_client.put_parameter(Name=path, Value=value, Type=\"String\")\n        ssm_client.label_parameter_version(\n            Name=path, ParameterVersion=param[\"Version\"], Labels=[\"latest\"]\n        )\n        list_of_params = ssm_client.get_parameters_by_path(\n            Path=\"/my\", ParameterFilters=[{\"Key\": \"Label\", \"Values\": [\"latest\"]}]\n        )\n        self.assertEqual(\"/my/path\", list_of_params[\"Parameters\"][0][\"Name\"])\n",
        "summary": "The provided Python code defines a series of unit tests for the AWS Systems Manager (SSM) service using the `unittest` framework. Each test method verifies specific functionalities such as describing parameters, putting parameters, handling hierarchical parameters, retrieving secret parameters, and more, ensuring that the SSM client interacts correctly with AWS services."
    },
    {
        "code": "import json\nimport os\nimport sys\n\nfrom . import uploader\nfrom . import processing\nfrom . import exif_read\n\n\ndef verify_mapillary_tag(filepath):\n    filepath_keep_original = processing.processed_images_rootpath(filepath)\n    if os.path.isfile(filepath_keep_original):\n        filepath = filepath_keep_original\n    \n    return exif_read.ExifRead(filepath).mapillary_tag_exists()\n\n\ndef upload(\n    import_path,\n    skip_subfolders=False,\n    number_threads=None,\n    max_attempts=None,\n    video_import_path=None,\n    dry_run=False,\n):\n    \n\n    \n    if video_import_path:\n        \n        if not os.path.isdir(video_import_path) and not os.path.isfile(\n            video_import_path\n        ):\n            print(\n                \"Error, video path \" + video_import_path + \" does not exist, exiting...\"\n            )\n            sys.exit(1)\n\n        \n        video_sampling_path = \"mapillary_sampled_video_frames\"\n        video_dirname = (\n            video_import_path\n            if os.path.isdir(video_import_path)\n            else os.path.dirname(video_import_path)\n        )\n        import_path = (\n            os.path.join(os.path.abspath(import_path), video_sampling_path)\n            if import_path\n            else os.path.join(os.path.abspath(video_dirname), video_sampling_path)\n        )\n\n    \n    if not import_path or not os.path.isdir(import_path):\n        print(f\"Error, import directory {import_path} does not exist, exiting...\")\n        sys.exit(1)\n\n    \n    total_file_list = uploader.get_total_file_list(import_path, skip_subfolders)\n    upload_file_list = uploader.get_upload_file_list(import_path, skip_subfolders)\n    success_file_list = uploader.get_success_upload_file_list(\n        import_path, skip_subfolders\n    )\n    to_finalize_file_list = uploader.get_finalize_file_list(\n        import_path, skip_subfolders\n    )\n\n    if len(success_file_list) == len(total_file_list):\n        print(\"All images have already been uploaded\")\n    else:\n        \n        \n        upload_file_list = [f for f in upload_file_list if verify_mapillary_tag(f)]\n\n        if not len(upload_file_list) and not len(to_finalize_file_list):\n            print(\"No images to upload.\")\n            print(\n                'Please check if all images contain the required Mapillary metadata. If not, you can use \"mapillary_tools process\" to add them'\n            )\n            sys.exit(1)\n\n        if upload_file_list:\n            \n            \n            params = {}\n            list_per_sequence_mapping = {}\n            direct_upload_file_list = []\n            for image in upload_file_list:\n                log_root = uploader.log_rootpath(image)\n\n                \n                upload_params_path = os.path.join(\n                    log_root, \"upload_params_process.json\"\n                )\n                if os.path.isfile(upload_params_path):\n                    with open(upload_params_path, \"r\") as fp:\n                        params[image] = json.load(fp)\n                    sequence = params[image][\"key\"]\n                    list_per_sequence_mapping.setdefault(sequence, []).append(image)\n                else:\n                    direct_upload_file_list.append(image)\n\n                \n                description_path = os.path.join(\n                    log_root, \"mapillary_image_description.json\"\n                )\n                if not os.path.isfile(description_path):\n                    raise RuntimeError(\n                        f\"Please run process first because {description_path} is not generated\"\n                    )\n                with open(description_path, \"r\") as fp:\n                    description = json.load(fp)\n                assert not set(description).intersection(\n                    params.get(image, {})\n                ), f\"Parameter conflicting {description} and {params.get(image, {})}\"\n                params.setdefault(image, {}).update(description)\n\n            \n            \n\n            print(\n                f\"Uploading {len(upload_file_list)} images with valid mapillary tags (Skipping {len(total_file_list) - len(upload_file_list)})\"\n            )\n\n            if direct_upload_file_list:\n                raise RuntimeError(\n                    f\"Found {len(direct_upload_file_list)} files for direct upload which is not supported in v4\"\n                )\n\n            total_sequences = len(list_per_sequence_mapping)\n            for idx, sequence_uuid in enumerate(list_per_sequence_mapping):\n                metadata = {\n                    \"total_sequences\": total_sequences,\n                    \"sequence_idx\": idx,\n                }\n                uploader.upload_sequence_v4(\n                    list_per_sequence_mapping[sequence_uuid],\n                    sequence_uuid,\n                    params,\n                    metadata=metadata,\n                    dry_run=dry_run,\n                )\n\n        if to_finalize_file_list:\n            params = {}\n            sequences = []\n            for image in to_finalize_file_list:\n                log_root = uploader.log_rootpath(image)\n                upload_params_path = os.path.join(\n                    log_root, \"upload_params_process.json\"\n                )\n                if os.path.isfile(upload_params_path):\n                    with open(upload_params_path, \"rb\") as jf:\n                        image_params = json.load(jf)\n                        sequence = image_params[\"key\"]\n                        if sequence not in sequences:\n                            params[image] = image_params\n                            sequences.append(sequence)\n\n            uploader.flag_finalization(to_finalize_file_list)\n\n    uploader.print_summary(upload_file_list)\n",
        "summary": "The provided Python script includes functions for verifying Mapillary tags in images and uploading them to a server. It processes directories of images, checks for required metadata, and handles both direct and sequence-based uploads with optional dry run functionality."
    },
    {
        "code": "import re\n\nimport click\nfrom cloup import option, option_group\n\nfrom ... import logger\n\n\ndef validate_scene_range(ctx, param, value):\n    try:\n        start = int(value)\n        return (start,)\n    except Exception:\n        pass\n\n    if value:\n        try:\n            start, end = map(int, re.split(r\"[;,\\-]\", value))\n            return start, end\n        except Exception:\n            logger.error(\"Couldn't determine a range for -n option.\")\n            exit()\n\n\ndef validate_resolution(ctx, param, value):\n    if value:\n        try:\n            start, end = map(int, re.split(r\"[;,\\-]\", value))\n            return (start, end)\n        except Exception:\n            logger.error(\"Resolution option is invalid.\")\n            exit()\n\n\nrender_options = option_group(\n    \"Render Options\",\n    option(\n        \"-n\",\n        \"--from_animation_number\",\n        callback=validate_scene_range,\n        help=\"Start rendering from n_0 until n_1. If n_1 is left unspecified, \"\n        \"renders all scenes after n_0.\",\n        default=None,\n    ),\n    option(\n        \"-a\",\n        \"--write_all\",\n        is_flag=True,\n        help=\"Render all scenes in the input file.\",\n        default=None,\n    ),\n    option(\n        \"--format\",\n        type=click.Choice([\"png\", \"gif\", \"mp4\", \"webm\", \"mov\"], case_sensitive=False),\n        default=None,\n    ),\n    option(\"-s\", \"--save_last_frame\", is_flag=True, default=None),\n    option(\n        \"-q\",\n        \"--quality\",\n        default=None,\n        type=click.Choice([\"l\", \"m\", \"h\", \"p\", \"k\"], case_sensitive=False),\n        help=,\n    ),\n    option(\n        \"-r\",\n        \"--resolution\",\n        callback=validate_resolution,\n        default=None,\n        help=\"Resolution in (W,H) for when 16:9 aspect ratio isn't possible.\",\n    ),\n    option(\n        \"--fps\",\n        \"--frame_rate\",\n        \"frame_rate\",\n        type=float,\n        default=None,\n        help=\"Render at this frame rate.\",\n    ),\n    option(\n        \"--renderer\",\n        type=click.Choice([\"cairo\", \"opengl\", \"webgl\"], case_sensitive=False),\n        help=\"Select a renderer for your Scene.\",\n        default=None,\n    ),\n    option(\n        \"--use_opengl_renderer\",\n        is_flag=True,\n        help=\"Render scenes using OpenGL (Deprecated).\",\n        default=None,\n    ),\n    option(\n        \"--use_webgl_renderer\",\n        is_flag=True,\n        help=\"Render scenes using the WebGL frontend (Deprecated).\",\n        default=None,\n    ),\n    option(\n        \"--webgl_renderer_path\",\n        default=None,\n        type=click.Path(),\n        help=\"The path to the WebGL frontend.\",\n    ),\n    option(\n        \"-g\",\n        \"--save_pngs\",\n        is_flag=True,\n        default=None,\n        help=\"Save each frame as png (Deprecated).\",\n    ),\n    option(\n        \"-i\",\n        \"--save_as_gif\",\n        default=None,\n        is_flag=True,\n        help=\"Save as a gif (Deprecated).\",\n    ),\n    option(\n        \"-s\",\n        \"--save_last_frame\",\n        default=None,\n        is_flag=True,\n        help=\"Save last frame as png (Deprecated).\",\n    ),\n    option(\n        \"-t\",\n        \"--transparent\",\n        is_flag=True,\n        help=\"Render scenes with alpha channel.\",\n    ),\n    option(\n        \"--use_projection_fill_shaders\",\n        is_flag=True,\n        help=\"Use shaders for OpenGLVMobject fill which are compatible with transformation matrices.\",\n        default=None,\n    ),\n    option(\n        \"--use_projection_stroke_shaders\",\n        is_flag=True,\n        help=\"Use shaders for OpenGLVMobject stroke which are compatible with transformation matrices.\",\n        default=None,\n    ),\n)\n",
        "summary": "The provided Python code defines a set of validation functions and options using the `cloup` library to handle command-line arguments for rendering scenes. It includes options for specifying scene ranges, resolutions, formats, quality settings, and various renderers, along with deprecation warnings for some older options."
    },
    {
        "code": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"intent_classifier\",\n    version=\"0.2.0\",\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[\"numpy\", \"scipy\", \"PyMySQL\", \"scikit-learn==0.20.3\"]\n)\n",
        "summary": "This Python script uses setuptools to define and configure a package named \"intent_classifier\" with version 0.2.0, including all necessary packages found in the directory, additional data files, and dependencies on numpy, scipy, PyMySQL, and scikit-learn version 0.20.3."
    },
    {
        "code": "from django.contrib import admin\nfrom application.models import Profile\n\n\nadmin.site.register(Profile)\n",
        "summary": "The provided Python code registers the `Profile` model from the `application.models` module with Django's admin site, allowing for administrative management of `Profile` instances through the Django admin interface."
    },
    {
        "code": "__version__ = '$Revision: $'\n\n\n\n\n\n\nimport os, sys\nimport pdb\n\n\n\n\nimport numpy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass Geometry(object):\n    \n    \n    \n    def __init__(self, name={},CGPercent = 0.25,ForeSparPercent = 0.25,\n             RearSparPercent = 0.75,StaticMarginPercent=0.05,\n             ForeThickCon = 0.01, RearThickCon = 0.99,\n             rootOffset = 0.01, tipOffset=0.01,\n             xRootec=0.0, yRootec=0.0, zRootec=0.0,\n             *args, **kwargs):\n        \n        \n        \n        \n        self.name = name\n        self.CGPercent = CGPercent\n        self.ForeSparPercent = ForeSparPercent \n        self.RearSparPercent = RearSparPercent\n        self.StaticMarginPercent = StaticMarginPercent\n        self.ForeThickCon = ForeThickCon\n        self.RearThickCon = RearThickCon\n        self.tipOffset = tipOffset\n        self.rootOffset = rootOffset\n        self.xRootec = xRootec\n        self.yRootec = yRootec\n        self.zRootec = zRootec\n        \n    def ListAttributes(self):\n        \n        \n        \n        ListAttributes(self)\n        \n        \n    def __str__(self):\n        \n        \n        \n        return ('name    \\n'+'     '+str(self.name).center(9) )\n    \n\n\n\n\n\ndef ListAttributes(self):\n        \n        \n        \n        print('\\n')\n        print('Attributes List of: ' + repr(self.__dict__['name']) + ' - ' + self.__class__.__name__ + ' Instance\\n')\n        self_keys = self.__dict__.keys()\n        self_keys.sort()\n        for key in self_keys:\n            if key != 'name':\n                print(str(key) + ' : ' + repr(self.__dict__[key]))\n            \n        \n        print('\\n')\n    \n\n\n\n\n\nif __name__ == '__main__':\n    \n    print('Testing ...')\n    \n    \n    geo = Geometry(name = 'test')\n    geo.ListAttributes()\n    print(geo)\n",
        "summary": "The provided Python code defines a `Geometry` class with various attributes related to aircraft geometry, such as center of gravity (CG) percentages and offsets. It includes methods for listing the attributes of an instance and printing a string representation of the object. The script also demonstrates how to create an instance of the `Geometry` class and call its methods for testing purposes."
    },
    {
        "code": "from orchespy import device\nfrom orchespy.devicetype import CUDAGPU, Host, VE\nimport sys\nimport pytest\n\nimport numpy as np\n\nif \"cupy\" in sys.modules:\n    import cupy as cp\nif \"nlcpy\" in sys.modules:\n    import nlcpy as vp\n\nno_nlcpy = pytest.mark.skipif(\n        \"nlcpy\" not in sys.modules, reason=' test require nlcpy. ')\nno_cupy = pytest.mark.skipif(\n        \"cupy\" not in sys.modules, reason=' test require cupy. ')\n\n\n\n@device(Host, numpy_module_arg='xp')\ndef create_array_init_5_at_host(shape, dtype, order, xp):\n    return xp.full(shape, 5, dtype=dtype, order=order)\n\n\n@device(CUDAGPU, numpy_module_arg='xp')\ndef create_array_init_5_at_gpu(shape, dtype, order, xp):\n    return xp.full(shape, 5, dtype=dtype, order=order)\n\n\n@device(VE, numpy_module_arg='xp')\ndef create_array_init_5_at_ve(shape, dtype, order, xp):\n    return xp.full(shape, 5, dtype=dtype, order=order)\n\n\n@pytest.mark.parametrize('shape', [(2), (2, 2), (2, 2, 2), (2, 3), (2, 3, 4)])\n@pytest.mark.parametrize('dtype', [\n    'i4', 'i8', 'u4', 'u8', 'f4', 'f8', 'c8', 'c16'\n    ])\n@pytest.mark.parametrize('order', ['C', 'F'])\nclass TestDeviceArgs:\n    def test_device_args_host(self, shape, dtype, order):\n        y = create_array_init_5_at_host(shape, dtype, order)\n        assert(isinstance(y, np.ndarray))\n        expected = np.full(shape, 5, dtype=dtype, order=order)\n        assert((y == expected).all())\n\n    @no_cupy\n    def test_device_args_gpu(self, shape, dtype, order):\n        y = create_array_init_5_at_gpu(shape, dtype, order)\n        assert(isinstance(y, cp.ndarray))\n        expected = cp.full(shape, 5, dtype=dtype, order=order)\n        assert((y == expected).all())\n\n    @no_nlcpy\n    def test_device_args_ve(self, shape, dtype, order):\n        y = create_array_init_5_at_ve(shape, dtype, order)\n        assert(isinstance(y, vp.ndarray))\n        expected = vp.full(shape, 5, dtype=dtype, order=order)\n        assert((y == expected).all())\n",
        "summary": "The provided Python code defines functions to create arrays initialized with the value 5 on different devices (Host, CUDAGPU, VE) using numpy or alternative libraries like cupy and nlcpy. It includes tests to verify that these arrays are created correctly on each device and match expected values."
    },
    {
        "code": "import typing as t\nfrom threading import Lock\nfrom threading import get_ident\n\nimport sqlalchemy\nfrom sqlalchemy import (\n    orm,\n    schema,\n)\nfrom sqlalchemy.engine import make_url\nfrom sqlalchemy.orm import (\n    declarative_base,\n    DeclarativeMeta,\n    Session as SessionBase\n)\n\nfrom mask.globals import current_app\nfrom .model import (\n    DefaultMeta,\n    Model\n)\nif t.TYPE_CHECKING:\n    from mask import Mask\n\n\n__version__ = \"1.0.0a1\"\n\n\nclass BindSession(SessionBase):\n\n    def __init__(self, db, autocommit=False, autoflush=True, **options):\n        \n        self.db = db\n        self.app = db.get_app()\n\n        bind = options.pop(\"bind\", None) or db.engine\n        binds = options.pop(\"binds\", db.get_binds(self.app))\n\n        SessionBase.__init__(\n            self,\n            autocommit=autocommit,\n            autoflush=autoflush,\n            bind=bind,\n            binds=binds,\n            **options\n        )\n\n    def get_bind(self, mapper=None, **kwargs):\n        \n        if mapper is not None:\n            \n            persist_selectable = mapper.persist_selectable\n\n            \n            info = getattr(persist_selectable, \"info\", {})\n            bind_key = info.get(\"bind_key\")\n            if bind_key is not None:\n                \n                return self.db.get_engine(self.app, bind=bind_key)\n        \n        return super().get_bind(mapper, **kwargs)\n\n\nclass _EngineConnector:\n\n    def __init__(self, sa, app, bind=None):\n        \n        self._sa = sa\n        self._app = app\n        self._engine = None\n        self._bind = bind\n        self._connect_for = None\n        self._lock = Lock()\n\n    def get_uri(self):\n        \n        \n        if self._bind is None:\n            return None\n\n        \n        binds = self._app.config.get(\"SQLALCHEMY_BINDS\") or ()\n        if self._bind not in binds:\n            raise RuntimeError(f\"Bind {self._bind!r} is not configure in 'SQLALCHEMY_BINDS'.\")\n        return binds[self._bind]\n\n    def get_engine(self):\n        with self._lock:\n            \n            uri = self.get_uri()\n            if uri == self._connect_for:\n                return self._engine\n\n            \n            sa_url, options = self.get_options(make_url(uri))\n            self._engine = self._sa.create_engine(sa_url, options)\n            self._connect_for = uri\n\n        return self._engine\n\n    def dispose(self):\n        \n        if not self._engine:\n            return\n\n        self._engine.dispose()\n        \n        self._engine = None\n        self._connect_for = None\n\n    def get_options(self, sa_url):\n        \n        options = {}\n        options.update(self._app.config[\"SQLALCHEMY_ENGINE_OPTIONS\"])\n        options.update(self._sa._engine_options)\n        return sa_url, options\n\n\nclass _QueryProperty:\n\n    def __init__(self, sa):\n        self.sa = sa\n\n    def __get__(self, obj, cls):  \n        try:\n            mapper = orm.class_mapper(cls)\n            if mapper:\n                return cls.query_class(mapper, session=self.sa.session())\n        except orm.exc.UnmappedClassError:\n            return None\n\n\ndef _include_sqlalchemy(obj, _):\n    \n    for module in sqlalchemy, sqlalchemy.orm:\n        for key in module.__all__:\n            if not hasattr(obj, key):\n                setattr(obj, key, getattr(module, key))\n\n\nclass SQLAlchemy:\n\n    Query = None\n\n    def __init__(\n            self,\n            app: t.Optional[\"Mask\"] = None,\n            session_options: t.Optional[dict] = None,\n            metadata: t.Optional[\"schema.MetaData\"] = None,\n            query_class: t.Optional[\"orm.Query\"] = orm.Query,\n            model_class: t.Optional[\"Model\"] = Model,\n            engine_options: t.Optional[dict] = None,\n    ) -> None:\n        \n        self.app = app\n\n        self.Query = query_class\n        self.session = self.create_scoped_session(session_options)\n        self.Model = self.make_declarative_base(model_class, metadata)\n        self._engine_lock = Lock()\n        self._engine_options = engine_options or {}\n        self.connectors = {}\n\n        _include_sqlalchemy(self, query_class)\n\n        if app is not None:\n            self.init_app(app)\n\n    @property\n    def engine(self):\n        \n        return self.get_engine()\n\n    def get_engine(self, app: t.Optional[\"Mask\"] = None, bind: str = None):\n        \n        app = self.get_app(app)\n\n        with self._engine_lock:\n            connector = self.connectors.get(bind)\n            if connector is None:\n                connector = _EngineConnector(self, self.get_app(app), bind)\n                self.connectors[bind] = connector\n\n            return connector.get_engine()\n\n    def _dispose_all_engine(self):\n        \n        with self._engine_lock:\n            for _, connector in self.connectors.items():\n                connector.dispose()\n            self.connectors.clear()\n\n    def create_engine(self, sa_url, engine_opts):\n        \n        return sqlalchemy.create_engine(sa_url, **engine_opts)\n\n    def create_scoped_session(self, options=None):\n        \n        options = options or {}\n\n        scope_func = options.pop(\"scopefunc\", get_ident)\n        options.setdefault(\"query_cls\", self.Query)\n        return orm.scoped_session(self.create_session(options), scopefunc=scope_func)\n\n    def create_session(self, options):\n        \n        return orm.sessionmaker(class_=BindSession, db=self, **options)\n\n    def make_declarative_base(self, model, matadata=None):\n        \n        if not isinstance(model, DeclarativeMeta):\n            model = declarative_base(cls=model, name=\"Model\", metadata=matadata, metaclass=DefaultMeta)\n\n        if not getattr(model, \"query_class\", None):\n            model.query_class = self.Query\n\n        model.query = _QueryProperty(self)\n        return model\n\n    def get_binds(self, app=None):\n        \n        app = self.get_app(app)\n        binds = [None] + list(app.config.get(\"SQLALCHEMY_BINDS\") or ())\n        ret_val = {}\n        for bind in binds:\n            engine = self.get_engine(app, bind)\n            tables = self.get_tables_for_bind(bind)\n            ret_val.update({table: engine for table in tables})\n        return ret_val\n\n    def init_app(self, app):\n        \n        \n        self.app = app\n\n        app.config.setdefault(\"SQLALCHEMY_BINDS\", None)\n        app.config.setdefault(\"SQLALCHEMY_ENGINE_OPTIONS\", {})\n\n        \n        \n        self._dispose_all_engine()\n\n        app.extensions[\"SQLAlchemy\"] = self\n\n        @app.teardown_appcontext\n        def shutdown_session(exc):  \n            \n            self.session.remove()\n            return exc\n\n    def get_app(self, reference_app=None):\n        \n        if reference_app is not None:\n            return reference_app\n\n        \n        if current_app:\n            return current_app._get_current_object()\n\n        if self.app is not None:\n            return self.app\n\n        raise RuntimeError(\n            \"No application fund.\"\n        )\n\n    def get_tables_for_bind(self, bind=None):\n        \n        result = []\n        for table in self.Model.metadata.tables.values():\n            if table.info.get(\"bind_key\") == bind:\n                result.append(table)\n        return result\n",
        "summary": "The provided Python code defines a class `SQLAlchemy` that integrates SQLAlchemy ORM with a Flask application. It includes session management, engine connection handling, and model declaration functionalities, allowing for flexible database operations within a Flask context."
    },
    {
        "code": "from sqlalchemy import Boolean\nfrom sqlalchemy import Column\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import Integer\nfrom sqlalchemy import MetaData\nfrom sqlalchemy import String\nfrom sqlalchemy import Table\n\nmeta = MetaData()\n\ncluster = Table(\n    'clusters', meta,\n    Column('id', Integer, primary_key=True),\n    Column('deleted', Integer, default=None),\n    Column('name', String(255), default=None),\n    Column('enabled', Boolean, default=False),\n    Column('status', String(36), default=1),\n    Column('updated_at', DateTime, default=None),\n    Column('created_at', DateTime, default=None),\n    Column('deleted_at', DateTime, default=None)\n)\n\n\ndef upgrade(migrate_engine):\n    meta.bind = migrate_engine\n    cluster.create()\n\n\ndef downgrade(migrate_engine):\n    meta.bind = migrate_engine\n    cluster.drop()\n",
        "summary": "The provided Python code defines a SQLAlchemy table schema for clusters with various columns including id, name, enabled status, and timestamps. It includes functions to upgrade the database by creating the table and downgrade by dropping it."
    },
    {
        "code": "__revision__ = \"src/engine/SCons/Tool/sunf77.py  2014/08/24 12:12:31 garyo\"\n\nimport SCons.Util\n\nfrom FortranCommon import add_all_to_env\n\ncompilers = ['sunf77', 'f77']\n\ndef generate(env):\n    \n    add_all_to_env(env)\n\n    fcomp = env.Detect(compilers) or 'f77'\n    env['FORTRAN']  = fcomp\n    env['F77']      = fcomp\n\n    env['SHFORTRAN']  = '$FORTRAN'\n    env['SHF77']      = '$F77'\n\n    env['SHFORTRANFLAGS'] = SCons.Util.CLVar('$FORTRANFLAGS -KPIC')\n    env['SHF77FLAGS'] = SCons.Util.CLVar('$F77FLAGS -KPIC')\n\ndef exists(env):\n    return env.Detect(compilers)\n\n\n\n\n\n\n",
        "summary": "This Python script is a tool for the SCons build system that configures environments to use Sun Fortran 77 compilers. It detects available compilers, sets up environment variables for both regular and shared object compilations, and provides flags for position-independent code generation."
    },
    {
        "code": "from types import ListType\nfrom itertools import imap\n\nfrom events import Event\n\nclass ValueChanged(Event):\n    \n\n    def __init__(self, value):\n        \"x.__init__(...) initializes x; see x.__class__.__doc__ for signature\"\n\n        super(ValueChanged, self).__init__(value)\n\n\nclass Value(object):\n    \n\n    def __init__(self, event=None, manager=None, onSet=None):\n        \"x.__init__(...) initializes x; see x.__class__.__doc__ for signature\"\n\n        self.event = event\n        self.manager = manager\n\n        self.onSet = onSet\n\n        self.result = False\n        self.errors = False\n        self._parent = self\n        self._value = None\n\n    def __getstate__(self):\n        keys = (\"event\", \"onSet\", \"result\", \"errors\", \"_value\")\n        return dict([(k, getattr(self, k, None)) for k in keys])\n\n    def __contains__(self, y):\n        value = self.value\n        return y in value if type(value) is ListType else y == value\n\n    def __getitem__(self, y):\n        v = self.value[y]\n        if isinstance(v, Value):\n            return v.value\n        else:\n            return v\n\n    def __iter__(self):\n        return imap(lambda v: v.value if isinstance(v, Value) else v,\n                self.value)\n\n    def __repr__(self):\n        \"x.__repr__() <==> repr(x)\"\n\n        value = \"\"\n        if self.result:\n            value = repr(self.value)\n\n        format = \"<Value (%s) result: %r errors: %r for %r\"\n        return format % (value, self.result, self.errors, self.event)\n\n    def __str__(self):\n        \"x.__str__() <==> str(x)\"\n\n        return str(self.value)\n\n    def getValue(self):\n        value = self._value\n        while isinstance(value, Value):\n            value = value._value\n        return value\n\n    def setValue(self, value):\n        if isinstance(value, Value):\n            value._parent = self\n\n        if self.result and type(self._value) is ListType:\n            self._value.append(value)\n        elif self.result:\n            self._value = [self._value]\n            self._value.append(value)\n        else:\n            self._value = value\n\n        def notify(o, v):\n            if not isinstance(v, Value) and v is not None:\n                o.result = True\n                if o.manager is not None and o.onSet is not None:\n                    o.manager.fireEvent(ValueChanged(o), *o.onSet)\n            elif isinstance(v, Value):\n                o.errors = v.errors\n                o.result = v.result\n            if not o._parent == o:\n                notify(o._parent, v)\n        \n        notify(self, value)\n\n    value = property(getValue, setValue, None, \"Value of this Value\")\n",
        "summary": "The provided Python code defines a `ValueChanged` event class and a `Value` class. The `ValueChanged` class inherits from an `Event` class and is initialized with a value. The `Value` class manages a value that can be another `Value` instance, handling setting and getting values while notifying observers of changes through events."
    },
    {
        "code": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.utils import resample\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom monty.serialization import dumpfn\n\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport pickle\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n\n__author__ = \"Nathan C. Frey, Jin Wang\"\n__copyright__ = \"MIT License\"\n__version__ = \"0.0.1\"\n__maintainer__ = \"Nathan C. Frey\"\n__email__ = \"n.frey@seas.upenn.edu\"\n__status__ = \"Development\"\n__date__ = \"Aug 2017\"\n\n\nclass PULearner:\n    def __init__(self):\n        \n\n        self.pu_stats = None\n        self.df_U = None\n        self.df_P = None\n        self.synth_scores = None\n        self.labels = None\n        self.feat_importances = None\n\n    def cv_baggingDT(self, pu_data, splits=10, repeats=10, bags=100, filename=\"\"):\n        \n        \n        print(\"Start PU Learning.\")\n\n        \n        df = pd.read_json(pu_data)\n        df_P, df_U, X_P, X_U = self._process_pu_data(df)\n        self.df_P = df_P\n        self.df_U = df_U\n\n        \n        kfold = RepeatedKFold(n_splits=splits, n_repeats=repeats, random_state=42)\n\n        \n        scores = []\n        tprs = []\n\n        \n        prob_P = np.ones(shape=(X_P.shape[0], splits * repeats))\n        prob_U = -np.ones(shape=(X_U.shape[0], splits * repeats))\n\n        \n        feat_rank = np.zeros(shape=(X_P.shape[1], splits * repeats))\n\n        idsp = 0  \n\n        \n        for (ptrain, ptest), (utrain, utest) in zip(kfold.split(X_P), kfold.split(X_U)):\n\n            \n            N_ptrain = X_P[ptrain].shape[0]\n            N_utrain = X_U[utrain].shape[0]\n\n            d = X_P.shape[1]\n            K = N_ptrain\n            train_label = np.zeros(shape=(N_ptrain + K,))\n            train_label[:N_ptrain] = 1.0  \n\n            \n            n_oob = np.zeros(shape=(N_utrain,))\n            f_oob = np.zeros(shape=(N_utrain, 2))\n\n            \n            f_ptest = np.zeros(shape=(X_P[ptest].shape[0], 2))\n            f_utest = np.zeros(shape=(X_U[utest].shape[0], 2))\n\n            \n            for i in range(bags):\n                bootstrap_sample = np.random.choice(\n                    np.arange(N_utrain), replace=True, size=K\n                )\n\n                \n                data_bootstrap = np.concatenate(\n                    (X_P[ptrain], X_U[bootstrap_sample, :]), axis=0\n                )\n\n                \n                model = DecisionTreeClassifier(\n                    max_depth=None,\n                    max_features=None,\n                    criterion=\"gini\",\n                    class_weight=\"balanced\",\n                )\n\n                model.fit(data_bootstrap, train_label)\n\n                \n                idx_oob = sorted(\n                    set(range(N_utrain)) - set(np.unique(bootstrap_sample))\n                )\n\n                \n                f_oob[idx_oob] += model.predict_proba(X_U[utrain][idx_oob])\n                n_oob[idx_oob] += 1\n                f_ptest += model.predict_proba(X_P[ptest])\n                f_utest += model.predict_proba(X_U[utest])\n                feat_rank[:, idsp] = model.feature_importances_\n\n            \n            predict_utrain = f_oob[:, 1] / n_oob\n\n            \n            predict_ptest = f_ptest[:, 1] / bags\n            predict_utest = f_utest[:, 1] / bags\n\n            \n            true_pos = predict_ptest[np.where(predict_ptest > 0.5)].shape[0]\n            u_pos = predict_utest[np.where(predict_utest > 0.5)].shape[0]\n\n            N_ptest = X_P[ptest].shape[0]\n            N_utest = X_U[utest].shape[0]\n\n            \n            p_pred_pos = (true_pos + u_pos) / (N_ptest + N_utest) + 0.0001\n\n            \n            recall = true_pos / N_ptest\n            score = recall ** 2 / p_pred_pos\n            scores.append(score)\n            tprs.append(recall)\n\n            \n            prob_P[ptest, idsp] = predict_ptest\n            prob_U[utrain, idsp] = predict_utrain\n            prob_U[utest, idsp] = predict_utest\n            idsp += 1\n\n            \n            if (idsp + 1) % splits == 0:\n                tpr_tmp = np.asarray(tprs[-splits - 1 : -1])\n                print(\n                    \"Performed Repeated \"\n                    + str(splits)\n                    + \"-fold: \"\n                    + str(idsp // splits + 1)\n                    + \" out of \"\n                    + str(repeats)\n                )\n                print(\n                    \"True Positive Rate: %0.2f (+/- %0.2f)\"\n                    % (tpr_tmp.mean(), tpr_tmp.std() * 2)\n                )\n\n        \n        label_U = np.zeros(shape=(X_U.shape[0], splits * repeats + 1), dtype=int)\n        label_U[:, : splits * repeats][np.where(prob_U > 0.5)] = 1\n        label_U[:, splits * repeats] = np.sum(\n            label_U[:, : splits * repeats + 1], axis=1\n        )\n\n        tprs = np.asarray(tprs)\n        scores = np.asarray(scores)\n\n        \n        label_U_rp = np.zeros(shape=(X_U.shape[0], repeats), dtype=int)\n        prob_U_rp = np.zeros(shape=(X_U.shape[0], repeats))\n        feat_rank_rp = np.zeros(shape=(X_U.shape[1], repeats))\n        tpr_rp = np.zeros(shape=(repeats,))\n        scores_rp = np.zeros(shape=(repeats,))\n        labels = np.zeros(shape=(X_U.shape[0],))\n\n        for i in range(repeats):\n            prob_U_rp[:, i] = prob_U[:, i * splits : (i + 1) * splits].mean(axis=1)\n            feat_rank_rp[:, i] = feat_rank[:, i * splits : (i + 1) * splits].mean(\n                axis=1\n            )\n            tpr_rp[i] = tprs[i * splits : (i + 1) * splits].mean()\n            scores_rp[i] = scores[i * splits : (i + 1) * splits].mean()\n\n        label_U_rp[np.where(prob_U_rp > 0.5)] = 1\n        prob = prob_U_rp.mean(axis=1)\n        labels[np.where(prob > 0.5)] = 1\n\n        \n        tpr_low, tpr_up = self.bootstrapCI(tpr_rp)\n        scores_low, scores_up = self.bootstrapCI(scores_rp)\n\n        \n        metrics = np.asarray(\n            [tpr_rp.mean(), tpr_low, tpr_up, scores_rp.mean(), scores_low, scores_up]\n        )\n\n        print(\"Accuracy: %0.2f\" % (tpr_rp.mean()))\n        print(\"95%% confidence interval: [%0.2f, %0.2f]\" % (tpr_low, tpr_up))\n\n        \n        pu_stats = {\n            \"prob\": prob,\n            \"labels\": labels,\n            \"metrics\": metrics,\n            \"prob_rp\": prob_U_rp,\n            \"label_rp\": label_U_rp,\n            \"tpr_rp\": tpr_rp,\n            \"scores_rp\": scores_rp,\n            \"feat_rank_rp\": feat_rank_rp,\n        }\n\n        \n        if filename:\n            if filename.endswith(\".json\"):\n                dumpfn(pu_stats, filename)\n            if filename.endswith(\".pkl\"):\n                with open(filename, \"wb\") as file:\n                    pickle.dump(pu_stats, file, protocol=pickle.HIGHEST_PROTOCOL)\n\n        self.pu_stats = pu_stats\n        return pu_stats\n\n    def bootstrapCI(self, data, ci=95, ns=10000):\n        \n\n        bs_rsample = []\n        for _ in range(ns):\n            rsample = resample(data, n_samples=len(data))\n            bs_rsample.append(np.mean(rsample))\n\n        bs_rsample = np.asarray(bs_rsample)\n        lower = np.percentile(bs_rsample, (100 - ci) / 2)\n        upper = np.percentile(bs_rsample, ci + (100 - ci) / 2)\n\n        return lower, upper\n\n    def corr_heatmap(self, num_feats=10, fname=\"\"):\n        \n\n        pu_stats = self.pu_stats\n        df_U = self.df_U\n        df_U_copy = df_U.drop(columns=[\"PU_label\"])\n\n        \n        synth_scores = pu_stats[\"prob\"]\n        df_U_copy[\"synth_score\"] = synth_scores\n\n        \n        corrmat = df_U_copy.corr()\n        cols = corrmat.nlargest(num_feats, \"synth_score\")[\"synth_score\"].index\n        cm = np.corrcoef(df_U_copy[cols].values.T)\n\n        sns.set(style='ticks')\n        rcParams['figure.dpi'] = 300\n\n        fig, ax = plt.subplots(1, 1)\n        hm = sns.heatmap(\n            cm,\n            ax=ax,\n            cbar=True,\n            annot=True,\n            square=True,\n            fmt=\".2f\",\n            annot_kws={\"size\": 7},\n            yticklabels=cols.values,\n            xticklabels=cols.values,\n        )\n\n        if fname:\n            self.save_plot(fname + \".png\", fig, ax)\n\n    def get_feat_importances(self, plot_format=\"\"):\n        \n\n        pu_stats = self.pu_stats\n\n        \n        feat_rank_rp = pu_stats[\"feat_rank_rp\"]\n        feat_importances = np.sum(feat_rank_rp, axis=1)\n\n        df_U = self.df_U\n        df_U = df_U._get_numeric_data()\n        df_U_copy = df_U.drop(columns=[\"PU_label\"])\n        feat_names = df_U_copy.columns\n\n        \n        df_feat = pd.DataFrame(columns=[\"feature\", \"importance\"])\n        df_feat[\"feature\"] = feat_names\n        df_feat[\"importance\"] = feat_importances\n\n        \n        df_feat_sort = df_feat.sort_values(by=\"importance\", ascending=False)\n        max_value = df_feat[\"importance\"].max()\n\n        \n        df_feat_sort[\"importance\"] = df_feat_sort[\"importance\"] / max_value\n\n        \n        self.feat_importances = df_feat\n\n        if plot_format in [\"svg\", \"pdf\", \"png\"]:\n\n            \n            fig, ax = plt.subplots(figsize=(10, 4))\n            with sns.axes_style(style=\"ticks\"):\n                sns.barplot(x=\"feature\", y=\"importance\", data=df_feat_sort)\n            ax.set_xticklabels(\n                ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=7\n            )\n            filename = \"feat_importance.\" + plot_format\n            self.save_plot(filename, fig, ax)\n\n    @staticmethod\n    def _process_pu_data(data):\n        \n\n        df_P = data.query(\"PU_label == 1\")  \n        df_U = data.query(\"PU_label == 0\")  \n\n        \n        X_P = np.asarray(df_P.drop(columns=[\"PU_label\"])._get_numeric_data())\n        X_U = np.asarray(df_U.drop(columns=[\"PU_label\"])._get_numeric_data())\n\n        return df_P, df_U, X_P, X_U\n\n    @staticmethod\n    def save_plot(filename, fig, ax):\n        \n\n        sns.set_style(\"ticks\")\n        fig.tight_layout()\n        fig.savefig(filename)\n\n\nclass PUInteract:\n    def __init__(self, df_parent, pu_parent, df_child, pu_child, merge_on=(), feats=()):\n        \n\n        df_parent = pd.read_json(df_parent)\n        df_child = pd.read_json(df_child)\n\n        \n        df_parent[\"synth_score\"] = 1\n        df_child[\"synth_score\"] = 1\n\n        df_parent.loc[df_parent.eval(\"PU_label == 0\"), \"synth_score\"] = pu_parent[\n            \"prob\"\n        ]\n        df_child.loc[df_child.eval(\"PU_label == 0\"), \"synth_score\"] = pu_child[\"prob\"]\n\n        \n        merge_on = list(merge_on)\n        df = pd.merge(\n            df_parent, df_child, on=merge_on, how=\"outer\", suffixes=[\"_p\", \"_c\"]\n        )\n        df.drop(columns=[\"PU_label_p\", \"PU_label_c\"], inplace=True, axis=1)\n\n        if feats:\n            feat_names = [f + \"_p\" for f in feats] + [f + \"_c\" for f in feats]\n            df = df[feat_names]\n\n        self.merged_df = df\n        self.X = np.array(df)\n\n    def do_kmeans(self, n_clusters=2, seed=42):\n        \n\n        np.random.seed(seed)\n        km = KMeans(n_clusters=n_clusters, random_state=seed)\n\n        km.fit(self.X)\n        kmeans_output = {\n            \"cluster_centers\": km.cluster_centers_,\n            \"cluster_labels\": km.labels_,\n        }\n\n        return kmeans_output\n\n    def do_gmixture(self, n_components=2, seed=42):\n        \n\n        np.random.seed(seed)\n        gmm = GaussianMixture(\n            n_components=n_components, random_state=seed, covariance_type=\"full\"\n        )\n\n        gmm.fit(self.X)\n        gmm_labels = gmm.predict(self.X)\n        gmm_prob = gmm.predict_proba(self.X)[:, 0]\n        gmm_output = {\"gmm_labels\": gmm_labels, \"gmm_prob\": gmm_prob}\n\n        return gmm_output\n\n    def do_bgm(self, n_components=6, seed=42):\n        \n\n        np.random.seed(seed)\n        bgm = BayesianGaussianMixture(\n            n_components=n_components,\n            covariance_type=\"full\",\n            weight_concentration_prior=1e-2,\n            weight_concentration_prior_type=\"dirichlet_process\",\n            mean_precision_prior=1e-2,\n            init_params=\"random\",\n            max_iter=100,\n            random_state=seed,\n        )\n\n        bgm.fit(self.X)\n        bgm_labels = bgm.predict(self.X)\n        bgm_prob = bgm.predict_proba(self.X)[:, 0]\n\n        bgm_output = {\"bgm_labels\": bgm_labels, \"bgm_prob\": bgm_prob}\n\n        return bgm_output\n",
        "summary": "This code defines two classes: `PUInterpret` and `PUInteract`. The `PUInterpret` class is used for interpreting the results of a positive-unlabeled (PU) learning algorithm, while the `PUInteract` class is used for interacting with data from multiple sources.\n\n### Class: PUInterpret\n\nThe `PUInterpret` class has several methods and attributes:\n\n1. **Initialization (`__init__`)**:\n   - Takes a DataFrame `data`, a dictionary `pu_results`, and optional parameters like `n_clusters`, `seed`, etc.\n   - Processes the data to separate positive and unlabeled samples.\n   - Fits clustering algorithms (KMeans, GaussianMixture, BayesianGaussianMixture) if specified.\n\n2. **Data Processing (`_process_pu_data`)**:\n   - Splits the DataFrame into positive and unlabeled samples based on the `PU_label`.\n\n3. **Clustering Methods**:\n   - `do_kmeans`: Fits KMeans clustering.\n   - `do_gmixture`: Fits GaussianMixture clustering.\n   - `do_bgm`: Fits BayesianGaussianMixture clustering.\n\n4. **Feature Importance**:\n   - Calculates feature importance based on the ranking of features across different clusters.\n\n5. **Saving Plots (`save_plot`)**:\n   - Saves plots generated by seaborn to files in various formats (SVG, PDF, PNG).\n\n### Class: PUInteract\n\nThe `PUInteract` class is used for merging data from two sources and performing clustering or mixture modeling on the merged data:\n\n1. **Initialization (`__init__`)**:\n   - Takes DataFrames for parent and child datasets, along with a list of features to merge.\n   - Merges the datasets based on specified columns and keeps only the selected features.\n\n2. **Clustering Methods**:\n   - `do_kmeans`: Fits KMeans clustering on the merged data.\n   - `do_gmixture`: Fits GaussianMixture clustering on the merged data.\n   - `do_bgm`: Fits BayesianGaussianMixture clustering on the merged data.\n\n### Usage Example\n\nHere's a basic example of how you might use these classes:\n\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans, GaussianMixture, BayesianGaussianMixture\n\n# Load data\ndf_parent = pd.read_json('parent_data.json')\ndf_child = pd.read_json('child_data.json')\n\n# Initialize PUInterpret with parent and child data\npu_interpret = PUInterpret(df_parent, pu_results, df_child, pu_results)\n\n# Perform clustering\nkmeans_output = pu_interpret.do_kmeans(n_clusters=2)\ngmm_output = pu_interpret.do_gmixture(n_components=2)\n\n# Initialize PUInteract with parent and child data\npu_interact = PUInteract(df_parent, pu_results, df_child, pu_results, merge_on=['id'], feats=['feature1', 'feature2'])\n\n# Perform clustering on merged data\nkmeans_output_merged = pu_interact.do_kmeans(n_clusters=2)\ngmm_output_merged = pu_interact.do_gmixture(n_components=2)\n```\n\nThis example demonstrates how to load data, initialize the classes with the necessary parameters, and perform clustering on both individual datasets and their merged version."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nfrom absl.testing import absltest\n\nfrom tests import test_constants\nfrom validate import generate_universe\n\n_DEFAULT_ONTOLOGY_LOCATION = test_constants.ONTOLOGY_ROOT\n_BAD_MODIFIED_ONTOLOGY = path.join(test_constants.TEST_RESOURCES, 'BAD',\n                                   'BAD_FORMAT')\n_NONEXISTENT_LOCATION = path.join(test_constants.TEST_ROOT, 'nonexistent')\n_EMPTY_FOLDER = path.join(test_constants.TEST_RESOURCES, 'BAD', 'BAD_EMPTY')\n\n\nclass GenerateUniverseTest(absltest.TestCase):\n\n  def testCanGenerateUniverse(self):\n    universe = generate_universe.BuildUniverse(_DEFAULT_ONTOLOGY_LOCATION)\n    self.assertTrue(universe)\n\n  def testCatchInvalidModifiedOntology(self):\n    with self.assertRaises(Exception) as context:\n      generate_universe.BuildUniverse(_BAD_MODIFIED_ONTOLOGY)\n    self.assertIn('no longer valid', str(context.exception))\n\n  def testModifiedTypesCatchesNonexistent(self):\n    self.assertRaises(Exception,\n                      generate_universe.BuildUniverse(_NONEXISTENT_LOCATION))\n\n  def testModifiedTypesCatchesEmpty(self):\n    self.assertRaises(Exception, generate_universe.BuildUniverse(_EMPTY_FOLDER))\n\n\nif __name__ == '__main__':\n  absltest.main()\n",
        "summary": "The provided Python code defines a test suite for the `generate_universe` module using the `absltest` framework. It includes tests to verify that the universe can be generated from a default ontology location, that an exception is raised when encountering a modified ontology with invalid format, and that exceptions are thrown when attempting to generate a universe from non-existent or empty locations."
    },
    {
        "code": "import pytest\n\nimport numpy as np\n\nskl = pytest.importorskip(\"sklearn\")\n\nimport smlb\nfrom smlb.learners.scikit_learn.gaussian_process_regression_sklearn import GaussianProcessRegressionSklearn\n\n\ndef test_GaussianProcessRegressionSklearn_1():\n    \n\n    \n    kernel = skl.gaussian_process.kernels.DotProduct(sigma_0=0, sigma_0_bounds=\"fixed\")\n    gpr = GaussianProcessRegressionSklearn(kernel=kernel, optimizer=None, rng=1)\n    train_data = smlb.TabularData(data=np.array([[-1], [1]]), labels=np.array([-1, 1]))\n    valid_data = smlb.TabularData(data=np.array([[-2], [-1], [0], [1], [2]]))\n    preds = gpr.fit(train_data).apply(valid_data)\n    mean, stddev = preds.mean, preds.stddev\n\n    assert np.allclose(mean, [-2, -1, 0, 1, 2])\n    assert stddev[0] > stddev[1] > stddev[2] < stddev[3] < stddev[4]\n\n\ndef test_GaussianProcessRegressionSklearn_2():\n    \n\n    kernel = skl.gaussian_process.kernels.DotProduct(\n        sigma_0=0, sigma_0_bounds=\"fixed\"\n    ) + skl.gaussian_process.kernels.WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e-5))\n    gpr = GaussianProcessRegressionSklearn(kernel=kernel, rng=1)\n    n = 100\n    train_data = smlb.TabularData(\n        data=np.ones(shape=(n, 1)) * 2, labels=np.ones(shape=n) * 3\n    )\n    valid_data = smlb.TabularData(data=train_data.samples())\n    preds = gpr.fit(train_data).apply(valid_data)\n\n    assert preds.has_signal_part and preds.has_noise_part\n    conf, noise = preds.signal_part, preds.noise_part\n\n    assert np.allclose(conf.mean, train_data.labels())\n    assert np.allclose(conf.stddev, np.ones(n) * np.sqrt(1e-5), atol=1e-3)\n\n    assert (preds.mean == conf.mean).all()\n    assert np.allclose(preds.stddev, np.ones(n) * np.sqrt(np.square(conf.stddev) + 1e-5))\n\n    assert np.allclose(noise.mean, np.zeros(shape=n))\n    assert np.allclose(noise.stddev, np.sqrt(1e-5))\n\n\ndef test_GaussianProcessRegressionSklearn_3():\n    \n\n    kernel = skl.gaussian_process.kernels.DotProduct(\n        sigma_0=0, sigma_0_bounds=\"fixed\"\n    ) + skl.gaussian_process.kernels.WhiteKernel(noise_level=1, noise_level_bounds=(1e-5, 1e5))\n    gpr = GaussianProcessRegressionSklearn(kernel=kernel, rng=1)\n    n, nlsd = 100, 0.5\n    data = smlb.TabularData(data=np.ones(shape=(n, 1)) * 2, labels=np.ones(shape=n) * 3)\n    data = smlb.LabelNoise(noise=smlb.NormalNoise(stddev=nlsd, rng=1)).fit(data).apply(data)\n    preds = gpr.fit(data).apply(data)\n\n    assert preds.has_signal_part and preds.has_noise_part\n    conf, noise = preds.signal_part, preds.noise_part\n\n    assert np.allclose(conf.mean, np.ones(n) * 3, atol=1e-1)\n    assert np.allclose(conf.stddev, np.ones(n) * nlsd, atol=1e-1)\n\n    assert (preds.mean == conf.mean).all()\n    assert np.allclose(preds.stddev, np.sqrt(np.square(conf.stddev) + np.square(nlsd)), atol=1e-1)\n\n    assert np.allclose(noise.mean, np.zeros(shape=n))\n    assert np.allclose(noise.stddev, nlsd, atol=1e-1)\n",
        "summary": "The provided Python code defines a series of tests for the `GaussianProcessRegressionSklearn` class from the `smlb.learners.scikit_learn.gaussian_process_regression_sklearn` module using the `pytest` framework and `numpy`. The tests validate the functionality of Gaussian process regression with different kernels, including a simple dot product kernel and a combination of dot product and white noise kernels, ensuring that predictions are accurate and handle label noise appropriately."
    },
    {
        "code": "def picking_number(n, arr):\n    max_combinations = 0\n    for i in range(n):\n        combination = arr.count(arr[i]) + arr.count(arr[i] + 1)\n        if combination > max_combinations: \n            max_combinations = combination\n    return max_combinations\n\nn = int(input().strip())\na = [int(a_temp) for a_temp in input().strip().split(' ')]\nprint (picking_number(n, a))",
        "summary": "The Python code defines a function `picking_number` that takes an integer `n` and a list of integers `arr`. It calculates the maximum number of occurrences of any number and its consecutive number in the list. The main part of the code reads input values for `n` and `a`, then calls `picking_number` with these inputs and prints the result."
    },
    {
        "code": "import os\nimport sys\nfrom vmaf.core.quality_runner import QualityRunner\nfrom vmaf.core.result_store import FileSystemResultStore\nfrom vmaf.routine import run_remove_results_for_dataset\nfrom vmaf.tools.misc import import_python_file\n\n__copyright__ = \"Copyright 2016-2020, Netflix, Inc.\"\n__license__ = \"BSD+Patent\"\n\n\ndef print_usage():\n    quality_runner_types = ['VMAF', 'PSNR', 'SSIM', 'MS_SSIM']\n    print(\"usage: \" + os.path.basename(sys.argv[0]) + \\\n          \" quality_type dataset_filepath\\n\")\n    print(\"quality_type:\\n\\t\" + \"\\n\\t\".join(quality_runner_types) +\"\\n\")\n\n\ndef main():\n    if len(sys.argv) < 3:\n        print_usage()\n        return 2\n\n    try:\n        quality_type = sys.argv[1]\n        dataset_filepath = sys.argv[2]\n    except ValueError:\n        print_usage()\n        return 2\n\n    try:\n        dataset = import_python_file(dataset_filepath)\n    except Exception as e:\n        print(\"Error: \" + str(e))\n        return 1\n\n    try:\n        runner_class = QualityRunner.find_subclass(quality_type)\n    except:\n        print_usage()\n        return 2\n\n    result_store = FileSystemResultStore()\n\n    run_remove_results_for_dataset(result_store, dataset, runner_class)\n\n    return 0\n\n\nif __name__ == '__main__':\n    ret = main()\n    exit(ret)\n",
        "summary": "This Python script is designed to evaluate video quality using various metrics such as VMAF, PSNR, SSIM, and MS_SSIM. It reads a dataset file, selects the appropriate quality runner based on user input, and stores the results in a file system result store. The script includes error handling for incorrect usage and issues with importing the dataset file."
    },
    {
        "code": "import logging\nimport os\nfrom tempfile import mkstemp\n\nimport pandas as pd\nfrom box import Box\n\n\n\nlogger = logging.getLogger(__name__)  \n\n\ndef pd_export(\n    dataframe: pd.DataFrame,\n    export_type: str,\n    df_name: str,\n    temp_name: bool = False,\n    df_name_prefix: str = \"\",\n    df_name_suffix: str = \"\",\n    dir_name: str = \".\",\n    config_box: Box = None,\n    index=True,\n    header=True,\n) -> str:\n    \n\n    if temp_name and dir_name != \"\":\n        filepath = mkstemp(suffix=df_name_suffix, prefix=df_name_prefix, dir=dir_name)[\n            1\n        ]\n\n    elif config_box and dir_name == \"\":\n        filepath = os.path.join(\n            config_box.extracttempdir,\n            f\"{df_name_prefix}{df_name}{df_name_suffix}.{export_type}\",\n        )\n    else:\n        filename = f\"{df_name_prefix}{df_name}{df_name_suffix}.{export_type}\"\n        filepath = os.path.join(dir_name, filename)\n\n    logger.info(\"Creating %s file %s from dataframe.\", export_type, filepath)\n\n    if export_type == \"parquet\":\n        dataframe.to_parquet(path=filepath, index=index)\n    elif export_type == \"csv\":\n        dataframe.to_csv(filepath, index=index, header=header)\n\n    return filepath\n\n\ndef pd_colupdate(dataframe: pd.DataFrame, coldict: dict) -> pd.DataFrame:\n    \n\n    logger.info(\"Renaming and filtering dataframe columns using coldict key:values.\")\n\n    \n    dataframe = dataframe.rename(columns=coldict)\n\n    \n    dataframe = dataframe[[val for key, val in coldict.items()]].copy()\n\n    return dataframe\n",
        "summary": "The provided Python code includes two functions. The `pd_export` function exports a pandas DataFrame to a file of specified type (either 'parquet' or 'csv') with options for temporary file creation and custom naming conventions. The `pd_colupdate` function updates the columns of a DataFrame by renaming them according to a dictionary mapping and filtering out any columns not included in the mapping. Both functions utilize logging for informational purposes during their execution."
    },
    {
        "code": "from .MidiInfo import *",
        "summary": "The provided Python code imports all symbols from the module named `MidiInfo`."
    },
    {
        "code": "import argparse\nimport json\nimport urllib.request\n\nif __name__ == '__main__':\n\n  parser = argparse.ArgumentParser ()\n  parser.add_argument ('-v', '--verbose', help = 'Enable Verbose Mode', action = 'store_true')\n  parser.add_argument ('-ip', help = 'IP Address to Test')\n  args = parser.parse_args ()\n\n  if args.ip:\n    location_url = 'http://ipinfo.io/{:}/json'.format(args.ip)\n  else:\n    location_url = 'http://ipinfo.io/json'\n\n  if args.verbose:\n    print ('Retrieving location information ...')\n\n  location_facts = json.loads ((urllib.request.urlopen (location_url).read ())\n                                                       .decode (\"utf-8\"))\n\n  print ('This IP is in {:}, {:}, {:}.'.format (location_facts ['city'],\n                                                location_facts ['region'],\n                                                location_facts ['country']))\n\n  if args.verbose:\n    print ('All done.')\n",
        "summary": "The Python script uses the `argparse` module to handle command-line arguments for enabling verbose mode and specifying an IP address. It then fetches location information from the ipinfo.io API using `urllib.request`, parses the JSON response, and prints the city, region, and country associated with the provided IP address or the default IP if none is specified. If verbose mode is enabled, it also outputs status messages indicating the start and completion of the operation."
    },
    {
        "code": "import os\nimport json\nfrom pathlib import Path\n\nimport jimi\n\n\ndbCollectionName = \"model\"\n\nclass _model(jimi.db._document):\n    name = str()\n    className = str()\n    classType = str()\n    location = str()\n    hidden = bool()\n    manifest = dict()\n\n    _dbCollection = jimi.db.db[dbCollectionName]\n\n    def new(self,name,className,classType,location,hidden):\n        self.name = name\n        self.className = className\n        self.classType = classType\n        self.location = location\n        self.hidden = hidden\n        self.acl = { \"ids\":[ { \"accessID\":\"0\",\"delete\": True,\"read\": True,\"write\": True } ] }\n        return super(_model, self).new()\n\n    def classObject(self):\n        \n        try:\n            mod = __import__(\"{0}\".format(self.location), fromlist=[\"{0}\".format(self.className)])\n        except ModuleNotFoundError:\n            jimi.logging.debug(\"Error unable to find class='{0}', className='{1}', classType='{2}', location='{3}'\".format(self.classID,self.className,self.classType,self.location),-1)\n            if self.classType == \"_action\":\n                return jimi.action._action\n            elif self.classType == \"_trigger\":\n                return jimi.trigger._trigger\n            else:\n                return jimi.db._document\n            \n        class_ = getattr(mod, \"{0}\".format(self.className))\n        \n        class_.manifest__ = self.manifest\n        return class_\n\ndef registerModel(name,className,classType,location,hidden=False):\n    \n    results = _model(False).query(query={ \"name\" : name })[\"results\"]\n    if len(results) == 0:\n        return _model().new(name,className,classType,location,hidden)\n    else:\n        if jimi.logging.debugEnabled:\n            jimi.logging.debug(\"Register model failed as it already exists modelName='{0}', className='{1}', classType='{2}', location='{3}'\".format(name,className,classType,location),4)\n\ndef deregisterModel(name,className,classType,location):\n    loadModels = _model(False).query(query={ \"name\" : name})[\"results\"]\n    if loadModels:\n        loadModels = loadModels[0]\n        \n        \n        \n        \n        results = _model().api_delete(query={ \"name\" : name, \"classType\" : classType })\n        if results[\"result\"]:\n            return True\n    if jimi.logging.debugEnabled:\n        jimi.logging.debug(\"deregister model failed modelName='{0}', className='{1}', classType='{2}', location='{3}'\".format(name,className,classType,location),4)\n\ndef getClassID(name):\n    loadModels = _model(False).query(query={ \"name\" : name})[\"results\"]\n    if loadModels:\n        loadModels = loadModels[0]\n        return loadModels[\"_id\"]\n    return None\n\ndef loadModel(modelName):\n    results = _model(False).query(query={ \"name\" : modelName })[\"results\"]\n    if len(results) == 1:\n        results = results[0]\n        _class = _model().get(results[\"_id\"])\n        return _class\n    return None\n\ndef getClassObject(classID,sessionData):\n    return _model().getAsClass(id=classID)\n\n\nif jimi.api.webServer:\n    if not jimi.api.webServer.got_first_request:\n        if jimi.api.webServer.name == \"jimi_web\":\n            @jimi.api.webServer.route(jimi.api.base+\"models/\", methods=[\"GET\"])\n            def getModels():\n                result = []\n                jimi.api.g.sessionData\n                models = _model(False).query(jimi.api.g.sessionData,query={ \"_id\" : { \"$exists\": True } })[\"results\"]\n                for model in models:\n                    result.append(model[\"name\"])\n                return { \"models\" : result }, 200\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/\", methods=[\"GET\"])\n            def getModel(modelName):\n                class_ = loadModel(modelName).classObject()\n                if class_:\n                    results = _model(False).query(jimi.api.g.sessionData,query={ \"className\" : class_.__name__ })[\"results\"]\n                    if len(results) == 1:\n                        results = results[0]\n                        return class_().query(jimi.api.g.sessionData,query={ \"classID\" : results[\"_id\"] },fields=[\"_id\",\"name\",\"classType\"]), 200\n                return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/extra/\", methods=[\"GET\"])\n            def getModelExtra(modelName):\n                class_ = loadModel(modelName).classObject()\n                if class_:\n                    results = _model(False).query(jimi.api.g.sessionData,query={ \"className\" : class_.__name__ })[\"results\"]\n                    if len(results) == 1:\n                        results = results[0]\n                        results = class_(False).query(jimi.api.g.sessionData,query={ \"classID\" : results[\"_id\"] },fields=[\"_id\",\"name\",\"classType\",\"lastUpdateTime\"])[\"results\"]\n                        ids = [ x[\"_id\"] for x in results ]\n                        \n                        ConductsCache = jimi.conduct._conduct().query(query={ \"$or\" : [ { \"flow.triggerID\" : { \"$in\" : ids } }, { \"flow.actionID\" : { \"$in\" : ids } } ] },fields=[\"_id\",\"name\",\"flow\"])[\"results\"]\n                        for result in results:\n                            usedIn = []\n                            for ConductCache in ConductsCache:\n                                for flow in ConductCache[\"flow\"]:\n                                    if \"triggerID\" in flow:\n                                        if flow[\"triggerID\"] == result[\"_id\"]:\n                                            usedIn.append({ \"conductID\" :  ConductCache[\"_id\"], \"conductName\" : ConductCache[\"name\"] })\n                                    if \"actionID\" in flow:\n                                        if flow[\"actionID\"] == result[\"_id\"]:\n                                            usedIn.append({ \"conductID\" :  ConductCache[\"_id\"], \"conductName\" : ConductCache[\"name\"] })\n                            result[\"whereUsed\"] = usedIn\n                        return { \"results\" : results }, 200\n                return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/all/\", methods=[\"GET\"])\n            def getModelAndChildren(modelName):\n                class_ = loadModel(modelName).classObject()\n                classIDs = []\n                if class_:\n                    results = _model(False).query(jimi.api.g.sessionData,query={ \"className\" : class_.__name__ })[\"results\"]\n                    if len(results) == 1:\n                        results = results[0]\n                        classIDs.append(results[\"_id\"])\n                        results = _model(False).query(jimi.api.g.sessionData,query={ \"classType\" : results[\"className\"] })[\"results\"]\n                        for result in results:\n                            classIDs.append(result[\"_id\"])\n\n                        result = []\n                        for classID in classIDs:\n                            for foundObject in class_(False).query(jimi.api.g.sessionData,query={ \"classID\" : classID })[\"results\"]:\n                                result.append(foundObject)\n\n                        return { \"results\" : result}, 200\n                else:\n                    return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/schema/\", methods=[\"GET\"])\n            def getModelSchema(modelName):\n                class_ = loadModel(modelName)\n                if class_:\n                    access = jimi.db.ACLAccess(jimi.api.g.sessionData,class_.acl,\"read\")\n                    if access:\n                        return class_.classObject()(False).api_getSchema(), 200\n                    else:\n                        return {}, 403\n                else:\n                    return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/<objectID>/\", methods=[\"GET\"])\n            def getModelObject(modelName,objectID):\n                class_ = loadModel(modelName).classObject()\n                if class_:\n                    classObject = class_(False).getAsClass(jimi.api.g.sessionData,id=objectID)\n                    if classObject:\n                        classObject = classObject[0]\n                        members = jimi.helpers.classToJson(classObject)\n                        return members, 200\n                    else:\n                        return {}, 404\n                else:\n                    return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/<objectID>/\", methods=[\"DELETE\"])\n            def deleteModelObject(modelName,objectID):\n                class_ = loadModel(modelName)\n                if class_:\n                    _class = class_.classObject()(False).getAsClass(jimi.api.g.sessionData,id=objectID)\n                    if len(_class) == 1:\n                        _class = _class[0]\n                        access = jimi.db.ACLAccess(jimi.api.g.sessionData,_class.acl,\"delete\")\n                        if access:\n                            if \"_id\" in jimi.api.g.sessionData:\n                                jimi.audit._audit().add(\"model\",\"delete\",{ \"_id\" : jimi.api.g.sessionData[\"_id\"], \"user\" : jimi.api.g.sessionData[\"user\"], \"modelName\" : modelName, \"objectID\" : objectID })\n                            else:\n                                jimi.audit._audit().add(\"model\",\"delete\",{ \"user\" : \"system\", \"objectID\" : objectID })\n                            result = class_.classObject()(False).api_delete(id=objectID)\n                            if result[\"result\"]:\n                                return result, 200\n                        else:\n                            return {}, 403\n                return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/\", methods=[\"PUT\"])\n            def newModelObject(modelName):\n                class_ = loadModel(modelName)\n                if class_:\n                    access = jimi.db.ACLAccess(jimi.api.g.sessionData,class_.acl,\"read\")\n                    if access:\n                        class_ = class_.classObject()(False)\n                        if jimi.api.g.sessionData:\n                            class_.acl = { \"ids\" : [ { \"accessID\" : jimi.api.g.sessionData[\"primaryGroup\"], \"read\" : True, \"write\" : True, \"delete\" : True } ] }\n                        newObjectID = super(type(class_), class_).new().inserted_id\n                        if \"_id\" in jimi.api.g.sessionData:\n                            jimi.audit._audit().add(\"model\",\"create\",{ \"_id\" : jimi.api.g.sessionData[\"_id\"], \"user\" : jimi.api.g.sessionData[\"user\"], \"modelName\" : modelName, \"objectID\" : str(newObjectID) })\n                        else:\n                            jimi.audit._audit().add(\"model\",\"create\",{ \"user\" : \"system\", \"objectID\" : str(newObjectID) })\n                        return { \"_id\" : str(newObjectID) }, 200\n                return {}, 404\n\n            @jimi.api.webServer.route(jimi.api.base+\"models/<modelName>/<objectID>/\", methods=[\"POST\"])\n            def updateModelObject(modelName,objectID):\n                class_ = loadModel(modelName)\n                if class_:\n                    data = json.loads(jimi.api.request.data)\n                    updateItemsList = []\n                    changeLog = {}\n                    _class = class_.classObject()(False).getAsClass(jimi.api.g.sessionData,id=objectID)\n                    if len(_class) == 1:\n                        _class = _class[0]\n                        \n                        access = jimi.db.ACLAccess(jimi.api.g.sessionData,_class.acl,\"write\")\n                        adminBypass = False\n                        if \"admin\" in jimi.api.g.sessionData:\n                            if jimi.api.g.sessionData[\"admin\"]:\n                                adminBypass = True\n                        if access:\n                            for dataKey, dataValue in data.items():\n                                fieldAccessPermitted = True\n                                \n                                \n                                if _class.acl != {} and not adminBypass:\n                                    fieldAccessPermitted = jimi.db.fieldACLAccess(jimi.api.g.sessionData,_class.acl,dataKey,\"write\")\n\n                                if fieldAccessPermitted:\n                                    \n                                    if dataKey != \"_id\":\n                                        if hasattr(_class, dataKey):\n                                            changeLog[dataKey] = {}\n                                            changeLog[dataKey][\"currentValue\"] = getattr(_class, dataKey)\n                                            if type(getattr(_class, dataKey)) is str:\n                                                if _class.setAttribute(dataKey, str(dataValue),sessionData=jimi.api.g.sessionData):\n                                                    updateItemsList.append(dataKey)\n                                                    changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                            elif type(getattr(_class, dataKey)) is int:\n                                                try:\n                                                    if _class.setAttribute(dataKey, int(dataValue),sessionData=jimi.api.g.sessionData):\n                                                        updateItemsList.append(dataKey)\n                                                        changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                                except ValueError:\n                                                    if _class.setAttribute(dataKey, 0,sessionData=jimi.api.g.sessionData):\n                                                        updateItemsList.append(dataKey)\n                                                        changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                            elif type(getattr(_class, dataKey)) is float:\n                                                try:\n                                                    if _class.setAttribute(dataKey, float(dataValue),sessionData=jimi.api.g.sessionData):\n                                                        updateItemsList.append(dataKey)\n                                                        changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                                except ValueError:\n                                                    if _class.setAttribute(dataKey, 0,sessionData=jimi.api.g.sessionData):\n                                                        updateItemsList.append(dataKey)\n                                                        changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                            elif type(getattr(_class, dataKey)) is bool:\n                                                \n                                                if type(dataValue) is str:\n                                                    if dataValue.lower() == \"true\":\n                                                        dataValue = True\n                                                    else:\n                                                        dataValue = False\n                                                if _class.setAttribute(dataKey, dataValue,sessionData=jimi.api.g.sessionData):\n                                                    updateItemsList.append(dataKey)\n                                                    changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                                            elif type(getattr(_class, dataKey)) is dict or type(getattr(_class, dataKey)) is list:\n                                                if dataValue:\n                                                    if _class.setAttribute(dataKey, json.loads(dataValue),sessionData=jimi.api.g.sessionData):\n                                                        updateItemsList.append(dataKey)\n                                                        changeLog[dataKey][\"newValue\"] = getattr(_class, dataKey)\n                            \n                            if updateItemsList:\n                                \n                                if \"_id\" in jimi.api.g.sessionData:\n                                    jimi.audit._audit().add(\"model\",\"update\",{ \"_id\" : jimi.api.g.sessionData[\"_id\"], \"user\" : jimi.api.g.sessionData[\"user\"], \"objects\" : changeLog, \"modelName\" : modelName, \"objectID\" : objectID })\n                                else:\n                                    jimi.audit._audit().add(\"model\",\"update\",{ \"user\" : \"system\", \"objects\" : changeLog, \"modelName\" : modelName, \"objectID\" : objectID })\n                                _class.update(updateItemsList,sessionData=jimi.api.g.sessionData,revisioning=True)\n                            return {}, 200\n                        else:\n                            return {}, 403\n                return {}, 404\n",
        "summary": "This code appears to be a Python script that defines several routes for interacting with a database using the Flask web framework. The script includes functions for creating, reading, updating, and deleting (CRUD) operations on various models or collections in the database.\nThe script uses the `json` module to parse incoming JSON data from HTTP requests, and it uses the `jimi.api.request.data` attribute to access this data. It also uses the `jimi.api.g.sessionData` attribute to store session-specific data for each user.\nThe script includes several helper functions for working with database objects, such as `setAttribute`, which is used to update a specific field on an object, and `update`, which is used to save changes made to an object back to the database.\nOverall, this code provides a basic framework for building a web application that interacts with a database using Flask."
    },
    {
        "code": "import json\n\nfrom unittest import TestCase\nfrom time import sleep\nfrom cs3api4lab.tests.share_test_base import ShareTestBase\nfrom traitlets.config import LoggingConfigurable\nimport urllib.parse\n\nclass TestLocks(ShareTestBase, TestCase):\n    einstein_id = '4c510ada-c86b-4815-8820-42cdf82c3d51'\n    einstein_idp = 'cernbox.cern.ch'\n    marie_id = 'f7fbf8c8-139b-4376-b307-cf0a8c2d0d9c'\n    marie_idp = 'cesnet.cz'\n    richard_id = '932b4540-8d16-481e-8ef4-588e4b6b151c'\n    richard_idp = 'example.org'\n    receiver_role = 'viewer'\n    receiver_grantee_type = 'user'\n    file_path = '/home/test_locks.txt'\n    shared_file_path = '/reva/einstein/test_locks.txt'\n    storage_id = '123e4567-e89b-12d3-a456-426655440000'\n    share_id = None\n    conflict_name = None\n    \n    def test_lock_created_when_file_written(self):\n        self.file_name = self.file_path + self.get_random_suffix()\n\n        try:\n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n            self.file_api.write_file(self.file_name, 'content')\n\n            file_ref = self.storage_logic.get_unified_file_ref(self.file_name, '/')\n            file_info = self.storage_logic._stat_internal(file_ref).info\n\n            self.assertTrue(file_info.arbitrary_metadata.metadata)\n            self.assertIn(\"lock_einstein_cernbox.cern.ch_4c510ada-c86b-4815-8820-42cdf82c3d51\", file_info.arbitrary_metadata.metadata)\n            \n            lock = json.loads(urllib.parse.unquote(file_info.arbitrary_metadata.metadata[\"lock_einstein_cernbox.cern.ch_4c510ada-c86b-4815-8820-42cdf82c3d51\"]))\n            self.assertEquals(lock['username'], 'einstein')\n            self.assertEquals(lock['idp'], 'cernbox.cern.ch')\n            self.assertEquals(lock['opaque_id'], '4c510ada-c86b-4815-8820-42cdf82c3d51')\n\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n\n    def test_lock_created_when_file_read(self):\n        self.file_name = self.file_path + self.get_random_suffix()\n\n        try:\n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n\n            for chunk in self.file_api.read_file(self.file_name):\n                continue\n\n            file_ref = self.storage_logic.get_unified_file_ref(self.file_name, '/')\n            file_info = self.storage_logic._stat_internal(file_ref).info\n\n            self.assertTrue(file_info.arbitrary_metadata.metadata)\n            self.assertIn(\"lock_einstein_cernbox.cern.ch_4c510ada-c86b-4815-8820-42cdf82c3d51\", file_info.arbitrary_metadata.metadata)\n            \n            lock = json.loads(urllib.parse.unquote(file_info.arbitrary_metadata.metadata[\"lock_einstein_cernbox.cern.ch_4c510ada-c86b-4815-8820-42cdf82c3d51\"]))\n            self.assertEquals(lock['username'], 'einstein')\n            self.assertEquals(lock['idp'], 'cernbox.cern.ch')\n            self.assertEquals(lock['opaque_id'], '4c510ada-c86b-4815-8820-42cdf82c3d51')\n\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n\n    def test_write_file_locked_conflict_created(self):\n        suffix = self.get_random_suffix()\n        self.file_name = self.file_path + suffix\n        shared_name = self.shared_file_path + suffix\n\n        try:\n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n\n            self.file_api.write_file(self.file_name, 'content')\n            self.conflict_name = self.richard_file_api.write_file(shared_name, \"richard_content\")\n            \n            lock_stat = self.richard_file_api.stat(self.conflict_name)\n            self.assertEqual(lock_stat['filepath'], self.conflict_name)\n\n            content = self.read_file_content(self.richard_file_api, self.conflict_name)\n            self.assertEqual(content, 'richard_content', 'File ' + self.file_name + ' should contain the string: ' + 'richard_content')\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n            if self.conflict_name:\n                self.remove_test_file('richard', self.conflict_name)\n\n    def test_write_dir_file_locked(self):\n        suffix = self.get_random_suffix()\n        self.file_name = '/home/testdir/test_locks.txt' + suffix\n        shared_name = '/reva/einstein/testdir/test_locks.txt' + suffix\n\n        try:\n            try:\n                self.file_api.create_directory('/home/testdir')\n            except:\n                pass \n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n\n            self.file_api.write_file(self.file_name, 'content')\n            self.conflict_name = self.richard_file_api.write_file(shared_name, \"richard_content\")\n            \n            lock_stat = self.richard_file_api.stat(self.conflict_name)\n            self.assertEqual(lock_stat['filepath'], self.conflict_name)\n\n            content = self.read_file_content(self.richard_file_api, self.conflict_name)\n            self.assertEqual(content, 'richard_content', 'File ' + self.file_name + ' should contain the string: ' + 'richard_content')\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n            if self.conflict_name:\n                self.remove_test_file('richard', self.conflict_name)\n\n    def test_write_file_lock_expired(self):\n        suffix = self.get_random_suffix()\n        self.file_name = self.file_path + suffix\n        shared_name = self.shared_file_path + suffix\n\n        try:\n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n            self.file_api.write_file(self.file_name, 'content')\n\n            sleep(12)\n            self.richard_file_api.write_file(shared_name, \"richard_content\")\n\n            content = self.read_file_content(self.richard_file_api, shared_name)\n            self.assertEqual(content, 'richard_content', 'File ' + self.file_name + ' should contain the string: ' + 'richard_content')\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n\n    def test_write_by_lock_owner_file_locked(self):\n        self.file_name = self.file_path + self.get_random_suffix()\n\n        try:\n            created_share = self.create_share('einstein', self.richard_id, self.richard_idp, self.file_name)\n            self.share_id = created_share['opaque_id']\n            self.file_api.write_file(self.file_name, 'content')\n\n            self.file_api.write_file(self.file_name, 'new_content')\n\n            content = self.read_file_content(self.file_api, self.file_name)\n            self.assertEqual(content, 'new_content', 'File ' + self.file_name + ' should contain the string: ' + 'new_content')\n\n        finally:\n            if self.share_id:\n                self.remove_test_share('einstein', self.share_id)\n            self.remove_test_file('einstein', self.file_name)\n",
        "summary": "The provided Python code defines a test class `TestLocks` that extends `ShareTestBase` and `TestCase`. It includes several methods to test file locking mechanisms, such as creating locks when files are written or read, handling conflicts during writes, and ensuring locks expire after a certain period. Each method sets up a test environment, performs operations on shared files, and cleans up afterward."
    },
    {
        "code": "from pathlib import Path\nfrom typing import Any, Dict\n\nimport torchvision.transforms as pth_transforms\nfrom classy_vision.dataset.transforms import build_transform, register_transform\nfrom classy_vision.dataset.transforms.classy_transform import ClassyTransform\nfrom classy_vision.generic.registry_utils import import_all_modules\n\n\n\n\n\n_TRANSFORMS_WITH_LABELS = [\"ImgRotatePil\", \"ShuffleImgPatches\"]\n_TRANSFORMS_WITH_COPIES = [\n    \"ImgReplicatePil\",\n    \"ImgPilToPatchesAndImage\",\n    \"ImgPilToMultiCrop\",\n]\n_TRANSFORMS_WITH_GROUPING = [\"ImgPilMultiCropRandomApply\"]\n\n\n\n@register_transform(\"SSLTransformsWrapper\")\nclass SSLTransformsWrapper(ClassyTransform):\n    \n\n    def __init__(self, indices, **args):\n        \n        self.indices = set(indices)\n        self.name = args[\"name\"]\n        self.transform = build_transform(args)\n\n    def _is_transform_with_labels(self):\n        \n        if self.name in _TRANSFORMS_WITH_LABELS:\n            return True\n        return False\n\n    def _is_transform_with_copies(self):\n        \n        if self.name in _TRANSFORMS_WITH_COPIES:\n            return True\n        return False\n\n    def _is_grouping_transform(self):\n        \n        if self.name in _TRANSFORMS_WITH_GROUPING:\n            return True\n        return False\n\n    def __call__(self, sample):\n        \n        \n        indices = self.indices if self.indices else set(range(len(sample[\"data\"])))\n\n        if self._is_grouping_transform():\n            \n            \n            \n            output = self.transform(sample[\"data\"])\n            sample[\"data\"] = output\n        else:\n            for idx in indices:\n                output = self.transform(sample[\"data\"][idx])\n                if self._is_transform_with_labels():\n                    sample[\"data\"][idx] = output[0]\n                    sample[\"label\"][-1] = output[1]\n                else:\n                    sample[\"data\"][idx] = output\n\n        if self._is_transform_with_copies():\n            \n            \n            sample[\"data\"] = [val for sublist in sample[\"data\"] for val in sublist]\n            \n            num_times = len(sample[\"data\"])\n            sample[\"label\"] = sample[\"label\"] * num_times\n            sample[\"data_valid\"] = sample[\"data_valid\"] * num_times\n            sample[\"data_idx\"] = sample[\"data_idx\"] * num_times\n        return sample\n\n    @classmethod\n    def from_config(cls, config: Dict[str, Any]) -> \"SSLTransformsWrapper\":\n        indices = config.get(\"indices\", [])\n        return cls(indices, **config)\n\n\ndef get_transform(input_transforms_list):\n    \n    output_transforms = []\n    for transform_config in input_transforms_list:\n        transform = SSLTransformsWrapper.from_config(transform_config)\n        output_transforms.append(transform)\n    return pth_transforms.Compose(output_transforms)\n\n\nFILE_ROOT = Path(__file__).parent\nimport_all_modules(FILE_ROOT, \"vissl.data.ssl_transforms\")\n\n__all__ = [\"SSLTransformsWrapper\", \"get_transform\"]\n",
        "summary": "The provided Python code defines a custom transform class `SSLTransformsWrapper` that extends the functionality of existing transforms by wrapping them and applying additional logic based on the type of transform. It also includes utility functions to register, build, and compose these transforms for use in a specific application context, likely related to semi-supervised learning or other data augmentation techniques."
    },
    {
        "code": "import _plotly_utils.basevalidators\n\n\nclass CategoryarraysrcValidator(_plotly_utils.basevalidators.SrcValidator):\n\n    def __init__(\n        self,\n        plotly_name='categoryarraysrc',\n        parent_name='layout.scene.zaxis',\n        **kwargs\n    ):\n        super(CategoryarraysrcValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type='none',\n            role='info',\n            **kwargs\n        )\n",
        "summary": "The `CategoryarraysrcValidator` class is a subclass of `_plotly_utils.basevalidators.SrcValidator` designed to validate the source of category array data for the z-axis in a 3D scene layout, ensuring it adheres to specific validation rules without allowing edits."
    },
    {
        "code": "import os\n\nfrom setuptools import setup, find_packages\n\nhere = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(here, 'README.txt')) as f:\n    README = f.read()\nwith open(os.path.join(here, 'CHANGES.txt')) as f:\n    CHANGES = f.read()\n\nrequires = [\n    'elasticsearch',\n    'pyramid',\n    'pyramid_chameleon',\n    'pyramid_debugtoolbar',\n    'gunicorn',\n]\n\ntests_requires = [\n    'mocker'\n]\n\nsetup(name='wayta',\n      version='1.0b',\n      description='A tool to suggest the name of an institution or country in the original form and language.',\n      long_description=README + '\\n\\n' + CHANGES,\n      classifiers=[\n        \"Programming Language :: Python\",\n        \"Framework :: Pyramid\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: Internet :: WWW/HTTP :: WSGI :: Application\",\n        ],\n      author='SciELO',\n      author_email='tecnologia@scielo.org',\n      url='http://docs.scielo.org',\n      keywords='web pyramid pylons',\n      packages=find_packages(),\n      include_package_data=True,\n      zip_safe=False,\n      install_requires=requires,\n      setup_requires=[\"nose>=1.0\", \"coverage\"],\n      tests_require=tests_requires,\n      test_suite=\"nose.collector\",\n      entry_points=,\n      )\n",
        "summary": "This Python script sets up a package named 'wayta' using setuptools, including dependencies for Elasticsearch, Pyramid web framework, and other utilities like Chameleon templating and Gunicorn server. It reads version information and descriptions from external files and specifies test requirements for the project."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport errno\nimport functools\nimport glob as _glob\nimport os\nimport shutil\nimport threading\n\nimport six\n\n\nclass _GFileBase(six.Iterator):\n  \n\n  \n  def _synchronized(fn):\n    \n    @functools.wraps(fn)\n    def sync(self, *args, **kwargs):\n      \n      \n      if hasattr(self, '_locker'): self._locker.lock()\n      try:\n        return fn(self, *args, **kwargs)\n      finally:\n        if hasattr(self, '_locker'): self._locker.unlock()\n    return sync\n  \n\n  def __init__(self, name, mode, locker):\n    \n    self._name = name\n    self._mode = mode\n    self._locker = locker\n    self._fp = open(name, mode)\n\n  def __enter__(self):\n    \n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    \n    self.close()\n\n  @_synchronized\n  def __del__(self):\n    \n    \n    \n    if hasattr(self, '_fp'): self._fp.close()\n\n  @_synchronized\n  def flush(self):\n    \n    return self._fp.flush()\n\n  @property\n  @_synchronized\n  def closed(self):\n    \n    return self._fp.closed\n\n  @_synchronized\n  def write(self, data):\n    \n    self._fp.write(data)\n\n  @_synchronized\n  def writelines(self, seq):\n    \n    self._fp.writelines(seq)\n\n  @_synchronized\n  def tell(self):\n    \n    return self._fp.tell()\n\n  @_synchronized\n  def seek(self, offset, whence=0):\n    \n    self._fp.seek(offset, whence)\n\n  @_synchronized\n  def truncate(self, new_size=None):\n    \n    self._fp.truncate(new_size)\n\n  @_synchronized\n  def readline(self, max_length=-1):\n    \n    return self._fp.readline(max_length)\n\n  @_synchronized\n  def readlines(self, sizehint=None):\n    \n    if sizehint is not None:\n      return self._fp.readlines(sizehint)\n    else:\n      return self._fp.readlines()\n\n  def __iter__(self):\n    \n    return self\n\n  \n  def __next__(self):\n    \n    return next(self._fp)\n\n  @_synchronized\n  def Size(self):   \n    \n    cur = self.tell()\n    try:\n      self.seek(0, 2)\n      size = self.tell()\n    finally:\n      self.seek(cur)\n    return size\n\n  @_synchronized\n  def read(self, n=-1):\n    \n    return self._fp.read(n)\n\n  @_synchronized\n  def close(self):\n    \n    self._fp.close()\n\n  \n  \n  _synchronized = staticmethod(_synchronized)\n\n\nclass GFile(_GFileBase):\n  \n\n  def __init__(self, name, mode='r'):\n    super(GFile, self).__init__(name, mode, _Pythonlocker())\n\n\nclass FastGFile(_GFileBase):\n  \n\n  def __init__(self, name, mode='r'):\n    super(FastGFile, self).__init__(name, mode, _Nulllocker())\n\n\n\n\nclass _Pythonlocker(object):\n  \n\n  def __init__(self):\n    self._lock = threading.RLock()\n\n  def lock(self):\n    self._lock.acquire()\n\n  def unlock(self):\n    self._lock.release()\n\n\nclass _Nulllocker(object):\n  \n\n  def lock(self):\n    pass\n\n  def unlock(self):\n    pass\n\n\ndef Exists(path):   \n  \n  return os.path.exists(path)\n\n\ndef IsDirectory(path):   \n  \n  return os.path.isdir(path)\n\n\ndef Glob(glob):   \n  \n  return _glob.glob(glob)\n\n\ndef MkDir(path, mode=0o755):  \n  \n  os.mkdir(path, mode)\n\n\ndef MakeDirs(path, mode=0o755):  \n  \n  \n  \n  if path:\n    os.makedirs(path, mode)\n\n\ndef RmDir(directory):   \n  \n  os.rmdir(directory)\n\n\ndef Remove(path):   \n  \n  os.remove(path)\n\n\ndef Rename(oldpath, newpath, overwrite=False):\n  \n  if not overwrite and Exists(newpath) and not IsDirectory(newpath):\n    raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), newpath)\n  os.rename(oldpath, newpath)\n\n\ndef DeleteRecursively(path):   \n  \n  if IsDirectory(path):\n    shutil.rmtree(path)\n  else:\n    Remove(path)\n\n\ndef ListDirectory(directory, return_dotfiles=False):  \n  \n  files = os.listdir(directory)\n  if not return_dotfiles:\n    files = [f for f in files if not f.startswith('.')]\n  return files\n\n\ndef Walk(top, topdown=1, onerror=None):\n  \n  return os.walk(top, topdown=topdown, onerror=onerror)\n\n\ndef Stat(path):   \n  \n  statinfo = os.stat(path)\n  filestat = collections.namedtuple('FileStat', ['mtime'])\n  filestat.mtime = statinfo.st_mtime\n  return filestat\n\n\ndef Copy(oldpath, newpath, overwrite=False):\n  \n  if not overwrite and Exists(newpath):\n    raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), newpath)\n  shutil.copy(oldpath, newpath)\n\n\ndef Open(name, mode='r'):\n  \n  return GFile(name, mode=mode)\n",
        "summary": "The provided Python code defines a set of classes and functions for file operations with synchronization support. It includes `GFile` and `FastGFile` classes that extend `_GFileBase`, providing methods like read, write, and seek with thread safety using locks. The module also offers utility functions such as checking if a path exists, creating directories, removing files, and copying files, all of which are synchronized to ensure safe concurrent access."
    },
    {
        "code": "import uuid\nfrom msrest.pipeline import ClientRawResponse\n\nfrom .. import models\n\n\nclass EdgeNodesOperations(object):\n    \n\n    models = models\n\n    def __init__(self, client, config, serializer, deserializer):\n\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self.api_version = \"2017-04-02\"\n\n        self.config = config\n\n    def list(\n            self, custom_headers=None, raw=False, **operation_config):\n        \n        def internal_paging(next_link=None, raw=False):\n\n            if not next_link:\n                \n                url = self.list.metadata['url']\n\n                \n                query_parameters = {}\n                query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n            else:\n                url = next_link\n                query_parameters = {}\n\n            \n            header_parameters = {}\n            header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n            if self.config.generate_client_request_id:\n                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n            if custom_headers:\n                header_parameters.update(custom_headers)\n            if self.config.accept_language is not None:\n                header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n            \n            request = self._client.get(url, query_parameters)\n            response = self._client.send(\n                request, header_parameters, stream=False, **operation_config)\n\n            if response.status_code not in [200]:\n                raise models.ErrorResponseException(self._deserialize, response)\n\n            return response\n\n        \n        deserialized = models.EdgeNodePaged(internal_paging, self._deserialize.dependencies)\n\n        if raw:\n            header_dict = {}\n            client_raw_response = models.EdgeNodePaged(internal_paging, self._deserialize.dependencies, header_dict)\n            return client_raw_response\n\n        return deserialized\n    list.metadata = {'url': '/providers/Microsoft.Cdn/edgenodes'}\n",
        "summary": "The `EdgeNodesOperations` class in Python defines methods for interacting with edge nodes through an API, including listing them. It handles pagination internally and supports custom headers and raw responses."
    },
    {
        "code": "import geopandas\nimport shapely.geometry\ngdf = geopandas.GeoDataFrame(geometry=[shapely.geometry.Point(x, x) for x in [5,4,3,2]])\ngdf.index.name = 'id'\ngdf.to_file(\"test.geojson\", index=True, driver='GeoJSON')\ngdf.to_file(\"test.geojson1\", driver='GeoJSON')",
        "summary": "The Python code creates a GeoDataFrame with points at coordinates (5,5), (4,4), (3,3), and (2,2). It then saves this data to two GeoJSON files: \"test.geojson\" with an index column named 'id', and \"test.geojson1\" without an index."
    },
    {
        "code": "r\nfrom sage.rings.integer import Integer\n\nfrom sage.groups.group import FiniteGroup\nfrom sage.structure.unique_representation import UniqueRepresentation\nfrom sage.categories.action import Action\nfrom sage.combinat.permutation import Permutation\nfrom sage.groups.semimonomial_transformations.semimonomial_transformation import SemimonomialTransformation\n\n\nclass SemimonomialTransformationGroup(FiniteGroup, UniqueRepresentation):\n    r\n    Element = SemimonomialTransformation\n\n    def __init__(self, R, len):\n        r\n        if not R.is_field():\n            raise NotImplementedError('the ring must be a field')\n        self._R = R\n        self._len = len\n\n        from sage.categories.finite_groups import FiniteGroups\n        super(SemimonomialTransformationGroup, self).__init__(category=FiniteGroups())\n\n    def _element_constructor_(self, arg1, v=None, perm=None, autom=None, check=True):\n        r\n        from sage.categories.homset import End\n        R = self.base_ring()\n        if arg1 == 0:\n            if v is None:\n                v = [R.one()] * self.degree()\n            if perm is None:\n                perm = Permutation(range(1, self.degree() + 1))\n            if autom is None:\n                autom = R.hom(R.gens())\n\n            if check:\n                try:\n                    v = [R(x) for x in v]\n                except TypeError:\n                    raise TypeError('the vector attribute %s ' % v +\n                                    'should be iterable')\n                if len(v) != self.degree():\n                    raise ValueError('the length of the vector is %s,' % len(v) +\n                                     ' should be %s' % self.degree())\n                if not all(x.parent() is R and x.is_unit() for x in v):\n                    raise ValueError('there is at least one element in the ' +\n                                     'list %s not lying in %s ' % (v, R) +\n                                     'or which is not invertible')\n                try:\n                    perm = Permutation(perm)\n                except TypeError:\n                    raise TypeError('the permutation attribute %s ' % perm +\n                                    'could not be converted to a permutation')\n                if len(perm) != self.degree():\n                    txt = 'the permutation length is {}, should be {}'\n                    raise ValueError(txt.format(len(perm), self.degree()))\n\n                try:\n                    if autom.parent() != End(R):\n                        autom = End(R)(autom)\n                except TypeError:\n                    raise TypeError('%s of type %s' % (autom, type(autom)) +\n                                    ' is not coerceable to an automorphism')\n            return self.Element(self, v, perm, autom)\n        else:\n            try:\n                if arg1.parent() is self:\n                    return arg1\n            except AttributeError:\n                pass\n            try:\n                from sage.rings.integer import Integer\n                if Integer(arg1) == 1:\n                    return self()\n            except TypeError:\n                pass\n            raise TypeError('the first argument must be an integer' +\n                            ' or an element of this group')\n\n    def base_ring(self):\n        r\n        return self._R\n\n    def degree(self) -> Integer:\n        r\n        return self._len\n\n    def _an_element_(self):\n        r\n        R = self.base_ring()\n        v = [R.primitive_element()] + [R.one()] * (self.degree() - 1)\n        p = Permutation([self.degree()] + [i for i in range(1, self.degree())])\n\n        if not R.is_prime_field():\n            f = R.hom([R.gen()**R.characteristic()])\n        else:\n            f = R.Hom(R).identity()\n        return self(0, v, p, f)\n\n    def __contains__(self, item) -> bool:\n        r\n        try:\n            self(item, check=True)\n        except TypeError:\n            return False\n        return True\n\n    def gens(self):\n        r\n        from sage.groups.perm_gps.permgroup_named import SymmetricGroup\n        R = self.base_ring()\n        l = [self(v=([R.primitive_element()] + [R.one()] * (self.degree() - 1)))]\n        for g in SymmetricGroup(self.degree()).gens():\n            l.append(self(perm=Permutation(g)))\n        if R.is_field() and not R.is_prime_field():\n            l.append(self(autom=R.hom([R.primitive_element()**R.characteristic()])))\n        return l\n\n    def order(self) -> Integer:\n        r\n        from sage.functions.other import factorial\n        from sage.categories.homset import End\n        n = self.degree()\n        R = self.base_ring()\n        if R.is_field():\n            multgroup_size = len(R) - 1\n            autgroup_size = R.degree()\n        else:\n            multgroup_size = R.unit_group_order()\n            autgroup_size = len([x for x in End(R) if x.is_injective()])\n        return multgroup_size**n * factorial(n) * autgroup_size\n\n    def _get_action_(self, X, op, self_on_left):\n        r\n        if self_on_left:\n            try:\n                A = SemimonomialActionVec(self, X)\n                return A\n            except ValueError:\n                pass\n\n            try:\n                A = SemimonomialActionMat(self, X)\n                return A\n            except ValueError:\n                pass\n\n        return None\n\n    def _repr_(self) -> str:\n        r\n        return ('Semimonomial transformation group over %s' % self.base_ring() +\n                ' of degree %s' % self.degree())\n\n    def _latex_(self) -> str:\n        r\n        from sage.groups.perm_gps.permgroup_named import SymmetricGroup\n        ring_latex = self.base_ring()._latex_()\n        return ('\\\\left(' + ring_latex + '^' + str(self.degree()) + '\\\\wr' +\n                SymmetricGroup(self.degree())._latex_() +\n                ' \\\\right) \\\\rtimes \\\\operatorname{Aut}(' + ring_latex + ')')\n\n\nclass SemimonomialActionVec(Action):\n    r\n    def __init__(self, G, V, check=True):\n        r\n        if check:\n            from sage.modules.free_module import FreeModule_generic\n            if not isinstance(G, SemimonomialTransformationGroup):\n                raise ValueError('%s is not a semimonomial group' % G)\n            if not isinstance(V, FreeModule_generic):\n                raise ValueError('%s is not a free module' % V)\n            if V.ambient_module() != V:\n                raise ValueError('%s is not equal to its ambient module' % V)\n            if V.dimension() != G.degree():\n                raise ValueError('%s has a dimension different to the degree of %s' % (V, G))\n            if V.base_ring() != G.base_ring():\n                raise ValueError('%s and %s have different base rings' % (V, G))\n\n        Action.__init__(self, G, V.dense_module())\n\n    def _act_(self, a, b):\n        r\n        b = b.apply_map(a.get_autom())\n        b = self.codomain()(a.get_perm().action(b))\n        return b.pairwise_product(self.codomain()(a.get_v_inverse()))\n\n\nclass SemimonomialActionMat(Action):\n    r\n    def __init__(self, G, M, check=True):\n        r\n        if check:\n            from sage.matrix.matrix_space import MatrixSpace\n            if not isinstance(G, SemimonomialTransformationGroup):\n                raise ValueError('%s is not a semimonomial group' % G)\n            if not isinstance(M, MatrixSpace):\n                raise ValueError('%s is not a matrix space' % M)\n            if M.ncols() != G.degree():\n                raise ValueError('the number of columns of %s' % M +\n                                 ' and the degree of %s are different' % G)\n            if M.base_ring() != G.base_ring():\n                raise ValueError('%s and %s have different base rings' % (M, G))\n        Action.__init__(self, G, M)\n\n    def _act_(self, a, b):\n        r\n        return self.codomain()([a * x for x in b.rows()])\n",
        "summary": "The provided Python code defines a class `SemimonomialTransformationGroup` that represents a semimonomial transformation group over a field. It includes methods for constructing elements, checking membership, generating the group, and computing its order. The class also defines actions on vector spaces and matrix spaces using semimonomial transformations."
    },
    {
        "code": "import numpy as np\n\nclass StaticFns:\n\n    @staticmethod\n    def termination_fn(obs, act, next_obs):\n\n        done = np.array([False]).repeat(len(obs))\n        done = done[:,None]\n        return done\n",
        "summary": "The provided Python code defines a static method `termination_fn` within the class `StaticFns`. This method takes three parameters: `obs`, `act`, and `next_obs`. It initializes an array of `False` values with the same length as `obs`, reshapes it to have one column, and returns this array."
    },
    {
        "code": "from copy import deepcopy\nimport networkx_mod as nx\nfrom networkx_mod.classes.graph import Graph\nfrom networkx_mod.exception import NetworkXError\nimport networkx_mod.convert as convert\n__author__ = .join(['Aric Hagberg (hagberg@lanl.gov)',\n                            'Pieter Swart (swart@lanl.gov)',\n                            'Dan Schult(dschult@colgate.edu)'])\n\nclass DiGraph(Graph):\n    \n    def __init__(self, data=None, **attr):\n        \n        self.node_dict_factory = ndf = self.node_dict_factory\n        self.adjlist_dict_factory = self.adjlist_dict_factory\n        self.edge_attr_dict_factory = self.edge_attr_dict_factory\n\n        self.graph = {} \n        self.node = ndf() \n        \n        \n        \n        self.adj = ndf()  \n        self.pred = ndf()  \n        self.succ = self.adj  \n\n        \n        if data is not None:\n            convert.to_networkx_mod_graph(data,create_using=self)\n        \n        self.graph.update(attr)\n        self.edge=self.adj\n\n\n    def add_node(self, n, attr_dict=None, **attr):\n        \n        \n        if attr_dict is None:\n            attr_dict=attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\\\n                    \"The attr_dict argument must be a dictionary.\")\n        if n not in self.succ:\n            self.succ[n] = self.adjlist_dict_factory()\n            self.pred[n] = self.adjlist_dict_factory()\n            self.node[n] = attr_dict\n        else: \n            self.node[n].update(attr_dict)\n\n\n    def add_nodes_from(self, nodes, **attr):\n        \n        for n in nodes:\n            \n            \n            \n            try:\n                if n not in self.succ:\n                    self.succ[n] = self.adjlist_dict_factory()\n                    self.pred[n] = self.adjlist_dict_factory()\n                    self.node[n] = attr.copy()\n                else:\n                    self.node[n].update(attr)\n            except TypeError:\n                nn,ndict = n\n                if nn not in self.succ:\n                    self.succ[nn] = self.adjlist_dict_factory()\n                    self.pred[nn] = self.adjlist_dict_factory()\n                    newdict = attr.copy()\n                    newdict.update(ndict)\n                    self.node[nn] = newdict\n                else:\n                    olddict = self.node[nn]\n                    olddict.update(attr)\n                    olddict.update(ndict)\n\n    def remove_node(self, n):\n        \n        try:\n            nbrs=self.succ[n]\n            del self.node[n]\n        except KeyError: \n            raise NetworkXError(\"The node %s is not in the digraph.\"%(n,))\n        for u in nbrs:\n            del self.pred[u][n] \n        del self.succ[n]          \n        for u in self.pred[n]:\n            del self.succ[u][n] \n        del self.pred[n]          \n\n\n    def remove_nodes_from(self, nbunch):\n        \n        for n in nbunch:\n            try:\n                succs=self.succ[n]\n                del self.node[n]\n                for u in succs:\n                    del self.pred[u][n] \n                del self.succ[n]          \n                for u in self.pred[n]:\n                    del self.succ[u][n] \n                del self.pred[n]          \n            except KeyError:\n                pass \n\n\n    def add_edge(self, u, v, attr_dict=None, **attr):\n        \n        \n        if attr_dict is None:\n            attr_dict=attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\\\n                    \"The attr_dict argument must be a dictionary.\")\n        \n        if u not in self.succ:\n            self.succ[u]= self.adjlist_dict_factory()\n            self.pred[u]= self.adjlist_dict_factory()\n            self.node[u] = {}\n        if v not in self.succ:\n            self.succ[v]= self.adjlist_dict_factory()\n            self.pred[v]= self.adjlist_dict_factory()\n            self.node[v] = {}\n        \n        datadict=self.adj[u].get(v,self.edge_attr_dict_factory())\n        datadict.update(attr_dict)\n        self.succ[u][v]=datadict\n        self.pred[v][u]=datadict\n\n    def add_edges_from(self, ebunch, attr_dict=None, **attr):\n        \n        \n        if attr_dict is None:\n            attr_dict=attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\\\n                    \"The attr_dict argument must be a dict.\")\n        \n        for e in ebunch:\n            ne = len(e)\n            if ne==3:\n                u,v,dd = e\n                assert hasattr(dd,\"update\")\n            elif ne==2:\n                u,v = e\n                dd = {}\n            else:\n                raise NetworkXError(\\\n                    \"Edge tuple %s must be a 2-tuple or 3-tuple.\"%(e,))\n            if u not in self.succ:\n                self.succ[u] = self.adjlist_dict_factory()\n                self.pred[u] = self.adjlist_dict_factory()\n                self.node[u] = {}\n            if v not in self.succ:\n                self.succ[v] = self.adjlist_dict_factory()\n                self.pred[v] = self.adjlist_dict_factory()\n                self.node[v] = {}\n            datadict=self.adj[u].get(v,self.edge_attr_dict_factory())\n            datadict.update(attr_dict)\n            datadict.update(dd)\n            self.succ[u][v] = datadict\n            self.pred[v][u] = datadict\n\n\n    def remove_edge(self, u, v):\n        \n        try:\n            del self.succ[u][v]\n            del self.pred[v][u]\n        except KeyError:\n            raise NetworkXError(\"The edge %s-%s not in graph.\"%(u,v))\n\n\n    def remove_edges_from(self, ebunch):\n        \n        for e in ebunch:\n            (u,v)=e[:2]  \n            if u in self.succ and v in self.succ[u]:\n                del self.succ[u][v]\n                del self.pred[v][u]\n\n\n    def has_successor(self, u, v):\n        \n        return (u in self.succ and v in self.succ[u])\n\n    def has_predecessor(self, u, v):\n        \n        return (u in self.pred and v in self.pred[u])\n\n    def successors_iter(self,n):\n        \n        try:\n            return iter(self.succ[n])\n        except KeyError:\n            raise NetworkXError(\"The node %s is not in the digraph.\"%(n,))\n\n    def predecessors_iter(self,n):\n        \n        try:\n            return iter(self.pred[n])\n        except KeyError:\n            raise NetworkXError(\"The node %s is not in the digraph.\"%(n,))\n\n    def successors(self, n):\n        \n        return list(self.successors_iter(n))\n\n    def predecessors(self, n):\n        \n        return list(self.predecessors_iter(n))\n\n\n    \n    neighbors = successors\n    neighbors_iter = successors_iter\n\n    def edges_iter(self, nbunch=None, data=False, default=None):\n        \n        if nbunch is None:\n            nodes_nbrs=self.adj.items()\n        else:\n            nodes_nbrs=((n,self.adj[n]) for n in self.nbunch_iter(nbunch))\n        if data is True:\n            for n,nbrs in nodes_nbrs:\n                for nbr,ddict in nbrs.items():\n                    yield (n,nbr,ddict)\n        elif data is not False:\n            for n,nbrs in nodes_nbrs:\n                for nbr,ddict in nbrs.items():\n                    d=ddict[data] if data in ddict else default\n                    yield (n,nbr,d)\n        else:\n            for n,nbrs in nodes_nbrs:\n                for nbr in nbrs:\n                    yield (n,nbr)\n\n    \n    out_edges_iter=edges_iter\n    out_edges=Graph.edges\n\n    def in_edges_iter(self, nbunch=None, data=False):\n        \n        if nbunch is None:\n            nodes_nbrs=self.pred.items()\n        else:\n            nodes_nbrs=((n,self.pred[n]) for n in self.nbunch_iter(nbunch))\n        if data:\n            for n,nbrs in nodes_nbrs:\n                for nbr,data in nbrs.items():\n                    yield (nbr,n,data)\n        else:\n            for n,nbrs in nodes_nbrs:\n                for nbr in nbrs:\n                    yield (nbr,n)\n\n    def in_edges(self, nbunch=None, data=False):\n        \n        return list(self.in_edges_iter(nbunch, data))\n\n    def degree_iter(self, nbunch=None, weight=None):\n        \n        if nbunch is None:\n            nodes_nbrs=zip(iter(self.succ.items()),iter(self.pred.items()))\n        else:\n            nodes_nbrs=zip(\n                ((n,self.succ[n]) for n in self.nbunch_iter(nbunch)),\n                ((n,self.pred[n]) for n in self.nbunch_iter(nbunch)))\n\n        if weight is None:\n            for (n,succ),(n2,pred) in nodes_nbrs:\n                yield (n,len(succ)+len(pred))\n        else:\n        \n            for (n,succ),(n2,pred) in nodes_nbrs:\n               yield (n,\n                      sum((succ[nbr].get(weight,1) for nbr in succ))+\n                      sum((pred[nbr].get(weight,1) for nbr in pred)))\n\n\n    def in_degree_iter(self, nbunch=None, weight=None):\n        \n        if nbunch is None:\n            nodes_nbrs=self.pred.items()\n        else:\n            nodes_nbrs=((n,self.pred[n]) for n in self.nbunch_iter(nbunch))\n\n        if weight is None:\n            for n,nbrs in nodes_nbrs:\n                yield (n,len(nbrs))\n        else:\n        \n            for n,nbrs in nodes_nbrs:\n                yield (n, sum(data.get(weight,1) for data in nbrs.values()))\n\n\n    def out_degree_iter(self, nbunch=None, weight=None):\n        \n        if nbunch is None:\n            nodes_nbrs=self.succ.items()\n        else:\n            nodes_nbrs=((n,self.succ[n]) for n in self.nbunch_iter(nbunch))\n\n        if weight is None:\n            for n,nbrs in nodes_nbrs:\n                yield (n,len(nbrs))\n        else:\n        \n            for n,nbrs in nodes_nbrs:\n                yield (n, sum(data.get(weight,1) for data in nbrs.values()))\n\n\n    def in_degree(self, nbunch=None, weight=None):\n        \n        if nbunch in self:      \n            return next(self.in_degree_iter(nbunch,weight))[1]\n        else:           \n            return dict(self.in_degree_iter(nbunch,weight))\n\n    def out_degree(self, nbunch=None, weight=None):\n        \n        if nbunch in self:      \n            return next(self.out_degree_iter(nbunch,weight))[1]\n        else:           \n            return dict(self.out_degree_iter(nbunch,weight))\n\n    def clear(self):\n        \n        self.succ.clear()\n        self.pred.clear()\n        self.node.clear()\n        self.graph.clear()\n\n\n    def is_multigraph(self):\n        \n        return False\n\n\n    def is_directed(self):\n        \n        return True\n\n    def to_directed(self):\n        \n        return deepcopy(self)\n\n    def to_undirected(self, reciprocal=False):\n        \n        H=Graph()\n        H.name=self.name\n        H.add_nodes_from(self)\n        if reciprocal is True:\n            H.add_edges_from( (u,v,deepcopy(d))\n                              for u,nbrs in self.adjacency_iter()\n                              for v,d in nbrs.items()\n                              if v in self.pred[u])\n        else:\n            H.add_edges_from( (u,v,deepcopy(d))\n                              for u,nbrs in self.adjacency_iter()\n                              for v,d in nbrs.items() )\n        H.graph=deepcopy(self.graph)\n        H.node=deepcopy(self.node)\n        return H\n\n\n    def reverse(self, copy=True):\n        \n        if copy:\n            H = self.__class__(name=\"Reverse of (%s)\"%self.name)\n            H.add_nodes_from(self)\n            H.add_edges_from( (v,u,deepcopy(d)) for u,v,d\n                              in self.edges(data=True) )\n            H.graph=deepcopy(self.graph)\n            H.node=deepcopy(self.node)\n        else:\n            self.pred,self.succ=self.succ,self.pred\n            self.adj=self.succ\n            H=self\n        return H\n\n\n    def subgraph(self, nbunch):\n        \n        bunch = self.nbunch_iter(nbunch)\n        \n        H = self.__class__()\n        \n        for n in bunch:\n            H.node[n]=self.node[n]\n        \n        H_succ=H.succ\n        H_pred=H.pred\n        self_succ=self.succ\n        \n        for n in H:\n            H_succ[n]=H.adjlist_dict_factory()\n            H_pred[n]=H.adjlist_dict_factory()\n        \n        for u in H_succ:\n            Hnbrs=H_succ[u]\n            for v,datadict in self_succ[u].items():\n                if v in H_succ:\n                    \n                    Hnbrs[v]=datadict\n                    H_pred[v][u]=datadict\n        H.graph=self.graph\n        return H\n",
        "summary": "This code defines a class `DiGraph` which represents a directed graph. The class provides methods for adding and removing nodes and edges, checking connectivity, finding paths, and calculating various graph properties such as degree, in-degree, out-degree, and centrality measures.\n\nHere's a brief overview of some key features:\n\n1. **Adding/Removing Nodes and Edges**:\n   - `add_node()`, `remove_node()`\n   - `add_edge()`, `remove_edge()`\n\n2. **Graph Properties**:\n   - `number_of_nodes()`, `number_of_edges()`\n   - `degree()`, `in_degree()`, `out_degree()`\n\n3. **Path Finding**:\n   - `has_path()`, `shortest_path()`\n\n4. **Subgraph Extraction**:\n   - `subgraph()`\n\n5. **Graph Operations**:\n   - `reverse()`: Returns a new graph with edges reversed.\n   - `to_undirected()`: Converts the directed graph to an undirected one.\n\n6. **Utility Methods**:\n   - `clear()`\n   - `is_directed()`, `is_multigraph()`\n\nThe class uses dictionaries to store adjacency lists for both incoming and outgoing edges, allowing efficient operations on the graph structure. It also supports optional data associated with each edge, which can be accessed using the `edges()` method.\n\nThis implementation provides a solid foundation for working with directed graphs in Python, offering flexibility and performance suitable for various applications."
    },
    {
        "code": "import logging\nimport logging.config\nimport os\n\nfrom celery.utils.log import get_task_logger\nfrom dotenv import load_dotenv\nfrom flask import Flask\nfrom flask_login import LoginManager\n\nfrom config import config, Config\nfrom .AfricasTalkingGateway import gateway\nfrom .database import db, redis\n\ndotenv_path = os.path.join(os.path.join(os.path.dirname(__file__), \"..\"), \".env\")\nload_dotenv(dotenv_path)\n\n__version__ = \"0.2.0\"\n__author__ = \"npiusdan@gmail.com\"\n__description__ = \"Nerds Microfinance application\"\n__email__ = \"npiusdan@gmail.com\"\n__copyright__ = \"MIT LICENCE\"\n\nlogin_manager = LoginManager()\n\ncelery_logger = get_task_logger(__name__)\n\n\ndef create_celery():\n    from celery import Celery\n\n    celery = Celery(\n        __name__,\n        backend=Config.CELERY_RESULT_BACKEND,\n        broker=Config.CELERY_BROKER_URL\n    )\n    return celery\n\n\ncelery = create_celery()\n\n\ndef create_app(config_name):\n    app = Flask(__name__)\n    \n    app.config.from_object(config[config_name])\n    config[config_name].init_app(app)\n\n    \n    login_manager.init_app(app)\n\n    \n    redis.init_app(app)\n    db.init_app(app)\n\n    \n    gateway.init_app(app=app)\n\n    \n    celery.conf.update(app.config)\n\n    class ContextTask(celery.Task):\n        def __call__(self, *args, **kwargs):\n            with app.app_context():\n                return self.run(*args, **kwargs)\n\n    celery.Task = ContextTask\n\n    \n    from app.ussd import ussd as ussd_bp\n\n    app.register_blueprint(ussd_bp)\n\n    \n    from app.util import setup_logging\n    from config import basedir\n\n    if app.debug:\n        logging_level = logging.DEBUG\n    else:\n        logging_level = logging.INFO\n    path = os.path.join(basedir, \"app_logger.yaml\")\n    setup_logging(default_level=logging_level, logger_file_path=path)\n    return app\n",
        "summary": "This Python code sets up a Flask application with Celery for asynchronous tasks, Flask-Login for user authentication, and integrates various configurations and extensions such as databases, Redis, and logging. It also defines routes and initializes the application based on different configuration environments."
    },
    {
        "code": "from botocore.exceptions import CapacityNotAvailableError\nfrom botocore.retries import bucket\nfrom tests import unittest\n\n\nclass FakeClock(bucket.Clock):\n    def __init__(self, timestamp_sequences):\n        self.timestamp_sequences = timestamp_sequences\n        self.sleep_call_amounts = []\n\n    def sleep(self, amount):\n        self.sleep_call_amounts.append(amount)\n\n    def current_time(self):\n        return self.timestamp_sequences.pop(0)\n\n\nclass TestTokenBucket(unittest.TestCase):\n    def setUp(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n\n    def create_token_bucket(self, max_rate=10, min_rate=0.1):\n        return bucket.TokenBucket(max_rate=max_rate, clock=self.clock,\n                                  min_rate=min_rate)\n\n    def test_can_acquire_amount(self):\n        self.timestamp_sequences.extend([\n            \n            \n            1,\n            2,\n            3,\n            4,\n            5,\n        ])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_can_change_max_capacity_lower(self):\n        \n        self.timestamp_sequences.extend([1, 2, 3, 4, 5])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        \n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n        \n        self.timestamp_sequences.append(5)\n        token_bucket.max_rate = 1\n        \n        self.timestamp_sequences.extend([6, 7, 8, 9, 10])\n        for _ in range(5):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_max_capacity_is_at_least_one(self):\n        token_bucket = self.create_token_bucket()\n        self.timestamp_sequences.append(1)\n        token_bucket.max_rate = 0.5\n        self.assertEqual(token_bucket.max_rate, 0.5)\n        self.assertEqual(token_bucket.max_capacity, 1)\n\n    def test_acquire_fails_on_non_block_mode_returns_false(self):\n        self.timestamp_sequences.extend([\n            \n            0,\n            \n            1\n        ])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        with self.assertRaises(CapacityNotAvailableError):\n            token_bucket.acquire(100, block=False)\n\n    def test_can_retrieve_at_max_send_rate(self):\n        self.timestamp_sequences.extend([\n            \n            1 + 0.1 * i for i in range(20)\n        ])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(20):\n            self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_acquiring_blocks_when_capacity_reached(self):\n        \n        token_bucket = self.create_token_bucket(max_rate=10)\n        self.timestamp_sequences.extend([\n            \n            0.1,\n            \n            \n            \n            0.15,\n            \n            0.2,\n            \n            \n            \n            0.300001,\n        ])\n        self.assertTrue(token_bucket.acquire(1, block=False))\n        self.assertEqual(token_bucket.available_capacity, 0)\n        self.assertTrue(token_bucket.acquire(1, block=True))\n        self.assertEqual(token_bucket.available_capacity, 0)\n        self.assertTrue(token_bucket.acquire(1, block=False))\n\n    def test_rate_cant_go_below_min(self):\n        token_bucket = self.create_token_bucket(max_rate=1, min_rate=0.2)\n        self.timestamp_sequences.append(1)\n        token_bucket.max_rate = 0.1\n        self.assertEqual(token_bucket.max_rate, 0.2)\n        self.assertEqual(token_bucket.max_capacity, 1)\n",
        "summary": "The provided Python code defines a `FakeClock` class to simulate time for testing purposes and a `TestTokenBucket` class that uses the `unittest` framework to test the functionality of a token bucket implementation from the `botocore.retries.bucket` module. The tests cover various scenarios such as acquiring tokens, changing the maximum rate, handling capacity limits, and ensuring the rate does not fall below a minimum value."
    },
    {
        "code": "import warnings\nfrom typing import Callable, Dict, Optional, Sequence, Tuple\n\nfrom google.api_core import grpc_helpers  \nfrom google.api_core import gapic_v1  \nfrom google import auth  \nfrom google.auth import credentials  \nfrom google.auth.transport.grpc import SslCredentials  \n\nimport grpc  \n\nfrom google.ads.googleads.v7.resources.types import ad_group_simulation\nfrom google.ads.googleads.v7.services.types import ad_group_simulation_service\nfrom .base import AdGroupSimulationServiceTransport, DEFAULT_CLIENT_INFO\n\n\nclass AdGroupSimulationServiceGrpcTransport(AdGroupSimulationServiceTransport):\n    \n\n    def __init__(\n        self,\n        *,\n        host: str = \"googleads.googleapis.com\",\n        credentials: credentials.Credentials = None,\n        credentials_file: str = None,\n        scopes: Sequence[str] = None,\n        channel: grpc.Channel = None,\n        api_mtls_endpoint: str = None,\n        client_cert_source: Callable[[], Tuple[bytes, bytes]] = None,\n        ssl_channel_credentials: grpc.ChannelCredentials = None,\n        quota_project_id: Optional[str] = None,\n        client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,\n    ) -> None:\n        \n        self._ssl_channel_credentials = ssl_channel_credentials\n\n        if channel:\n            \n            \n            credentials = False\n\n            \n            self._grpc_channel = channel\n            self._ssl_channel_credentials = None\n        elif api_mtls_endpoint:\n            warnings.warn(\n                \"api_mtls_endpoint and client_cert_source are deprecated\",\n                DeprecationWarning,\n            )\n\n            host = (\n                api_mtls_endpoint\n                if \":\" in api_mtls_endpoint\n                else api_mtls_endpoint + \":443\"\n            )\n\n            if credentials is None:\n                credentials, _ = auth.default(\n                    scopes=self.AUTH_SCOPES, quota_project_id=quota_project_id\n                )\n\n            \n            \n            if client_cert_source:\n                cert, key = client_cert_source()\n                ssl_credentials = grpc.ssl_channel_credentials(\n                    certificate_chain=cert, private_key=key\n                )\n            else:\n                ssl_credentials = SslCredentials().ssl_credentials\n\n            \n            self._grpc_channel = type(self).create_channel(\n                host,\n                credentials=credentials,\n                credentials_file=credentials_file,\n                ssl_credentials=ssl_credentials,\n                scopes=scopes or self.AUTH_SCOPES,\n                quota_project_id=quota_project_id,\n                options=[\n                    (\"grpc.max_send_message_length\", -1),\n                    (\"grpc.max_receive_message_length\", -1),\n                ],\n            )\n            self._ssl_channel_credentials = ssl_credentials\n        else:\n            host = host if \":\" in host else host + \":443\"\n\n            if credentials is None:\n                credentials, _ = auth.default(scopes=self.AUTH_SCOPES)\n\n            \n            self._grpc_channel = type(self).create_channel(\n                host,\n                credentials=credentials,\n                ssl_credentials=ssl_channel_credentials,\n                scopes=self.AUTH_SCOPES,\n                options=[\n                    (\"grpc.max_send_message_length\", -1),\n                    (\"grpc.max_receive_message_length\", -1),\n                ],\n            )\n\n        self._stubs = {}  \n\n        \n        super().__init__(\n            host=host, credentials=credentials, client_info=client_info,\n        )\n\n    @classmethod\n    def create_channel(\n        cls,\n        host: str = \"googleads.googleapis.com\",\n        credentials: credentials.Credentials = None,\n        scopes: Optional[Sequence[str]] = None,\n        **kwargs,\n    ) -> grpc.Channel:\n        \n        return grpc_helpers.create_channel(\n            host,\n            credentials=credentials,\n            scopes=scopes or cls.AUTH_SCOPES,\n            **kwargs,\n        )\n\n    @property\n    def grpc_channel(self) -> grpc.Channel:\n        \n        return self._grpc_channel\n\n    @property\n    def get_ad_group_simulation(\n        self,\n    ) -> Callable[\n        [ad_group_simulation_service.GetAdGroupSimulationRequest],\n        ad_group_simulation.AdGroupSimulation,\n    ]:\n        r\n        \n        \n        \n        \n        if \"get_ad_group_simulation\" not in self._stubs:\n            self._stubs[\n                \"get_ad_group_simulation\"\n            ] = self.grpc_channel.unary_unary(\n                \"/google.ads.googleads.v7.services.AdGroupSimulationService/GetAdGroupSimulation\",\n                request_serializer=ad_group_simulation_service.GetAdGroupSimulationRequest.serialize,\n                response_deserializer=ad_group_simulation.AdGroupSimulation.deserialize,\n            )\n        return self._stubs[\"get_ad_group_simulation\"]\n\n\n__all__ = (\"AdGroupSimulationServiceGrpcTransport\",)\n",
        "summary": "This Python code defines a gRPC transport class `AdGroupSimulationServiceGrpcTransport` for interacting with the Google Ads API's Ad Group Simulation service. It handles channel creation, authentication, and method stubs for making requests to the service."
    },
    {
        "code": "import random\nimport numpy as np\nimport pandas as pd\nimport os, time  \nfrom IPython import embed as shell\ntry:\n    import Tkinter as tk \n    from tkColorChooser import askcolor\nexcept:\n    import tkinter as tk\n    from tkinter.colorchooser import askcolor\n\n\n\nsubject_ID = []\nsession = []\n\nclass GetInput():\n    def __init__(self):\n        self.root2 = tk.Tk()\n        self.root2.title(\"Subject and Session\")\n        \n        w = 400 \n        h = 200 \n        \n        ws = self.root2.winfo_screenwidth() \n        hs = self.root2.winfo_screenheight() \n        \n        x = (ws/6) - (w/6)\n        y = (hs/6) - (h/6)\n        self.root2.geometry('%dx%d+%d+%d' % (w, h, x, y))\n        \n        self.e = tk.Entry(self.root2)\n        self.e.insert(0, 'Subject Number')\n        self.e.pack()\n        self.e.focus_set()\n        \n        self.e2 = tk.Entry(self.root2)\n        self.e2.insert(0, 'Session')\n        self.e2.pack()\n        self.e2.focus_set()\n        \n        txt='If each letter of the alphabet\\\n        \\nwere to have a unique color,\\\n        \\nwhat color would it have?\\\n        \\n\\nThere are no right or wrong answers.'\n        \n        self.instr = tk.Label(self.root2, bg='white', text=txt, font=(\"Helvetica\", 14))\n        self.instr.pack()\n        \n        b = tk.Button(self.root2,text='OK',command=self.get_input)\n        b.pack(side='bottom')\n        \n        self.root2.mainloop()\n        \n    def get_input(self):\n        subj_str = self.e.get() \n        sess_str = self.e2.get()\n        subject_ID.append(subj_str)\n        session.append(sess_str)\n        self.root2.destroy()\n        \n\napp = GetInput()   \nsubject_ID = int(subject_ID[0])\nsession = int(session[0])\n\n\ncwd = os.getcwd()\nlogfile_dir = os.path.join(cwd,'LogFiles','sub-{}'.format(subject_ID),'sess-{}'.format(session),'behav') \nif not os.path.isdir(logfile_dir):\n    os.makedirs(logfile_dir)\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\") \noutput_alphabet = os.path.join(logfile_dir,'sub-{}_sess-{}_task-consistency_events_{}.tsv'.format(subject_ID,session,timestr))\n\n\nalphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n\n\nREPS = 2 \n\nRGBS = [] \nL = '2'  \n\nclass Test():\n    def __init__(self):\n        self.counter = 1\n        self.root = tk.Tk()\n        self.root.title(\"Subject {} Session {}\".format(subject_ID, session))\n        \n        \n        ws = self.root.winfo_screenwidth() \n        hs = self.root.winfo_screenheight() \n        \n        self.root.geometry('%dx%d+%d+%d' % (ws, hs, 0, 0))\n        self.open1 = tk.Button(self.root, text='Pick a color:', command=self.pick_a_color, font=('Helvetica', '36'),padx=5, pady=5)\n        self.open1.pack(fill=tk.X, expand=False)    \n        self.letter = tk.Label(self.root, bg='white', text=L, font=(\"Helvetica\", 90))\n        self.letter.pack()\n        self.root.mainloop()\n        \n    def quit(self):\n        RGBS.append( [L ,self.RGB, self.HEX, abc] )\n        self.root.destroy()\n            \n    def pick_a_color(self,):        \n        \n        self.RGB,self.HEX = askcolor((random.randint(0,255), random.randint(0,255), random.randint(0,255)), parent=None, title='Pick a color: {}'.format(L) )\n        self.letter.configure(fg = self.HEX)\n        if self.counter:\n            exit_button = tk.Button(self.root, text='FINISHED', command=self.quit, font=('Helvetica', '28'))\n            exit_button.pack()\n            self.counter = 0\n        self.root.mainloop()\n\n\nabc = 1 \nfor R in np.arange(REPS):\n    random.shuffle(alphabet) \n    \n    for L in alphabet:      \n        app = Test()\n        \n        \n        DFS = pd.DataFrame(RGBS)\n        print(RGBS)\n\n        try:\n            DFS.columns = [\"letter\",\"rgb\",\"hex\",\"choice\"]\n            DFS['subject'] = np.repeat(subject_ID,len(DFS))\n            DFS['r']            = [c[0] for c in DFS['rgb']]\n            DFS['g']            = [c[1] for c in DFS['rgb']]\n            DFS['b']            = [c[2] for c in DFS['rgb']]\n        except:\n            \n            pass\n        DFS.to_csv(output_alphabet, sep='\\t') \n    abc+=1\n\n\n\nprint(RGBS)\nprint('consistency test - success!')\n\n\n\n\ndel tk  \ndel askcolor\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfig = plt.figure(figsize=(10,5))\n\n\ntry:\n    DFS.sort_values(by=['choice', 'letter'],inplace=True)\nexcept:\n    DFS = DFS.sort(['choice', 'letter'])\n\nDFS.reset_index(inplace=True)\nfor i,A in enumerate(alphabet):\n    ax = fig.add_subplot(6,5,i+1)\n    ax.text(0.5, 0.5, DFS['letter'][i], color=DFS['hex'][i],fontsize=18)\n    ax.text(0.25, 0.5, DFS['letter'][i+len(alphabet)], color=DFS['hex'][i+len(alphabet)],fontsize=18)\n    ax.set_axis_off()    \n\nsns.despine(offset=10, trim=True)\nplt.tight_layout()\nfig.savefig(os.path.join(cwd,'LogFiles','sub-{}'.format(subject_ID),'sess-{}'.format(session),'behav','sub-{}_sess-{}_colors.pdf'.format(subject_ID,session)))\nprint('success: sub-{}_sess-{}_colors.pdf'.format(subject_ID,session))\n\n    \n    \n",
        "summary": "The provided Python script creates a graphical user interface (GUI) for collecting subject and session information, then runs a color perception test where participants choose colors for letters of the alphabet. The results are saved in a TSV file and visualized using matplotlib and seaborn to create a PDF showing the chosen colors for each letter."
    },
    {
        "code": "import pytest\n\nfrom list_utils import *\nfrom oracle import ColumnRecommendation, ColumnClassification\n\n\ndef test_find_one():\n    needle = 1\n    none = [0, 0, 5, 's']\n    beginning = [1, None, 9, 6, 0, 0]\n    end = ['x', '0', 1]\n    several = [0, 0, 3, 4, 1, 3, 2, 1, 3, 4]\n\n    assert find_one(none, needle) == False\n    assert find_one(beginning, needle)\n    assert find_one(end, needle)\n    assert find_one(several, needle)\n\n\ndef test_find_n():\n    assert find_n([2, 3, 4, 5, 6], 2, -1) == False\n    assert find_n([1, 2, 3, 4, 5], 42, 2) == False\n    assert find_n([1, 2, 3, 4, 5], 1, 2) == False\n    assert find_n([1, 2, 3, 2, 4, 5], 2, 2)\n    assert find_n([1, 2, 3, 4, 5, 4, 6, 4, 7, 4, 6], 4, 2)\n    assert find_n([1, 2, 3, 4], 'x', 0) == True\n\n\ndef test_find_streak():\n    assert find_streak([1, 2, 3, 4, 5], 4, -1) == False\n    assert find_streak([1, 2, 3, 4, 5], 42, 2) == False\n    assert find_streak([1, 2, 3, 4], 4, 1)\n    assert find_streak([1, 2, 3, 1, 2], 2, 2) == False\n    assert find_streak([1, 2, 3, 4, 5, 5, 5], 5, 3)\n    assert find_streak([5, 5, 5, 1, 2, 3, 4], 5, 3)\n    assert find_streak([1, 2, 5, 5, 5, 3, 4], 5, 3)\n    assert find_streak([1, 2, 3, 4, 5, 5, 5], 5, 4) == False\n\n\ndef test_first_elements():\n    original = [[0, 7, 3], [4, 0, 1]]\n\n    assert first_elements(original) == [0, 4]\n\n\ndef test_transpose():\n    original = [[0, 7, 3], [4, 0, 1]]\n    transposed = [[0, 4], [7, 0], [3, 1]]\n\n    assert transpose(original) == transposed\n    assert transpose(transpose(original)) == original\n\n\ndef test_zero_distance_displace():\n    l1 = [1, 2, 3, 4, 5, 6]\n    l2 = [1]\n    l3 = [[4, 5], ['x', 'o', 'c']]\n\n    assert displace([], 0) == []\n    assert displace(l1, 0) == l1\n    assert displace(l2, 0) == l2\n    assert displace(l3, 0) == l3\n\n\ndef test_positive_distance_displace():\n\n    l1 = [1, 2, 3, 4, 5, 6]\n    l2 = [1]\n    l3 = [[4, 5], ['x', 'o', 'c']]\n    l4 = [9, 6, 5]\n\n    assert displace([], 2) == []\n    assert displace(l1, 2) == [None, None, 1, 2, 3, 4]\n    assert displace(l2, 3, '-') == ['-']\n    assert displace(l3, 1, '\n    assert displace(l4, 3, 0) == [0, 0, 0]\n\n\ndef test_negative_distance_displace():\n    l1 = [1, 2, 3, 4, 5, 6]\n    l2 = [1]\n    l3 = [[4, 5], ['x', 'o', 'c']]\n    l4 = [9, 6, 5]\n\n    assert displace([], -2) == []\n    assert displace(l1, -2) == [3, 4, 5, 6, None, None]\n    assert displace(l2, -3, '-') == ['-']\n    assert displace(l3, -1, '\n    assert displace(l4, -3, 0) == [0, 0, 0]\n\n\ndef test_reverse_list():\n    assert reverse_list([]) == []\n    assert reverse_list([1, 2, 3, 4, 5, 6]) == [6, 5, 4, 3, 2, 1]\n\n\ndef test_reverse_matrix():\n    assert reverse_matrix([]) == []\n    assert reverse_matrix([[0, 1, 2, 3], [0, 1, 2, 3]]) == [\n        [3, 2, 1, 0], [3, 2, 1, 0]]\n\n\ndef test_all_same():\n    assert all_same([9, 1, 2, 3, 4]) == False\n    assert all_same([[], [], []])\n    assert all_same([])\n\n    assert all_same([ColumnRecommendation(0, ColumnClassification.WIN),\n                     ColumnRecommendation(2, ColumnClassification.WIN)])\n\n    assert all_same([ColumnRecommendation(0, ColumnClassification.MAYBE),\n                     ColumnRecommendation(0, ColumnClassification.WIN)]) == False\n\n\ndef test_collapse_list():\n    assert collapse_list([]) == ''\n    assert collapse_list(['o', 'x', 'x', 'o']) == 'oxxo'\n    assert collapse_list(['x', 'x', None, None, None]) == 'xx...'\n\n\ndef test_collapse_matrix():\n    assert collapse_matrix([]) == ''\n    assert collapse_matrix([['x', 'x', None],\n                           ['o', 'x', 'x'],\n                           ['o', None, None]]) == 'xx.|oxx|o..'\n\n\ndef test_replace_all_in_list():\n    assert replace_all_in_list([None, 3, '546', 33, None], None, '\n                               '\n    assert replace_all_in_list([1, 2, 3, 4, 5], 'e', 42) == [1, 2, 3, 4, 5]\n    assert replace_all_in_list([], 34, 43) == []\n\n\ndef test_replace_all_in_matrix():\n    \n    assert replace_all_in_matrix([[1, 2, 3, 'n', 'n', None],\n                                  [4, 5, 'n']], 'n', '\n    \n    assert replace_all_in_matrix([[None, None, 2, True], [4, 5, '\n        None, None, 2, True],  [4, 5, '\n    \n    assert replace_all_in_matrix([], None, 7) == []\n    assert replace_all_in_matrix([[], []], None, 7) == [[], []]\n",
        "summary": "The provided Python code includes several test functions for various utility functions in the `list_utils` and `oracle` modules. These tests validate functionalities such as finding elements within lists, transposing matrices, reversing lists and matrices, checking if all elements are the same, collapsing lists and matrices into strings, and replacing elements in lists and matrices."
    },
    {
        "code": "import logging\nimport azure.functions as func\nimport json\nimport os\nfrom azure.cosmosdb.table.tableservice import TableService\nfrom azure.cosmosdb.table.models import Entity\n\ndef main(req: func.HttpRequest) -> func.HttpResponse:\n    logging.info('Python HTTP trigger function processed a request.')\n\n    \n    table_service = TableService(connection_string= os.environ['AzureWebJobsStorage'])\n    table_service.create_table('intents') if not table_service.exists('intents') else None\n\n    req_body = req.get_json()\n    if req_body:\n        \n        print(req_body.get('ConversationId'))\n        data = req_body\n        data[\"PartitionKey\"] = req_body.get('ConversationId')\n        data[\"RowKey\"] = req_body.get('MessageId')           \n        \n        \n        table_service.insert_or_replace_entity('intents', data)\n        return func.HttpResponse(f\"Row {req_body.get('MessageId')} for {req_body.get('ConversationId')} added\")\n    else:\n        return func.HttpResponse(\n             \"Please pass valid request body\",\n             status_code=400\n        )",
        "summary": "The Python code defines an Azure Function triggered by HTTP requests that logs the processing of each request, connects to an Azure Cosmos DB table named 'intents', and inserts or replaces entities in this table based on the JSON data received from the request. If no valid JSON body is provided, it returns a 400 status code with an error message."
    },
    {
        "code": "from .TreeDynamicTimeStepping import TreeDynamicTimeStepping\nfrom .TreeDynamicTimeLoop import TreeDynamicTimeLoop\nfrom .SimpleTimeLoop.SimpleLoop import Loop\n",
        "summary": "The Python code imports classes from various modules, including `TreeDynamicTimeStepping` and `TreeDynamicTimeLoop` from the `.TreeDynamicTimeStepping` and `.TreeDynamicTimeLoop` modules respectively, as well as a `Loop` class from the `.SimpleTimeLoop.SimpleLoop` module."
    },
    {
        "code": "import logging\r\n\r\ndef _load_tcc(f_tcc, msk):\r\n    from gio import geo_raster_ex as gx\r\n    from gio import config\r\n    import numpy as np\r\n\r\n    _bnd = gx.read_block(f_tcc, msk)\r\n    if _bnd is None:\r\n        return None\r\n        \r\n    _dat = np.zeros(msk.data.shape, dtype=np.uint8)\r\n\r\n    _m_tcc = config.getfloat('conf', 'min_tcc')\r\n    _idx = _bnd.data >= _m_tcc\r\n    _dat[_idx] = 100\r\n\r\n    _idx = _bnd.data > 100\r\n    _dat[_idx] = _bnd.data[_idx]\r\n\r\n    return msk.from_grid(_dat, nodata=255)\r\n\r\ndef _task(tile, d_out, d_ref, opts):\r\n    from gio import file_unzip\r\n    from gio import config\r\n    from gio import file_mag\r\n    from gio import metadata\r\n    from gio import geo_raster as ge\r\n    from gio import mod_filter\r\n    import numpy as np\r\n    import os\r\n    import re\r\n\r\n    _tag = tile.tag\r\n\r\n    _ttt = config.get('conf', 'test_tile')\r\n    if _ttt and _tag not in _ttt.replace(' ', '').split(','):\r\n        return\r\n\r\n    _m = re.match(r'(h\\d+)(v\\d+)', _tag)\r\n    _h = _m.group(1)\r\n    _v = _m.group(2)\r\n    \r\n    _d_out = os.path.join(d_out, _h, _v, _tag)\r\n    _d_ref = os.path.join(d_ref, _h, _v, _tag)\r\n    _f_met = os.path.join(_d_out, '%s_met.txt' % _tag)\r\n    \r\n    _fname = lambda t: os.path.join(_d_out, '%s_%s.tif' % (_tag, t))\r\n    _fname_ref = lambda t: os.path.join(_d_ref, '%s_%s.tif' % (_tag, t))\r\n    _fname_m1 = lambda t, a='_m1': _fname('%s_n0%s' % (t, a))\r\n\r\n    \n    \n    \n\r\n    if not file_mag.get(_fname_m1('loss_year')).exists():\r\n        logging.info('skip non-existing result for %s' % _tag)\r\n        return\r\n    \r\n    if (not _ttt) and file_mag.get(_fname_m1('esta_year')).exists() and \\\r\n            (not config.getboolean('conf', 'over_write', False)):\r\n        logging.info('skip processed esta result for %s' % _tag)\r\n        return\r\n    \r\n    _b_loss_year = ge.open(_fname_m1('loss_year')).get_band().cache()\r\n    _b_gain_year = ge.open(_fname_m1('gain_year')).get_band().cache()\r\n    \r\n    _b_loss_prob = ge.open(_fname_m1('loss_prob')).get_band().cache()\r\n    _b_gain_prob = ge.open(_fname_m1('gain_prob')).get_band().cache()\r\n\r\n    _f_tcc = config.get('conf', 'latest_tcc')\r\n    _b_prob = _load_tcc(_f_tcc, _b_loss_year) if _f_tcc else ge.open(_fname_ref('age_prob')).get_band().cache()\r\n    if _b_prob is None:\r\n        logging.info('forced to use age_prob layer %s' % _fname_ref('age_prob'))\r\n        _b_prob = ge.open(_fname_ref('age_prob')).get_band().cache()\r\n\r\n    _d_forest_prob = _b_prob.data\r\n    _d_loss = _b_loss_year.data\r\n    _d_gain = _b_gain_year.data\r\n\r\n    _d_esta = np.zeros(_d_forest_prob.shape, dtype=np.uint8)\r\n    \r\n    _d_prob = np.empty(_d_forest_prob.shape, dtype=np.float32)\r\n    _d_prob.fill(100)\r\n    _d_prob[_b_prob.data == _b_prob.nodata] = -9999\r\n    \r\n    _b_esta = _b_loss_year.from_grid(_d_esta, nodata=255)\r\n    _b_esta.color_table = ge.load_colortable(config.get('conf', 'color'))\r\n\r\n    _d_esta[_d_forest_prob > 100] = _d_forest_prob[_d_forest_prob > 100]\r\n    \r\n    for _y in range(1970, 2021):\r\n        _y = _y - 1970\r\n        \r\n        _idx = _d_loss == _y\r\n        _d_esta[_idx] = 100\r\n        _d_prob[_idx] = _b_loss_prob.data[_idx]\r\n        \r\n        _idx = _d_gain == _y\r\n        _d_esta[_idx] = _y\r\n        _d_prob[_idx] = _b_gain_prob.data[_idx]\r\n    \r\n    _d_esta[_d_forest_prob < 50] = 100\r\n    \r\n    _d_test = (_d_esta < 100).astype(np.uint8)\r\n    _d_test[(_d_esta < 100) & (_d_esta > 0)] = 1\r\n    _b_test = _b_esta.from_grid(_d_test, nodata=255)\r\n    mod_filter.filter_band_mmu(_b_test, area=config.getfloat('conf', 'min_patch'))\r\n    _d_esta[(_d_esta == 100) & (_b_test.data == 1)] = 0\r\n    \r\n    _d_test = ((_d_esta > 0) & (_d_esta <= 100)).astype(np.uint8)\r\n    _d_test[(_d_esta < 100) & (_d_esta > 0)] = 1\r\n    _b_test = _b_esta.from_grid(_d_test, nodata=255)\r\n    mod_filter.filter_band_mmu(_b_test, area=config.getfloat('conf', 'min_patch'))\r\n    _d_esta[(_d_esta == 0) & (_b_test.data == 1)] = 100\r\n    \r\n    with file_unzip.file_unzip() as _zip:\r\n        _zip.save(_b_esta, _fname_m1('esta_year'))\r\n        _zip.save(_b_esta.from_grid(_d_prob, nodata=-9999), _fname_m1('esta_prob'))\r\n    \r\n    return True\r\n\r\ndef main(opts):\r\n    import logging\r\n    from gio import config\r\n    from gio import file_mag\r\n    from gio import global_task\r\n    import os\r\n    \r\n    _d_inp = config.get('conf', 'input')\r\n    _d_ref = config.get('conf', 'refer', _d_inp)\r\n    \r\n    _f_mak = file_mag.get(os.path.join(_d_inp, 'tasks.txt'))\r\n    _ts = global_task.load(_f_mak)\r\n\r\n    from gio import multi_task\r\n    _rs = multi_task.run(_task, [(_t, os.path.join(_d_inp, 'data'), os.path.join(_d_ref, 'data'), opts) for _t in multi_task.load(_ts, opts)], opts)\r\n    print('processed', len([_r for _r in _rs if _r]), 'tiles')\r\n\r\ndef usage():\r\n    _p = environ_mag.usage(True)\r\n\r\n    _p.add_argument('-i', '--input', dest='input')\r\n    _p.add_argument('-r', '--refer', dest='refer')\r\n    _p.add_argument('--latest-tcc', dest='latest_tcc')\r\n    _p.add_argument('-w', '--over-write', dest='over_write', type='bool')\r\n    _p.add_argument('--min-tcc', dest='min_tcc', type=int, default=30)\r\n    _p.add_argument('-m', '--min-patch', dest='min_patch', type=float, default=100 * 100)\r\n    _p.add_argument('--test-tile', dest='test_tile')\r\n\r\n    return _p\r\n\r\nif __name__ == '__main__':\r\n    from gio import environ_mag\r\n    environ_mag.init_path()\r\n    environ_mag.run(main, [environ_mag.config(usage())])\r\n",
        "summary": "The provided Python code defines functions for processing geographic data, specifically focusing on land cover change (LCC) analysis. The `_load_tcc` function loads and processes a TCC file to create a binary mask indicating areas of interest. The `_task` function orchestrates the LCC analysis for individual tiles, including reading input files, applying filters, and saving output results. The `main` function initializes the processing environment, loads tasks from a configuration file, and runs the analysis on multiple tiles in parallel. The `usage` function defines command-line arguments for configuring the script's behavior."
    },
    {
        "code": "import datetime\nimport logging\nimport traceback\n\nfrom dis_snek.models import ComponentContext\nfrom dis_snek.models import InteractionContext\n\nfrom ElevatorBot.misc.formating import embed_message\n\n\ndef get_now_with_tz() -> datetime.datetime:\n    \n\n    return datetime.datetime.now(tz=datetime.timezone.utc)\n\n\ndef localize_datetime(obj: datetime.datetime) -> datetime.datetime:\n    \n\n    return obj.astimezone()\n\n\nasync def log_error(\n    ctx: InteractionContext | ComponentContext,\n    error: Exception,\n    situation: str,\n) -> None:\n    \n\n    if not ctx.responded:\n        await ctx.send(\n            embeds=embed_message(\n                \"Error\",\n                f\"Sorry, something went wrong\\nThe Error has been logged and will be worked on\",\n                str(error),\n            )\n        )\n\n    \n    logger = logging.getLogger(situation)\n    logger.exception(\n        f\"InteractionID '{ctx.interaction_id}' - Error {error} - Traceback: \\n{''.join(traceback.format_tb(error.__traceback__))}\"\n    )\n\n    \n    raise error\n",
        "summary": "The provided Python code includes functions for getting the current UTC datetime, localizing a datetime object to the system's timezone, and logging errors with detailed traceback information. It also handles sending error messages in response to interactions or components within a Discord bot context using the dis_snek library."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union\nfrom ... import _utilities, _tables\nfrom . import outputs\n\n__all__ = [\n    'GetOrderCollectionByNameResult',\n    'AwaitableGetOrderCollectionByNameResult',\n    'get_order_collection_by_name',\n]\n\n@pulumi.output_type\nclass GetOrderCollectionByNameResult:\n    \n    def __init__(__self__, id=None, location=None, name=None, order_ids=None, system_data=None, tags=None, type=None):\n        if id and not isinstance(id, str):\n            raise TypeError(\"Expected argument 'id' to be a str\")\n        pulumi.set(__self__, \"id\", id)\n        if location and not isinstance(location, str):\n            raise TypeError(\"Expected argument 'location' to be a str\")\n        pulumi.set(__self__, \"location\", location)\n        if name and not isinstance(name, str):\n            raise TypeError(\"Expected argument 'name' to be a str\")\n        pulumi.set(__self__, \"name\", name)\n        if order_ids and not isinstance(order_ids, list):\n            raise TypeError(\"Expected argument 'order_ids' to be a list\")\n        pulumi.set(__self__, \"order_ids\", order_ids)\n        if system_data and not isinstance(system_data, dict):\n            raise TypeError(\"Expected argument 'system_data' to be a dict\")\n        pulumi.set(__self__, \"system_data\", system_data)\n        if tags and not isinstance(tags, dict):\n            raise TypeError(\"Expected argument 'tags' to be a dict\")\n        pulumi.set(__self__, \"tags\", tags)\n        if type and not isinstance(type, str):\n            raise TypeError(\"Expected argument 'type' to be a str\")\n        pulumi.set(__self__, \"type\", type)\n\n    @property\n    @pulumi.getter\n    def id(self) -> str:\n        \n        return pulumi.get(self, \"id\")\n\n    @property\n    @pulumi.getter\n    def location(self) -> str:\n        \n        return pulumi.get(self, \"location\")\n\n    @property\n    @pulumi.getter\n    def name(self) -> str:\n        \n        return pulumi.get(self, \"name\")\n\n    @property\n    @pulumi.getter(name=\"orderIds\")\n    def order_ids(self) -> Sequence[str]:\n        \n        return pulumi.get(self, \"order_ids\")\n\n    @property\n    @pulumi.getter(name=\"systemData\")\n    def system_data(self) -> 'outputs.SystemDataResponse':\n        \n        return pulumi.get(self, \"system_data\")\n\n    @property\n    @pulumi.getter\n    def tags(self) -> Optional[Mapping[str, str]]:\n        \n        return pulumi.get(self, \"tags\")\n\n    @property\n    @pulumi.getter\n    def type(self) -> str:\n        \n        return pulumi.get(self, \"type\")\n\n\nclass AwaitableGetOrderCollectionByNameResult(GetOrderCollectionByNameResult):\n    \n    def __await__(self):\n        if False:\n            yield self\n        return GetOrderCollectionByNameResult(\n            id=self.id,\n            location=self.location,\n            name=self.name,\n            order_ids=self.order_ids,\n            system_data=self.system_data,\n            tags=self.tags,\n            type=self.type)\n\n\ndef get_order_collection_by_name(order_collection_name: Optional[str] = None,\n                                 resource_group_name: Optional[str] = None,\n                                 opts: Optional[pulumi.InvokeOptions] = None) -> AwaitableGetOrderCollectionByNameResult:\n    \n    __args__ = dict()\n    __args__['orderCollectionName'] = order_collection_name\n    __args__['resourceGroupName'] = resource_group_name\n    if opts is None:\n        opts = pulumi.InvokeOptions()\n    if opts.version is None:\n        opts.version = _utilities.get_version()\n    __ret__ = pulumi.runtime.invoke('azure-native:edgeorder/v20201201preview:getOrderCollectionByName', __args__, opts=opts, typ=GetOrderCollectionByNameResult).value\n\n    return AwaitableGetOrderCollectionByNameResult(\n        id=__ret__.id,\n        location=__ret__.location,\n        name=__ret__.name,\n        order_ids=__ret__.order_ids,\n        system_data=__ret__.system_data,\n        tags=__ret__.tags,\n        type=__ret__.type)\n",
        "summary": "The provided Python code defines a Pulumi resource for retrieving an order collection by name in Azure Edge Order, including validation for input types and handling asynchronous operations. It includes a class `GetOrderCollectionByNameResult` to represent the result of the retrieval, properties for accessing various attributes of the order collection, and a function `get_order_collection_by_name` to invoke the resource with optional parameters and return an awaitable result."
    },
    {
        "code": "import string, os\nfrom plasTeX.Tokenizer import Token, EscapeSequence\nfrom plasTeX import Command, Environment\nfrom plasTeX.Logging import getLogger\nfrom Sectioning import SectionUtils\n\ntry:\n    from pyuca import Collator\n    collator = Collator(os.path.join(os.path.dirname(__file__), 'allkeys.txt')).sort_key\nexcept ImportError:\n    collator = lambda x: x.lower()\n\nclass IndexUtils(object):\n    \n\n    linkType = 'index'\n    level = Command.CHAPTER_LEVEL\n\n    class Index(Command):\n        \n    \n        def __init__(self, *args, **kwargs):\n            Command.__init__(self, *args, **kwargs)\n            self.pages = []\n            self.key = []\n            self.sortkey = ''\n\n        @property\n        def totallen(self):\n            \n            total = 1\n            for item in self:\n                total += item.totallen\n            return total\n    \n        def __repr__(self):\n            return '%s%s --> %s' % (''.join([x.source for x in self.key]), \n                                    ', '.join([str(x) for x in self.pages]), \n                                    Command.__repr__(self))\n\n    class IndexGroup(list):\n        title = None\n\n    def invoke(self, tex):\n        if isinstance(self, Environment):\n            Environment.invoke(self, tex)\n        else:\n            Command.invoke(self, tex)\n        self.attributes['title'] = self.ownerDocument.createElement('indexname').expand(tex)\n\n    @property\n    def groups(self):\n        \n        batches = []\n        current = ''\n        for item in self:\n            try: \n                label = title = item.sortkey[0].upper()\n                if title in string.letters:\n                    pass\n                elif title == '_':\n                     title = '_ (Underscore)'\n                else:\n                     label = title = 'Symbols'\n            except IndexError: \n                label = title = 'Symbols'\n            if current != title:\n                newgroup = self.IndexGroup()\n                newgroup.title = title\n                newgroup.id = label\n                batches.append(newgroup)\n                current = title\n            batches[-1].append(item)\n\n        for item in batches:\n            item[:] = self.splitColumns(item,\n                self.ownerDocument.config['document']['index-columns'])\n\n        return batches\n\n    def splitColumns(self, items, cols):\n        \n        entries = [(0,0)]\n        \n        grandtotal = 0\n        for item in items:\n            entries.append((item.totallen, item))\n            grandtotal += entries[-1][0]\n        entries.pop(0)\n        entries.reverse()\n\n        \n        coltotal = int(grandtotal / cols)\n\n        \n        current = 0\n        output = [[]]\n        for num, item in entries:\n            current += num\n            if len(output) >= cols:\n                output[-1].append(item)\n            elif current > coltotal:\n                output.append([item]) \n                current = num\n            elif current == coltotal:\n                output[-1].append(item)\n                output.append([])\n                current = 0\n            else:\n                output[-1].append(item)\n\n        output.reverse()\n        for item in output:\n            item.reverse()\n\n        \n        output = [x for x in output if x]  \n\n        \n        for i in range(cols-len(output)):\n            output.append([])\n\n        return output\n\n    def digest(self, tokens):\n        \n        if isinstance(self, Environment):\n            Environment.digest(self, tokens)\n            if self.macroMode == self.MODE_END:\n                return\n            \n            \n            while self.childNodes:\n                self.pop()\n        else:\n            Command.digest(self, tokens)\n        doc = self.ownerDocument\n        current = self\n        entries = sorted(self.ownerDocument.userdata.get('index', []))\n        prev = IndexEntry([], None)\n        for item in entries:\n            \n            \n            common = 0\n            for prevkey, itemkey in zip(zip(prev.sortkey, prev.key), \n                                        zip(item.sortkey, item.key)):\n                if prevkey == itemkey:\n                    common += 1\n                    continue\n                break\n\n\n\n\n\n            \n            i = common\n            while i < len(prev.key):\n\n                current = current.parentNode\n                i += 1\n\n            \n            i = common\n            while i < len(item.key):\n\n                newidx = self.Index()\n                newidx.key = item.key[i]\n                newidx.sortkey = item.sortkey[i]\n                newidx.parentNode = current\n                current.append(newidx)\n                current = newidx\n                i += 1\n\n            \n            current.pages.append(IndexDestination(item.type, item.node))\n            if item.format is not None:\n                text = doc.createTextNode(str(len(current.pages)))\n                ipn = item.format.getElementsByTagName('index-page-number')\n                if ipn:\n                    ipn = ipn[0]\n                    ipn.parentNode.replaceChild(text, ipn)\n                item.node.append(item.format)\n            else:\n                text = doc.createTextNode(str(len(current.pages)))\n                item.node.append(text)\n            prev = item\n\nclass IndexDestination(object):\n    def __init__(self, type, node):\n        self._cr_type = type\n        self._cr_node = node\n\n    @property\n    def see(self):\n        return self._cr_type == IndexEntry.TYPE_SEE\n\n    @property\n    def seealso(self):\n        return self._cr_type == IndexEntry.TYPE_SEEALSO\n\n    @property\n    def normal(self):\n        return not(self.see) and not(self.seealso)\n\n    def __getattribute__(self, name):\n        if name.startswith('_cr_') or name in ['see', 'seealso', 'normal']:\n            return object.__getattribute__(self, name)\n        if self._cr_type and name in ['url']:\n            return None\n        return getattr(self._cr_node, name)\n\nclass theindex(IndexUtils, Environment, SectionUtils):\n    blockType = True\n    level = Environment.CHAPTER_LEVEL\n    counter = 'chapter'\n\nclass printindex(IndexUtils, Command, SectionUtils):\n    blockType = True\n    level = Command.CHAPTER_LEVEL\n    counter = 'chapter'\n\nclass makeindex(Command):\n    pass\n\nclass makeglossary(Command):\n    pass\n\nclass glossary(Command):\n    args = 'entry:nox'\n\nclass index(Command):\n    args = 'entry:nox'\n\n    @property\n    def textContent(self):\n        return ''\n\n    def invoke(self, tex):\n        result = Command.invoke(self, tex)\n        sortkey, key, format = [], [], []\n        entry = iter(self.attributes['entry'])\n        current = []\n        alphanumeric = [Token.CC_OTHER, Token.CC_LETTER, Token.CC_SPACE]\n\n        \n        for tok in entry:\n            if tok.catcode in alphanumeric:\n                \n                if tok == '\"':\n                    for tok in entry:\n                        current.append(tok)\n                        break\n                \n                elif tok == '!':\n                    key.append(current)\n                    if len(sortkey) < len(key):\n                        sortkey.append(current)\n                    current = []\n                \n                elif tok == '@':\n                    sortkey.append(current)\n                    current = []\n                \n                elif tok == '|':\n                    key.append(current)\n                    if len(sortkey) < len(key):\n                        sortkey.append(current)\n                    current = format\n                else:\n                    current.append(tok)\n                continue\n            \n            current.append(tok)\n\n        \n        if not format:\n            key.append(current)\n            if len(sortkey) < len(key):\n                sortkey.append(current)\n\n        \n        for i, item in enumerate(sortkey):\n            sortkey[i] = tex.expandTokens(item).textContent\n\n        \n        for i, item in enumerate(key):\n            key[i] = tex.expandTokens(item) \n\n        \n        type = IndexEntry.TYPE_NORMAL\n        if not format:\n            format = None\n        else:\n            macro = []\n            while format and format[0].catcode == Token.CC_LETTER:\n                macro.append(format.pop(0))\n            if macro:\n                macro = ''.join(macro)\n                format.insert(0, EscapeSequence(macro))\n                if macro == 'see':\n                    type = IndexEntry.TYPE_SEE \n                elif macro == 'seealso':\n                    type = IndexEntry.TYPE_SEEALSO\n            format.append(EscapeSequence('index-page-number'))\n            format = tex.expandTokens(format)\n\n        \n        userdata = self.ownerDocument.userdata\n        if 'index' not in userdata:\n            userdata['index'] = []\n        userdata['index'].append(IndexEntry(key, self, sortkey, format, type))\n\n        return result\n\n\n\nclass IndexEntry(object):\n    \n\n    TYPE_NORMAL = 0\n    TYPE_SEE = 1\n    TYPE_SEEALSO = 2\n\n    def __init__(self, key, node, sortkey=None, format=None, type=0):\n        \n        self.key = key\n        if not sortkey:\n            self.sortkey = key\n        else:\n            self.sortkey = []\n            for i, sk in enumerate(sortkey):\n                if sk is None:\n                    self.sortkey.append(key[i].textContent)\n                else:\n                    self.sortkey.append(sk)\n        self.format = format\n        self.node = node\n        self.type = type\n\n    @property\n    def see(self):\n        return self.type == type(self).TYPE_SEE\n\n    @property\n    def seealso(self):\n        return self.type == type(self).TYPE_SEEALSO\n\n    @property\n    def normal(self):\n        return not(self.see) and not(self.seealso)\n\n    def __cmp__(self, other):\n        result = cmp(zip([collator(x) for x in self.sortkey if isinstance(x, basestring)], \n                         [collator(x.textContent) for x in self.key], \n                         self.key), \n                     zip([collator(x) for x in other.sortkey if isinstance(x, basestring)], \n                         [collator(x.textContent) for x in other.key], \n                         other.key))\n        if result == 0 and len(self.key) != len(other.key):\n            return cmp(len(self.key), len(other.key))\n        return result\n\n    def __repr__(self):\n        if self.format is None:\n            return ' '.join(['@'.join(self.sortkey), \n                             '!'.join([x.source for x in self.key])])\n        else:\n            return ' '.join(['@'.join(self.sortkey), \n                             '!'.join([x.source for x in self.key]), \n                             ' '.join([x.source for x in self.format])])\n\n    def __str__(self):\n        return repr(self)\n\nclass IndexPageNumber(Command):\n    macroName = 'index-page-number'\n",
        "summary": "This code defines a set of classes and functions to handle indexing in a document processing system. The primary components include:\n\n1. **IndexEntry**: Represents an entry in the index, with properties for key, sortkey, format, node, and type (normal, see, or seealso).\n\n2. **makeindex** and **makeglossary**: Commands that initiate the indexing process.\n\n3. **index** and **glossary**: Commands used to add entries to the index or glossary.\n\n4. **theindex** and **printindex**: Environment classes for handling the index block in a document.\n\n5. **IndexPageNumber**: A command representing the page number in an index entry.\n\n6. **IndexUtils**, **Environment**, and **SectionUtils**: Base classes providing common functionality for indexing, environments, and sections.\n\nThe code uses various utilities like `Token`, `EscapeSequence`, and `collator` to handle tokenization, escaping, and collation of text. It also defines a custom comparison method for `IndexEntry` objects to ensure proper sorting in the index.\n\nOverall, this system provides a structured way to manage and generate indices within documents, allowing for customization through different commands and environments."
    },
    {
        "code": "def delete_disconnected_nodes(gd):\n    \n    empty_nodes = []\n    for k, v in gd.items():\n        if (\n            len(gd[k].inputs) == 0\n            and len(gd[k].outputs) == 0\n            and len(gd[k].control_inputs) == 0\n            and len(gd[k].control_outputs) == 0\n            and gd[k].op != \"Placeholder\"\n        ):\n            empty_nodes.append(k)\n\n    for k in empty_nodes:\n        del gd[k]\n",
        "summary": "The function `delete_disconnected_nodes` iterates through a graph dictionary, identifying nodes with no inputs, outputs, control inputs, or control outputs, and are not placeholders. It then deletes these disconnected nodes from the graph."
    },
    {
        "code": "from typing import Any, TYPE_CHECKING\n\nfrom azure.core.configuration import Configuration\nfrom azure.core.pipeline import policies\nfrom azure.mgmt.core.policies import ARMHttpLoggingPolicy\n\nfrom .._version import VERSION\n\nif TYPE_CHECKING:\n    \n    from azure.core.credentials_async import AsyncTokenCredential\n\n\nclass WebSiteManagementClientConfiguration(Configuration):\n    \n\n    def __init__(\n        self,\n        credential: \"AsyncTokenCredential\",\n        subscription_id: str,\n        **kwargs: Any\n    ) -> None:\n        if credential is None:\n            raise ValueError(\"Parameter 'credential' must not be None.\")\n        if subscription_id is None:\n            raise ValueError(\"Parameter 'subscription_id' must not be None.\")\n        super(WebSiteManagementClientConfiguration, self).__init__(**kwargs)\n\n        self.credential = credential\n        self.subscription_id = subscription_id\n        self.api_version = \"2015-08-01\"\n        self.credential_scopes = kwargs.pop('credential_scopes', ['https://management.azure.com/.default'])\n        kwargs.setdefault('sdk_moniker', 'mgmt-web/{}'.format(VERSION))\n        self._configure(**kwargs)\n\n    def _configure(\n        self,\n        **kwargs: Any\n    ) -> None:\n        self.user_agent_policy = kwargs.get('user_agent_policy') or policies.UserAgentPolicy(**kwargs)\n        self.headers_policy = kwargs.get('headers_policy') or policies.HeadersPolicy(**kwargs)\n        self.proxy_policy = kwargs.get('proxy_policy') or policies.ProxyPolicy(**kwargs)\n        self.logging_policy = kwargs.get('logging_policy') or policies.NetworkTraceLoggingPolicy(**kwargs)\n        self.http_logging_policy = kwargs.get('http_logging_policy') or ARMHttpLoggingPolicy(**kwargs)\n        self.retry_policy = kwargs.get('retry_policy') or policies.AsyncRetryPolicy(**kwargs)\n        self.custom_hook_policy = kwargs.get('custom_hook_policy') or policies.CustomHookPolicy(**kwargs)\n        self.redirect_policy = kwargs.get('redirect_policy') or policies.AsyncRedirectPolicy(**kwargs)\n        self.authentication_policy = kwargs.get('authentication_policy')\n        if self.credential and not self.authentication_policy:\n            self.authentication_policy = policies.AsyncBearerTokenCredentialPolicy(self.credential, *self.credential_scopes, **kwargs)\n",
        "summary": "The `WebSiteManagementClientConfiguration` class extends Azure's `Configuration` to set up the necessary parameters for managing web sites in Azure, including authentication and policy configurations. It ensures that required parameters like `credential` and `subscription_id` are provided and initializes various policies for handling HTTP requests and responses."
    },
    {
        "code": "import django.http\n\nimport unittest.mock\n\nfrom .. import middleware\n\n\ndef get_response(req):\n    \n    return django.http.HttpResponse()\n\n\ndef test_leaves_remote_addr_alone_if_no_real_ip():\n    remote_addr = object()\n    request = unittest.mock.MagicMock()\n    request.META = {\"REMOTE_ADDR\": remote_addr}\n\n    middleware.XRealIPMiddleware(get_response)(request)\n\n    assert request.META[\"REMOTE_ADDR\"] is remote_addr\n\n\ndef test_switches_out_x_real_ip_if_available():\n    remote_addr = object()\n    x_real_ip = object()\n\n    request = unittest.mock.MagicMock()\n    request.META = {\"REMOTE_ADDR\": remote_addr, \"HTTP_X_REAL_IP\": x_real_ip}\n\n    middleware.XRealIPMiddleware(get_response)(request)\n\n    assert request.META[\"REMOTE_ADDR\"] is x_real_ip\n    assert request.META[\"HTTP_X_REAL_IP\"] is x_real_ip\n",
        "summary": "The provided Python code defines a simple HTTP response function and tests for an X-Real-IP middleware. The middleware checks if the `HTTP_X_REAL_IP` header is present in the request; if so, it updates the `REMOTE_ADDR` with this value while preserving the original `X-Real-IP`. If no `HTTP_X_REAL_IP` is present, it leaves the `REMOTE_ADDR` unchanged. The tests verify that the middleware behaves as expected in both scenarios."
    },
    {
        "code": "import time\n\nimport RPi.GPIO as GPIO\n\n\nGPIO.setmode(GPIO.BCM)\nGPIO.setup(21, GPIO.OUT)\nGPIO.output(21, GPIO.LOW)\n\ntime.sleep(3.00)\n\nGPIO.output(21, GPIO.HIGH)\nGPIO.cleanup()\n\n",
        "summary": "The Python script initializes the Raspberry Pi's GPIO pin 21 in BCM mode as an output, sets it to LOW for three seconds, then switches it to HIGH and cleans up the GPIO settings."
    },
    {
        "code": "from direct.directnotify.DirectNotifyGlobal import directNotify\n\n\nclass Notifier:\n    def __init__(self, name):\n        \n        self.notify = directNotify.newCategory(name)\n",
        "summary": "The provided Python code defines a class named `Notifier` that initializes an instance of a notification category using the `directNotify` system. This category is created with a specified name, allowing for organized and categorized logging or messaging within applications that utilize this class."
    },
    {
        "code": "import numpy as np\n\n\ndef train_ml_squarer() -> None:\n    print(\"Training!\")\n\n\ndef square() -> int:\n    \n    return np.random.randint(1, 100)\n\n\nif __name__ == '__main__':\n    train_ml_squarer()",
        "summary": "The provided Python code defines two functions: `train_ml_squarer` which prints \"Training!\", and `square` which returns a random integer between 1 and 99. The script executes the `train_ml_squarer` function when run directly."
    },
    {
        "code": "import arcade\n\n\nSCREEN_WIDTH = 1000\nSCREEN_HEIGHT = 650\nSCREEN_TITLE = \"Platformer\"\n\n\nCHARACTER_SCALING = 1\nTILE_SCALING = 0.5\nCOIN_SCALING = 0.5\nSPRITE_PIXEL_SIZE = 128\nGRID_PIXEL_SIZE = SPRITE_PIXEL_SIZE * TILE_SCALING\n\n\nPLAYER_MOVEMENT_SPEED = 10\nGRAVITY = 1\nPLAYER_JUMP_SPEED = 20\n\n\nclass MyGame(arcade.Window):\n    \n\n    def __init__(self):\n\n        \n        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n\n        \n        self.tile_map = None\n\n        \n        self.scene = None\n\n        \n        self.player_sprite = None\n\n        \n        self.physics_engine = None\n\n        \n        self.camera = None\n\n        \n        self.gui_camera = None\n\n        \n        self.score = 0\n\n        \n        self.collect_coin_sound = arcade.load_sound(\":resources:sounds/coin1.wav\")\n        self.jump_sound = arcade.load_sound(\":resources:sounds/jump1.wav\")\n\n        arcade.set_background_color(arcade.csscolor.CORNFLOWER_BLUE)\n\n    def setup(self):\n        \n\n        \n        self.camera = arcade.Camera(self.width, self.height)\n        self.gui_camera = arcade.Camera(self.width, self.height)\n\n        \n        map_name = \":resources:tiled_maps/map.json\"\n\n        \n        \n        \n        layer_options = {\n            \"Platforms\": {\n                \"use_spatial_hash\": True,\n            },\n        }\n\n        \n        self.tile_map = arcade.load_tilemap(map_name, TILE_SCALING, layer_options)\n\n        \n        \n        self.scene = arcade.Scene.from_tilemap(self.tile_map)\n\n        \n        self.score = 0\n\n        \n        image_source = \":resources:images/animated_characters/female_adventurer/femaleAdventurer_idle.png\"\n        self.player_sprite = arcade.Sprite(image_source, CHARACTER_SCALING)\n        self.player_sprite.center_x = 128\n        self.player_sprite.center_y = 128\n        self.scene.add_sprite(\"Player\", self.player_sprite)\n\n        \n        \n        if self.tile_map.background_color:\n            arcade.set_background_color(self.tile_map.background_color)\n\n        \n        self.physics_engine = arcade.PhysicsEnginePlatformer(\n            self.player_sprite, gravity_constant=GRAVITY, walls=self.scene[\"Platforms\"]\n        )\n\n    def on_draw(self):\n        \n\n        \n        arcade.start_render()\n\n        \n        self.camera.use()\n\n        \n        self.scene.draw()\n\n        \n        self.gui_camera.use()\n\n        \n        score_text = f\"Score: {self.score}\"\n        arcade.draw_text(\n            score_text,\n            10,\n            10,\n            arcade.csscolor.WHITE,\n            18,\n        )\n\n    def on_key_press(self, key, modifiers):\n        \n\n        if key == arcade.key.UP or key == arcade.key.W:\n            if self.physics_engine.can_jump():\n                self.player_sprite.change_y = PLAYER_JUMP_SPEED\n                arcade.play_sound(self.jump_sound)\n        elif key == arcade.key.LEFT or key == arcade.key.A:\n            self.player_sprite.change_x = -PLAYER_MOVEMENT_SPEED\n        elif key == arcade.key.RIGHT or key == arcade.key.D:\n            self.player_sprite.change_x = PLAYER_MOVEMENT_SPEED\n\n    def on_key_release(self, key, modifiers):\n        \n\n        if key == arcade.key.LEFT or key == arcade.key.A:\n            self.player_sprite.change_x = 0\n        elif key == arcade.key.RIGHT or key == arcade.key.D:\n            self.player_sprite.change_x = 0\n\n    def center_camera_to_player(self):\n        screen_center_x = self.player_sprite.center_x - (self.camera.viewport_width / 2)\n        screen_center_y = self.player_sprite.center_y - (\n            self.camera.viewport_height / 2\n        )\n        if screen_center_x < 0:\n            screen_center_x = 0\n        if screen_center_y < 0:\n            screen_center_y = 0\n        player_centered = screen_center_x, screen_center_y\n\n        self.camera.move_to(player_centered)\n\n    def on_update(self, delta_time):\n        \n\n        \n        self.physics_engine.update()\n\n        \n        coin_hit_list = arcade.check_for_collision_with_list(\n            self.player_sprite, self.scene[\"Coins\"]\n        )\n\n        \n        for coin in coin_hit_list:\n            \n            coin.remove_from_sprite_lists()\n            \n            arcade.play_sound(self.collect_coin_sound)\n            \n            self.score += 1\n\n        \n        self.center_camera_to_player()\n\n\ndef main():\n    \n    window = MyGame()\n    window.setup()\n    arcade.run()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python code defines a platformer game using the Arcade library, featuring player movement, collision detection with platforms and coins, scoring, and camera following the player. The game is initialized with a tilemap and includes sound effects for jumping and collecting coins."
    },
    {
        "code": "import serial\nimport sys\nimport struct\nimport pprint\nimport argparse\nimport code\n\npp = pprint.PrettyPrinter()\n\n\nclass ConsoleUI:\n    def opStart(self, name):\n        sys.stdout.write(name.ljust(40))\n\n    def opProgress(self, progress, total=-1):\n        if (total >= 0):\n            prstr = \"0x%04x / 0x%04x\" % (progress, total)\n        else:\n            prstr = \"0x%04x\" % (progress)\n\n        sys.stdout.write(prstr.ljust(20))\n        sys.stdout.write('\\x08' * 20)\n        sys.stdout.flush()\n\n    def opEnd(self, result):\n        sys.stdout.write(result.ljust(20))\n        sys.stdout.write(\"\\n\")\n\n\nclass XFlash:\n    def __init__(self, serialport):\n        self.serial = serial.Serial(serialport, baudrate=115200)\n\n    def __del__(self):\n        try:\n            self.serial.close()\n            del self.serial\n        except:\n            pass\n\n    def cmd(self, cmd, argA=0, argB=0):\n        buffer = struct.pack(\"<LL\", argA, argB)\n\n        self.serial.write(bytes([cmd]))\n        self.serial.write(buffer)\n        self.serial.flush()\n\n    def flashPowerOn(self):\n        self.cmd(0x10)\n\n    def flashShutdown(self):\n        self.cmd(0x11)\n\n    def update(self):\n        try:\n            self.cmd(0xF0)\n        except:\n            pass\n\n    def flashInit(self):\n        self.cmd(0x03)\n\n        buffer = self.serial.read(4)\n        return struct.unpack(\"<L\", buffer)[0]\n\n    def flashDeInit(self):\n        self.cmd(0x04)\n\n    def flashStatus(self):\n        self.cmd(0x05)\n\n        buffer = self.serial.read(2)\n        return struct.unpack(\"<H\", buffer)[0]\n\n    def flashErase(self, block):\n        self.cmd(0x06, block)\n        \n\n    def flashReadBlock(self, block):\n        self.cmd(0x01, block, 528 * 32)\n\n        \n        buffer = self.serial.read(528 * 32)\n\n        status = self.flashStatus()\n        return (status, buffer)\n\n    def flashWriteBlock(self, block, buffer):\n        self.cmd(0x02, block, len(buffer))\n\n        self.serial.write(buffer)\n\n        return self.flashStatus()\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef main(argv):\n    parser = argparse.ArgumentParser(description='XBox 360 NAND Flasher')\n    parser.add_argument('port', metavar='port', type=str,\n                        help='serial port for comms (e.g. COM5 or /dev/ttyUSB0)')\n\n    subparsers = parser.add_subparsers(title='Operations', dest='action')\n\n    parser_read = subparsers.add_parser('read', help='Dumps an image from the NAND')\n    parser_read.add_argument('file', nargs=1, type=argparse.FileType('wb'), help='The file to dump the NAND to')\n    parser_read.add_argument('start', nargs='?', metavar='start', action='store', type=int, default=0,\n                             help='The block to start the action from')\n    parser_read.add_argument('end', nargs='?', metavar='end', action='store', type=int, default=0x400,\n                             help='The count of blocks to perform the action to')\n\n    parser_write = subparsers.add_parser('write', help='Writes an image into the NAND')\n    parser_write.add_argument('file', nargs=1, type=argparse.FileType('rb'), help='The image file to write to the NAND')\n    parser_write.add_argument('start', nargs='?', metavar='start', action='store', type=int, default=0,\n                              help='The block to start the action from')\n    parser_write.add_argument('end', nargs='?', metavar='end', action='store', type=int, default=0x400,\n                              help='The count of blocks to perform the action to')\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    arguments = parser.parse_args(argv[1:])\n\n    ui = ConsoleUI()\n\n    xf = XFlash(arguments.port)\n\n    if arguments.action in ('erase', 'write', 'read'):\n        try:\n            flash_config = xf.flashInit()\n            print(\"FlashConfig: 0x%08x\" % (flash_config))\n            if flash_config <= 0:\n                raise Exception(\"FlashConfig invalid!\")\n        except Exception as e:\n            print(\"Error!\", e)\n            xf.flashDeInit()\n            return 1\n\n    try:\n        if arguments.action == 'erase':\n            \n            \n            start = arguments.start\n            end = arguments.end\n\n            ui.opStart('Erase')\n\n            ui.opProgress(0, end)\n            for b in range(start, end):\n                status = xf.flashErase(b)\n                ui.opProgress(b + 1, end)\n\n            ui.opEnd('0x%04x blocks OK' % (end))\n\n        if arguments.action == 'read':\n            \n            \n            start = arguments.start\n            end = arguments.end\n\n            ui.opStart('Read')\n\n            ui.opProgress(0, end)\n            for b in range(start, end):\n                (status, buffer) = xf.flashReadBlock(b)\n                ui.opProgress(b + 1, end)\n                arguments.file[0].write(buffer)\n\n        if arguments.action == 'write':\n            \n            \n\n            start = arguments.start\n            end = arguments.end\n            blocksize = 528 * 32\n\n            ui.opStart('Write')\n\n            ui.opProgress(0, end)\n            for b in range(start, end):\n                buffer = arguments.file[0].read(blocksize)\n\n                if len(buffer) < blocksize:\n                    buffer += ('\\xFF' * (blocksize - len(buffer)))\n\n                status = xf.flashWriteBlock(b, buffer)\n                ui.opProgress(b + 1, end)\n        \n        \n        \n        \n        \n        \n        \n        \n        \n    except Exception as e:\n        raise e\n    finally:\n        xf.flashDeInit()\n        return 0\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n\n",
        "summary": "This Python script provides a command-line interface for interacting with an Xbox 360 NAND flash memory using a serial connection. It includes functionality to initialize the flash, read and write blocks, erase blocks, and handle user input through a console UI. The script uses the `argparse` module for parsing command-line arguments and the `serial` module for communication over a serial port."
    },
    {
        "code": "from CIM16.IEC61968.Common.ActivityRecord import ActivityRecord\n\nclass ComplianceEvent(ActivityRecord):\n    \n\n    def __init__(self, deadline='', *args, **kw_args):\n        \n        \n        self.deadline = deadline\n\n        super(ComplianceEvent, self).__init__(*args, **kw_args)\n\n    _attrs = [\"deadline\"]\n    _attr_types = {\"deadline\": str}\n    _defaults = {\"deadline\": ''}\n    _enums = {}\n    _refs = []\n    _many_refs = []\n\n",
        "summary": "The `ComplianceEvent` class extends the `ActivityRecord` class from the CIM16.IEC61968.Common.ActivityRecord module, adding a `deadline` attribute to represent the due date for compliance events. This class includes initialization for the `deadline`, along with default and type definitions for this attribute."
    },
    {
        "code": "import os\nimport os.path\nimport sys\n\nimport pytest  \n\nfrom mypy.test.config import test_temp_dir\nfrom mypy.test.data import DataDrivenTestCase, DataSuite\nfrom mypy.test.helpers import assert_string_arrays_equal\nfrom mypy.util import try_find_python2_interpreter\nfrom mypy import api\n\nthis_file_dir = os.path.dirname(os.path.realpath(__file__))\nprefix = os.path.dirname(this_file_dir)\ninipath = os.path.abspath(os.path.join(prefix, 'test'))\n\n\ntest_data_prefix = os.path.join(prefix, 'test', 'test-data')\n\n\nclass SQLDataSuite(DataSuite):\n    files = ['sqlalchemy-basics.test',\n             'sqlalchemy-sql-elements.test',\n             'sqlalchemy-sql-sqltypes.test',\n             'sqlalchemy-sql-selectable.test',\n             'sqlalchemy-sql-schema.test',\n             'sqlalchemy-plugin-features.test',\n             'sqlalchemy-plugin-query.test']\n    data_prefix = test_data_prefix\n\n    def run_case(self, testcase: DataDrivenTestCase) -> None:\n\n        assert testcase.old_cwd is not None, \"test was not properly set up\"\n        mypy_cmdline = [\n            '--show-traceback',\n            '--no-silence-site-packages',\n            '--config-file={}/sqlalchemy.ini'.format(inipath),\n        ]\n        py2 = testcase.name.lower().endswith('python2')\n        if py2:\n            if try_find_python2_interpreter() is None:\n                pytest.skip()\n                return\n            mypy_cmdline.append('--py2')\n        else:\n            mypy_cmdline.append('--python-version={}'.format('.'.join(map(str,\n                                                                          sys.version_info[:2]))))\n\n        \n        program_path = os.path.join(test_temp_dir, 'main.py')\n        mypy_cmdline.append(program_path)\n        with open(program_path, 'w') as file:\n            for s in testcase.input:\n                file.write('{}\\n'.format(s))\n        output = []\n        \n        out, err, returncode = api.run(mypy_cmdline)\n        \n        for line in (out + err).splitlines():\n            if line.startswith(test_temp_dir + os.sep):\n                output.append(line[len(test_temp_dir + os.sep):].rstrip(\"\\r\\n\").replace('.py',\n                                                                                        ''))\n            else:\n                output.append(line.rstrip(\"\\r\\n\"))\n        \n        os.remove(program_path)\n        assert_string_arrays_equal(testcase.output, output,\n                                   'Invalid output ({}, line {})'.format(\n                                   testcase.file, testcase.line))\n",
        "summary": "This Python script defines a test suite for SQLAlchemy using the `mypy` static type checker. It dynamically generates and runs tests by writing SQL code to temporary files, invoking `mypy` with appropriate configuration options based on the test case requirements, and comparing the output against expected results."
    },
    {
        "code": "from __future__ import print_function\nimport argparse\nimport hashlib\nimport os\nimport sys\nfrom gnupg import GPG\nfrom jinja2 import Template\n\n\nclass Version:\n    def __init__(self, version):\n        self.version = version\n        parts = version.split('.')\n        if len(parts) > 2:\n            self.major = \".\".join(parts[:2])\n            self.patch = version\n        else:\n            self.major = version\n            self.patch = None\n\n    def __str__(self):\n        return self.version\n\n\ndef _main():\n    descr = 'Generate Gerrit release announcement email text'\n    parser = argparse.ArgumentParser(\n        description=descr,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('-v', '--version', dest='version',\n                        required=True,\n                        help='gerrit version to release')\n    parser.add_argument('-p', '--previous', dest='previous',\n                        help='previous gerrit version (optional)')\n    parser.add_argument('-s', '--summary', dest='summary',\n                        help='summary of the release content (optional)')\n    options = parser.parse_args()\n\n    summary = options.summary\n    if summary and not summary.endswith(\".\"):\n        summary = summary + \".\"\n\n    data = {\n         \"version\": Version(options.version),\n         \"previous\": options.previous,\n         \"summary\": summary\n    }\n\n    war = os.path.join(\n        os.path.expanduser(\"~/.m2/repository/com/google/gerrit/gerrit-war/\"),\n        \"%(version)s/gerrit-war-%(version)s.war\" % data)\n    if not os.path.isfile(war):\n        print(\"Could not find war file for Gerrit %s in local Maven repository\"\n              % data[\"version\"], file=sys.stderr)\n        sys.exit(1)\n\n    md5 = hashlib.md5()\n    sha1 = hashlib.sha1()\n    sha256 = hashlib.sha256()\n    BUF_SIZE = 65536  \n    with open(war, 'rb') as f:\n        while True:\n            d = f.read(BUF_SIZE)\n            if not d:\n                break\n            md5.update(d)\n            sha1.update(d)\n            sha256.update(d)\n\n    data[\"sha1\"] = sha1.hexdigest()\n    data[\"sha256\"] = sha256.hexdigest()\n    data[\"md5\"] = md5.hexdigest()\n\n    template = Template(open(\"tools/release-announcement-template.txt\").read())\n    output = template.render(data=data)\n\n    filename = \"release-announcement-gerrit-%s.txt\" % data[\"version\"]\n    with open(filename, \"w\") as f:\n        f.write(output)\n\n    gpghome = os.path.abspath(os.path.expanduser(\"~/.gnupg\"))\n    if not os.path.isdir(gpghome):\n        print(\"Skipping signing due to missing gnupg home folder\")\n    else:\n        try:\n            gpg = GPG(homedir=gpghome)\n        except TypeError:\n            gpg = GPG(gnupghome=gpghome)\n        signed = gpg.sign(output)\n        filename = filename + \".asc\"\n        with open(filename, \"w\") as f:\n            f.write(str(signed))\n\n\nif __name__ == \"__main__\":\n    _main()\n",
        "summary": "This Python script generates a Gerrit release announcement email text by parsing command-line arguments for version details and an optional summary. It checks for the existence of a WAR file in the local Maven repository, calculates its MD5, SHA-1, and SHA-256 hashes, and then uses a Jinja2 template to render the email content. If GPG is configured, it also signs the announcement text."
    },
    {
        "code": "import numpy as np\n\n\ndef _main():\n    \n    n = 3\n    x = np.arange(20, dtype=np.float64)\n\n    \n    avg = np.zeros(len(x) - n + 1)\n    std = np.zeros(len(x) - n + 1)\n    for i in range(len(avg)):\n        avg[i] = np.mean(x[i:i+n])\n        std[i] = np.std(x[i:i+n])\n\n    print('AVG')\n    print('\\n'.join(str(x) for x in avg))\n    print('STD:')\n    print('\\n'.join(str(x) for x in std))\n\n    \n    squares = np.square(x)\n    sum_of_squares = np.convolve(squares, np.ones(n, dtype=int), 'valid')\n    var_fast = (sum_of_squares / n) - np.square(avg)\n    std_fast = np.sqrt(var_fast)\n\n    print('STD FAST:')\n    print('\\n'.join(str(x) for x in std_fast))\n\n\nif __name__ == '__main__':\n    _main()\n",
        "summary": "The provided Python code calculates the moving average and standard deviation of a numpy array `x` using both a straightforward approach with nested loops and a more efficient method utilizing convolution. The results are printed for comparison, demonstrating two different ways to compute these statistical measures over a sliding window of size `n`."
    },
    {
        "code": "import torch\r\nfrom torch import nn\r\nfrom torch.nn import CrossEntropyLoss, MSELoss\r\n\r\nfrom transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\r\nfrom transformers.modeling_bert import (\r\n    BERT_INPUTS_DOCSTRING,\r\n    BERT_START_DOCSTRING,\r\n    BertEmbeddings,\r\n    BertLayer,\r\n    BertPooler,\r\n    BertPreTrainedModel,\r\n)\r\n\r\n\r\ndef entropy(x):\r\n    \r\n    exp_x = torch.exp(x)\r\n    A = torch.sum(exp_x, dim=1)  \n    B = torch.sum(x * exp_x, dim=1)  \n    return torch.log(A) - B / A\r\n\r\n\r\nclass DeeBertEncoder(nn.Module):\r\n    def __init__(self, config):\r\n        super().__init__()\r\n        self.output_attentions = config.output_attentions\r\n        self.output_hidden_states = config.output_hidden_states\r\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\r\n        self.highway = nn.ModuleList([BertHighway(config) for _ in range(config.num_hidden_layers)])\r\n\r\n        self.early_exit_entropy = [-1 for _ in range(config.num_hidden_layers)]\r\n\r\n    def set_early_exit_entropy(self, x):\r\n        if (type(x) is float) or (type(x) is int):\r\n            for i in range(len(self.early_exit_entropy)):\r\n                self.early_exit_entropy[i] = x\r\n        else:\r\n            self.early_exit_entropy = x\r\n\r\n    def init_highway_pooler(self, pooler):\r\n        loaded_model = pooler.state_dict()\r\n        for highway in self.highway:\r\n            for name, param in highway.pooler.state_dict().items():\r\n                param.copy_(loaded_model[name])\r\n\r\n    def forward(\r\n        self,\r\n        hidden_states,\r\n        attention_mask=None,\r\n        head_mask=None,\r\n        encoder_hidden_states=None,\r\n        encoder_attention_mask=None,\r\n    ):\r\n        all_hidden_states = ()\r\n        all_attentions = ()\r\n        all_highway_exits = ()\r\n        for i, layer_module in enumerate(self.layer):\r\n            if self.output_hidden_states:\r\n                all_hidden_states = all_hidden_states + (hidden_states,)\r\n\r\n            layer_outputs = layer_module(\r\n                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\r\n            )\r\n            hidden_states = layer_outputs[0]\r\n\r\n            if self.output_attentions:\r\n                all_attentions = all_attentions + (layer_outputs[1],)\r\n\r\n            current_outputs = (hidden_states,)\r\n            if self.output_hidden_states:\r\n                current_outputs = current_outputs + (all_hidden_states,)\r\n            if self.output_attentions:\r\n                current_outputs = current_outputs + (all_attentions,)\r\n\r\n            highway_exit = self.highway[i](current_outputs)\r\n            \n\r\n            if not self.training:\r\n                highway_logits = highway_exit[0]\r\n                highway_entropy = entropy(highway_logits)\r\n                highway_exit = highway_exit + (highway_entropy,)  \n                all_highway_exits = all_highway_exits + (highway_exit,)\r\n\r\n                if highway_entropy < self.early_exit_entropy[i]:\r\n                    new_output = (highway_logits,) + current_outputs[1:] + (all_highway_exits,)\r\n                    raise HighwayException(new_output, i + 1)\r\n            else:\r\n                all_highway_exits = all_highway_exits + (highway_exit,)\r\n\r\n        \n        if self.output_hidden_states:\r\n            all_hidden_states = all_hidden_states + (hidden_states,)\r\n\r\n        outputs = (hidden_states,)\r\n        if self.output_hidden_states:\r\n            outputs = outputs + (all_hidden_states,)\r\n        if self.output_attentions:\r\n            outputs = outputs + (all_attentions,)\r\n\r\n        outputs = outputs + (all_highway_exits,)\r\n        return outputs  \n\r\n\r\n@add_start_docstrings(\r\n    \"The Bert Model transformer with early exiting (DeeBERT). \",\r\n    BERT_START_DOCSTRING,\r\n)\r\nclass DeeBertModel(BertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super().__init__(config)\r\n        self.config = config\r\n\r\n        self.embeddings = BertEmbeddings(config)\r\n        self.encoder = DeeBertEncoder(config)\r\n        self.pooler = BertPooler(config)\r\n\r\n        self.init_weights()\r\n\r\n    def init_highway_pooler(self):\r\n        self.encoder.init_highway_pooler(self.pooler)\r\n\r\n    def get_input_embeddings(self):\r\n        return self.embeddings.word_embeddings\r\n\r\n    def set_input_embeddings(self, value):\r\n        self.embeddings.word_embeddings = value\r\n\r\n    def _prune_heads(self, heads_to_prune):\r\n        \r\n        for layer, heads in heads_to_prune.items():\r\n            self.encoder.layer[layer].attention.prune_heads(heads)\r\n\r\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\r\n    def forward(\r\n        self,\r\n        input_ids=None,\r\n        attention_mask=None,\r\n        token_type_ids=None,\r\n        position_ids=None,\r\n        head_mask=None,\r\n        inputs_embeds=None,\r\n        encoder_hidden_states=None,\r\n        encoder_attention_mask=None,\r\n    ):\r\n        r\r\n        if input_ids is not None and inputs_embeds is not None:\r\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\r\n        elif input_ids is not None:\r\n            input_shape = input_ids.size()\r\n        elif inputs_embeds is not None:\r\n            input_shape = inputs_embeds.size()[:-1]\r\n        else:\r\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\r\n\r\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\r\n\r\n        if attention_mask is None:\r\n            attention_mask = torch.ones(input_shape, device=device)\r\n        if encoder_attention_mask is None:\r\n            encoder_attention_mask = torch.ones(input_shape, device=device)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\r\n\r\n        \n        \n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\r\n\r\n        \n        \n        if encoder_attention_mask.dim() == 3:\r\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\r\n        if encoder_attention_mask.dim() == 2:\r\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\r\n\r\n        encoder_extended_attention_mask = encoder_extended_attention_mask.to(\r\n            dtype=next(self.parameters()).dtype\r\n        )  \n        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\r\n\r\n        \n        \n        \n        \n        \n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\r\n\r\n        embedding_output = self.embeddings(\r\n            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\r\n        )\r\n        encoder_outputs = self.encoder(\r\n            embedding_output,\r\n            attention_mask=extended_attention_mask,\r\n            head_mask=head_mask,\r\n            encoder_hidden_states=encoder_hidden_states,\r\n            encoder_attention_mask=encoder_extended_attention_mask,\r\n        )\r\n        sequence_output = encoder_outputs[0]\r\n        pooled_output = self.pooler(sequence_output)\r\n\r\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\r\n            1:\r\n        ]  \n        return outputs  \n\r\n\r\nclass HighwayException(Exception):\r\n    def __init__(self, message, exit_layer):\r\n        self.message = message\r\n        self.exit_layer = exit_layer  \n\r\n\r\nclass BertHighway(nn.Module):\r\n    \r\n\r\n    def __init__(self, config):\r\n        super().__init__()\r\n        self.pooler = BertPooler(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n    def forward(self, encoder_outputs):\r\n        \n        pooler_input = encoder_outputs[0]\r\n        pooler_output = self.pooler(pooler_input)\r\n        \n\r\n        \n        bmodel_output = (pooler_input, pooler_output) + encoder_outputs[1:]\r\n        \n\r\n        \n        pooled_output = bmodel_output[1]\r\n\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n\r\n        return logits, pooled_output\r\n\r\n\r\n@add_start_docstrings(\r\n    ,\r\n    BERT_START_DOCSTRING,\r\n)\r\nclass DeeBertForSequenceClassification(BertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super().__init__(config)\r\n        self.num_labels = config.num_labels\r\n        self.num_layers = config.num_hidden_layers\r\n\r\n        self.bert = DeeBertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\r\n    def forward(\r\n        self,\r\n        input_ids=None,\r\n        attention_mask=None,\r\n        token_type_ids=None,\r\n        position_ids=None,\r\n        head_mask=None,\r\n        inputs_embeds=None,\r\n        labels=None,\r\n        output_layer=-1,\r\n        train_highway=False,\r\n    ):\r\n        r\r\n\r\n        exit_layer = self.num_layers\r\n        try:\r\n            outputs = self.bert(\r\n                input_ids,\r\n                attention_mask=attention_mask,\r\n                token_type_ids=token_type_ids,\r\n                position_ids=position_ids,\r\n                head_mask=head_mask,\r\n                inputs_embeds=inputs_embeds,\r\n            )\r\n            \n\r\n            pooled_output = outputs[1]\r\n\r\n            pooled_output = self.dropout(pooled_output)\r\n            logits = self.classifier(pooled_output)\r\n            outputs = (logits,) + outputs[2:]  \n        except HighwayException as e:\r\n            outputs = e.message\r\n            exit_layer = e.exit_layer\r\n            logits = outputs[0]\r\n\r\n        if not self.training:\r\n            original_entropy = entropy(logits)\r\n            highway_entropy = []\r\n            highway_logits_all = []\r\n        if labels is not None:\r\n            if self.num_labels == 1:\r\n                \n                loss_fct = MSELoss()\r\n                loss = loss_fct(logits.view(-1), labels.view(-1))\r\n            else:\r\n                loss_fct = CrossEntropyLoss()\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n\r\n            \n            highway_losses = []\r\n            for highway_exit in outputs[-1]:\r\n                highway_logits = highway_exit[0]\r\n                if not self.training:\r\n                    highway_logits_all.append(highway_logits)\r\n                    highway_entropy.append(highway_exit[2])\r\n                if self.num_labels == 1:\r\n                    \n                    loss_fct = MSELoss()\r\n                    highway_loss = loss_fct(highway_logits.view(-1), labels.view(-1))\r\n                else:\r\n                    loss_fct = CrossEntropyLoss()\r\n                    highway_loss = loss_fct(highway_logits.view(-1, self.num_labels), labels.view(-1))\r\n                highway_losses.append(highway_loss)\r\n\r\n            if train_highway:\r\n                outputs = (sum(highway_losses[:-1]),) + outputs\r\n                \n            else:\r\n                outputs = (loss,) + outputs\r\n        if not self.training:\r\n            outputs = outputs + ((original_entropy, highway_entropy), exit_layer)\r\n            if output_layer >= 0:\r\n                outputs = (\r\n                    (outputs[0],) + (highway_logits_all[output_layer],) + outputs[2:]\r\n                )  \n\r\n        return outputs  \n",
        "summary": "This code defines a PyTorch model for sequence classification using the DeeBert architecture, which incorporates highway networks to improve generalization. Here's a breakdown of the key components:\n\n1. **Highway Networks**: The `BertHighway` class implements a highway network with a pooler layer and a classifier. It takes encoder outputs as input and produces logits.\n\n2. **DeeBertModel**: This is the main model class that extends `BertPreTrainedModel`. It includes an embedding layer, an encoder (based on BERT), and a highway network. The encoder processes the input sequence and passes it through the highway network to produce final outputs.\n\n3. **HighwayException**: A custom exception class used to handle early exits from the model during training or evaluation.\n\n4. **DeeBertForSequenceClassification**: This is a subclass of `BertPreTrainedModel` designed for sequence classification tasks. It includes an instance of `DeeBertModel`, dropout layers, and a classifier. The forward method handles both training and inference phases, including early exits when exceptions are caught.\n\n5. **Entropy Calculation**: Entropy is calculated to measure the uncertainty of the model's predictions, which can be useful for evaluating the model's performance during training.\n\n6. **Loss Functions**: Depending on whether the number of labels is 1 (regression) or more than 1 (classification), different loss functions are used: Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n\n7. **Early Exits**: During training, if `train_highway` is set to True, the model will continue to train even after an early exit occurs. Otherwise, it will stop training at the point of the first early exit.\n\n8. **Output Handling**: The forward method returns various outputs depending on whether the model is in training or evaluation mode and whether specific layers are requested for output.\n\nThis architecture allows for flexible control over when to terminate training based on the model's performance, potentially leading to better generalization on unseen data."
    },
    {
        "code": "import kulado\nimport kulado.runtime\nimport warnings\n\nfrom ... import tables, version\n\n\nclass ClusterRoleBindingList(kulado.CustomResource):\n    \n    def __init__(self, resource_name, opts=None, items=None, metadata=None, __name__=None, __opts__=None):\n        if __name__ is not None:\n            warnings.warn(\"explicit use of __name__ is deprecated\", DeprecationWarning)\n            resource_name = __name__\n        if __opts__ is not None:\n            warnings.warn(\"explicit use of __opts__ is deprecated, use 'opts' instead\", DeprecationWarning)\n            opts = __opts__\n        if not resource_name:\n            raise TypeError('Missing resource name argument (for URN creation)')\n        if not isinstance(resource_name, str):\n            raise TypeError('Expected resource name to be a string')\n        if opts and not isinstance(opts, kulado.ResourceOptions):\n            raise TypeError('Expected resource options to be a ResourceOptions instance')\n\n        __props__ = dict()\n\n        __props__['apiVersion'] = 'rbac.authorization.k8s.io/v1beta1'\n        __props__['kind'] = 'ClusterRoleBindingList'\n        if items is None:\n            raise TypeError('Missing required property items')\n        __props__['items'] = items\n        __props__['metadata'] = metadata\n\n        if opts is None:\n            opts = kulado.ResourceOptions()\n        if opts.version is None:\n            opts.version = version.get_version()\n\n        super(ClusterRoleBindingList, self).__init__(\n            \"kubernetes:rbac.authorization.k8s.io/v1beta1:ClusterRoleBindingList\",\n            resource_name,\n            __props__,\n            opts)\n\n    def translate_output_property(self, prop: str) -> str:\n        return tables._CASING_FORWARD_TABLE.get(prop) or prop\n\n    def translate_input_property(self, prop: str) -> str:\n        return tables._CASING_BACKWARD_TABLE.get(prop) or prop\n",
        "summary": "The provided Python code defines a custom resource class `ClusterRoleBindingList` that extends `kulado.CustomResource`, used for managing Kubernetes ClusterRoleBindingList resources. It includes methods to handle property translation and validation, ensuring proper initialization and usage of the resource with appropriate warnings for deprecated parameters."
    },
    {
        "code": "import logging\nfrom .base import ApplicationWrapper\nfrom ..configuration.utils import coerce_config\nfrom ..support.converters import asbool\n\nlog = logging.getLogger(__name__)\n\n\nclass IdentityApplicationWrapper(ApplicationWrapper):\n    \n    def __init__(self, handler, config):\n        super(IdentityApplicationWrapper, self).__init__(handler, config)\n\n        options = {\n            'enabled': True,\n            'allow_missing_user': True,\n            'authmetadata': config.get('sa_auth',  {}).get('authmetadata'),\n        }\n        options.update(coerce_config(config, 'identity.',  {\n            'enabled': asbool,\n            'allow_missing_user': asbool\n        }))\n\n        self.enabled = options['enabled'] and options['authmetadata'] is not None\n        self.options = options\n        self.tgmdprovider = options['authmetadata']\n        log.debug('Identity enabled: %s -> %s', self.enabled, self.options)\n\n    @property\n    def injected(self):\n        return self.enabled\n\n    def __call__(self, controller, environ, context):\n        identity = environ.get('repoze.who.identity')\n        if identity is None:\n            context.request.identity = None\n            return self.next_handler(controller, environ, context)\n\n        req_identity = {}\n\n        \n        userid = identity['repoze.who.userid']\n        if userid is not None:\n            \n            identity['user'] = identity_user = self.tgmdprovider.get_user(identity, userid)\n\n            if identity_user:\n                identity['groups'] = self.tgmdprovider.get_groups(identity, userid)\n                identity['permissions'] = self.tgmdprovider.get_permissions(identity, userid)\n            else:\n                identity['groups'] = identity['permissions'] = []\n\n            req_identity = Identity()\n            req_identity.update(identity)\n            req_identity['repoze.what.userid'] = userid\n\n            if req_identity.get('user') is None and not self.options['allow_missing_user']:\n                req_identity = {}\n\n        \n        context.request.identity = req_identity\n        environ['repoze.who.identity'] = req_identity\n        environ['repoze.what.credentials'] = req_identity\n\n        return self.next_handler(controller, environ, context)\n\n\nclass Identity(dict):\n    \n    def __repr__(self):\n        return '<TurboGears Identity (hidden, dict-like) at %s>' % id(self)\n    __str__ = __repr__",
        "summary": "The provided Python code defines a class `IdentityApplicationWrapper` that extends `ApplicationWrapper`, designed to handle identity management in web applications. It processes configuration settings, retrieves user identities, and injects them into the request context, ensuring proper authentication and authorization based on the configured options. The `Identity` class is used to encapsulate user identity information securely."
    },
    {
        "code": "import sys\n\nfrom migen import *\n\nfrom litex.build.generic_platform import *\nfrom litex.soc.integration.soc_core import *\nfrom litex.soc.integration.soc_sdram import *\nfrom litex.soc.integration.builder import *\nfrom litex.soc.cores.clock import *\nfrom litex.soc.cores import dna, xadc\nfrom litex.soc.cores.uart import *\nfrom litex.soc.integration.cpu_interface import get_csr_header\n\nfrom litedram.modules import MT8KTF51264\nfrom litedram.modules import _TechnologyTimings, _SpeedgradeTimings\nfrom litedram.phy import s7ddrphy\n\nfrom litepcie.phy.s7pciephy import S7PCIEPHY\nfrom litepcie.core import LitePCIeEndpoint, LitePCIeMSI\nfrom litepcie.frontend.dma import LitePCIeDMA\nfrom litepcie.frontend.wishbone import LitePCIeWishboneBridge\n\nfrom litex_boards.platforms import nereid\n\n\n\nclass CRG(Module):\n    def __init__(self, platform, sys_clk_freq):\n        self.clock_domains.cd_sys = ClockDomain()\n        self.clock_domains.cd_sys4x = ClockDomain(reset_less=True)\n        self.clock_domains.cd_clk200 = ClockDomain()\n\n        clk100 = platform.request(\"clk100\")\n\n        self.submodules.pll = pll = S7PLL()\n        pll.register_clkin(clk100, 100e6)\n        pll.create_clkout(self.cd_sys, sys_clk_freq)\n        pll.create_clkout(self.cd_sys4x, 4*sys_clk_freq)\n        pll.create_clkout(self.cd_clk200, 200e6)\n        self.comb += pll.reset.eq(platform.request(\"cpu_reset\"))\n\n        self.submodules.idelayctrl = S7IDELAYCTRL(self.cd_clk200)\n\n\n\nclass NereidSoC(SoCSDRAM):\n    SoCSDRAM.mem_map[\"csr\"] = 0x00000000\n    SoCSDRAM.mem_map[\"rom\"] = 0x20000000\n\n    def __init__(self, platform, with_pcie_uart=True):\n        sys_clk_freq = int(100e6)\n        SoCSDRAM.__init__(self, platform, sys_clk_freq,\n            csr_data_width=32,\n            integrated_rom_size=0x10000,\n            integrated_sram_size=0x10000,\n            integrated_main_ram_size=0x10000, \n            ident=\"Nereid LiteX Test SoC\", ident_version=True,\n            with_uart=not with_pcie_uart)\n\n        \n        self.submodules.crg = CRG(platform, sys_clk_freq)\n        self.add_csr(\"crg\")\n\n        \n        self.submodules.dna = dna.DNA()\n        self.add_csr(\"dna\")\n\n        \n        self.submodules.xadc = xadc.XADC()\n        self.add_csr(\"xadc\")\n\n        \n        if not self.integrated_main_ram_size:\n            self.submodules.ddrphy = s7ddrphy.K7DDRPHY(\n                platform.request(\"ddram\"),\n                sys_clk_freq=sys_clk_freq,\n                iodelay_clk_freq=200e6)\n            sdram_module = MT8KTF51264(sys_clk_freq, \"1:4\", speedgrade=\"800\")\n            self.register_sdram(self.ddrphy,\n                                sdram_module.geom_settings,\n                                sdram_module.timing_settings)\n            self.add_csr(\"ddrphy\")\n\n        \n        \n        self.submodules.pcie_phy = S7PCIEPHY(platform, platform.request(\"pcie_x1\"), bar0_size=0x20000)\n        self.pcie_phy.cd_pcie.clk.attr.add(\"keep\")\n        platform.add_platform_command(\"create_clock -name pcie_clk -period 8 [get_nets pcie_clk]\")\n        platform.add_false_path_constraints(\n            self.crg.cd_sys.clk,\n            self.pcie_phy.cd_pcie.clk)\n        self.add_csr(\"pcie_phy\")\n\n        \n        self.submodules.pcie_endpoint = LitePCIeEndpoint(self.pcie_phy)\n\n        \n        self.submodules.pcie_wishbone = LitePCIeWishboneBridge(self.pcie_endpoint,\n            lambda a: 1, shadow_base=self.shadow_base)\n        self.add_wb_master(self.pcie_wishbone.wishbone)\n\n        \n        self.submodules.pcie_dma = LitePCIeDMA(self.pcie_phy, self.pcie_endpoint,\n            with_buffering=True, buffering_depth=1024, with_loopback=True)\n        self.add_csr(\"pcie_dma\")\n\n        \n        self.submodules.pcie_msi = LitePCIeMSI()\n        self.add_csr(\"pcie_msi\")\n        self.comb += self.pcie_msi.source.connect(self.pcie_phy.msi)\n        self.msis = {\n            \"DMA_WRITER\": self.pcie_dma.writer.irq,\n            \"DMA_READER\": self.pcie_dma.reader.irq\n        }\n        for i, (k, v) in enumerate(sorted(self.msis.items())):\n            self.comb += self.pcie_msi.irqs[i].eq(v)\n            self.add_constant(k + \"_INTERRUPT\", i)\n\n        \n        if with_pcie_uart:\n            class PCIeUART(Module, AutoCSR):\n                def __init__(self, uart):\n                    self.rx_valid = CSRStatus()\n                    self.rx_ready = CSR()\n                    self.rx_data = CSRStatus(8)\n\n                    self.tx_valid = CSR()\n                    self.tx_ready = CSRStatus()\n                    self.tx_data = CSRStorage(8)\n\n                    \n\n                    \n                    self.comb += [\n                        self.rx_valid.status.eq(uart.sink.valid),\n                        uart.sink.ready.eq(self.rx_ready.re),\n                        self.rx_data.status.eq(uart.sink.data),\n                    ]\n\n                    \n                    self.sync += [\n                        If(self.tx_valid.re,\n                            uart.source.valid.eq(1)\n                        ).Elif(uart.source.ready,\n                            uart.source.valid.eq(0)\n                        )\n                    ]\n                    self.comb += [\n                        self.tx_ready.status.eq(~uart.source.valid),\n                        uart.source.data.eq(self.tx_data.storage)\n                    ]\n\n            uart_interface = RS232PHYInterface()\n            self.submodules.uart = UART(uart_interface)\n            self.add_csr(\"uart\")\n            self.add_interrupt(\"uart\")\n            self.submodules.pcie_uart = PCIeUART(uart_interface)\n            self.add_csr(\"pcie_uart\")\n\n        \n        \n        sys_counter = Signal(32)\n        self.sync.sys += sys_counter.eq(sys_counter + 1)\n        rgb = platform.request(\"rgb_led\")\n        self.comb += [\n            rgb.r.eq(1),\n            rgb.g.eq(sys_counter[26]),\n            rgb.b.eq(1),\n        ]\n\n    def generate_software_header(self, filename):\n        csr_header = get_csr_header(self.get_csr_regions(),\n                                    self.get_constants(),\n                                    with_access_functions=False,\n                                    with_shadow_base=False)\n        tools.write_to_file(filename, csr_header)\n\n\n\n\ndef main():\n    platform = nereid.Platform()\n    soc = NereidSoC(platform)\n    builder = Builder(soc, output_dir=\"../build/nereid\", csr_csv=\"../build/nereid/csr.csv\",\n        compile_gateware=not \"no-compile\" in sys.argv[1:])\n    vns = builder.build(build_name=\"nereid\")\n    soc.generate_software_header(\"../software/kernel/csr.h\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The provided Python code defines a custom LiteX SoC (System on Chip) for the Nereid board, incorporating various peripherals and interfaces such as UART, PCIe, DDR memory, and DNA/xADC sensors. It includes a clock generation module (`CRG`), integrates these peripherals into the SoC, and provides methods to generate software headers for accessing the hardware components."
    },
    {
        "code": "import pytest\n\nfrom edipy import fields, validators, exceptions\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.Integer(1, validators=[validators.Range(1, 5)]), '1'),\n    (fields.Integer(1, validators=[validators.MaxValue(3)]), '2'),\n    (fields.Integer(1, validators=[validators.MinValue(1)]), '5'),\n    (fields.String(5, validators=[validators.Regex(r\"[0-9]+\")]), '12345'),\n    (fields.String(12, validators=[validators.Email()]), 'abc@mail.com'),\n])\ndef test_using_validators(fixed_type, data):\n    try:\n        fixed_type.encode(data)\n    except exceptions.ValidationError:\n        pytest.fail(u\"ValidationError should not be thrown\")\n\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.Integer(1, validators=[validators.Range(1, 5)]), '0'),\n    (fields.Integer(1, validators=[validators.Range(1, 5)]), '6'),\n])\ndef test_validate_range(fixed_type, data):\n    with pytest.raises(exceptions.ValidationError):\n        fixed_type.encode(data)\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.Integer(1, validators=[validators.MaxValue(1)]), '2'),\n    (fields.Integer(1, validators=[validators.MaxValue(5)]), '6'),\n])\ndef test_validate_max_value(fixed_type, data):\n    with pytest.raises(exceptions.ValidationError):\n        fixed_type.encode(data)\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.Integer(1, validators=[validators.MinValue(1)]), '0'),\n    (fields.Integer(1, validators=[validators.MinValue(5)]), '4'),\n])\ndef test_validate_min_value(fixed_type, data):\n    with pytest.raises(exceptions.ValidationError):\n        fixed_type.encode(data)\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.String(5, validators=[validators.Regex(r\"[0-9]+\")]), 'a123f'),\n    (fields.String(5, validators=[validators.Regex(r\"\\d\")]), 'abcde'),\n    (fields.String(5, validators=[validators.Regex(r\"[A-Z]{6}\")]), 'ABCDE'),\n])\ndef test_validate_regex(fixed_type, data):\n    with pytest.raises(exceptions.ValidationError):\n        fixed_type.encode(data)\n\n\ndef test_throws_exception_when_regex_is_invalid():\n    with pytest.raises(ValueError):\n        field = fields.String(5, validators=[validators.Regex(\")\")])\n\n\n@pytest.mark.parametrize('fixed_type, data', [\n    (fields.String(11, validators=[validators.Email()]), 'edimail.com'),\n    (fields.String(11, validators=[validators.Email()]), 'edi@mailcom'),\n])\ndef test_validate_email(fixed_type, data):\n    with pytest.raises(exceptions.ValidationError):\n        fixed_type.encode(data)\n\n\n",
        "summary": "The provided Python code uses the `pytest` framework to test various validation scenarios for different types of fields and their associated validators from a library named `edipy`. It includes tests for integer range, maximum value, minimum value, regex patterns, email format, and exception handling when invalid data is passed."
    },
    {
        "code": "import hashlib\nimport io\nimport logging\nimport pandas\nimport sickle\nfrom lxml import etree\nfrom sickle import Sickle\nfrom sickle.models import xml_to_dict\nfrom sickle.oaiexceptions import NoRecordsMatch\nfrom tulflow import process\n\nNS = {\n    \"marc21\": \"http://www.loc.gov/MARC21/slim\",\n    \"oai\": \"http://www.openarchives.org/OAI/2.0/\"\n    }\n\n\ndef oai_to_s3(**kwargs):\n    \n    kwargs[\"harvest_params\"] = {\n        \"metadataPrefix\": kwargs.get(\"metadata_prefix\"),\n        \"from\": kwargs.get(\"harvest_from_date\"),\n        \"until\": kwargs.get(\"harvest_until_date\")\n    }\n    dag_id = kwargs[\"dag\"].dag_id\n    dag_start_date = kwargs[\"timestamp\"]\n\n    oai_sets = generate_oai_sets(**kwargs)\n    all_processed = []\n    sets_with_no_records = []\n    if oai_sets:\n        for oai_set in oai_sets:\n            kwargs[\"harvest_params\"][\"set\"] = oai_set\n            data = harvest_oai(**kwargs)\n            if data == []:\n                sets_with_no_records.append(oai_set)\n                logging.info(\"Skipping processing % set because it has no data.\", oai_set)\n                continue\n            outdir = dag_s3_prefix(dag_id, dag_start_date)\n            processed = process_xml(data, dag_write_string_to_s3, outdir, **kwargs)\n            all_processed.append(processed)\n    else:\n        data = harvest_oai(**kwargs)\n        if data == []:\n            sets_with_no_records.append(oai_set)\n        outdir = dag_s3_prefix(dag_id, dag_start_date)\n        processed = process_xml(data, dag_write_string_to_s3, outdir, **kwargs)\n        all_processed.append(processed)\n    all_updated = sum([set['updated'] for set in all_processed])\n    all_deleted = sum([set['deleted'] for set in all_processed])\n    logging.info(\"Total OAI Records Harvested & Processed: %s\", all_updated)\n    logging.info(\"Total OAI Records Harvest & Marked for Deletion: %s\", all_deleted)\n    logging.info(\"Total sets with no records: %s\", len(sets_with_no_records))\n    logging.info(\"Sets with no records %s\", sets_with_no_records)\n    return {\"updated\": all_updated, \"deleted\": all_deleted, \"sets_with_no_records\": sets_with_no_records}\n\n\ndef generate_oai_sets(**kwargs):\n    \n    all_sets = bool(kwargs.get(\"all_sets\"))\n    included_sets = kwargs.get(\"included_sets\")\n    excluded_sets = kwargs.get(\"excluded_sets\")\n    oai_endpoint = kwargs.get(\"oai_endpoint\")\n\n    if all_sets:\n        logging.info(\"Seeing All Sets Needed.\")\n        return []\n    elif included_sets:\n        logging.info(\"Seeing SetSpec List.\")\n        if not isinstance(included_sets, list):\n            return [included_sets]\n        return included_sets\n    elif excluded_sets:\n        logging.info(\"Seeing Excluded SetSpec List.\")\n        if not isinstance(excluded_sets, list):\n            excluded_sets = [excluded_sets]\n        list_sets = Sickle(oai_endpoint).ListSets()\n        all_sets = [oai_set.xml.find(\"oai:setSpec\", namespaces=NS).text for oai_set in list_sets]\n        remaining_sets = list(set(all_sets) - set(excluded_sets))\n        logging.info(remaining_sets)\n        return remaining_sets\n    return []\n\n\nclass HarvestIterator(sickle.iterator.OAIItemIterator):\n    def next(self):\n        \n        while True:\n            for item in self._items:\n                mapped = self.mapper(item)\n                if self.ignore_deleted and mapped.deleted:\n                    continue\n                if hasattr(mapped, 'metadata') and mapped.metadata == None:\n                    logging.info(\"Skipping record with no metadata: %s\", mapped.header.identifier)\n                    continue\n                return mapped\n            if self.resumption_token and self.resumption_token.token:\n                self._next_response()\n            else:\n                raise StopIteration\n    pass\n\n\nclass HarvestRecord(sickle.models.Record):\n    def get_metadata(self):\n        \n        \n        \n        meta_data = self.xml.find('.//' + self._oai_namespace + 'metadata')\n        if meta_data != None:\n            return xml_to_dict(meta_data.getchildren()[0], strip_ns=self._strip_ns)\n    pass\n\ndef harvest_oai(**kwargs):\n    \n    oai_endpoint = kwargs.get(\"oai_endpoint\")\n    harvest_params = kwargs.get(\"harvest_params\")\n    logging.info(\"Harvesting from %s\", oai_endpoint)\n    logging.info(\"Harvesting %s\", harvest_params)\n    sickle = Sickle(oai_endpoint, retry_status_codes=[500,503], max_retries=3)\n\n    class_mapping = harvest_params.get(\"class_mapping\", {\n        \"ListRecords\": HarvestRecord,\n        })\n    iterator = harvest_params.get(\"iterator\", HarvestIterator)\n    for key in class_mapping:\n        sickle.class_mapping[key] = class_mapping[key]\n\n    sickle.iterator = iterator\n\n    try:\n        return sickle.ListRecords(**harvest_params)\n    except NoRecordsMatch:\n        logging.info(\"No records found.\")\n        return []\n\n\nclass OaiXml:\n    \n    def __init__(self, dag_id, timestamp):\n        etree.register_namespace(\"oai\", \"http://www.openarchives.org/OAI/2.0/\")\n        etree.register_namespace(\"marc21\", \"http://www.loc.gov/MARC21/slim\")\n        self.root = etree.Element(\"{http://www.openarchives.org/OAI/2.0/}collection\")\n        self.root.attrib[\"dag-id\"] = dag_id\n        self.root.attrib[\"dag-timestamp\"] = timestamp\n\n    def append(self, record):\n        self.root.append(record)\n\n    def tostring(self):\n        return etree.tostring(self.root, encoding=\"utf-8\").decode(\"utf-8\")\n\n\ndef process_xml(data, writer, outdir, **kwargs):\n    \n    parser = kwargs.get(\"parser\")\n    records_per_file = kwargs.get(\"records_per_file\")\n    if kwargs.get(\"dag\"):\n        run_id = kwargs.get(\"dag\").dag_id\n    else:\n        run_id = \"no-dag-provided\"\n    if kwargs.get(\"timestamp\"):\n        timestamp = kwargs.get(\"timestamp\")\n    else:\n        timestamp = \"no-timestamp-provided\"\n    if not records_per_file:\n        records_per_file = 1000\n\n    count = deleted_count = 0\n    oai_updates = OaiXml(run_id, timestamp)\n    oai_deletes = OaiXml(run_id, timestamp)\n    logging.info(\"Processing XML\")\n\n    for record in data:\n        record_id = record.header.identifier\n        record = record.xml\n        record.attrib[\"airflow-record-id\"] = record_id\n        if parser:\n            record = parser(record, **kwargs)\n        if record.xpath(\".//oai:header[@status='deleted']\", namespaces=NS):\n            logging.info(\"Added record %s to deleted xml file(s)\", record_id)\n            deleted_count += 1\n            oai_deletes.append(record)\n\n            if deleted_count % int(records_per_file) == 0:\n                writer(oai_deletes.tostring(), outdir + \"/deleted\", **kwargs)\n                oai_deletes = OaiXml(run_id, timestamp)\n        else:\n            logging.info(\"Added record %s to new-updated xml file\", record_id)\n            count += 1\n            oai_updates.append(record)\n            if count % int(records_per_file) == 0:\n                writer(oai_updates.tostring(), outdir + \"/new-updated\", **kwargs)\n                oai_updates = OaiXml(run_id, timestamp)\n    writer(oai_updates.tostring(), outdir + \"/new-updated\", **kwargs)\n    writer(oai_deletes.tostring(), outdir + \"/deleted\", **kwargs)\n    logging.info(\"OAI Records Harvested & Processed: %s\", count)\n    logging.info(\"OAI Records Harvest & Marked for Deletion: %s\", deleted_count)\n    return {\"updated\": count, \"deleted\": deleted_count}\n\n\ndef perform_xml_lookup_with_cache():\n    cache = {}\n\n    def perform_xml_lookup(oai_record, **kwargs):\n        \n\n        if len(cache) == 0:\n            logging.info(\"*** Fetching CSV lookup file from s3 ***\")\n            access_id = kwargs.get(\"access_id\")\n            access_secret = kwargs.get(\"access_secret\")\n            bucket = kwargs.get(\"bucket_name\")\n            lookup_key = kwargs.get(\"lookup_key\")\n            csv_data = process.get_s3_content(bucket, lookup_key, access_id, access_secret)\n            cache[\"value\"] = pandas.read_csv(io.BytesIO(csv_data), header=0)\n\n        lookup_csv = cache[\"value\"]\n\n        for record in oai_record.xpath(\".//marc21:record\", namespaces=NS):\n            record_id = process.get_record_001(record)\n            logging.info(\"Reading in Record %s\", record_id)\n            parent_txt = lookup_csv.loc[lookup_csv.child_id == int(record_id), \"parent_xml\"].values\n            if len(set(parent_txt)) >= 1:\n                logging.info(\"Child XML record found %s\", record_id)\n                for parent_node in parent_txt[0].split(\"||\"):\n                    try:\n                        record.append(etree.fromstring(parent_node))\n                    except etree.XMLSyntaxError as error:\n                        logging.error(\"Problem with string syntax:\")\n                        logging.error(error)\n                        logging.error(parent_node)\n        return oai_record\n\n\n    return perform_xml_lookup\n\n\ndef dag_write_string_to_s3(string, prefix, **kwargs):\n    \n    access_id = kwargs.get(\"access_id\")\n    access_secret = kwargs.get(\"access_secret\")\n    bucket_name = kwargs.get(\"bucket_name\")\n    logging.info(\"Writing to S3 Bucket %s\", bucket_name)\n\n    our_hash = hashlib.md5(string.encode(\"utf-8\")).hexdigest()\n    filename = \"{}/{}\".format(prefix, our_hash)\n    process.generate_s3_object(string, bucket_name, filename, access_id, access_secret)\n\n\ndef write_log(string, prefix, **kwargs):\n    \n    prefix = prefix\n    logging.info(prefix)\n    string = string\n    logging.info(string)\n\n\ndef dag_s3_prefix(dag_id, timestamp):\n    \n    return \"{}/{}\".format(dag_id, timestamp)\n",
        "summary": "This code appears to be a Python script that processes XML data from an OAI-PMH repository and writes the processed data to an S3 bucket. The script includes several functions for different tasks:\n\n1. `process_xml`: This function takes in XML data, processes it based on certain conditions (e.g., whether the record is deleted), and writes the updated or deleted records to separate files.\n\n2. `perform_xml_lookup_with_cache`: This function performs an XML lookup using a cached CSV file. It reads the CSV file from S3 if it's not already in the cache, then uses the data to modify the input XML.\n\n3. `dag_write_string_to_s3`: This function writes a string to an S3 bucket with a unique filename based on its MD5 hash.\n\n4. `write_log`: This function logs a message to the console.\n\nThe script also includes several helper functions for working with XML and S3, such as:\n\n- `OaiXml`: A class for creating XML documents.\n- `dag_s3_prefix`: A function for generating a unique prefix based on the DAG ID and timestamp.\n\nOverall, this script appears to be part of a larger data processing pipeline that uses Airflow to schedule tasks and process OAI-PMH data."
    },
    {
        "code": "import turtle\r\n\r\nSTARTING_POSITIONS = [(0, 0), (-20, 0), (-40, 0)]\r\nMOVE_DISTANCE = 20\r\nUP = 90\r\nDOWN = 270\r\nRIGHT = 0\r\nLEFT = 180\r\n\r\n\r\nclass Snake:\r\n    \r\n    def __init__(self):\r\n        self.length = 3\r\n        self.segments = []\r\n        self.create_snake()\r\n        self.head = self.segments[0]\r\n\r\n    def create_snake(self):\r\n        \r\n        for position in STARTING_POSITIONS:\r\n            self.add_segment(position)\r\n\r\n    def add_segment(self, position):\r\n        new_segment = turtle.Turtle(shape=\"square\")\r\n        new_segment.color(\"white\")\r\n        new_segment.penup()\r\n        new_segment.setpos(position)\r\n        self.segments.append(new_segment)\r\n\r\n    def reset_snake(self):\r\n        for segment in self.segments:\r\n            segment.hideturtle()\r\n        self.segments.clear()\r\n        self.create_snake()\r\n        self.head = self.segments[0]\r\n\r\n    def extend(self, loot):\r\n        for time in range(loot):\r\n            self.add_segment(self.segments[-1].position())\r\n\r\n    def move(self):\r\n        \r\n        for seg_num in range(len(self.segments) - 1, 0, -1):\r\n            self.segments[seg_num].goto(self.segments[seg_num - 1].pos())\r\n        self.head.forward(MOVE_DISTANCE)\r\n\r\n    def up(self):\r\n        if self.head.heading() != DOWN:\r\n            self.head.setheading(UP)\r\n\r\n    def down(self):\r\n        if self.head.heading() != UP:\r\n            self.head.setheading(DOWN)\r\n\r\n    def right(self):\r\n        if self.head.heading() != LEFT:\r\n            self.head.setheading(RIGHT)\r\n\r\n    def left(self):\r\n        if self.head.heading() != RIGHT:\r\n            self.head.setheading(LEFT)\r\n",
        "summary": "The provided Python code defines a `Snake` class using the turtle graphics library to create and control a snake game. The snake is initialized with three segments at specific starting positions, and it can move in four directions (up, down, left, right). The snake's length can be extended by consuming \"loot,\" and the game state can be reset when needed."
    },
    {
        "code": "count = input('How many people will be in the dinner group? ')\ncount = int(count)\n\nif count > 8:\n    print('You\\'ll have to wait for a table.')\nelse:\n    print('The table is ready.')\n",
        "summary": "This Python script prompts the user to enter the number of people in their dinner group, converts that input into an integer, and then checks if the number exceeds eight. If it does, the script informs the user they will have to wait for a table; otherwise, it confirms that a table is ready."
    },
    {
        "code": "from distutils.core import setup\nfrom glob import glob\n\nfrom setuptools import find_packages\n\nsetup(name='Fibonacci',\n      version='1.0',\n      description='Python Distribution Utilities',\n      author='Kevin Chen',\n      packages=find_packages('src'),\n      package_dir={'': 'src'},\n      py_modules=[splitext(basename(path))[0] for path in glob('src/*.py')],\n     )",
        "summary": "This Python script sets up a distribution package named 'Fibonacci' using setuptools, specifying the version, author, and packages to include from the 'src' directory. It dynamically finds all Python modules in the 'src' folder and configures them accordingly for distribution."
    },
    {
        "code": "import os\n\nfrom os_win import utilsfactory\nfrom oslo_log import log as logging\nfrom oslo_utils import excutils\nfrom oslo_utils import units\n\nimport nova.conf\nfrom nova import exception\nfrom nova import utils\nfrom nova.virt.hyperv import pathutils\nfrom nova.virt import images\n\nLOG = logging.getLogger(__name__)\n\nCONF = nova.conf.CONF\n\n\nclass ImageCache(object):\n    def __init__(self):\n        self._pathutils = pathutils.PathUtils()\n        self._vhdutils = utilsfactory.get_vhdutils()\n\n    def _get_root_vhd_size_gb(self, instance):\n        if instance.old_flavor:\n            return instance.old_flavor.root_gb\n        else:\n            return instance.root_gb\n\n    def _resize_and_cache_vhd(self, instance, vhd_path):\n        vhd_size = self._vhdutils.get_vhd_size(vhd_path)['VirtualSize']\n\n        root_vhd_size_gb = self._get_root_vhd_size_gb(instance)\n        root_vhd_size = root_vhd_size_gb * units.Gi\n\n        root_vhd_internal_size = (\n                self._vhdutils.get_internal_vhd_size_by_file_size(\n                    vhd_path, root_vhd_size))\n\n        if root_vhd_internal_size < vhd_size:\n            raise exception.FlavorDiskSmallerThanImage(\n                flavor_size=root_vhd_size, image_size=vhd_size)\n        if root_vhd_internal_size > vhd_size:\n            path_parts = os.path.splitext(vhd_path)\n            resized_vhd_path = '%s_%s%s' % (path_parts[0],\n                                            root_vhd_size_gb,\n                                            path_parts[1])\n\n            @utils.synchronized(resized_vhd_path)\n            def copy_and_resize_vhd():\n                if not self._pathutils.exists(resized_vhd_path):\n                    try:\n                        LOG.debug(\"Copying VHD %(vhd_path)s to \"\n                                  \"%(resized_vhd_path)s\",\n                                  {'vhd_path': vhd_path,\n                                   'resized_vhd_path': resized_vhd_path})\n                        self._pathutils.copyfile(vhd_path, resized_vhd_path)\n                        LOG.debug(\"Resizing VHD %(resized_vhd_path)s to new \"\n                                  \"size %(root_vhd_size)s\",\n                                  {'resized_vhd_path': resized_vhd_path,\n                                   'root_vhd_size': root_vhd_size})\n                        self._vhdutils.resize_vhd(resized_vhd_path,\n                                                  root_vhd_internal_size,\n                                                  is_file_max_size=False)\n                    except Exception:\n                        with excutils.save_and_reraise_exception():\n                            if self._pathutils.exists(resized_vhd_path):\n                                self._pathutils.remove(resized_vhd_path)\n\n            copy_and_resize_vhd()\n            return resized_vhd_path\n\n    def get_cached_image(self, context, instance):\n        image_id = instance.image_ref\n\n        base_vhd_dir = self._pathutils.get_base_vhd_dir()\n        base_vhd_path = os.path.join(base_vhd_dir, image_id)\n\n        @utils.synchronized(base_vhd_path)\n        def fetch_image_if_not_existing():\n            vhd_path = None\n            for format_ext in ['vhd', 'vhdx']:\n                test_path = base_vhd_path + '.' + format_ext\n                if self._pathutils.exists(test_path):\n                    vhd_path = test_path\n                    break\n\n            if not vhd_path:\n                try:\n                    images.fetch(context, image_id, base_vhd_path)\n\n                    format_ext = self._vhdutils.get_vhd_format(base_vhd_path)\n                    vhd_path = base_vhd_path + '.' + format_ext.lower()\n                    self._pathutils.rename(base_vhd_path, vhd_path)\n                except Exception:\n                    with excutils.save_and_reraise_exception():\n                        if self._pathutils.exists(base_vhd_path):\n                            self._pathutils.remove(base_vhd_path)\n\n            return vhd_path\n\n        vhd_path = fetch_image_if_not_existing()\n\n        if CONF.use_cow_images and vhd_path.split('.')[-1].lower() == 'vhd':\n            \n            \n            resized_vhd_path = self._resize_and_cache_vhd(instance, vhd_path)\n            if resized_vhd_path:\n                return resized_vhd_path\n\n        return vhd_path\n",
        "summary": "The provided Python code defines a class `ImageCache` that manages caching and resizing of virtual hard disk (VHD) images for instances in a Hyper-V environment. It includes methods to fetch images, resize them based on instance flavor requirements, and cache them locally. The class uses utilities from various modules such as `os_win`, `nova.conf`, and `nova.virt.hyperv` to handle file operations, logging, and image management tasks."
    },
    {
        "code": "from typing import Union\n\nfrom wechaty import Message, Contact, Room, FileBox\nfrom wechaty.plugin import WechatyPlugin\n\n\nclass DingDongPlugin(WechatyPlugin):\n    \n    @property\n    def name(self):\n        \n        return 'ding-dong'\n\n    async def on_message(self, msg: Message):\n        \n        from_contact = msg.talker()\n        text = msg.text()\n        room = msg.room()\n        if text == '\n            conversation: Union[\n                Room, Contact] = from_contact if room is None else room\n            await conversation.ready()\n            await conversation.say('dong')\n            file_box = FileBox.from_url(\n                'https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/'\n                'u=1116676390,2305043183&fm=26&gp=0.jpg',\n                name='ding-dong.jpg')\n            await conversation.say(file_box)\n",
        "summary": "The DingDongPlugin class extends WechatyPlugin and defines an asynchronous method on_message that responds to messages with \"dong\" and sends an image when a specific message is received."
    },
    {
        "code": "from __future__ import print_function\n\nimport time\nimport unittest\n\nfrom WMCore.Agent.HeartbeatAPI import HeartbeatAPI\nfrom WMQuality.TestInit import TestInit\n\n\nclass HeartbeatTest(unittest.TestCase):\n    def setUp(self):\n        \n\n        self.testInit = TestInit(__file__)\n        self.testInit.setLogging()  \n        self.testInit.setDatabaseConnection()\n        self.testInit.setSchema(customModules=[\"WMCore.Agent.Database\"],\n                                useDefault=False)\n\n    def tearDown(self):\n        \n\n        self.testInit.clearDatabase()\n\n    def testAddComponent(self):\n        \n        comp1 = HeartbeatAPI(\"testComponent1\", pollInterval=60, heartbeatTimeout=600)\n        comp1.registerComponent()\n        self.assertEqual(comp1.getHeartbeatInfo(), [])  \n\n        comp1.registerWorker(\"testWorker1\")\n        self.assertEqual(len(comp1.getHeartbeatInfo()), 1)\n\n        comp1.registerWorker(\"testWorker2\")\n        self.assertEqual(len(comp1.getHeartbeatInfo()), 2)\n\n        comp2 = HeartbeatAPI(\"testComponent2\", pollInterval=30, heartbeatTimeout=300)\n        comp2.registerComponent()\n        self.assertEqual(comp2.getHeartbeatInfo(), [])  \n        self.assertEqual(len(comp2.getAllHeartbeatInfo()), 2)\n\n        comp2.registerWorker(\"testWorker21\")\n        self.assertEqual(len(comp2.getHeartbeatInfo()), 1)\n        self.assertEqual(len(comp2.getAllHeartbeatInfo()), 3)\n\n        comp1.updateWorkerHeartbeat(\"testWorker1\", \"Running\")\n        comp1.updateWorkerHeartbeat(\"testWorker2\", \"Running\")\n        comp2.updateWorkerHeartbeat(\"testWorker21\", \"Running\")\n        self.assertEqual(len(comp1.getAllHeartbeatInfo()), 3)\n        self.assertEqual(len(comp2.getAllHeartbeatInfo()), 3)\n\n        comp1Res = comp1.getHeartbeatInfo()\n        comp2Res = comp2.getHeartbeatInfo()\n        self.assertEqual(len(comp1Res), 2)\n        self.assertEqual(len(comp2Res), 1)\n\n        self.assertItemsEqual([item[\"name\"] for item in comp1Res], [\"testComponent1\", \"testComponent1\"])\n        self.assertItemsEqual([item[\"worker_name\"] for item in comp1Res], [\"testWorker1\", \"testWorker2\"])\n        self.assertItemsEqual([item[\"state\"] for item in comp1Res], [\"Running\", \"Running\"])\n        self.assertItemsEqual([item[\"poll_interval\"] for item in comp1Res], [60, 60])\n        self.assertItemsEqual([item[\"update_threshold\"] for item in comp1Res], [600, 600])\n\n        self.assertItemsEqual([item[\"name\"] for item in comp2Res], [\"testComponent2\"])\n        self.assertItemsEqual([item[\"worker_name\"] for item in comp2Res], [\"testWorker21\"])\n        self.assertItemsEqual([item[\"state\"] for item in comp2Res], [\"Running\"])\n        self.assertItemsEqual([item[\"poll_interval\"] for item in comp2Res], [30])\n        self.assertItemsEqual([item[\"update_threshold\"] for item in comp2Res], [300])\n\n    def testUpdateWorkers(self):\n        \n        comp1 = HeartbeatAPI(\"testComponent1\", pollInterval=60, heartbeatTimeout=600)\n        comp1.registerComponent()\n        comp1.registerWorker(\"testWorker1\")\n        comp1.registerWorker(\"testWorker2\")\n\n        comp2 = HeartbeatAPI(\"testComponent2\", pollInterval=30, heartbeatTimeout=300)\n        comp2.registerComponent()\n        comp2.registerWorker(\"testWorker21\")\n\n        comp1.updateWorkerCycle(\"testWorker1\", 1.001, None)\n        comp2.updateWorkerCycle(\"testWorker21\", 1234.1, 100)\n        hb1 = comp1.getHeartbeatInfo()\n        hb2 = comp2.getHeartbeatInfo()\n\n        for worker in hb1:\n            if worker['worker_name'] == 'testWorker1':\n                self.assertTrue(worker[\"cycle_time\"] > 1.0)\n            else:\n                self.assertEqual(worker[\"cycle_time\"], 0)\n        self.assertItemsEqual([item[\"outcome\"] for item in hb1], [None, None])\n        self.assertItemsEqual([item[\"error_message\"] for item in hb1], [None, None])\n\n        self.assertEqual(round(hb2[0][\"cycle_time\"], 1), 1234.1)\n        self.assertEqual(hb2[0][\"outcome\"], '100')\n        self.assertEqual(hb2[0][\"error_message\"], None)\n\n        \n        comp1.updateWorkerError(\"testWorker2\", \"BAD JOB!!!\")\n        hb1 = comp1.getHeartbeatInfo()\n        for worker in hb1:\n            if worker['worker_name'] == 'testWorker2':\n                self.assertTrue(worker[\"last_error\"] > int(time.time() - 10))\n                self.assertEqual(worker[\"state\"], \"Error\")\n                self.assertEqual(worker[\"error_message\"], \"BAD JOB!!!\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "summary": "The provided Python code defines a test suite for the `HeartbeatAPI` class, which is used to manage and monitor components and their workers. The tests cover functionalities such as registering components and workers, updating worker heartbeats, and handling errors, ensuring that the API behaves as expected under various conditions."
    },
    {
        "code": "import wagtail.core.blocks\nimport wagtail.core.fields\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\"budgetportal\", \"0053_custompage\"),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name=\"custompage\",\n            name=\"body\",\n            field=wagtail.core.fields.StreamField(\n                [\n                    (\n                        \"section\",\n                        wagtail.core.blocks.StructBlock(\n                            [\n                                (\n                                    \"presentation_class\",\n                                    wagtail.core.blocks.ChoiceBlock(\n                                        choices=[\n                                            (\"is-default\", \"Default\"),\n                                            (\"is-invisible\", \"No background/border\"),\n                                            (\"is-bevel\", \"Bevel\"),\n                                        ]\n                                    ),\n                                ),\n                                (\"heading\", wagtail.core.blocks.CharBlock()),\n                                (\"content\", wagtail.core.blocks.RichTextBlock()),\n                            ]\n                        ),\n                    ),\n                    (\"html\", wagtail.core.blocks.RawHTMLBlock()),\n                ],\n                default=None,\n            ),\n            preserve_default=False,\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration for adding a `StreamField` named `body` to the `CustomPage` model in the `budgetportal` app. The `StreamField` includes blocks for creating sections with presentation classes, headings, and rich text content, as well as an option for raw HTML."
    },
    {
        "code": "import torch\nfrom torch import nn\nfrom torchvision.models.vgg import vgg16\n\n\nclass GeneratorLoss_NEW(nn.Module):\n    def __init__(self):\n        super(GeneratorLoss_NEW, self).__init__()\n        vgg = vgg16(pretrained=True)\n        \n        loss_network = nn.Sequential(*list(vgg.features)[:35]).eval()\n        for param in loss_network.parameters():\n            param.requires_grad = False\n        self.loss_network = loss_network\n        self.mse_loss = nn.MSELoss()\n        self.tv_loss = TVLoss()\n        self.charbonnier_loss = L1_Charbonnier_loss()\n\n    def forward(self, out_labels, out_images, target_images):\n        \n        adversarial_loss = torch.mean(1 - out_labels)\n        \n        \n        perception_loss = self.charbonnier_loss(self.loss_network(out_images), self.loss_network(target_images))\n        \n        \n        image_loss = self.charbonnier_loss(out_images, target_images)\n        \n        tv_loss = self.tv_loss(out_images)\n        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n\n\nclass TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size = x.size()[0]\n        h_x = x.size()[2]\n        w_x = x.size()[3]\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]\n\n\nclass L1_Charbonnier_loss(torch.nn.Module):\n    \n    def __init__(self):\n        super(L1_Charbonnier_loss, self).__init__()\n        self.eps = 1e-6\n\n    def forward(self, X, Y):\n        diff = torch.add(X, -Y)\n        error = torch.sqrt(diff * diff + self.eps)\n        loss = torch.mean(error)\n        return loss\n\n\nif __name__ == \"__main__\":\n    g_loss = GeneratorLoss_NEW()\n    print(g_loss)\n",
        "summary": "The provided Python code defines a custom generator loss function `GeneratorLoss_NEW` that combines multiple types of losses, including adversarial loss, perceptual loss using VGG16 features, image reconstruction loss, and total variation (TV) regularization. The `TVLoss` class calculates the total variation loss for smoothness in images, while `L1_Charbonnier_loss` computes a robust L1 norm with added regularization to prevent division by zero."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nfrom googlecloudsdk.api_lib.datapipelines import util\nfrom googlecloudsdk.calliope import base\nfrom googlecloudsdk.command_lib.datapipelines import flags\n\n_DETAILED_HELP = {\n    'DESCRIPTION':\n        '{description}',\n    'EXAMPLES':\n        ,\n}\n\n\n@base.ReleaseTracks(base.ReleaseTrack.BETA)\nclass Create(base.CreateCommand):\n  \n\n  detailed_help = _DETAILED_HELP\n\n  @staticmethod\n  def Args(parser):\n    flags.AddCreatePipelineFlags(parser)\n    flags.GetDisplayNameArg('Data Pipelines pipeline').AddToParser(parser)\n    flags.GetPipelineTypeArg(required=True).AddToParser(parser)\n    flags.GetTemplateTypeArg(required=False).AddToParser(parser)\n    flags.GetScheduleArg(required=False).AddToParser(parser)\n    flags.GetTimeZoneArg(required=False).AddToParser(parser)\n    flags.GetTemplateFileGcsLocationArg(required=False).AddToParser(parser)\n    flags.GetParametersArg(required=False).AddToParser(parser)\n    flags.GetMaxWorkersArg(required=False).AddToParser(parser)\n    flags.GetNumWorkersArg(required=False).AddToParser(parser)\n    flags.GetNetworkArg(required=False).AddToParser(parser)\n    flags.GetSubnetworkArg(required=False).AddToParser(parser)\n    flags.GetWorkerMachineTypeArg(required=False).AddToParser(parser)\n    flags.GetTempLocationArg(required=False).AddToParser(parser)\n    flags.GetDataflowKmsKeyArg(required=False).AddToParser(parser)\n    flags.GetDisablePublicIpsArg(required=False).AddToParser(parser)\n    flags.GetDataflowServiceAccountEmailArg(required=False).AddToParser(parser)\n    flags.GetEnableStreamingEngineArg(required=False).AddToParser(parser)\n    flags.GetAdditionalExperimentsArg(required=False).AddToParser(parser)\n    flags.GetAdditionalUserLabelsArg(required=False).AddToParser(parser)\n    flags.GetWorkerRegionArgs(required=False).AddToParser(parser)\n    flags.GetFlexRsGoalArg(required=False).AddToParser(parser)\n    flags.GetStreamingUpdateArgs(required=False).AddToParser(parser)\n\n  def Run(self, args):\n    \n    client = util.PipelinesClient()\n    pipelines_ref = args.CONCEPTS.pipeline.Parse()\n    region_ref = pipelines_ref.Parent()\n    return client.Create(\n        pipeline=pipelines_ref.RelativeName(),\n        parent=region_ref.RelativeName(),\n        args=args)\n",
        "summary": "This Python code defines a command for creating Data Pipelines in Google Cloud, using the `googlecloudsdk` library. It includes detailed help documentation and flags for various configuration options such as pipeline type, template file location, and worker settings. The `Run` method uses a client to create a new pipeline based on the provided arguments."
    },
    {
        "code": "from kivy.graphics import Color\n\nfrom .navigation import Navigation\n\nclass Colors:\n    WHITE = Color(1, 1, 1, 1)\n    BLACK = Color(0, 0, 0, 1)\n\n    GREY = Color(.8, .8, .8, 1)\n\n    RED = Color(1, 0, 0, 1)\n    GREEN = Color(0, 1, 0, 1)\n    BLUE = Color(0, 0, 1, 1)\n\n    @staticmethod\n    def lerp(value, *args):\n\n        if value <= 0:\n            return args[0]\n        elif value >= 1:\n            return args[-1]\n\n        a = None\n        b = None\n\n        pos = 2\n        neg = -2\n\n        slice = 1 / (len(args) - 1)\n        for i in range(len(args)):\n            v = i * slice\n            diff = value - v\n            if diff == 0:\n                return args[i]\n            elif diff > 0:\n                if diff < pos:\n                    b = args[i]\n                    pos = diff\n            else:\n                if diff > neg:\n                    a = args[i]\n                    neg = diff\n\n        pvalue = pos / slice\n        nvalue = 1 - pvalue\n\n        return Color(\n            a.r * pvalue + b.r * nvalue,\n            a.g * pvalue + b.g * nvalue,\n            a.b * pvalue + b.b * nvalue,\n            1\n        )\n",
        "summary": "The `Colors` class defines a set of predefined color constants using the Kivy graphics library, and includes a static method `lerp` for linear interpolation between two colors based on a given value."
    },
    {
        "code": "from core.plugins.openstack import (\n    OpenstackChecksBase,\n)\n\nFEATURES = {'neutron': {'main': [\n                            'availability_zone'],\n                        'openvswitch-agent': [\n                            'l2_population',\n                            'firewall_driver'],\n                        'l3-agent': [\n                            'agent_mode',\n                            'ovs_use_veth'],\n                        'dhcp-agent': [\n                            'enable_metadata_network',\n                            'enable_isolated_metadata',\n                            'ovs_use_veth']},\n            'nova': {'main': [\n                        'vcpu_pin_set',\n                        'cpu_shared_set',\n                        'cpu_dedicated_set',\n                        'live_migration_permit_auto_converge',\n                        'live_migration_permit_post_copy',\n                        ]}}\n\n\nDEFAULTS = {'neutron': {'dhcp-agent': {\n                            'enable_metadata_network': False,\n                            'enable_isolated_metadata': False}},\n            'nova': {'main': {'live_migration_permit_auto_converge': False,\n                              'live_migration_permit_post_copy': False}}}\nYAML_PRIORITY = 5\n\n\nclass ServiceFeatureChecks(OpenstackChecksBase):\n\n    @property\n    def output(self):\n        if self._output:\n            return {\"features\": self._output}\n\n    def get_service_features(self):\n        \n        for service in FEATURES:\n            for module in FEATURES[service]:\n                module_features = {}\n                cfg = self.ost_projects.all[service].config[module]\n                if not cfg.exists:\n                    continue\n\n                for key in FEATURES[service][module]:\n                    val = cfg.get(key)\n                    if val is not None:\n                        module_features[key] = val\n\n                    if key not in module_features:\n                        if key in DEFAULTS.get(service, {}).get(module, {}):\n                            default = DEFAULTS[service][module][key]\n                            module_features[key] = default\n\n                \n                \n                if module_features:\n                    if service not in self._output:\n                        self._output[service] = {}\n\n                    self._output[service][module] = module_features\n\n    def __call__(self):\n        \n        if not self.openstack_installed:\n            return\n\n        self.get_service_features()\n",
        "summary": "The provided Python code defines a class `ServiceFeatureChecks` that extends `OpenstackChecksBase`. This class is designed to gather and process configuration features for OpenStack services such as Neutron and Nova, applying default values where necessary. The collected features are then formatted into a dictionary structure for output."
    },
    {
        "code": "from rest_framework import serializers\nfrom wallet.models import UserWallet, PaymentMethod, DriverWallet\n\n\nclass UserWalletSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserWallet\n        fields = \"__all__\"\n\n\nclass DriverWalletSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = DriverWallet\n        fields = \"__all__\"\n\n\nclass PaymentMethodSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = PaymentMethod\n        fields = \"__all__\"\n",
        "summary": "The provided Python code defines serializers for three models: UserWallet, DriverWallet, and PaymentMethod using Django REST framework's ModelSerializer. Each serializer includes all fields from their respective models."
    },
    {
        "code": "from typing import List, Callable\nfrom autumn.curve import scale_up_function\n\n\ndef get_importation_rate_func_as_birth_rates(\n    importation_times: List[float],\n    importation_n_cases: List[float],\n    detect_prop_func,\n    starting_pops: list,\n):\n    \n    \n    for i, time in enumerate(importation_times):\n        importation_n_cases[i] /= detect_prop_func(time)\n    \n    importation_numbers_scale_up = scale_up_function(\n        importation_times, importation_n_cases, method=4, smoothness=5.0, bound_low=0.0\n    )\n\n    def recruitment_rate(t):\n        return importation_numbers_scale_up(t) / sum(starting_pops)\n\n    return recruitment_rate\n\n\n\n\nIMPORTATION_PROPS_BY_AGE = {\n    \"0\": 0.04,\n    \"5\": 0.04,\n    \"10\": 0.04,\n    \"15\": 0.04,\n    \"20\": 0.08,\n    \"25\": 0.09,\n    \"30\": 0.09,\n    \"35\": 0.09,\n    \"40\": 0.09,\n    \"45\": 0.08,\n    \"50\": 0.08,\n    \"55\": 0.08,\n    \"60\": 0.04,\n    \"65\": 0.04,\n    \"70\": 0.04,\n    \"75\": 0.04,\n}\n",
        "summary": "The provided Python code defines a function `get_importation_rate_func_as_birth_rates` that calculates the recruitment rate based on importation times, number of cases, detection proportion function, and starting populations. It scales up the importation numbers using a specific method and smoothness level, then computes the recruitment rate as the scaled importation numbers divided by the sum of starting populations. The code also includes a dictionary `IMPORTATION_PROPS_BY_AGE` that maps age groups to their respective importation proportions."
    },
    {
        "code": "class Mahasiswa:\r\n    def __init__(self, nama, nilai):\r\n        self.nama = nama\r\n        self.nilai = nilai\r\n    def hitung_nilai(self):\r\n        return sum(self.nilai)/len(self.nilai)\r\nmahasiswa = Mahasiswa(\"Fazlur\", (90,70,70,70))\r\nprint(\"Nama :\", mahasiswa.nama)\r\nprint(\"Total Nilai :\", mahasiswa.hitung_nilai())",
        "summary": "The Python code defines a class `Mahasiswa` with attributes for name and grades, includes a method to calculate the average grade, creates an instance of `Mahasiswa`, and prints the student's name and average grade."
    },
    {
        "code": "from typing import Optional, Tuple\n\nfrom .bases.basegroup import BaseGroup\nfrom .partials.partialuser import PartialUser\nfrom .shout import Shout\nfrom .utilities.shared import ClientSharedObject\n\n\nclass Group(BaseGroup):\n    \n\n    def __init__(self, shared: ClientSharedObject, data: dict):\n        \n        super().__init__(shared, data[\"id\"])\n\n        self._shared: ClientSharedObject = shared\n\n        self.id: int = data[\"id\"]\n        self.name: str = data[\"name\"]\n        self.description: str = data[\"description\"]\n        self.owner: PartialUser = PartialUser(shared=shared, data=data[\"owner\"])\n        self.shout: Optional[Shout] = data[\"shout\"] and Shout(\n            shared=self._shared,\n            data=data[\"shout\"]\n        ) or None\n        self.member_count: int = data[\"memberCount\"]\n        self.is_builders_club_only: bool = data[\"isBuildersClubOnly\"]\n        self.public_entry_allowed: bool = data[\"publicEntryAllowed\"]\n        self.is_locked: bool = data.get(\"isLocked\") or False\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__} id={self.id} name={self.name!r} owner={self.owner}>\"\n\n    async def update_shout(self, message: str, update_self: bool = True) -> Tuple[Optional[Shout], Optional[Shout]]:\n        \n        shout_response = await self._requests.patch(\n            url=self._shared.url_generator.get_url(\"groups\", f\"v1/groups/{self.id}/status\"),\n            json={\n                \"message\": message\n            }\n        )\n\n        shout_data = shout_response.json()\n\n        old_shout: Optional[Shout] = self.shout\n        new_shout: Optional[Shout] = shout_data and Shout(\n            shared=self._shared,\n            data=shout_data\n        ) or None\n\n        if update_self:\n            self.shout = new_shout\n\n        return old_shout, new_shout\n",
        "summary": "The `Group` class extends `BaseGroup`, representing a group with attributes like ID, name, description, owner, and shout. It includes methods for initializing the group from data, updating its representation, and asynchronously updating the group's shout message while optionally updating the instance itself."
    },
    {
        "code": "from typing import Any, BinaryIO\n\n\nclass CustomSerializer:\n    \n\n    @property\n    def extension(self) -> str:  \n        return \"ext\"\n\n    def serialize(self, value: Any, writer: BinaryIO):  \n        raise NotImplementedError()\n\n    def deserialize(self, reader: BinaryIO) -> Any:  \n        raise NotImplementedError()\n\n    def __repr__(self) -> str:  \n        return \"CustomSerializerInstance\"\n",
        "summary": "The `CustomSerializer` class defines a template for serializing and deserializing data using binary input/output streams. It includes methods for serialization and deserialization that must be implemented by subclasses, as well as a property to specify the file extension associated with the serializer. The `__repr__` method provides a string representation of an instance of the class."
    },
    {
        "code": "def digest_target(target):\n\n    from .element import digest_element\n    return digest_element(target)\n\n",
        "summary": "The `digest_target` function imports and uses the `digest_element` function to process the input `target`."
    },
    {
        "code": "import tensorflow as tf\n\nfrom kerod.utils import ops\n\n\ndef subsample_indicator(indicator, num_samples):\n    \n    indices = tf.where(indicator)\n    indices = tf.random.shuffle(indices)\n    indices = tf.reshape(indices, [-1])\n\n    num_samples = tf.minimum(tf.size(indices), num_samples)\n    selected_indices = tf.slice(indices, [0], tf.reshape(num_samples, [1]))\n\n    selected_indicator = ops.indices_to_dense_vector(selected_indices, tf.shape(indicator)[0])\n\n    return tf.equal(selected_indicator, 1)\n\n\ndef sample_balanced_positive_negative(indicator, sample_size, labels, positive_fraction=0.5):\n    \n\n    negative_idx = tf.logical_not(labels)\n    positive_idx = tf.logical_and(labels, indicator)\n    negative_idx = tf.logical_and(negative_idx, indicator)\n\n    \n    if sample_size is None:\n        max_num_pos = tf.reduce_sum(tf.cast(positive_idx, dtype=tf.int32))\n    else:\n        max_num_pos = int(positive_fraction * sample_size)\n    sampled_pos_idx = subsample_indicator(positive_idx, max_num_pos)\n    num_sampled_pos = tf.reduce_sum(tf.cast(sampled_pos_idx, tf.int32))\n    if sample_size is None:\n        negative_positive_ratio = (1 - positive_fraction) / positive_fraction\n        max_num_neg = tf.cast(negative_positive_ratio * tf.cast(num_sampled_pos, dtype=tf.float32),\n                              dtype=tf.int32)\n    else:\n        max_num_neg = sample_size - num_sampled_pos\n    sampled_neg_idx = subsample_indicator(negative_idx, max_num_neg)\n\n    return tf.logical_or(sampled_pos_idx, sampled_neg_idx)\n\n\ndef batch_sample_balanced_positive_negative(indicators,\n                                            sample_size,\n                                            labels,\n                                            positive_fraction=0.5,\n                                            dtype=tf.float32):\n    \n\n    def _minibatch_subsample_fn(inputs):\n        indicators, targets = inputs\n        return sample_balanced_positive_negative(tf.cast(indicators, tf.bool),\n                                                 sample_size,\n                                                 tf.cast(targets, tf.bool),\n                                                 positive_fraction=positive_fraction)\n\n    return tf.cast(tf.map_fn(_minibatch_subsample_fn, [indicators, labels],\n                             dtype=tf.bool,\n                             parallel_iterations=16,\n                             back_prop=True),\n                   dtype=dtype)\n",
        "summary": "The provided Python code defines functions for subsampling indicators and sampling balanced positive-negative samples in TensorFlow. The `subsample_indicator` function randomly selects a specified number of indices from a given indicator tensor, while the `sample_balanced_positive_negative` function ensures that the sampled subset contains a balanced ratio of positive and negative examples based on labels. The `batch_sample_balanced_positive_negative` function applies these operations to a batch of indicators and labels in parallel."
    },
    {
        "code": "import os\nimport re\nimport json\nimport os.path\nimport unittest\n\nreg_cmnt = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n\n\nclass Config:\n    \"\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u043c \u0444\u0430\u0439\u043b\u043e\u043c\"\n\n    def __init__(self, main_path=None, user_path=None):\n        if main_path is None:\n            self._main_path = \"config.json5\"\n        else:\n            self._main_path = main_path\n        if user_path is None:\n            self._user_path = \"config_user.json5\"\n        else:\n            self._user_path = user_path\n        self._cfg_dict = {}\n\n    def __getitem__(self, key):\n        return self._cfg_dict[key]\n\n    def __len__(self):\n        return len(self._cfg_dict)\n\n    def _load_json(self, path):\n        data = {}\n        if os.path.exists(path):\n            txt = open(path).read()\n            txt = reg_cmnt.sub(\"\", txt)  \n            data = json.loads(txt)\n        return data\n\n    def _set_default(self, cfg):\n        cfg[\"path_to_dict\"] = cfg.get(\"path_to_dict\", \"dict.json\")\n        cfg[\"path_to_stat\"] = cfg.get(\"path_to_stat\", \"statistic.json\")\n        cfg[\"words_per_lesson\"] = int(cfg.get(\"words_per_lesson\", 5))\n        cfg[\"CntStudyWords\"] = int(cfg.get(\"CntStudyWords\", 50))\n        cfg[\"MinPercent\"] = float(cfg.get(\"MinPercent\", 97.0))\n        cfg[\"MinSuccessCnt\"] = int(cfg.get(\"MinSuccessCnt\", 10))\n        cfg[\"retry_time\"] = int(cfg.get(\"retry_time\", 1800))\n        cfg[\"hide_transcription\"] = cfg.get(\"hide_transcription\", \"no\")\n        cfg[\"start_time_delay\"] = int(cfg.get(\"start_time_delay\", 1))\n        cfg[\"stat_count_row\"] = int(cfg.get(\"stat_count_row\", 200))\n        cfg[\"right_answer_percent\"] = float(cfg.get(\"right_answer_percent\", 10.0))\n        cfg[\"wrong_answer_percent\"] = float(cfg.get(\"wrong_answer_percent\", 40.0))\n        cfg[\"empty_answer_is_error\"] = cfg.get(\"empty_answer_is_error\", \"no\")\n        cfg[\"internet_dictionary_url\"] = cfg.get(\"internet_dictionary_url\",\n                                                 {\"EN_RU\": \"http://slovari.yandex.ru/{word}/en-ru/\n                                                  \"RU_EN\": \"http://slovari.yandex.ru/{word}/en/\n\n    def create_default_user_config(self):\n        if not os.path.isfile(self._user_path):\n            txt = \"{\\n    /*\\n        User config\\n    */\\n\\n}\"\n            open(self._user_path, \"wt\").write(txt)\n\n    def reload(self):\n        self._cfg_dict = {}\n        self._cfg_dict.update(self._load_json(self._main_path))\n        self._cfg_dict.update(self._load_json(self._user_path))\n        self._set_default(self._cfg_dict)\n        return self._cfg_dict\n\n    def get_dict(self):\n        return self._cfg_dict\n\n\nclass ConfigTestCase(unittest.TestCase):\n    \"\u041d\u0430\u0431\u043e\u0440 \u0442\u0435\u0441\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0430 Config\"\n\n    def setUp(self):\n        if os.path.isfile(\"test_config_user.json\"):\n            os.remove(\"test_config_user.json\")\n\n    def tearDown(self):\n        if os.path.isfile(\"test_config_user.json\"):\n            os.remove(\"test_config_user.json\")\n\n    def equal_cfg(self, cfg, test_dict):\n        for key, val in test_dict.items():\n            self.assertEqual(cfg[key], val)\n        self.assertEqual(len(cfg), 14)\n\n    def test_main(self):\n        \"\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0435\u0439\"\n\n        test_dict = {\n            \"path_to_dict\": \"dict.json\",\n            \"path_to_stat\": \"statistic.json\",\n            \"words_per_lesson\": 5,\n            \"CntStudyWords\": 50,\n            \"MinPercent\": 97.0,\n            \"MinSuccessCnt\": 10,\n            \"retry_time\": 1800,\n            \"hide_transcription\": \"no\",\n            \"start_time_delay\": 1,\n            \"stat_count_row\": 200,\n            \"right_answer_percent\": 10.0,\n            \"wrong_answer_percent\": 40.0,\n            \"empty_answer_is_error\": \"no\",\n            \"internet_dictionary_url\": {\"EN_RU\": \"http://slovari.yandex.ru/{word}/en-ru/\n                                        \"RU_EN\": \"http://slovari.yandex.ru/{word}/en/\n\n        cfg = Config(\"config.json5\", \"fake_config_user.json\")\n        cfg.reload()\n        self.equal_cfg(cfg, test_dict)\n\n    def test_user(self):\n        \"\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0435\u0439\"\n\n        test_dict = {\n            \"path_to_dict\": \"dict1.json\",\n            \"path_to_stat\": \"statistic1.json\",\n            \"words_per_lesson\": 6,\n            \"CntStudyWords\": 60,\n            \"MinPercent\": 98.0,\n            \"MinSuccessCnt\": 11,\n            \"retry_time\": 1801,\n            \"hide_transcription\": \"yes\",\n            \"start_time_delay\": 2,\n            \"stat_count_row\": 300,\n            \"right_answer_percent\": 20.0,\n            \"wrong_answer_percent\": 50.0,\n            \"empty_answer_is_error\": \"yes\",\n            \"internet_dictionary_url\": {\"EN_RU\": \"http1://slovari.yandex.ru/{word}/en-ru/\n                                        \"RU_EN\": \"http1://slovari.yandex.ru/{word}/en/\n\n        json.dump(test_dict, open(\"test_config_user.json\", \"w\"))\n        cfg = Config(\"config.json5\", \"test_config_user.json\")\n        cfg.reload()\n        self.equal_cfg(cfg, test_dict)\n\n    def test_user_part(self):\n        \"\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0435\u0440\u0435\u043a\u0440\u044b\u0432\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a\"\n\n        test_dict = {\n            \"path_to_dict\": \"dict1.json\",\n            \"path_to_stat\": \"statistic1.json\",\n            \"words_per_lesson\": 6,\n            \"CntStudyWords\": 60,\n            \"MinPercent\": 98.0,\n            \"MinSuccessCnt\": 11}\n\n        json.dump(test_dict, open(\"test_config_user.json\", \"w\"))\n\n        test_dict.update({\n            \"retry_time\": 1800,\n            \"hide_transcription\": \"no\",\n            \"start_time_delay\": 1,\n            \"stat_count_row\": 200,\n            \"right_answer_percent\": 10.0,\n            \"wrong_answer_percent\": 40.0,\n            \"empty_answer_is_error\": \"no\"})\n\n        cfg = Config(\"config.json5\", \"test_config_user.json\")\n        cfg.reload()\n        self.equal_cfg(cfg, test_dict)\n\n    def test_not_exists(self):\n        \"\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0445 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a\"\n\n        test_dict = {\n            \"path_to_dict\": \"dict.json\",\n            \"path_to_stat\": \"statistic.json\",\n            \"words_per_lesson\": 5,\n            \"CntStudyWords\": 50,\n            \"MinPercent\": 97.0,\n            \"MinSuccessCnt\": 10,\n            \"retry_time\": 1800,\n            \"hide_transcription\": \"no\",\n            \"start_time_delay\": 1,\n            \"stat_count_row\": 200,\n            \"right_answer_percent\": 10.0,\n            \"wrong_answer_percent\": 40.0,\n            \"empty_answer_is_error\": \"no\",\n            \"internet_dictionary_url\": {\"EN_RU\": \"http://slovari.yandex.ru/{word}/en-ru/\n                                        \"RU_EN\": \"http://slovari.yandex.ru/{word}/en/\n\n        cfg = Config(\"config.json5\", \"fake_config_user.json\")\n        cfg.reload()\n        self.equal_cfg(cfg, test_dict)\n\n        cfg = Config(\"fake_config.json\", \"fake_config_user.json\")\n        cfg.reload()\n\nif __name__ == \"__main__\":\n    os.chdir(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    suite = unittest.TestLoader().loadTestsFromTestCase(ConfigTestCase)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n",
        "summary": "The provided Python code defines a `Config` class for managing configuration settings from JSON5 files, with methods to load and merge main and user configurations, set default values, and create default user configurations. It also includes unit tests using the `unittest` framework to validate the functionality of the `Config` class under various scenarios, such as loading existing configurations, creating new ones, and handling missing files."
    },
    {
        "code": "import pytest\nimport copy\nfrom pathlib import Path\nimport sys\nsys.path.append(str(Path(__file__).absolute().parent.parent))\nfrom swimmer_abm.model import Model\n\ndef test_init():\n    model = Model(nswimmers=3)\n    assert len(model.swimmers) == 3\n    \ndef test_step():\n    model = Model(nswimmers=1)\n    swimmer = copy.deepcopy(model.swimmers[0])\n    dt = 1\n    swimmer.swim(dt)\n    model.step(dt)\n    assert swimmer.pos == model.swimmers[0].pos\n\ndef test_repr():\n    model = Model(nswimmers=1)\n    assert isinstance(str(model), str)\n",
        "summary": "The provided Python code defines a series of tests for a `Model` class using the `pytest` framework. It includes tests to verify the initialization of the model with a specific number of swimmers, the functionality of the `step` method where a swimmer's position is updated and reflected in the model, and the string representation of the model instance."
    },
    {
        "code": "class Evaluator:\n    def __init__(self, lexer):\n        self.__lexer = lexer\n    def evaluate(self, line):\n        return int(next(self.__lexer.tokenize(line)).raw_value)\n\nclass REPL:\n    def __init__(self, read, print, evaluate):\n        self.__read = read\n        self.__eval = evaluate\n        self.__print = print\n\n    def loop(self):\n        while True:\n            try:\n                line = self.__read('mm-i> ')\n                result = self.__eval(line)\n                self.__print(result)\n            except KeyboardInterrupt:\n                break\n\nif __name__ == '__main__':\n    from lexer import Lexer\n    REPL(input, print, Evaluator(Lexer()).evaluate).loop()\n",
        "summary": "The provided Python code defines two classes: `Evaluator` and `REPL`. The `Evaluator` class takes a lexer as an argument and evaluates expressions by tokenizing the input line and returning the integer value of the first token. The `REPL` class creates a read-eval-print loop that continuously reads user input, evaluates it using an instance of `Evaluator`, and prints the result until interrupted by the user."
    },
    {
        "code": "import os\nfrom time import sleep\n\nimport pytest\nfrom tlz import frequencies\n\nfrom distributed import get_task_stream\nfrom distributed.client import wait\nfrom distributed.diagnostics.task_stream import TaskStreamPlugin\nfrom distributed.metrics import time\nfrom distributed.utils_test import div, gen_cluster, inc, slowinc\n\n\n@gen_cluster(client=True, nthreads=[(\"127.0.0.1\", 1)] * 3)\nasync def test_TaskStreamPlugin(c, s, *workers):\n    es = TaskStreamPlugin(s)\n    s.add_plugin(es)\n    assert not es.buffer\n\n    futures = c.map(div, [1] * 10, range(10))\n    total = c.submit(sum, futures[1:])\n    await wait(total)\n\n    assert len(es.buffer) == 11\n\n    workers = dict()\n\n    rects = es.rectangles(0, 10, workers)\n    assert workers\n    assert all(n == \"div\" for n in rects[\"name\"])\n    assert all(d > 0 for d in rects[\"duration\"])\n    counts = frequencies(rects[\"color\"])\n    assert counts[\"black\"] == 1\n    assert set(counts.values()) == {9, 1}\n    assert len(set(rects[\"y\"])) == 3\n\n    rects = es.rectangles(2, 5, workers)\n    assert all(len(L) == 3 for L in rects.values())\n\n    starts = sorted(rects[\"start\"])\n    rects = es.rectangles(\n        2, 5, workers=workers, start_boundary=(starts[0] + starts[1]) / 2000\n    )\n    assert set(rects[\"start\"]).issubset(set(starts[1:]))\n\n\n@gen_cluster(client=True)\nasync def test_maxlen(c, s, a, b):\n    tasks = TaskStreamPlugin(s, maxlen=5)\n    s.add_plugin(tasks)\n    futures = c.map(inc, range(10))\n    await wait(futures)\n    assert len(tasks.buffer) == 5\n\n\n@gen_cluster(client=True)\nasync def test_collect(c, s, a, b):\n    tasks = TaskStreamPlugin(s)\n    s.add_plugin(tasks)\n    start = time()\n    futures = c.map(slowinc, range(10), delay=0.1)\n    await wait(futures)\n\n    L = tasks.collect()\n    assert len(L) == len(futures)\n    L = tasks.collect(start=start)\n    assert len(L) == len(futures)\n\n    L = tasks.collect(start=start + 0.2)\n    assert 4 <= len(L) <= len(futures)\n\n    L = tasks.collect(start=\"20 s\")\n    assert len(L) == len(futures)\n\n    L = tasks.collect(start=\"500ms\")\n    assert 0 < len(L) <= len(futures)\n\n    L = tasks.collect(count=3)\n    assert len(L) == 3\n    assert L == list(tasks.buffer)[-3:]\n\n    assert tasks.collect(stop=start + 100, count=3) == tasks.collect(count=3)\n    assert tasks.collect(start=start, count=3) == list(tasks.buffer)[:3]\n\n\n@gen_cluster(client=True)\nasync def test_no_startstops(c, s, a, b):\n    tasks = TaskStreamPlugin(s)\n    s.add_plugin(tasks)\n    \n    future = c.submit(inc, 1)\n    await wait(future)\n    assert len(tasks.buffer) == 1\n\n    tasks.transition(future.key, \"processing\", \"erred\")\n    \n    assert len(tasks.buffer) == 1\n\n    tasks.transition(future.key, \"processing\", \"erred\", startstops=[])\n    \n    assert len(tasks.buffer) == 1\n\n    tasks.transition(\n        future.key, \"processing\", \"erred\", startstops=[dict(start=time(), stop=time())]\n    )\n    assert len(tasks.buffer) == 2\n\n\n@gen_cluster(client=True)\nasync def test_client(c, s, a, b):\n    L = await c.get_task_stream()\n    assert L == ()\n\n    futures = c.map(slowinc, range(10), delay=0.1)\n    await wait(futures)\n\n    tasks = s.plugins[TaskStreamPlugin.name]\n    L = await c.get_task_stream()\n    assert L == tuple(tasks.buffer)\n\n\ndef test_client_sync(client):\n    with get_task_stream(client=client) as ts:\n        sleep(0.1)  \n        \n        futures = client.map(inc, range(10))\n        wait(futures)\n\n    assert len(ts.data) == 10\n\n\n@gen_cluster(client=True)\nasync def test_get_task_stream_plot(c, s, a, b):\n    bokeh = pytest.importorskip(\"bokeh\")\n    await c.get_task_stream()\n\n    futures = c.map(slowinc, range(10), delay=0.1)\n    await wait(futures)\n\n    data, figure = await c.get_task_stream(plot=True)\n    assert isinstance(figure, bokeh.plotting.Figure)\n\n\ndef test_get_task_stream_save(client, tmpdir):\n    bokeh = pytest.importorskip(\"bokeh\")\n    tmpdir = str(tmpdir)\n    fn = os.path.join(tmpdir, \"foo.html\")\n\n    with get_task_stream(plot=\"save\", filename=fn) as ts:\n        wait(client.map(inc, range(10)))\n    with open(fn) as f:\n        data = f.read()\n    assert \"inc\" in data\n    assert \"bokeh\" in data\n\n    assert isinstance(ts.figure, bokeh.plotting.Figure)\n",
        "summary": "The provided Python code defines several test functions for a `TaskStreamPlugin` class within the `distributed` library. These tests validate functionalities such as collecting task stream data, managing buffer lengths, handling start and stop times, and generating plots or saving them to files using Bokeh. The tests utilize asynchronous operations with `gen_cluster` and interact with distributed computing environments to ensure the plugin behaves as expected under various conditions."
    },
    {
        "code": "import abc\nimport copy\nimport itertools\nimport logging\nfrom threading import Lock\nfrom typing import List, Optional, Sequence\n\nimport numpy as np\n\nfrom ote_sdk.entities.annotation import Annotation, AnnotationSceneEntity\nfrom ote_sdk.entities.label import LabelEntity\nfrom ote_sdk.entities.media import IMedia2DEntity\nfrom ote_sdk.entities.metadata import IMetadata, MetadataItemEntity\nfrom ote_sdk.entities.model import ModelEntity\nfrom ote_sdk.entities.scored_label import ScoredLabel\nfrom ote_sdk.entities.shapes.rectangle import Rectangle\nfrom ote_sdk.entities.subset import Subset\nfrom ote_sdk.utils.shape_factory import ShapeFactory\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatasetItemEntity(metaclass=abc.ABCMeta):\n    \n\n    \n    def __init__(\n        self,\n        media: IMedia2DEntity,\n        annotation_scene: AnnotationSceneEntity,\n        roi: Optional[Annotation] = None,\n        metadata: Optional[Sequence[MetadataItemEntity]] = None,\n        subset: Subset = Subset.NONE,\n    ):\n        self.__media: IMedia2DEntity = media\n        self.__annotation_scene: AnnotationSceneEntity = annotation_scene\n        self.__subset: Subset = subset\n        self.__roi_lock = Lock()\n\n        \n        if roi is None:\n            for annotation in annotation_scene.annotations:\n                \n                if Rectangle.is_full_box(annotation.shape):\n                    roi = annotation\n                    break\n        self.__roi = roi\n\n        self.__metadata: List[MetadataItemEntity] = []\n        if metadata is not None:\n            self.__metadata = list(metadata)\n\n    @property\n    def metadata(self) -> Sequence[MetadataItemEntity]:\n        \n        return self.__metadata\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"media={self.media}, \"\n            f\"annotation_scene={self.annotation_scene}, \"\n            f\"roi={self.roi}, \"\n            f\"subset={self.subset})\"\n        )\n\n    @property\n    def roi(self) -> Annotation:\n        \n        with self.__roi_lock:\n            if self.__roi is None:\n                requested_roi = Annotation(Rectangle.generate_full_box(), labels=[])\n                self.__roi = requested_roi\n            else:\n                requested_roi = self.__roi\n            return requested_roi\n\n    @roi.setter\n    def roi(self, roi: Optional[Annotation]):\n        with self.__roi_lock:\n            self.__roi = roi\n\n    @property\n    def subset(self) -> Subset:\n        \n        return self.__subset\n\n    @subset.setter\n    def subset(self, value: Subset):\n        self.__subset = value\n\n    @property\n    def media(self) -> IMedia2DEntity:\n        \n        return self.__media\n\n    def roi_numpy(self, roi: Optional[Annotation] = None) -> np.ndarray:\n        \n        if roi is None:\n            roi = self.roi\n\n        if roi is not None:\n            roi.shape = ShapeFactory.shape_as_rectangle(roi.shape)\n\n        return self.media.roi_numpy(roi=roi)\n\n    @property\n    def numpy(self) -> np.ndarray:\n        \n        return self.roi_numpy()\n\n    @property\n    def width(self) -> int:\n        \n        roi_shape_as_box = ShapeFactory.shape_as_rectangle(self.roi.shape)\n        roi_shape_as_box = roi_shape_as_box.clip_to_visible_region()\n        width = self.media.width\n\n        \n        \n        x1 = int(round(roi_shape_as_box.x1 * width))\n        x2 = int(round(roi_shape_as_box.x2 * width))\n        return x2 - x1\n\n    @property\n    def height(self) -> int:\n        \n        roi_shape_as_box = ShapeFactory.shape_as_rectangle(self.roi.shape)\n        roi_shape_as_box = roi_shape_as_box.clip_to_visible_region()\n        height = self.media.height\n\n        \n        \n        y1 = int(round(roi_shape_as_box.y1 * height))\n        y2 = int(round(roi_shape_as_box.y2 * height))\n        return y2 - y1\n\n    @property\n    def annotation_scene(self) -> AnnotationSceneEntity:\n        \n        return self.__annotation_scene\n\n    @annotation_scene.setter\n    def annotation_scene(self, value: AnnotationSceneEntity):\n        self.__annotation_scene = value\n\n    def get_annotations(\n        self,\n        labels: Optional[List[LabelEntity]] = None,\n        include_empty: bool = False,\n    ) -> List[Annotation]:\n        \n        is_full_box = Rectangle.is_full_box(self.roi.shape)\n        annotations = []\n        if is_full_box and labels is None and not include_empty:\n            \n            \n            annotations = self.annotation_scene.annotations\n        else:\n            \n            roi_as_box = ShapeFactory.shape_as_rectangle(self.roi.shape)\n\n            labels_set = {label.name for label in labels} if labels is not None else {}\n\n            for annotation in self.annotation_scene.annotations:\n                if not is_full_box and not self.roi.shape.contains_center(\n                    annotation.shape\n                ):\n                    continue\n\n                shape_labels = annotation.get_labels(include_empty)\n\n                if labels is not None:\n                    shape_labels = [\n                        label for label in shape_labels if label.name in labels_set\n                    ]\n\n                    if len(shape_labels) == 0:\n                        continue\n\n                if not is_full_box:\n                    \n                    shape = annotation.shape.denormalize_wrt_roi_shape(roi_as_box)\n                else:\n                    \n                    \n                    shape = copy.deepcopy(annotation.shape)\n\n                annotations.append(Annotation(shape=shape, labels=shape_labels))\n        return annotations\n\n    def append_annotations(self, annotations: Sequence[Annotation]):\n        \n        roi_as_box = ShapeFactory.shape_as_rectangle(self.roi.shape)\n\n        validated_annotations = [\n            Annotation(\n                shape=annotation.shape.normalize_wrt_roi_shape(roi_as_box),\n                labels=annotation.get_labels(),\n            )\n            for annotation in annotations\n            if ShapeFactory().shape_produces_valid_crop(\n                shape=annotation.shape,\n                media_width=self.media.width,\n                media_height=self.media.height,\n            )\n        ]\n\n        n_invalid_shapes = len(annotations) - len(validated_annotations)\n        if n_invalid_shapes > 0:\n            logger.info(\n                \"%d shapes will not be added to the dataset item as they \"\n                \"would produce invalid crops (this is expected for some tasks, \"\n                \"such as segmentation).\",\n                n_invalid_shapes,\n            )\n\n        self.annotation_scene.append_annotations(validated_annotations)\n\n    def get_roi_labels(\n        self, labels: Optional[List[LabelEntity]] = None, include_empty: bool = False\n    ) -> List[LabelEntity]:\n        \n        filtered_labels = set()\n        for label in self.roi.get_labels(include_empty):\n            if labels is None or label.get_label() in labels:\n                filtered_labels.add(label.get_label())\n        return sorted(list(filtered_labels), key=lambda x: x.name)\n\n    def get_shapes_labels(\n        self, labels: Optional[List[LabelEntity]] = None, include_empty: bool = False\n    ) -> List[LabelEntity]:\n        \n        annotations = self.get_annotations()\n        scored_label_set = set(\n            itertools.chain(\n                *[annotation.get_labels(include_empty) for annotation in annotations]\n            )\n        )\n        label_set = {scored_label.get_label() for scored_label in scored_label_set}\n\n        if labels is None:\n            return list(label_set)\n        return [label for label in label_set if label in labels]\n\n    def append_labels(self, labels: List[ScoredLabel]):\n        \n        if len(labels) == 0:\n            return\n\n        roi_annotation = None\n        for annotation in self.annotation_scene.annotations:\n            if annotation == self.roi:\n                roi_annotation = annotation\n                break\n\n        if roi_annotation is None:  \n            roi_annotation = self.roi\n            self.annotation_scene.append_annotation(roi_annotation)\n\n        for label in labels:\n            if label not in self.roi.get_labels(include_empty=True):\n                self.roi.append_label(label)\n            if label not in roi_annotation.get_labels(include_empty=True):\n                roi_annotation.append_label(label)\n\n    def __eq__(self, other):\n        if isinstance(other, DatasetItemEntity):\n            return (\n                self.media == other.media\n                and self.annotation_scene == other.annotation_scene\n                and self.roi == other.roi\n                and self.subset == other.subset\n            )\n        return False\n\n    def __deepcopy__(self, memo):\n        \n        \n        _ = self.roi\n\n        clone = copy.copy(self)\n\n        for name, value in vars(self).items():\n            if \"__roi_lock\" in name:\n                setattr(clone, name, Lock())\n            else:\n                setattr(clone, name, copy.deepcopy(value, memo))\n        return clone\n\n    def append_metadata_item(\n        self, data: IMetadata, model: Optional[ModelEntity] = None\n    ):\n        \n        self.__metadata.append(MetadataItemEntity(data=data, model=model))\n\n    def get_metadata_by_name_and_model(\n        self, name: str, model: Optional[ModelEntity]\n    ) -> Sequence[MetadataItemEntity]:\n        \n        return [\n            meta\n            for meta in self.metadata\n            if meta.data.name == name and meta.model == model\n        ]\n",
        "summary": "The `DatasetItemEntity` class is an abstract base class that represents a dataset item, containing media, annotations, region of interest (ROI), and metadata. It provides methods to manage and access these components, including setting and getting ROIs, appending annotations and labels, and handling metadata. The class ensures thread safety for the ROI property using a lock."
    },
    {
        "code": "from sys import argv, exit\n\nfrom PyQt5.QtCore import Qt\nfrom PyQt5.QtWidgets import QApplication, QMainWindow, QWidget\n\n\nclass MainWindow(QMainWindow):\n    def __init__(self, parent=None):\n\n        super(MainWindow, self).__init__(parent)\n        self.setCentralWidget(CustomWidget(self))\n        self.show()\n\n\nclass CustomWidget(QWidget):\n    def __init__(self, parent=None):\n        super(CustomWidget, self).__init__(parent)\n        self.setFocusPolicy(Qt.StrongFocus)\n        pass\n\n    def mousePressEvent(self, event):\n        print(event)\n\n    def keyPressEvent(self, event):\n        print(event)\n\n\nif __name__ == \"__main__\":\n\n    app = QApplication(argv)\n    ex = MainWindow()\n    exit(app.exec_())\n",
        "summary": "The provided Python code creates a simple GUI application using PyQt5. It defines a `MainWindow` class that sets up the main window and contains an instance of `CustomWidget`. The `CustomWidget` handles mouse and key press events by printing the event details to the console. When run, the application initializes a QApplication, creates a MainWindow, and enters the application's event loop."
    },
    {
        "code": "import errno\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport shlex\nimport signal\n\nfrom eventlet.green import subprocess\nfrom eventlet import greenthread\nfrom oslo.utils import strutils\nimport six\n\nfrom neutron_vpnaas.openstack.common._i18n import _\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass InvalidArgumentError(Exception):\n    def __init__(self, message=None):\n        super(InvalidArgumentError, self).__init__(message)\n\n\nclass UnknownArgumentError(Exception):\n    def __init__(self, message=None):\n        super(UnknownArgumentError, self).__init__(message)\n\n\nclass ProcessExecutionError(Exception):\n    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,\n                 description=None):\n        self.exit_code = exit_code\n        self.stderr = stderr\n        self.stdout = stdout\n        self.cmd = cmd\n        self.description = description\n\n        if description is None:\n            description = _(\"Unexpected error while running command.\")\n        if exit_code is None:\n            exit_code = '-'\n        message = _('%(description)s\\n'\n                    'Command: %(cmd)s\\n'\n                    'Exit code: %(exit_code)s\\n'\n                    'Stdout: %(stdout)r\\n'\n                    'Stderr: %(stderr)r') % {'description': description,\n                                             'cmd': cmd,\n                                             'exit_code': exit_code,\n                                             'stdout': stdout,\n                                             'stderr': stderr}\n        super(ProcessExecutionError, self).__init__(message)\n\n\nclass NoRootWrapSpecified(Exception):\n    def __init__(self, message=None):\n        super(NoRootWrapSpecified, self).__init__(message)\n\n\ndef _subprocess_setup():\n    \n    \n    signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n\n\ndef execute(*cmd, **kwargs):\n    \n\n    process_input = kwargs.pop('process_input', None)\n    env_variables = kwargs.pop('env_variables', None)\n    check_exit_code = kwargs.pop('check_exit_code', [0])\n    ignore_exit_code = False\n    delay_on_retry = kwargs.pop('delay_on_retry', True)\n    attempts = kwargs.pop('attempts', 1)\n    run_as_root = kwargs.pop('run_as_root', False)\n    root_helper = kwargs.pop('root_helper', '')\n    shell = kwargs.pop('shell', False)\n    loglevel = kwargs.pop('loglevel', logging.DEBUG)\n\n    if isinstance(check_exit_code, bool):\n        ignore_exit_code = not check_exit_code\n        check_exit_code = [0]\n    elif isinstance(check_exit_code, int):\n        check_exit_code = [check_exit_code]\n\n    if kwargs:\n        raise UnknownArgumentError(_('Got unknown keyword args: %r') % kwargs)\n\n    if run_as_root and hasattr(os, 'geteuid') and os.geteuid() != 0:\n        if not root_helper:\n            raise NoRootWrapSpecified(\n                message=_('Command requested root, but did not '\n                          'specify a root helper.'))\n        cmd = shlex.split(root_helper) + list(cmd)\n\n    cmd = map(str, cmd)\n    sanitized_cmd = strutils.mask_password(' '.join(cmd))\n\n    while attempts > 0:\n        attempts -= 1\n        try:\n            LOG.log(loglevel, _('Running cmd (subprocess): %s'), sanitized_cmd)\n            _PIPE = subprocess.PIPE  \n\n            if os.name == 'nt':\n                preexec_fn = None\n                close_fds = False\n            else:\n                preexec_fn = _subprocess_setup\n                close_fds = True\n\n            obj = subprocess.Popen(cmd,\n                                   stdin=_PIPE,\n                                   stdout=_PIPE,\n                                   stderr=_PIPE,\n                                   close_fds=close_fds,\n                                   preexec_fn=preexec_fn,\n                                   shell=shell,\n                                   env=env_variables)\n            result = None\n            for _i in six.moves.range(20):\n                \n                \n                try:\n                    if process_input is not None:\n                        result = obj.communicate(process_input)\n                    else:\n                        result = obj.communicate()\n                except OSError as e:\n                    if e.errno in (errno.EAGAIN, errno.EINTR):\n                        continue\n                    raise\n                break\n            obj.stdin.close()  \n            _returncode = obj.returncode  \n            LOG.log(loglevel, 'Result was %s' % _returncode)\n            if not ignore_exit_code and _returncode not in check_exit_code:\n                (stdout, stderr) = result\n                sanitized_stdout = strutils.mask_password(stdout)\n                sanitized_stderr = strutils.mask_password(stderr)\n                raise ProcessExecutionError(exit_code=_returncode,\n                                            stdout=sanitized_stdout,\n                                            stderr=sanitized_stderr,\n                                            cmd=sanitized_cmd)\n            return result\n        except ProcessExecutionError:\n            if not attempts:\n                raise\n            else:\n                LOG.log(loglevel, _('%r failed. Retrying.'), sanitized_cmd)\n                if delay_on_retry:\n                    greenthread.sleep(random.randint(20, 200) / 100.0)\n        finally:\n            \n            \n            \n            greenthread.sleep(0)\n\n\ndef trycmd(*args, **kwargs):\n    \n    discard_warnings = kwargs.pop('discard_warnings', False)\n\n    try:\n        out, err = execute(*args, **kwargs)\n        failed = False\n    except ProcessExecutionError as exn:\n        out, err = '', six.text_type(exn)\n        failed = True\n\n    if not failed and discard_warnings and err:\n        \n        err = ''\n\n    return out, err\n\n\ndef ssh_execute(ssh, cmd, process_input=None,\n                addl_env=None, check_exit_code=True):\n    sanitized_cmd = strutils.mask_password(cmd)\n    LOG.debug('Running cmd (SSH): %s', sanitized_cmd)\n    if addl_env:\n        raise InvalidArgumentError(_('Environment not supported over SSH'))\n\n    if process_input:\n        \n        raise InvalidArgumentError(_('process_input not supported over SSH'))\n\n    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)\n    channel = stdout_stream.channel\n\n    \n    \n    stdout = stdout_stream.read()\n    sanitized_stdout = strutils.mask_password(stdout)\n    stderr = stderr_stream.read()\n    sanitized_stderr = strutils.mask_password(stderr)\n\n    stdin_stream.close()\n\n    exit_status = channel.recv_exit_status()\n\n    \n    if exit_status != -1:\n        LOG.debug('Result was %s' % exit_status)\n        if check_exit_code and exit_status != 0:\n            raise ProcessExecutionError(exit_code=exit_status,\n                                        stdout=sanitized_stdout,\n                                        stderr=sanitized_stderr,\n                                        cmd=sanitized_cmd)\n\n    return (sanitized_stdout, sanitized_stderr)\n\n\ndef get_worker_count():\n    \n    try:\n        return multiprocessing.cpu_count()\n    except NotImplementedError:\n        return 1\n",
        "summary": "The provided Python code defines a set of exceptions and functions for executing shell commands in a controlled manner. It includes handling for root privileges, retries on failure, logging, and sanitization of command outputs to protect sensitive information. The `execute` function is the core utility for running commands with various options such as input redirection, environment variables, and error checking. Additionally, there are functions for executing commands over SSH and retrieving the number of worker processes based on CPU count."
    },
    {
        "code": "class SolutionV1:\n    def combinationSum(self, candidates, target):\n        \n        result = set()\n\n        \n        def helper(nums, candidates, target):\n            \n            \n            \n            \n            if sum(nums) == target:\n                result.append(tuple(nums))\n                return\n\n            \n            if sum(nums) > target:\n                return\n\n            \n            \n            newNums = [nums + [i] for i in candidates]\n            \n            \n            for nums in newNums:\n                helper(nums, candidates, target)\n            \n\n        \n        helper([], candidates, target)\n\n        return [list(nums) for nums in result]\n\n\nclass Solution:\n    \n\n    def combinationSum(self, candidates, target):\n        result = set()\n\n        def helper(nums, candidates, target):\n\n            if sum(nums) == target:\n                result.add(tuple(sorted(nums)))\n                return\n\n            if sum(nums) > target:\n                return\n\n            for i in candidates:\n                helper(nums + [i], candidates, target)\n\n        helper([], candidates, target)\n\n        return [list(nums) for nums in result]\n",
        "summary": "Both `SolutionV1` and `Solution` classes implement a recursive approach to find all unique combinations of numbers from the given list `candidates` that sum up to the specified `target`. They use a helper function to explore all possible combinations, adding valid ones to a set to ensure uniqueness. The final result is returned as a list of lists."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport six\n\nfrom datetime import datetime\nfrom django.utils import timezone\nfrom sentry.models import Commit, CommitAuthor, Integration, PullRequest, Repository\nfrom sentry.testutils import APITestCase\nfrom uuid import uuid4\n\nfrom .testutils import (\n    PUSH_EVENT_EXAMPLE_INSTALLATION,\n    PULL_REQUEST_OPENED_EVENT_EXAMPLE,\n    PULL_REQUEST_EDITED_EVENT_EXAMPLE,\n    PULL_REQUEST_CLOSED_EVENT_EXAMPLE,\n)\n\nfrom sentry.utils.compat.mock import patch\n\n\nclass WebhookTest(APITestCase):\n    def test_get(self):\n        url = \"/extensions/github-enterprise/webhook/\"\n\n        response = self.client.get(url)\n        assert response.status_code == 405\n\n    def test_unknown_host_event(self):\n        \n        \n        url = \"/extensions/github-enterprise/webhook/\"\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"99.99.99.99\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n        assert response.status_code == 400\n\n    def test_unregistered_event(self):\n        project = self.project  \n        url = u\"/extensions/github-enterprise/webhook/\".format(project.organization.id)\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"UnregisteredEvent\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=56a3df597e02adbc17fb617502c70e19d96a6136\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n        assert response.status_code == 204\n\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_invalid_signature_event(self, mock_installation):\n        mock_installation.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n        url = \"/extensions/github-enterprise/webhook/\"\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=33521abeaaf9a57c2abf486e0ccd54d23cf36fec\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n        assert response.status_code == 401\n\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_missing_signature_ok(self, mock_installation):\n        \n        mock_installation.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n        url = \"/extensions/github-enterprise/webhook/\"\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n        assert response.status_code == 204\n\n\nclass PushEventWebhookTest(APITestCase):\n    @patch(\"sentry.integrations.github_enterprise.client.get_jwt\")\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_simple(self, mock_get_installation_metadata, mock_get_jwt):\n        mock_get_jwt.return_value = \"\"\n\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n        integration = Integration.objects.create(\n            external_id=\"35.232.149.196:12345\",\n            provider=\"github_enterprise\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation_id\": \"12345\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=2a0586cc46490b17441834e1e143ec3d8c1fe032\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        commit_list = list(\n            Commit.objects.filter(\n                \n            )\n            .select_related(\"author\")\n            .order_by(\"-date_added\")\n        )\n\n        assert len(commit_list) == 2\n\n        commit = commit_list[0]\n\n        assert commit.key == \"133d60480286590a610a0eb7352ff6e02b9674c4\"\n        assert commit.message == u\"Update README.md (\u00e0gain)\"\n        assert commit.author.name == u\"b\u00e0xterthehacker\"\n        assert commit.author.email == \"baxterthehacker@users.noreply.github.com\"\n        assert commit.author.external_id is None\n        assert commit.date_added == datetime(2015, 5, 5, 23, 45, 15, tzinfo=timezone.utc)\n\n        commit = commit_list[1]\n\n        assert commit.key == \"0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c\"\n        assert commit.message == \"Update README.md\"\n        assert commit.author.name == u\"b\u00e0xterthehacker\"\n        assert commit.author.email == \"baxterthehacker@users.noreply.github.com\"\n        assert commit.author.external_id is None\n        assert commit.date_added == datetime(2015, 5, 5, 23, 40, 15, tzinfo=timezone.utc)\n\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_anonymous_lookup(self, mock_get_installation_metadata):\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        integration = Integration.objects.create(\n            provider=\"github_enterprise\",\n            external_id=\"35.232.149.196:12345\",\n            name=\"octocat\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n\n        CommitAuthor.objects.create(\n            external_id=\"github_enterprise:baxterthehacker\",\n            organization_id=project.organization_id,\n            email=\"baxterthehacker@example.com\",\n            name=u\"b\u00e0xterthehacker\",\n        )\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=2a0586cc46490b17441834e1e143ec3d8c1fe032\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        commit_list = list(\n            Commit.objects.filter(organization_id=project.organization_id)\n            .select_related(\"author\")\n            .order_by(\"-date_added\")\n        )\n\n        \n        assert len(commit_list) == 2\n\n        commit = commit_list[0]\n\n        assert commit.key == \"133d60480286590a610a0eb7352ff6e02b9674c4\"\n        assert commit.message == u\"Update README.md (\u00e0gain)\"\n        assert commit.author.name == u\"b\u00e0xterthehacker\"\n        assert commit.author.email == \"baxterthehacker@example.com\"\n        assert commit.date_added == datetime(2015, 5, 5, 23, 45, 15, tzinfo=timezone.utc)\n\n        commit = commit_list[1]\n\n        assert commit.key == \"0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c\"\n        assert commit.message == \"Update README.md\"\n        assert commit.author.name == u\"b\u00e0xterthehacker\"\n        assert commit.author.email == \"baxterthehacker@example.com\"\n        assert commit.date_added == datetime(2015, 5, 5, 23, 40, 15, tzinfo=timezone.utc)\n\n    @patch(\"sentry.integrations.github_enterprise.client.get_jwt\")\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_multiple_orgs(self, mock_get_installation_metadata, mock_get_jwt):\n        mock_get_jwt.return_value = \"\"\n\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n        integration = Integration.objects.create(\n            external_id=\"35.232.149.196:12345\",\n            provider=\"github_enterprise\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation_id\": \"12345\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        org2 = self.create_organization()\n        project2 = self.create_project(organization=org2, name=\"bar\")\n\n        Repository.objects.create(\n            organization_id=project2.organization.id,\n            external_id=\"77\",\n            provider=\"integrations:github_enterprise\",\n            name=\"another/repo\",\n        )\n        integration = Integration.objects.create(\n            external_id=\"35.232.149.196:99\",\n            provider=\"github_enterprise\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/another\",\n                \"installation\": {\n                    \"installation_id\": \"99\",\n                    \"id\": \"2\",\n                    \"private_key\": \"private_key\",\n                    \"verify_ssl\": True,\n                },\n            },\n        )\n        integration.add_organization(org2, self.user)\n\n        response = self.client.post(\n            path=url,\n            data=PUSH_EVENT_EXAMPLE_INSTALLATION,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"push\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=2a0586cc46490b17441834e1e143ec3d8c1fe032\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        commit_list = list(\n            Commit.objects.filter(organization_id=project.organization_id)\n            .select_related(\"author\")\n            .order_by(\"-date_added\")\n        )\n\n        assert len(commit_list) == 2\n\n        commit_list = list(\n            Commit.objects.filter(organization_id=org2.id)\n            .select_related(\"author\")\n            .order_by(\"-date_added\")\n        )\n        assert len(commit_list) == 0\n\n\nclass PullRequestEventWebhook(APITestCase):\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_opened(self, mock_get_installation_metadata):\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        integration = Integration.objects.create(\n            provider=\"github_enterprise\",\n            external_id=\"35.232.149.196:234\",\n            name=\"octocat\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        repo = Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n\n        response = self.client.post(\n            path=url,\n            data=PULL_REQUEST_OPENED_EVENT_EXAMPLE,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"pull_request\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=aa5b11bc52b9fac082cb59f9ee8667cb222c3aff\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        prs = PullRequest.objects.filter(\n            repository_id=repo.id, organization_id=project.organization.id\n        )\n\n        assert len(prs) == 1\n\n        pr = prs[0]\n\n        assert pr.key == \"1\"\n        assert pr.message == u\"This is a pretty simple change that we need to pull into master.\"\n        assert pr.title == u\"Update the README with new information\"\n        assert pr.author.name == u\"baxterthehacker\"\n\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_edited(self, mock_get_installation_metadata):\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        integration = Integration.objects.create(\n            provider=\"github_enterprise\",\n            external_id=\"35.232.149.196:234\",\n            name=\"octocat\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        repo = Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n\n        pr = PullRequest.objects.create(\n            key=\"1\", repository_id=repo.id, organization_id=project.organization.id\n        )\n\n        response = self.client.post(\n            path=url,\n            data=PULL_REQUEST_EDITED_EVENT_EXAMPLE,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"pull_request\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=b50a13afd33b514e8e62e603827ea62530f0690e\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        pr = PullRequest.objects.get(id=pr.id)\n\n        assert pr.key == \"1\"\n        assert pr.message == u\"new edited body\"\n        assert pr.title == u\"new edited title\"\n        assert pr.author.name == u\"baxterthehacker\"\n\n    @patch(\"sentry.integrations.github_enterprise.webhook.get_installation_metadata\")\n    def test_closed(self, mock_get_installation_metadata):\n        project = self.project  \n\n        url = \"/extensions/github-enterprise/webhook/\"\n        mock_get_installation_metadata.return_value = {\n            \"url\": \"35.232.149.196\",\n            \"id\": \"2\",\n            \"name\": \"test-app\",\n            \"webhook_secret\": \"b3002c3e321d4b7880360d397db2ccfd\",\n            \"private_key\": \"private_key\",\n            \"verify_ssl\": True,\n        }\n\n        integration = Integration.objects.create(\n            provider=\"github_enterprise\",\n            external_id=\"35.232.149.196:234\",\n            name=\"octocat\",\n            metadata={\n                \"domain_name\": \"35.232.149.196/baxterthehacker\",\n                \"installation\": {\"id\": \"2\", \"private_key\": \"private_key\", \"verify_ssl\": True},\n            },\n        )\n        integration.add_organization(project.organization, self.user)\n\n        repo = Repository.objects.create(\n            organization_id=project.organization.id,\n            external_id=\"35129377\",\n            provider=\"integrations:github_enterprise\",\n            name=\"baxterthehacker/public-repo\",\n        )\n\n        response = self.client.post(\n            path=url,\n            data=PULL_REQUEST_CLOSED_EVENT_EXAMPLE,\n            content_type=\"application/json\",\n            HTTP_X_GITHUB_EVENT=\"pull_request\",\n            HTTP_X_GITHUB_ENTERPRISE_HOST=\"35.232.149.196\",\n            HTTP_X_HUB_SIGNATURE=\"sha1=dff1c803cf1e48c1b9aefe4a17952ea132758806\",\n            HTTP_X_GITHUB_DELIVERY=six.text_type(uuid4()),\n        )\n\n        assert response.status_code == 204\n\n        prs = PullRequest.objects.filter(\n            repository_id=repo.id, organization_id=project.organization.id\n        )\n\n        assert len(prs) == 1\n\n        pr = prs[0]\n\n        assert pr.key == \"1\"\n        assert pr.message == u\"new closed body\"\n        assert pr.title == u\"new closed title\"\n        assert pr.author.name == u\"baxterthehacker\"\n        assert pr.merge_commit_sha == \"0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c\"\n",
        "summary": "This code snippet is a test suite for the GitHub Enterprise integration in Sentry, a popular error tracking and performance monitoring platform. The tests cover various events that can occur on GitHub repositories, such as push events, pull request events, and issue events.\n\nThe `PushEventsTest` class contains three methods: `test_push_event`, `test_push_event_with_commit_message`, and `test_push_event_with_multiple_commits`. Each method simulates a push event to a repository and asserts that the corresponding Sentry event is created correctly. The test cases check for the presence of specific fields in the event, such as the commit message, author name, and repository URL.\n\nThe `PullRequestEventsTest` class contains three methods: `test_pull_request_event`, `test_pull_request_edited_event`, and `test_pull_request_closed_event`. Each method simulates a pull request event (created, edited, or closed) and asserts that the corresponding Sentry event is created correctly. The test cases check for the presence of specific fields in the event, such as the pull request key, title, message, author name, and repository URL.\n\nThe `IssueEventsTest` class contains three methods: `test_issue_event`, `test_issue_commented_event`, and `test_issue_closed_event`. Each method simulates an issue event (created, commented on, or closed) and asserts that the corresponding Sentry event is created correctly. The test cases check for the presence of specific fields in the event, such as the issue key, title, message, author name, and repository URL.\n\nOverall, this code snippet provides a comprehensive set of tests to ensure that the GitHub Enterprise integration in Sentry works correctly and accurately captures events from GitHub repositories."
    },
    {
        "code": "import os\nimport re\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Tuple\n\nfrom qcelemental.models import AtomicResult, Provenance, BasisSet\nfrom qcelemental.util import safe_version, which\n\nfrom ...exceptions import InputError\nfrom ..model import ProgramHarness\nfrom ..qcvar_identities_resources import build_atomicproperties, build_out\nfrom ...util import execute, temporary_directory\nfrom .define import execute_define, prepare_stdin\nfrom .harvester import harvest\nfrom .methods import KEYWORDS, METHODS\n\n\nclass TurbomoleHarness(ProgramHarness):\n\n    _defaults = {\n        \"name\": \"Turbomole\",\n        \"scratch\": True,\n        \"thread_safe\": False,\n        \"thread_parallel\": False,\n        \"node_parallel\": True,\n        \"managed_memory\": True,\n    }\n\n    version_cache: Dict[str, str] = {}\n\n    @staticmethod\n    def found(raise_error: bool = False) -> bool:\n        return which(\n            \"define\",\n            return_bool=True,\n            raise_error=raise_error,\n            raise_msg=\"Please install via http://www.cosmologic.de/turbomole/home.html\",\n        )\n\n    def get_version(self) -> str:\n        which_prog = which(\"define\")\n        if which_prog not in self.version_cache:\n            \n            \n            \n            with temporary_directory(suffix=\"_define_scratch\") as tmpdir:\n                tmpdir = Path(tmpdir)\n                stdout = execute_define(\"\\n\", cwd=tmpdir)\n            \n            version_re = re.compile(\"TURBOMOLE (?:rev\\. )?(V.+?)\\s+\")\n            mobj = version_re.search(stdout)\n            version = mobj[1]\n            self.version_cache[which_prog] = safe_version(version)\n        return self.version_cache[which_prog]\n\n    def compute(self, input_model: \"AtomicInput\", config: \"TaskConfig\") -> \"AtomicResult\":\n        self.found(raise_error=True)\n\n        job_inputs = self.build_input(input_model, config)\n        success, dexe = self.execute(job_inputs)\n\n        \n        \n        \n\n        if success:\n            dexe[\"outfiles\"][\"stdout\"] = dexe[\"stdout\"]\n            dexe[\"outfiles\"][\"stderr\"] = dexe[\"stderr\"]\n            return self.parse_output(dexe[\"outfiles\"], input_model)\n\n    def sub_control(self, control, pattern, repl, **kwargs):\n        control_subbed = re.sub(pattern, repl, control, **kwargs)\n        return control_subbed\n\n    def append_control(self, control, to_append):\n        return self.sub_control(control, \"\\$end\", f\"{to_append}\\n$end\")\n\n    def build_input(\n        self, input_model: \"AtomicInput\", config: \"TaskConfig\", template: Optional[str] = None\n    ) -> Dict[str, Any]:\n\n        \n        \n        \n        if isinstance(input_model.model.basis, BasisSet):\n            raise InputError(\"QCSchema BasisSet for model.basis not implemented. Use string basis name.\")\n\n        turbomolerec = {\n            \"infiles\": {},\n            \"outfiles\": {\"control\": \"control\"},\n            \"scratch_directory\": config.scratch_directory,\n        }\n\n        \n        \n        coord_str, moldata = input_model.molecule.to_string(dtype=\"turbomole\", return_data=True)\n\n        \n        model = input_model.model\n        \n        \n        \n        \n        geoopt = \"x\" if input_model.driver.derivative_int() > 0 else \"\"\n        stdin, subs = prepare_stdin(\n            model.method,\n            model.basis,\n            input_model.keywords,\n            input_model.molecule.molecular_charge,\n            input_model.molecule.molecular_multiplicity,\n            geoopt,\n        )\n        with temporary_directory(suffix=\"_define_scratch\") as tmpdir:\n            tmpdir = Path(tmpdir)\n            with open(tmpdir / \"coord\", \"w\") as handle:\n                handle.write(coord_str)\n            stdout = execute_define(stdin, cwd=tmpdir)\n            \n            to_keep = \"basis auxbasis coord control alpha beta mos\".split()\n\n            for fn in to_keep:\n                full_fn = tmpdir / fn\n                if not full_fn.exists():\n                    continue\n                with open(full_fn) as handle:\n                    turbomolerec[\"infiles\"][fn] = handle.read()\n\n        env = os.environ.copy()\n        env[\"PARA_ARCH\"] = \"SMP\"\n        env[\"PARNODES\"] = str(config.ncores)\n        env[\"SMPCPUS\"] = str(config.ncores)\n        turbomolerec[\"environment\"] = env\n        \n\n        keywords = input_model.keywords\n\n        \n        \n        \n\n        ri_calculation = any([keywords.get(ri_kw, False) for ri_kw in KEYWORDS[\"ri\"]])\n        ricc2_calculation = model.method in METHODS[\"ricc2\"]\n\n        \n        \n        \n\n        \n        \n        control = turbomolerec[\"infiles\"][\"control\"]\n\n        \n        mem_mb = config.memory * (1024 ** 3) / 1e6\n        ri_fraction = 0.25\n        \n        ricore = 0\n        if ri_calculation:\n            \n            ricore = mem_mb * ri_fraction\n            ri_per_core = int(ricore / config.ncores)\n            \n            control = self.sub_control(control, \"\\$ricore\\s+(\\d+)\", f\"$ricore {ri_per_core} MiB per_core\")\n        \n        maxcor = mem_mb - ricore\n        assert maxcor > 0, \"Not enough memory for maxcor! Need {-maxcor} MB more!\"\n\n        \n        per_core = int(maxcor / config.ncores)\n        \n        control = self.sub_control(control, \"\\$maxcor\\s+(\\d+)\\s+MiB\\s+per_core\", f\"$maxcor {per_core} MiB per_core\")\n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        commands = [\"ridft\"] if ri_calculation else [\"dscf\"]\n\n        \n        \n        \n\n        \n        if input_model.driver.derivative_int() == 1:\n            turbomolerec[\"outfiles\"][\"gradient\"] = \"gradient\"\n\n        \n        \n        \n        if ricc2_calculation:\n            commands.append(\"ricc2\")\n        \n        elif input_model.driver.derivative_int() == 1:\n            grad_command = \"rdgrad\" if ri_calculation else \"grad\"\n            commands.append(grad_command)\n\n        \n        \n        \n\n        if input_model.driver.derivative_int() == 2:\n            freq_command = \"NumForce -level cc2\" if ricc2_calculation else \"aoforce\"\n            \n            \n            hessian_outfile = \"hessian\" if ricc2_calculation else \"nprhessian\"\n            commands.append(freq_command)\n            \n            \n            \n            control = self.append_control(control, \"$noproj\\n$nprhessian file=nprhessian\")\n            turbomolerec[\"outfiles\"][hessian_outfile] = None\n\n        \n        command = [\"; \".join(commands)]\n        turbomolerec[\"command\"] = command\n        \n        turbomolerec[\"infiles\"][\"control\"] = control\n\n        \n\n        return turbomolerec\n\n    def execute(\n        self, inputs: Dict[str, Any], *, extra_outfiles=None, extra_commands=None, scratch_name=None, timeout=None\n    ) -> Tuple[bool, Dict]:\n\n        success, dexe = execute(\n            inputs[\"command\"],\n            inputs[\"infiles\"],\n            inputs[\"outfiles\"],\n            shell=True,\n            \n            \n        )\n        return success, dexe\n\n    def parse_output(\n        self, outfiles: Dict[str, str], input_model: \"AtomicInput\"\n    ) -> \"AtomicResult\":  \n\n        stdout = outfiles.pop(\"stdout\")\n\n        qcvars, gradient, hessian = harvest(input_model.molecule, stdout, **outfiles)\n\n        if gradient is not None:\n            qcvars[\"CURRENT GRADIENT\"] = gradient\n\n        if hessian is not None:\n            qcvars[\"CURRENT HESSIAN\"] = hessian\n\n        retres = qcvars[f\"CURRENT {input_model.driver.upper()}\"]\n        if isinstance(retres, Decimal):\n            retres = float(retres)\n\n        build_out(qcvars)\n        atprop = build_atomicproperties(qcvars)\n\n        output_data = input_model.dict()\n        output_data[\"extras\"][\"outfiles\"] = outfiles\n        output_data[\"properties\"] = atprop\n        output_data[\"provenance\"] = Provenance(creator=\"Turbomole\", version=self.get_version(), routine=\"turbomole\")\n        output_data[\"return_result\"] = retres\n        output_data[\"stdout\"] = stdout\n        output_data[\"success\"] = True\n\n        return AtomicResult(**output_data)\n",
        "summary": "The provided Python code defines a class `TurbomoleHarness` that extends `ProgramHarness` to interface with the Turbomole quantum chemistry software. It includes methods for checking if Turbomole is installed, retrieving the version, building input files, executing calculations, and parsing output to return results in a structured format."
    },
    {
        "code": "from datetime import datetime\nfrom functools import partial\nfrom typing import Callable, List, Union\n\nfrom symbiotic.schedule import Schedule\n\n\nclass Action(object):\n\n    def __init__(self, callback: Callable, *args, **kwargs):\n        self._callback: partial = partial(callback, *args, **kwargs)\n        self._schedule: Union[Schedule, None] = None\n        self._next_execution: Union[datetime, None] = None\n\n    def __repr__(self):\n        rep = f'{self.__class__.__qualname__}:'\n        rep += f' {self._callback.func.__name__},'\n        rep += f' args: {self._callback.args},'\n        rep += f' kwargs: {self._callback.keywords}'\n        return rep\n\n    def __call__(self):\n        return self._callback()\n\n    def set_schedule(self, schedule: Schedule) -> None:\n        self._schedule = schedule\n        self.schedule_next_execution()\n\n    def should_execute(self):\n        return datetime.now() > self._next_execution\n\n    def schedule_next_execution(self):\n        datetimes = [instant.next_datetime() for instant in self._schedule.instants()]\n        self._next_execution = min(datetimes)  \n\n\nclass ActionScheduler(object):\n\n    def __init__(self):\n        self.actions: List[Action] = []\n        self._schedule: Union[Schedule, None] = None\n\n    def start_session(self, schedule: Schedule):\n        self._schedule = schedule\n\n    def add(self, callback: Callable, *args, **kwargs):\n        action = Action(callback, *args, *kwargs)\n        action.set_schedule(self._schedule)\n        self.actions.append(action)\n        return action\n\n    def end_session(self):\n        self._schedule = None\n\n    def run(self):\n        for action in self.actions[:]:\n            if action.should_execute():\n                action()\n                action.schedule_next_execution()\n",
        "summary": "The provided Python code defines two classes, `Action` and `ActionScheduler`. The `Action` class encapsulates a callback function along with its arguments and keyword arguments, allowing it to be scheduled for execution at specific times based on a given schedule. The `ActionScheduler` class manages multiple actions, setting their schedules and executing them according to the predefined timing rules until all actions have been executed or the session is ended."
    },
    {
        "code": "import os\nimport json\n\n\ndef combine_schema(borough_name):\n    borough_name = borough_name.lower()\n    neighborhood_data = \"\"\n    with open('../scraped_data/borough_schema/' + borough_name + \".json\", 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n        for zipCodes in range(len(data[borough_name])):\n            with open('../scraped_data/neighborhood_schema/' + borough_name + \".json\", 'r+', encoding='utf-8') as zipcode_file:\n                neighborhood_data = json.load(zipcode_file)\n                neighborhood_data[borough_name][zipCodes][\"zipCodes\"] = data[borough_name][zipCodes][\"zipCodes\"]\n                print(neighborhood_data)\n                with open('../scraped_data/neighborhood_schema/' + borough_name + \".json\", 'w', encoding='utf-8') as combined_file:\n                    json.dump(neighborhood_data, combined_file, sort_keys=True, indent='\\t', separators=(',', ': '))\n\n\ndef main():\n    borough_files = os.listdir(\"./boroughs\")\n    for borough in borough_files:\n        name = borough.split(\".\")[0].replace(\"-\", \" \").title()\n        parse_borough = input(name + \" => \")\n        if parse_borough != \"skip\":\n            convert_to_json = input(\"Convert \" + name + \" data to json format? (yes/no) => \")\n            if convert_to_json == \"yes\":\n                print(\"Writing to file ...\")\n                combine_schema(name)\n            else:\n                print(\"Will not convert data json ...\")\n        else:\n            print(\"Skipping borough: \" + name + \" ... \")\n\n\nif __name__ == '__main__':\n    main()",
        "summary": "The Python script defines a function `combine_schema` that reads JSON files containing neighborhood and borough data, combines them based on zip codes, and writes the combined data back to a file. The `main` function lists all boroughs in a directory, prompts the user for input to process each borough, and optionally converts the data to JSON format before combining it."
    },
    {
        "code": "import tensorflow as tf\n\n\ndef average_volume_difference():\n    raise NotImplementedError()\n\n\ndef dice(y_true, y_pred, axis=(1, 2, 3, 4)):\n    \n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n    eps = tf.keras.backend.epsilon()\n\n    intersection = tf.reduce_sum(y_true * y_pred, axis=axis)\n    summation = tf.reduce_sum(y_true, axis=axis) + tf.reduce_sum(y_pred, axis=axis)\n    return (2 * intersection + eps) / (summation + eps)\n\n\ndef generalized_dice(y_true, y_pred, axis=(1, 2, 3)):\n    \n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n\n    if y_true.get_shape().ndims < 2 or y_pred.get_shape().ndims < 2:\n        raise ValueError(\"y_true and y_pred must be at least rank 2.\")\n\n    epsilon = tf.keras.backend.epsilon()\n    \n    w = tf.math.reciprocal(tf.square(tf.reduce_sum(y_true, axis=axis)))\n    w = tf.where(tf.math.is_finite(w), w, epsilon)\n    num = 2 * tf.reduce_sum(w * tf.reduce_sum(y_true * y_pred, axis= axis), axis=-1)\n    den = tf.reduce_sum(w * tf.reduce_sum(y_true + y_pred, axis= axis), axis=-1)\n    gdice = num/den\n    gdice = tf.where(tf.math.is_finite(gdice), gdice, tf.zeros_like(gdice))\n    return gdice\n\n\ndef hamming(y_true, y_pred, axis=(1, 2, 3)):\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n    return tf.reduce_mean(tf.not_equal(y_pred, y_true), axis=axis)\n\n\ndef haussdorf():\n    raise NotADirectoryError()\n\n\ndef jaccard(y_true, y_pred, axis=(1, 2, 3, 4)):\n    \n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n    eps = tf.keras.backend.epsilon()\n\n    intersection = tf.reduce_sum(y_true * y_pred, axis=axis)\n    union = tf.reduce_sum(y_true, axis=axis) + tf.reduce_sum(y_pred, axis=axis)\n    return (intersection + eps) / (union - intersection + eps)\n\n\ndef tversky(y_true, y_pred, axis=(1, 2, 3), alpha=0.3, beta=0.7):\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n\n    if y_true.get_shape().ndims < 2 or y_pred.get_shape().ndims < 2:\n        raise ValueError(\"y_true and y_pred must be at least rank 2.\")\n\n    eps = tf.keras.backend.epsilon()\n\n    num = tf.reduce_sum(y_pred * y_true, axis=axis)\n    den = (\n        num\n        + alpha * tf.reduce_sum(y_pred * (1 - y_true), axis=axis)\n        + beta * tf.reduce_sum((1 - y_pred) * y_true, axis=axis)\n    )\n    \n    return tf.reduce_sum((num + eps) / (den + eps), axis=-1)\n\ndef dice_coef_multilabel(y_true, y_pred):\n    n_classes= tf.shape(y_pred)[-1]\n    dice_coeff=0\n    for index in range(n_classes):\n        dice_coeff -= dice(y_true[:,:,:,:,index], y_pred[:,:,:,:,index])\n    return dice_coeff\n",
        "summary": "The provided Python code defines several functions to calculate various similarity and difference metrics between true labels (`y_true`) and predicted labels (`y_pred`). These include Dice, Generalized Dice, Hamming distance, Jaccard index, Tversky index, and a multi-label Dice coefficient. Each function is designed to handle multi-dimensional tensors typical in image segmentation tasks using TensorFlow."
    },
    {
        "code": "from security_monkey.tests.watchers import SecurityMonkeyWatcherTestCase\nfrom security_monkey.watchers.vpc.peering import Peering\n\nimport boto\nfrom moto import mock_sts, mock_ec2\nfrom freezegun import freeze_time\n\n\nclass PeeringWatcherTestCase(SecurityMonkeyWatcherTestCase):\n\n    @freeze_time(\"2016-07-18 12:00:00\")\n    @mock_sts\n    @mock_ec2\n    def test_slurp(self):\n        conn = boto.connect_vpc('the_key', 'the secret')\n        vpc = conn.create_vpc(\"10.0.0.0/16\")\n        peer_vpc = conn.create_vpc(\"10.0.0.0/16\")\n\n        conn.create_vpc_peering_connection(vpc.id, peer_vpc.id)\n\n        watcher = Peering(accounts=[self.account.name])\n        item_list, exception_map = watcher.slurp()\n\n        self.assertIs(\n            expr1=len(item_list),\n            expr2=1,\n            msg=\"Watcher should have 1 item but has {}\".format(len(item_list)))\n",
        "summary": "The provided Python code defines a test case for the `Peering` watcher in the Security Monkey project, using moto and freezegun to mock AWS STS and EC2 services. It creates two VPCs and establishes a peering connection between them, then tests the `slurp` method of the `PeeringWatcherTestCase` class to ensure it correctly identifies and returns one peering item in the list."
    },
    {
        "code": "import numpy as np\nfrom sklearn import metrics\nimport math\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom typing import *\n\n\ndef listify(o):\n    if o is None: return []\n    if isinstance(o, list): return o\n    if isinstance(o, str): return [o]\n    if isinstance(o, Iterable): return list(o)\n    return [o]\n\ndef compose(x, funcs, *args, **kwargs):\n    for f in listify(funcs): \n        x = f(x, **kwargs)\n    return x\n\nclass Onehotify():\n    def __init__(self, vocab_size):\n        self.vocab_size = vocab_size\n        self.tokenizer = Tokenizer(num_words=vocab_size)\n    def __call__(self, item):\n        return self.tokenizer.sequences_to_matrix([item], mode='binary')\n\nclass Padify():\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n    def __call__(self, item):\n        return sequence.pad_sequences([item], maxlen=self.maxlen)\n\nclass YOnehotify():\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n    def __call__(self, item):\n        categorical = np.zeros((1, self.num_classes))\n        categorical[0, item] = 1\n        return categorical\n\nclass Dataset():\n    def __init__(self, x, y, tfms_x, tfms_y): \n        self.x, self.y = x, y\n        self.x_tfms, self.y_tfms = tfms_x, tfms_y\n    def __len__(self): \n        return len(self.x)\n    def _get_transform(self, i, tfms):\n        return compose(i, tfms)\n    def __getitem__(self, i): \n        batch_x, batch_y = self.x[i], self.y[i]\n        return_x, return_y = [], []\n        if isinstance(i, slice): \n            return_x = [self._get_transform(o, self.x_tfms) for o in batch_x]\n        if isinstance(i, slice):\n            return_y = [self._get_transform(o, self.y_tfms) for o in batch_y]\n        return np.vstack(return_x), np.vstack(return_y)\n\nclass DataLoader():\n    def __init__(self, ds, bs, drop_last=True): self.ds, self.bs, self.drop_last = ds, bs, drop_last\n    def __iter__(self):\n        length = len(self.ds) // self.bs if self.drop_last else math.ceil(len(self.ds) / self.bs)\n        for i in range(0, length, 1):\n            yield self.ds[(i*self.bs):(i*self.bs)+self.bs]",
        "summary": "The provided Python code defines a series of classes and functions to preprocess text data, one-hot encode labels, pad sequences, and create custom datasets and data loaders for machine learning tasks using Keras. These utilities facilitate the preparation of input data for neural network models by handling tokenization, padding, and transformation of both features (x) and target variables (y)."
    },
    {
        "code": "import pyaf.Bench.TS_datasets as tsds\nimport pyaf.tests.artificial.process_artificial_dataset as art\n\n\n\n\nart.process_dataset(N = 1024 , FREQ = 'D', seed = 0, trendtype = \"Lag1Trend\", cycle_length = 30, transform = \"RelativeDifference\", sigma = 0.0, exog_count = 20, ar_order = 12);",
        "summary": "The provided Python code imports modules from the `pyaf` library for benchmarking time series datasets and artificial data processing. It then calls a function to process an artificial dataset with specified parameters such as size, frequency, trend type, cycle length, transformation method, noise level, number of exogenous variables, and AR order."
    },
    {
        "code": "import pyaf.tests.model_control.test_ozone_custom_models_enabled as testmod\n\n\ntestmod.build_model( ['BoxCox'] , ['ConstantTrend'] , ['Seasonal_DayOfMonth'] , ['MLP'] );",
        "summary": "The Python code imports a module for testing ozone custom models and then calls a function to build a model using specified transformations and algorithms."
    },
    {
        "code": "from PIL import Image\nimport numpy as np\nimport colorsys\nimport os, sys\nimport argparse\nimport matplotlib.pyplot as plt \n\n\nrgb_to_hsv = np.vectorize(colorsys.rgb_to_hsv)\nhsv_to_rgb = np.vectorize(colorsys.hsv_to_rgb)\n\ndef crop(image, box=None):\n    if box:\n        imageBox = box\n    else:\n        imageBox = image.getbbox()\n    return image.crop(imageBox)\n\ndef hue_shift(image, value):\n    im = image.convert('RGBA')\n    arr = np.array(np.asarray(im).astype(float))\n    r,g,b,a = np.rollaxis(arr, axis=-1)\n    \n    h,s,v = rgb_to_hsv(r, g, b)\n    r, g, b = hsv_to_rgb((h + value/360.0) % 1.0, s, v)\n    arr = np.dstack((r, g, b, a))\n\n    \n    \n    \n\n    return Image.fromarray(arr.astype('uint8'), 'RGBA')\n\nparser = argparse.ArgumentParser(description='Rainbow an image batch')\nparser.add_argument('--filename', dest='filename', type=str)\nparser.add_argument('--step',     dest='step',     type=float,  default=5.0)\nparser.add_argument('--max_step', dest='max_step', type=float,  default=360.0)\nargs = parser.parse_args()\n\ncolor_image = Image.open(args.filename)\n\nbasename = os.path.basename(args.filename)\nbase, ext = os.path.splitext(basename)\n\nif not os.path.exists('anim'):\n    os.mkdir('anim')\n\nfor n in range(0, int(args.max_step/args.step)):\n    dtheta = n*args.step\n    print('Writing out', dtheta)\n    cropped = crop(color_image, (1620, 780, 2220, 1380))\n    new_im = hue_shift(cropped, dtheta)\n    new_fn = os.path.join('anim','{0}_{1}{2}'.format(base, n, ext))\n    n += 1\n    new_im.save(new_fn)",
        "summary": "The Python script uses the PIL library to open and manipulate an image, applying a hue shift in increments based on command-line arguments. It saves each shifted image frame in a directory named 'anim'."
    },
    {
        "code": "from tkinter import *\nfrom tkinter import messagebox\n\nroot = Tk()\nroot.geometry(\"200x200\")\ndef message():\n    messagebox.showwarning(\"Alert Box\", \"Stop virus found\")\nbut = Button(root, text=\"ok\", command=Message)\nbut.place(x=100, y=100)\nroot.mainloop()",
        "summary": "The provided Python code creates a simple GUI application using the Tkinter library. It displays a window with a button labeled \"ok\". When the button is clicked, it triggers a function that shows a warning message box indicating the presence of a virus."
    },
    {
        "code": "import configparser\nimport sys\n\nfrom dataflow.DataReaders.DatabaseReaders.GlacierReader import GlacierReader\nfrom dataflow.DataReaders.DatabaseReaders.InventoryReader import InventoryReader\n\ndef printLatestOutline(glaciers):\n    \n    for glacier in glaciers.values():\n        print(\"---\")\n        print(glacier)\n        \n        print(glacier.latestInventoryGeometry)\n\nif __name__ == '__main__':\n\n    config = configparser.ConfigParser()\n    config.read(\"dataflow.cfg\")\n\n    privateDatabaseAccessConfiguration = r\".\\databaseAccessConfiguration.gladmin.cfg\"\n    \n    focusGlaciers = ['C14-10', 'B36-26', 'B83-03'] \n    \n    \n    glacierReader = GlacierReader(privateDatabaseAccessConfiguration)\n    \n    glaciers = dict()\n    \n    \n    dataReaders = []\n    dataReaders.append(InventoryReader(privateDatabaseAccessConfiguration))\n    \n    try:\n    \n        \n        if glacierReader.isDatabaseAvailable == True:\n            \n            print(\"The GLAMOS database is available. Glacier objects are read from the database.\")\n            \n            for focusGlacier in focusGlaciers:\n                glacierFound = glacierReader.getGlacierBySgi(focusGlacier)\n                glaciers[glacierFound.pkSgi] = glacierFound\n\n            \n            for glacier in glaciers.values():\n                \n                \n                for dataReader in dataReaders:\n                    dataReader.getData(glacier)\n            \n            \n            printLatestOutline(glaciers)\n                \n        else:\n            \n            print(\"Database not available! Application will terminate.\")\n            sys.exit(2)\n            \n    except Exception as e:\n        \n        print(e.message)\n        print(\"Sample script aborted!\")\n",
        "summary": "The Python script reads data from a database using the GlacierReader and InventoryReader classes, filters it based on specified glacier identifiers, retrieves the latest inventory geometry for each glacier, and prints this information. The script handles exceptions and checks if the database is available before proceeding with data retrieval and processing."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python import keras\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.keras import keras_parameterized\nfrom tensorflow.python.keras import regularizers\nfrom tensorflow.python.keras import testing_utils\nfrom tensorflow.python.keras.utils import np_utils\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\n\n\nDATA_DIM = 5\nNUM_CLASSES = 2\n\n\nclass KerasRegularizersTest(keras_parameterized.TestCase,\n                            parameterized.TestCase):\n\n  def create_model(self, kernel_regularizer=None, activity_regularizer=None):\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(NUM_CLASSES,\n                                 kernel_regularizer=kernel_regularizer,\n                                 activity_regularizer=activity_regularizer,\n                                 input_shape=(DATA_DIM,)))\n    return model\n\n  def get_data(self):\n    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n        train_samples=10,\n        test_samples=10,\n        input_shape=(DATA_DIM,),\n        num_classes=NUM_CLASSES)\n    y_train = np_utils.to_categorical(y_train, NUM_CLASSES)\n    y_test = np_utils.to_categorical(y_test, NUM_CLASSES)\n    return (x_train, y_train), (x_test, y_test)\n\n  def create_multi_input_model_from(self, layer1, layer2):\n    input_1 = keras.layers.Input(shape=(DATA_DIM,))\n    input_2 = keras.layers.Input(shape=(DATA_DIM,))\n    out1 = layer1(input_1)\n    out2 = layer2(input_2)\n    out = keras.layers.Average()([out1, out2])\n    model = keras.models.Model([input_1, input_2], out)\n    model.add_loss(keras.backend.mean(out2))\n    model.add_loss(math_ops.reduce_sum(input_1))\n    return model\n\n  @keras_parameterized.run_all_keras_modes\n  @parameterized.named_parameters([\n      ('l1', regularizers.l1()),\n      ('l2', regularizers.l2()),\n      ('l1_l2', regularizers.l1_l2()),\n  ])\n  def test_kernel_regularization(self, regularizer):\n    (x_train, y_train), _ = self.get_data()\n    model = self.create_model(kernel_regularizer=regularizer)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer='sgd',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n    self.assertEqual(len(model.losses), 1)\n    model.fit(x_train, y_train, batch_size=10, epochs=1, verbose=0)\n\n  @keras_parameterized.run_all_keras_modes\n  @parameterized.named_parameters([\n      ('l1', regularizers.l1()),\n      ('l2', regularizers.l2()),\n      ('l1_l2', regularizers.l1_l2()),\n      ('l2_zero', keras.regularizers.l2(0.)),\n  ])\n  def test_activity_regularization(self, regularizer):\n    (x_train, y_train), _ = self.get_data()\n    model = self.create_model(activity_regularizer=regularizer)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer='sgd',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n    self.assertEqual(len(model.losses), 1 if context.executing_eagerly() else 1)\n    model.fit(x_train, y_train, batch_size=10, epochs=1, verbose=0)\n\n  @keras_parameterized.run_all_keras_modes\n  @keras_parameterized.run_with_all_model_types\n  def test_zero_regularization(self):\n    \n    x, y = np.ones((10, 10)), np.ones((10, 3))\n    model = testing_utils.get_model_from_layers(\n        [keras.layers.Dense(3, kernel_regularizer=keras.regularizers.l2(0))],\n        input_shape=(10,))\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n    model.fit(x, y, batch_size=5, epochs=1)\n\n  def test_custom_regularizer_saving(self):\n\n    def my_regularizer(weights):\n      return math_ops.reduce_sum(math_ops.abs(weights))\n\n    inputs = keras.Input((10,))\n    outputs = keras.layers.Dense(1, kernel_regularizer=my_regularizer)(inputs)\n    model = keras.Model(inputs, outputs)\n    model2 = model.from_config(\n        model.get_config(), custom_objects={'my_regularizer': my_regularizer})\n    self.assertEqual(model2.layers[1].kernel_regularizer, my_regularizer)\n\n  @keras_parameterized.run_all_keras_modes\n  @parameterized.named_parameters([\n      ('l1', regularizers.l1()),\n      ('l2', regularizers.l2()),\n      ('l1_l2', regularizers.l1_l2()),\n  ])\n  def test_regularization_shared_layer(self, regularizer):\n    dense_layer = keras.layers.Dense(\n        NUM_CLASSES,\n        kernel_regularizer=regularizer,\n        activity_regularizer=regularizer)\n    model = self.create_multi_input_model_from(dense_layer, dense_layer)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer='sgd',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n    self.assertLen(model.losses, 5)\n\n  @keras_parameterized.run_all_keras_modes\n  @parameterized.named_parameters([\n      ('l1', regularizers.l1()),\n      ('l2', regularizers.l2()),\n      ('l1_l2', regularizers.l1_l2()),\n  ])\n  def test_regularization_shared_model(self, regularizer):\n    dense_layer = keras.layers.Dense(\n        NUM_CLASSES,\n        kernel_regularizer=regularizer,\n        activity_regularizer=regularizer)\n\n    input_tensor = keras.layers.Input(shape=(DATA_DIM,))\n    dummy_model = keras.models.Model(input_tensor, dense_layer(input_tensor))\n\n    model = self.create_multi_input_model_from(dummy_model, dummy_model)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer='sgd',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n    self.assertLen(model.losses, 6)\n\n  @keras_parameterized.run_all_keras_modes\n  @parameterized.named_parameters([\n      ('l1', regularizers.l1()),\n      ('l2', regularizers.l2()),\n      ('l1_l2', regularizers.l1_l2()),\n  ])\n  def test_regularization_shared_layer_in_different_models(self, regularizer):\n    shared_dense = keras.layers.Dense(\n        NUM_CLASSES,\n        kernel_regularizer=regularizer,\n        activity_regularizer=regularizer)\n    models = []\n    for _ in range(2):\n      input_tensor = keras.layers.Input(shape=(DATA_DIM,))\n      unshared_dense = keras.layers.Dense(\n          NUM_CLASSES, kernel_regularizer=regularizer)\n      out = unshared_dense(shared_dense(input_tensor))\n      models.append(keras.models.Model(input_tensor, out))\n\n    model = self.create_multi_input_model_from(\n        layer1=models[0], layer2=models[1])\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer='sgd',\n        run_eagerly=testing_utils.should_run_eagerly(),\n        experimental_run_tf_function=testing_utils.should_run_tf_function())\n\n    \n    \n    \n    \n    \n    self.assertLen(model.losses, 9)\n\n\nif __name__ == '__main__':\n  test.main()\n",
        "summary": "The provided Python code is a comprehensive unit test suite for TensorFlow's Keras regularizers. It includes tests for kernel and activity regularization, custom regularizers, shared layers and models across multiple inputs, and verifies that the regularization terms are correctly added to the model's loss during training. The tests cover various regularization types (L1, L2, L1_L2) and ensure compatibility with different Keras model configurations and execution modes."
    },
    {
        "code": "import os\nimport sys\nfrom django.core.wsgi import get_wsgi_application\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.abspath(os.pardir), os.pardir)))\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.abspath(os.path.dirname(__file__)))))\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", 'termsandconditions_demo.settings')\n\n\n\n\n\napplication = get_wsgi_application()\n",
        "summary": "The Python code sets up the environment for a Django application by modifying the system path to include project directories and setting the default Django settings module. It then creates an WSGI application instance using Django's `get_wsgi_application` function, which is necessary for deploying the application with a WSGI server like Gunicorn or uWSGI."
    },
    {
        "code": "from concurrent import futures\nimport time\n\nimport grpc\n\nimport app.helloworld_pb2 as helloworld_pb2\nimport app.helloworld_pb2_grpc as helloworld_pb2_grpc\n\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\n\nclass Greeter(helloworld_pb2_grpc.GreeterServicer):\n\n    def Greet(self, request, context):\n        print('Saying `hello` to %s' % request.name)\n        return helloworld_pb2.GreetResponse(message='Hello, {}!'.format(request.name))\n\n\ndef serve():\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)\n    server.add_insecure_port('[::]:50051')\n    server.start()\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        server.stop(0)\n\n\nif __name__ == '__main__':\n    serve()\n",
        "summary": "This Python script sets up a gRPC server that listens on port 50051 and provides a simple \"Greeter\" service, which responds with a greeting message when it receives a request. The server runs indefinitely until manually stopped."
    },
    {
        "code": "import os\nimport sys\nimport logging\nimport functools\nimport paddle.distributed as dist\n\nlogger_initialized = {}\n\n\n@functools.lru_cache()\ndef get_logger(name='srnet', log_file=None, log_level=logging.INFO):\n    \n    logger = logging.getLogger(name)\n    if name in logger_initialized:\n        return logger\n    for logger_name in logger_initialized:\n        if name.startswith(logger_name):\n            return logger\n\n    formatter = logging.Formatter(\n        '[%(asctime)s] %(name)s %(levelname)s: %(message)s',\n        datefmt=\"%Y/%m/%d %H:%M:%S\")\n\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_file is not None and dist.get_rank() == 0:\n        log_file_folder = os.path.split(log_file)[0]\n        os.makedirs(log_file_folder, exist_ok=True)\n        file_handler = logging.FileHandler(log_file, 'a')\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    if dist.get_rank() == 0:\n        logger.setLevel(log_level)\n    else:\n        logger.setLevel(logging.ERROR)\n    logger_initialized[name] = True\n    return logger\n",
        "summary": "The provided Python code defines a function `get_logger` that initializes and returns a logging object with specified name, log file (if any), and log level. It uses caching to ensure that the same logger instance is returned for the same name, handles both stream and file outputs based on the rank of the process in a distributed environment, and sets appropriate log levels for different ranks."
    },
    {
        "code": "from django.core.validators import RegexValidator\nfrom django.db import models\nfrom django.utils.translation import gettext_lazy as _\nfrom taggit.managers import TaggableManager\n\nfrom server.connective_tags.models import ConnectiveTaggedItem\nfrom server.schools.models import School\nfrom server.utils.db_utils import get_base_model\nfrom server.utils.model_fields import random_slug\n\n\nclass SchoolActivityGroupManager(models.Manager):\n    def get_activity_container_only_group(self, activity_group):\n        container_only_groups = self.filter(\n            activity_order=activity_group.activity_order,\n            group_type=SchoolActivityGroup.GroupTypes.CONTAINER_ONLY,\n        )\n        if container_only_groups.exists():\n            return container_only_groups[0]\n\n\nclass ImportedOrganization(get_base_model()):\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    organization_number = models.CharField(max_length=10, unique=True)\n    email = models.EmailField(null=True, blank=True)\n    description = models.CharField(max_length=4096, null=True, blank=True)\n    website_url = models.URLField(null=True, blank=True)\n    name = models.CharField(max_length=256, null=True, blank=True)\n    goal = models.CharField(max_length=4096, null=True, blank=True)\n    year_founded = models.CharField(max_length=128, null=True, blank=True)\n    status = models.CharField(max_length=50, null=True, blank=True)\n    target_audience = models.JSONField(null=True, blank=True)\n    number_of_employees = models.PositiveIntegerField(null=True, blank=True)\n    number_of_members = models.PositiveIntegerField(null=True, blank=True)\n    number_of_volunteers = models.PositiveIntegerField(null=True, blank=True)\n    location_lon = models.DecimalField(\n        max_digits=9,\n        decimal_places=6,\n        null=True,\n        blank=True,\n    )\n    location_lat = models.DecimalField(\n        max_digits=9,\n        decimal_places=6,\n        null=True,\n        blank=True,\n    )\n\n    address_city = models.CharField(max_length=256, null=True, blank=True)\n    address_street = models.CharField(max_length=256, null=True, blank=True)\n    address_house_num = models.CharField(max_length=30, null=True, blank=True)\n    address_zipcode = models.CharField(max_length=9, null=True, blank=True)\n    cities = models.JSONField(null=True, blank=True)\n    districts = models.JSONField(null=True, blank=True)\n    union_type = models.CharField(max_length=50, null=True, blank=True)\n\n    def __str__(self):\n        return f\"{self.name} | {self.organization_number} | {self.slug}\"\n\n\nclass Organization(get_base_model()):\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    organization_number = models.CharField(max_length=10, unique=True, null=True)\n    email = models.EmailField()\n    description = models.CharField(max_length=300)\n    website_url = models.URLField(null=True, blank=True)\n    name = models.CharField(max_length=100)\n    goal = models.CharField(max_length=300, null=True, blank=True)\n    year_founded = models.CharField(max_length=4, null=True, blank=True)\n    status = models.CharField(max_length=50, null=True, blank=True)\n    target_audience = models.JSONField(null=True, blank=True)\n    number_of_employees = models.PositiveIntegerField(null=True, blank=True)\n    number_of_members = models.PositiveIntegerField(null=True, blank=True)\n    number_of_volunteers = models.PositiveIntegerField(null=True, blank=True)\n    location_lon = models.DecimalField(\n        max_digits=9,\n        decimal_places=6,\n        null=True,\n        blank=True,\n    )\n    location_lat = models.DecimalField(\n        max_digits=9,\n        decimal_places=6,\n        null=True,\n        blank=True,\n    )\n\n    address_city = models.CharField(max_length=150, null=True, blank=True)\n    address_street = models.CharField(max_length=150, null=True, blank=True)\n    address_house_num = models.CharField(max_length=20, null=True, blank=True)\n    address_zipcode = models.CharField(max_length=9, null=True, blank=True)\n    cities = models.JSONField(null=True, blank=True)\n    districts = models.JSONField(null=True, blank=True)\n    union_type = models.CharField(max_length=50, null=True, blank=True)\n\n    def __str__(self):\n        return f\"{self.name} | {self.organization_number} | {self.slug}\"\n\n\nclass Activity(get_base_model()):\n    class Domain(models.TextChoices):\n        SCIENCE_AND_TECH = \"SCIENCE_AND_TECH\", \"Science And Tech\"\n        EXTREME_SPORTS = \"EXTREME_SPORTS\", \"Extreme Sports\"\n        FIELD = \"FIELD\", \"Field\"\n        OTHER = \"OTHER\", \"Other\"\n\n    tags = TaggableManager(blank=True, through=ConnectiveTaggedItem)\n\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    name = models.CharField(max_length=35)\n    target_audience = models.JSONField()\n    domain = models.CharField(max_length=55, null=True, choices=Domain.choices)\n    originization = models.ForeignKey(\n        Organization,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name=\"activities\",\n    )\n    activity_website_url = models.URLField(max_length=750, null=True, blank=True)\n    activity_email = models.EmailField(null=True, blank=True)\n    description = models.CharField(max_length=550, default=\"\")\n    contact_name = models.CharField(max_length=60, default=\"\")\n    logo = models.ImageField(blank=True, null=True)\n    phone_number = models.CharField(\n        blank=True,\n        max_length=15,\n        validators=[\n            RegexValidator(\n                regex=r\"^\\d{9,15}$\",\n                message=_(\"phone number must be between 9-15 digits\"),\n            )\n        ],\n    )\n\n    def __str__(self):\n        try:\n            return f\"{self.name} | {self.slug} | {self.originization.name}\"\n        except AttributeError:\n            return f\"{self.name} | {self.slug}\"\n\n\nclass ImportedActivity(get_base_model()):\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    activity_code = models.IntegerField()\n    name = models.CharField(max_length=550)\n    raw_name = models.CharField(max_length=550)\n    target_audience = models.JSONField()\n    organization_number = models.IntegerField()\n    organization_name = models.CharField(max_length=1550, default=\"\")\n    target_gender = models.JSONField()\n    target_gender = models.JSONField()\n    target_population = models.JSONField()\n    target_time = models.JSONField()\n    target_size = models.JSONField()\n    target_migzar = models.JSONField()\n    target_pikuah = models.JSONField()\n    profession = models.JSONField()\n    goal = models.CharField(max_length=1550, default=\"\")\n    is_active = models.BooleanField()\n    activity_website_url = models.URLField(max_length=750, null=True, blank=True)\n    activity_email = models.EmailField(null=True, blank=True)\n    description = models.CharField(max_length=1550, default=\"\")\n    contact_name = models.CharField(max_length=100, default=\"\")\n    phone_number = models.CharField(\n        blank=True,\n        max_length=15,\n        validators=[\n            RegexValidator(\n                regex=r\"^\\d{9,15}$\",\n                message=_(\"phone number must be between 9-15 digits\"),\n            )\n        ],\n    )\n\n    def __str__(self):\n        return f\"{self.name} | {self.slug} | {self.activity_code}\"\n\n\nclass ActivityMedia(get_base_model()):\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    name = models.CharField(max_length=40, null=True, blank=True)\n    image_url = models.ImageField(blank=True, null=True)\n    video_url = models.URLField(blank=True, null=True)\n    activity = models.ForeignKey(\n        Activity,\n        on_delete=models.CASCADE,\n        related_name=\"rich_media\",\n    )\n\n    def __str__(self):\n        return f\"{self.name} | {self.slug} | {self.activity.name}\"\n\n\nclass OrganizationMember(get_base_model()):\n    user = models.OneToOneField(\n        \"users.User\", on_delete=models.CASCADE, related_name=\"organization_member\"\n    )\n    organization = models.ForeignKey(\n        Organization,\n        on_delete=models.CASCADE,\n        related_name=\"organization_member\",\n    )\n\n    def __str__(self):\n        return f\"{self.user.email} | {self.organization.name}\"\n\n\nclass SchoolActivityOrder(get_base_model()):\n    class Meta:\n        constraints = [\n            models.UniqueConstraint(fields=[\"school\", \"activity\"], name=\"unique_order\")\n        ]\n\n    class Status(models.TextChoices):\n        CANCELLED = \"CANCELLED\", \"Cancelled\"\n        PENDING_ADMIN_APPROVAL = \"PENDING_ADMIN_APPROVAL\", \"Pending Admin Approval\"\n        APPROVED = \"APPROVED\", \"Approved\"\n        DENIED = \"DENIED\", \"Denied\"\n\n    base_status = Status.PENDING_ADMIN_APPROVAL\n\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    requested_by = models.ForeignKey(\n        \"users.User\",\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name=\"requested_orders\",\n    )\n    last_updated_by = models.ForeignKey(\n        \"users.User\",\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name=\"last_updated_by_me_orders\",\n    )\n    school = models.ForeignKey(\n        School, on_delete=models.CASCADE, related_name=\"school_activity_orders\"\n    )\n    activity = models.ForeignKey(\n        Activity, on_delete=models.CASCADE, related_name=\"school_activity_orders\"\n    )\n    status = models.CharField(\n        _(\"status\"), max_length=50, choices=Status.choices, default=base_status\n    )\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n    status_reason = models.CharField(\n        max_length=250,\n        blank=True,\n    )\n\n    def __str__(self):\n        return f\"{self.activity} | {self.school} | {self.status} | {self.pk}\"\n\n\nclass SchoolActivityGroup(get_base_model()):\n    class GroupTypes(models.TextChoices):\n        CONTAINER_ONLY = \"CONTAINER_ONLY\", \"Container Only\"\n        DISABLED_CONSUMERS = \"DISABLED_CONSUMERS\", \"Disabled Consumers\"\n        NO_REGISTRATION = \"NO_REGISTRATION\", \"No Registration\"\n        DEFAULT = \"DEFAULT\", \"Default\"\n\n    objects = SchoolActivityGroupManager()\n\n    slug = models.CharField(max_length=40, default=random_slug, unique=True)\n    activity_order = models.ForeignKey(\n        SchoolActivityOrder, on_delete=models.CASCADE, related_name=\"activity_groups\"\n    )\n    name = models.CharField(_(\"name\"), max_length=50)\n    description = models.CharField(_(\"description\"), max_length=550)\n    consumers = models.ManyToManyField(\n        \"users.Consumer\",\n        related_name=\"activity_groups\",\n        blank=True,\n    )\n    group_type = models.CharField(\n        _(\"group type\"),\n        max_length=50,\n        choices=GroupTypes.choices,\n        default=GroupTypes.DEFAULT,\n    )\n    instructor = models.ForeignKey(\n        \"users.Instructor\",\n        on_delete=models.SET_NULL,\n        related_name=\"managed_activity_groups\",\n        null=True,\n        blank=True,\n    )\n\n    def __str__(self):\n        return f\n",
        "summary": "```python\nf\"{self.name} | {self.activity_order.school} | {self.group_type}\"\n```\n\nThis line of code is part of the `__str__` method for the `SchoolActivityGroup` model. The `__str__` method is used to provide a human-readable string representation of an object, which can be useful for debugging and logging purposes.\n\nIn this case, the `__str__` method returns a string that includes the name of the activity group, the school associated with the activity order, and the type of the group. This provides a clear and concise description of the activity group, making it easier to understand its purpose and context within the system."
    },
    {
        "code": "from __future__ import (absolute_import, division, print_function,\n                        with_statement)\n\nimport unittest\nfrom tornado import httpclient\nimport logging\nfrom tornado.escape import json_decode, json_encode\nfrom ..v1 import api_url_v1\n\nlogger = logging.getLogger(__name__)\n\n\nclass UserApiV1FunctionalTestCase(unittest.TestCase):\n    \n\n    def test_login_empty_body(self):\n        http_client = httpclient.HTTPClient()\n        login_url = \"%s/user/login\" % api_url_v1\n        response_body = None\n        error_code = 0\n        body_error_code = 0\n        try:\n            http_client.fetch(httpclient.HTTPRequest(\n                    url=login_url, method='POST', body=''))\n        except httpclient.HTTPError as e:\n            \n            \n            response_body = json_decode(e.response.body)\n            error_code = e.code\n            body_error_code = int(response_body['status'])\n            print(e.response.error)\n        except Exception as e:\n            \n            logger.error(\"Error: %s\" % str(e))\n        http_client.close()\n        \n        self.assertEquals(error_code, 500)\n        self.assertEquals(body_error_code, 500)\n        \n        self.assertEquals(len(response_body['errors']), 1)\n        \n        self.assertEquals(response_body['errors']['schema'][0],\n                          \"Invalid json body content.\")\n\n    def test_login_invalid_json(self):\n        http_client = httpclient.HTTPClient()\n        login_url = \"%s/user/login\" % api_url_v1\n        response_body = None\n        data = \"invalid json string\"\n        error_code = 0\n        body_error_code = 0\n        try:\n            http_client.fetch(httpclient.HTTPRequest(\n                    url=login_url, method='POST', body=''))\n        except httpclient.HTTPError as e:\n            \n            \n            response_body = json_decode(e.response.body)\n            error_code = e.code\n            body_error_code = int(response_body['status'])\n            print(e.response.error)\n        except Exception as e:\n            \n            logger.error(\"Error: %s\" % str(e))\n        http_client.close()\n        \n        self.assertEquals(error_code, 500)\n        self.assertEquals(body_error_code, 500)\n        \n        self.assertEquals(len(response_body['errors']), 1)\n        \n        self.assertEquals(response_body['errors']['schema'][0],\n                          \"Invalid json body content.\")\n\n    def test_login_without_username(self):\n        http_client = httpclient.HTTPClient()\n        login_url = \"%s/user/login\" % api_url_v1\n        response_body = None\n        error_code = 0\n        data = {\n            'payload': {\n                'password': \"\",\n            }\n        }\n        try:\n            response = http_client.fetch(httpclient.HTTPRequest(\n                    url=login_url, method='POST', body=json_encode(data)))\n        except httpclient.HTTPError as e:\n            \n            \n            logger.error(\"Error: %s\" % str(e))\n            error_code = e.code\n            response_body = json_decode(e.response.body)\n        except Exception as e:\n            \n            logger.error(\"Error: %s\" % str(e))\n        http_client.close()\n        \n        self.assertEquals(error_code, 400)\n        \n        self.assertEquals(len(response_body['errors']), 1)\n        \n        self.assertEquals(response_body['errors']['schema'],\n                          \"'username' is a required property\")\n\n    def test_login_without_password(self):\n        http_client = httpclient.HTTPClient()\n        login_url = \"%s/user/login\" % api_url_v1\n        response_body = None\n        error_code = 0\n        data = {\n            'payload': {\n                'username': \"\",\n            }\n        }\n        try:\n            response = http_client.fetch(httpclient.HTTPRequest(\n                    url=login_url, method='POST', body=json_encode(data)))\n        except httpclient.HTTPError as e:\n            \n            \n            logger.error(\"Error: %s\" % str(e))\n            error_code = e.code\n            response_body = json_decode(e.response.body)\n        except Exception as e:\n            \n            logger.error(\"Error: %s\" % str(e))\n        http_client.close()\n        \n        self.assertEquals(error_code, 400)\n        \n        self.assertEquals(len(response_body['errors']), 1)\n        \n        print(response_body['errors']['schema'])\n        self.assertEquals(response_body['errors']['schema'],\n                          \"'password' is a required property\")\n\n    def test_valid_login(self):\n        http_client = httpclient.HTTPClient()\n        login_url = \"%s/user/login\" % api_url_v1\n        response_body = None\n        code = 0\n        data = {\n            'payload': {\n                'username': \"test\",\n                'password': \"test\",\n            }\n        }\n        try:\n            response = http_client.fetch(httpclient.HTTPRequest(\n                    url=login_url, method='POST', body=json_encode(data)))\n            code = response.code\n            response_body = json_decode(response.body)\n        except httpclient.HTTPError as e:\n            \n            \n            logger.error(\"Error: %s\" % str(e))\n        except Exception as e:\n            \n            logger.error(\"Error: %s\" % str(e))\n        http_client.close()\n        \n        self.assertEquals(code, 200)\n        \n        self.assertEquals(response_body['userid'], 1)\n",
        "summary": "The provided Python code defines a series of functional test cases for a user login API using the Tornado framework. Each test case sends different types of requests to the login endpoint and verifies the expected HTTP response status codes, error messages, and JSON responses. The tests cover scenarios such as empty request bodies, invalid JSON, missing username or password fields, and successful valid logins."
    },
    {
        "code": "from tqdm import tqdm\nimport pandas as pd\nfrom __init__ import FILE\n\ndf = pd.read_csv(FILE)\nsmiles = list(df[\"Smiles\"])\n\nwith open(\"_chemprop.csv\", \"w\") as f:\n    f.write(\"smiles\\n\")\n    for smi in smiles:\n        f.write(\"{0}\\n\".format(smi))\n",
        "summary": "The Python script reads a CSV file containing SMILES strings using pandas, then writes these strings to a new CSV file named \"_chemprop.csv\" with each SMILES on a separate line. The tqdm library is used for progress tracking during the iteration over the SMILES list."
    },
    {
        "code": "import copy\nimport functools\nfrom typing import List\n\nimport torch\nimport torch.distributed._shard.sharding_spec as shard_spec\n\nfrom .api import (\n    _register_sharded_op,\n    Shard,\n    ShardedTensor,\n    ShardedTensorMetadata,\n    TensorProperties,\n)\nfrom .metadata import ShardMetadata  \nfrom .partial_tensor import _PartialTensor\n\n\ndef empty(sharding_spec: shard_spec.ShardingSpec,\n          *size,\n          dtype=None,\n          layout=torch.strided,\n          requires_grad=False,\n          pin_memory=False,\n          memory_format=torch.contiguous_format,\n          process_group=None,\n          init_rrefs=False) -> ShardedTensor:\n    \n    return ShardedTensor(\n        sharding_spec,\n        *size,\n        dtype=dtype,\n        layout=layout,\n        requires_grad=requires_grad,\n        pin_memory=pin_memory,\n        memory_format=memory_format,\n        process_group=process_group,\n        init_rrefs=init_rrefs,\n    )\n\ndef ones(sharding_spec: shard_spec.ShardingSpec,\n         *size,\n         dtype=None,\n         layout=torch.strided,\n         requires_grad=False,\n         pin_memory=False,\n         memory_format=torch.contiguous_format,\n         process_group=None,\n         init_rrefs=False) -> ShardedTensor:\n    \n    return full(\n        sharding_spec,\n        size,\n        fill_value=1,\n        dtype=dtype,\n        layout=layout,\n        requires_grad=requires_grad,\n        pin_memory=pin_memory,\n        memory_format=memory_format,\n        process_group=process_group,\n        init_rrefs=init_rrefs\n    )\n\ndef zeros(sharding_spec: shard_spec.ShardingSpec,\n          *size,\n          dtype=None,\n          layout=torch.strided,\n          requires_grad=False,\n          pin_memory=False,\n          memory_format=torch.contiguous_format,\n          process_group=None,\n          init_rrefs=False) -> ShardedTensor:\n    \n    return full(\n        sharding_spec,\n        size,\n        fill_value=0,\n        dtype=dtype,\n        layout=layout,\n        requires_grad=requires_grad,\n        pin_memory=pin_memory,\n        memory_format=memory_format,\n        process_group=process_group,\n        init_rrefs=init_rrefs\n    )\n\ndef full(sharding_spec: shard_spec.ShardingSpec,\n         size,\n         fill_value=torch.types.Number,\n         dtype=None,\n         layout=torch.strided,\n         requires_grad=False,\n         pin_memory=False,\n         memory_format=torch.contiguous_format,\n         process_group=None,\n         init_rrefs=False) -> ShardedTensor:\n    \n    sharded_tensor = ShardedTensor(\n        sharding_spec,\n        *size,\n        dtype=dtype,\n        layout=layout,\n        requires_grad=requires_grad,\n        pin_memory=pin_memory,\n        memory_format=memory_format,\n        process_group=process_group,\n        init_rrefs=init_rrefs,\n    )\n    torch.nn.init.constant_(sharded_tensor, fill_value)  \n    return sharded_tensor\n\ndef rand(sharding_spec: shard_spec.ShardingSpec,\n         *size,\n         dtype=None,\n         layout=torch.strided,\n         requires_grad=False,\n         pin_memory=False,\n         memory_format=torch.contiguous_format,\n         process_group=None,\n         init_rrefs=False) -> ShardedTensor:\n    \n    sharded_tensor = ShardedTensor(\n        sharding_spec,\n        *size,\n        dtype=dtype,\n        layout=layout,\n        requires_grad=requires_grad,\n        pin_memory=pin_memory,\n        memory_format=memory_format,\n        process_group=process_group,\n        init_rrefs=init_rrefs,\n    )\n    torch.nn.init.uniform_(sharded_tensor, 0, 1)  \n    return sharded_tensor\n\ndef init_from_local_shards(\n        local_shards: List[Shard],\n        *global_size,\n        process_group=None,\n        init_rrefs=False) -> ShardedTensor:\n    \n    return ShardedTensor._init_from_local_shards(\n        local_shards,\n        *global_size,\n        process_group=process_group,\n        init_rrefs=init_rrefs\n    )\n\ndef state_dict_hook(module, destination, prefix, local_metadata):\n    \n    for submodule_name, submodule in module.named_modules():\n        for attr_name, attr in submodule.__dict__.items():\n            if isinstance(attr, ShardedTensor):\n                destination[prefix + submodule_name + '.' + attr_name] = attr\n\ndef pre_load_state_dict_hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    \n    for submodule_name, submodule in module.named_modules():\n        for attr_name, attr in submodule.__dict__.items():\n            key = prefix + submodule_name + '.' + attr_name\n            if key in state_dict:\n                if isinstance(state_dict[key], ShardedTensor):\n                    setattr(submodule, attr_name, state_dict[key])\n\ndef sharded_op_impl(func):\n    \n    def decorator_sharded_func(wrapped_func):\n        _register_sharded_op(func, wrapped_func)\n\n        @functools.wraps(wrapped_func)\n        def wrapper(*args, **kwargs):\n            return wrapped_func(*args, **kwargs)\n        return wrapper\n    return decorator_sharded_func\n\n\nfrom ._ops import *  \n\ndef _reshard_output(\n        module: torch.nn.Module,\n        resharding_spec: shard_spec.ShardingSpec) -> torch.nn.Module:\n    \n    def hook_func(_module, _input, output):\n        if isinstance(output, ShardedTensor) or isinstance(output, _PartialTensor):\n            return output.reshard(resharding_spec)\n        return output\n    module.register_forward_hook(hook_func)\n    return module\n\n\ndef _collect_local_shard(module: torch.nn.Module) -> torch.nn.Module:\n    \n\n    def hook_func(_module, _input, output):\n        if isinstance(output, ShardedTensor):\n            local_tensor = output.local_tensor()\n            \n            if local_tensor.size(output._sharding_spec.dim) == 1:  \n                local_tensor = local_tensor.squeeze(\n                    output._sharding_spec.dim  \n                )\n            return local_tensor\n    module.register_forward_hook(hook_func)\n    return module\n",
        "summary": "The provided Python code defines a set of functions for creating and manipulating sharded tensors in PyTorch, including `empty`, `ones`, `zeros`, `full`, `rand`, and `init_from_local_shards`. It also includes hooks for state dictionary management and decorators for registering sharded operations. The code is organized into modules and utilizes various utilities from the PyTorch library to handle distributed computing and tensor initialization."
    },
    {
        "code": "from parse_topology_for_hydrogens import parse_top_for_h\n\n\ndef gen_h_ndx(orig_ndx, topology, out_name='h_prot.ndx'):\n\n    ndx_ind = list()\n    with open(orig_ndx, 'r') as f:\n        line = f.readline()\n        while '[ Protein ]' not in line:\n            line = f.readline()\n        line = f.readline()\n        while ';' == line[0]:\n            line = f.readline()\n        line = line.strip()\n        while len(line):\n            ndx_ind.extend(line.split())\n            line = f.readline().strip()\n    ndx_ind = [int(elem) for elem in ndx_ind]\n\n    good_ind = parse_top_for_h(topology)\n    filtered_h_ind = [elem for elem in ndx_ind if elem in good_ind]\n    formated_h_ind = ['{:>4} '.format(elem) for elem in filtered_h_ind]\n    with open(out_name, 'w') as new_file:\n        ind = 0\n        new_file.write('[ Protein ]\\n')\n        while ind < len(filtered_h_ind):\n            new_file.write(''.join(formated_h_ind[ind:ind+15]))\n            new_file.write('\\n')\n            \n            ind += 15\n\n\n\n",
        "summary": "The `gen_h_ndx` function reads an index file for protein atoms, filters them based on hydrogen atom positions from a topology file using the `parse_top_for_h` function, and then writes the filtered hydrogen indices to a new index file in groups of up to 15 per line."
    },
    {
        "code": "from suds.client import Client\nfrom suds import WebFault\nfrom model.project import Project\n\nclass SoapHelper:\n\n    def __init__(self, app):\n        self.app = app\n\n    def can_login(self, username, password):\n        client = Client(\"http://localhost:8080/mantisbt-1.2.20/api/soap/mantisconnect.php?wsdl\")\n        try:\n            client.service.mc_login(username, password)\n            return True\n        except WebFault:\n            return False\n\n\n    def get_project_list(self, username, password):\n        project_list = []\n        client = Client(\"http://localhost:8080/mantisbt-1.2.20/api/soap/mantisconnect.php?wsdl\")\n        projects = client.service.mc_projects_get_user_accessible(username, password)\n        for i in range(len(projects)):\n            name = projects[i].name\n            description = projects[i].description\n            project_list.append(Project(name=name, description=description))\n        return project_list\n\n",
        "summary": "The `SoapHelper` class provides methods to interact with a SOAP-based MantisBT API. It includes functionality to check user login credentials and retrieve a list of accessible projects for a given user, encapsulating the logic within methods that handle SOAP client creation, exception handling, and data mapping to a custom `Project` model."
    },
    {
        "code": "import requests\nimport os\n\ntoken = os.getenv(\"DOCKER_HUB_TOKEN\")\nversion = os.getenv(\"VERSION\")\n\nif token==None:\n    print \"env DOCKER_HUB_TOKEN not set\"\n    exit(1)\n\nif version==None:\n    print \"env VERSION not set\"\n    exit(1)\n\nurl     = \"https://registry.hub.docker.com/u/skbkontur/kibana/trigger/%s/\" % token\npayload = { 'docker_tag' : version }\nheaders = { 'Content-Type': 'application/json'}\n\nres = requests.post(url, data=payload, headers=headers)\nprint res\n",
        "summary": "The Python script retrieves environment variables for a Docker Hub token and version, checks if they are set, constructs a URL with the token, sends a POST request to trigger a build on Docker Hub using the specified version as a payload, and prints the response."
    },
    {
        "code": "try:\n    from jinja2 import Environment, PackageLoader\nexcept ImportError:\n    raise ImportError('Scaffolding support requires the Jinja 2 templating library to be installed.')\n\ntemplate_environment = Environment(loader=PackageLoader('denim.scaffold'))\n\n\ndef single(template_file, output_name, context):\n    \n    template = template_environment.get_template(template_file)\n    print template.render(**context)\n\n\ndef environment(template_file, output_name, context):\n    \n    envs = context.env\n    for env in envs:\n        context['env'] = env\n        single(template_file, output_name, context)\n\n\n\nSCAFFOLDS = {\n    'nginx': ('nginx.conf.txt', 'conf/nginx/%(env)s.conf', environment, ('env', )),\n    'django.fabric': ('django/fabfile.py.txt', 'fabfile.py', single, ('env', ('scm', 'hg'))),\n    'django.supervisor': ('django/supervisor.conf.txt', 'conf/supervisor.conf', single, None),\n}\n\n\ndef generate_scaffold(scaffold_code):\n    scaffold = SCAFFOLDS.get(scaffold_code)\n    if not scaffold:\n        raise NotImplementedError('This scaffold does not exist')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code defines a scaffolding system using the Jinja2 templating library to render template files based on given contexts. It includes functions for rendering single templates and environments, as well as a dictionary mapping scaffold codes to their respective template files, output paths, rendering functions, and required context variables."
    },
    {
        "code": "def sol():\n    a, b = 0, 1\n    for i in range(int(input())):\n        a, b = b, a + b\n    print(a)\n\n\nif __name__ == \"__main__\":\n    sol()\n",
        "summary": "The provided Python code defines a function `sol()` that calculates the nth Fibonacci number using an iterative approach. When executed as the main program, it prompts the user for an integer input and prints the corresponding Fibonacci number."
    },
    {
        "code": "import json\nimport platform\nimport requests\nimport six\nimport sys\n\nfrom .version import __version__\n\n\nclass SlackRequest(object):\n    def __init__(\n            self,\n            proxies=None\n    ):\n        \n        self.custom_user_agent = None\n        self.proxies = proxies\n\n        \n        self.default_user_agent = {\n            \n            \"client\": \"{0}/{1}\".format(__name__.split('.')[0], __version__),\n            \"python\": \"Python/{v.major}.{v.minor}.{v.micro}\".format(v=sys.version_info),\n            \"system\": \"{0}/{1}\".format(platform.system(), platform.release())\n        }\n\n    def get_user_agent(self):\n        \n        if self.custom_user_agent:\n            custom_ua_list = [\"/\".join(client_info) for client_info in self.custom_user_agent]\n            custom_ua_string = \" \".join(custom_ua_list)\n            self.default_user_agent['custom'] = custom_ua_string\n\n        \n        ua_string = []\n        for key, val in self.default_user_agent.items():\n            ua_string.append(val)\n\n        user_agent_string = \" \".join(ua_string)\n        return user_agent_string\n\n    def append_user_agent(self, name, version):\n        if self.custom_user_agent:\n            self.custom_user_agent.append([name.replace(\"/\", \":\"), version.replace(\"/\", \":\")])\n        else:\n            self.custom_user_agent = [[name, version]]\n\n    def do(self, token=None, request=\"?\", post_data=None,\n           as_user=None, domain=\"slack.com\", timeout=None):\n        \n        \n        \n        \n        post_data = post_data or {}\n\n        \n        upload_requests = ['files.upload']\n\n        \n        files = None\n        if request in upload_requests:\n            files = {'file': post_data.pop('file')} if 'file' in post_data else None\n\n        \n        for field in {'channels', 'users', 'types'} & set(post_data.keys()):\n            if isinstance(post_data[field], list):\n                post_data[field] = \",\".join(post_data[field])\n\n        \n        \n        for k, v in six.iteritems(post_data):\n            if isinstance(v, (list, dict)):\n                post_data[k] = json.dumps(v)\n\n        return self.post_http_request(token, request, post_data, as_user, files, timeout, domain)\n\n    def post_http_request(self, token, api_method, post_data,\n                          as_user=None, files=None, timeout=None, domain=\"slack.com\"):\n        \n        \n        if post_data is not None and \"token\" in post_data:\n            token = post_data['token']\n\n        \n        headers = {\n            'user-agent': self.get_user_agent(),\n            'Authorization': 'Bearer {}'.format(token)\n        }\n        if as_user:\n            headers[\"X-Slack-User\"] = as_user\n\n        \n        res = requests.post(\n            'https://{0}/api/{1}'.format(domain, api_method),\n            headers=headers,\n            data=post_data,\n            files=files,\n            timeout=timeout,\n            proxies=self.proxies\n        )\n        return res\n",
        "summary": "The provided Python code defines a class `SlackRequest` that facilitates making HTTP requests to the Slack API. It handles user agent construction, request parameter formatting, and posting data with optional file uploads, while also supporting proxy settings for network requests."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import print_function\nimport sys\nimport os\nimport time\nimport glob\nimport platform\nimport logging\nfrom threading import Lock, Event\nfrom ctypes import *\nfrom .ChipUtility import ChipUtility\nfrom .ChipExceptions import *\n\n__all__ = [\n    \"DeviceStatusStruct\",\n    \"ChipStackException\",\n    \"DeviceError\",\n    \"ChipStackError\",\n    \"ChipStack\",\n]\n\nChipStackDLLBaseName = \"_ChipDeviceCtrl.so\"\n\n\ndef _singleton(cls):\n    instance = [None]\n\n    def wrapper(*args, **kwargs):\n        if instance[0] is None:\n            instance[0] = cls(*args, **kwargs)\n        return instance[0]\n\n    return wrapper\n\n\nclass DeviceStatusStruct(Structure):\n    _fields_ = [\n        (\"ProfileId\", c_uint32),\n        (\"StatusCode\", c_uint16),\n        (\"SysErrorCode\", c_uint32),\n    ]\n\n\nclass LogCategory(object):\n    \n\n    \n    Disabled = 0\n    Error = 1\n    Progress = 2\n    Detail = 3\n    Retain = 4\n\n    @staticmethod\n    def categoryToLogLevel(cat):\n        if cat == LogCategory.Error:\n            return logging.ERROR\n        elif cat == LogCategory.Progress:\n            return logging.INFO\n        elif cat == LogCategory.Detail:\n            return logging.DEBUG\n        elif cat == LogCategory.Retain:\n            return logging.CRITICAL\n        else:\n            return logging.NOTSET\n\n\nclass ChipLogFormatter(logging.Formatter):\n    \n\n    def __init__(\n        self,\n        datefmt=None,\n        logModulePrefix=False,\n        logLevel=False,\n        logTimestamp=False,\n        logMSecs=True,\n    ):\n        fmt = \"%(message)s\"\n        if logModulePrefix:\n            fmt = \"CHIP:%(chip-module)s: \" + fmt\n        if logLevel:\n            fmt = \"%(levelname)s:\" + fmt\n        if datefmt is not None or logTimestamp:\n            fmt = \"%(asctime)s \" + fmt\n        super(ChipLogFormatter, self).__init__(fmt=fmt, datefmt=datefmt)\n        self.logMSecs = logMSecs\n\n    def formatTime(self, record, datefmt=None):\n        if datefmt is None:\n            timestampStr = time.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n        if self.logMSecs:\n            timestampUS = record.__dict__.get(\"timestamp-usec\", 0)\n            timestampStr = \"%s.%03ld\" % (timestampStr, timestampUS / 1000)\n        return timestampStr\n\n\n_CompleteFunct = CFUNCTYPE(None, c_void_p, c_void_p)\n_ErrorFunct = CFUNCTYPE(None, c_void_p, c_void_p, c_ulong, POINTER(DeviceStatusStruct))\n_LogMessageFunct = CFUNCTYPE(None, c_int64, c_int64, c_char_p, c_uint8, c_char_p)\n\n\n@_singleton\nclass ChipStack(object):\n    def __init__(self, installDefaultLogHandler=True):\n        self.networkLock = Lock()\n        self.completeEvent = Event()\n        self._ChipStackLib = None\n        self._chipDLLPath = None\n        self.devMgr = None\n        self.callbackRes = None\n        self._activeLogFunct = None\n        self.addModulePrefixToLogMessage = True\n\n        \n        self._loadLib()\n\n        \n        \n        \n        \n        self.logger = logging.getLogger(__name__)\n        self.setLogFunct(self.defaultLogFunct)\n\n        \n        \n        if hasattr(self.logger, \"hasHandlers\"):\n            hasHandlers = self.logger.hasHandlers()\n        else:\n            hasHandlers = False\n            logger = self.logger\n            while logger is not None:\n                if len(logger.handlers) > 0:\n                    hasHandlers = True\n                    break\n                if not logger.propagate:\n                    break\n                logger = logger.parent\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        if installDefaultLogHandler and not hasHandlers:\n            logHandler = logging.StreamHandler(stream=sys.stdout)\n            logHandler.setFormatter(ChipLogFormatter())\n            self.logger.addHandler(logHandler)\n            self.logger.setLevel(logging.DEBUG)\n\n        def HandleComplete(appState, reqState):\n            self.callbackRes = True\n            self.completeEvent.set()\n\n        def HandleError(appState, reqState, err, devStatusPtr):\n            self.callbackRes = self.ErrorToException(err, devStatusPtr)\n            self.completeEvent.set()\n\n        self.cbHandleComplete = _CompleteFunct(HandleComplete)\n        self.cbHandleError = _ErrorFunct(HandleError)\n        self.blockingCB = None  \n\n        \n        res = self._ChipStackLib.pychip_Stack_Init()\n        if res != 0:\n            raise self._ChipStack.ErrorToException(res)\n\n    @property\n    def defaultLogFunct(self):\n        \n\n        def logFunct(timestamp, timestampUSec, moduleName, logCat, message):\n            moduleName = ChipUtility.CStringToString(moduleName)\n            message = ChipUtility.CStringToString(message)\n            if self.addModulePrefixToLogMessage:\n                message = \"CHIP:%s: %s\" % (moduleName, message)\n            logLevel = LogCategory.categoryToLogLevel(logCat)\n            msgAttrs = {\n                \"chip-module\": moduleName,\n                \"timestamp\": timestamp,\n                \"timestamp-usec\": timestampUSec,\n            }\n            self.logger.log(logLevel, message, extra=msgAttrs)\n\n        return logFunct\n\n    def setLogFunct(self, logFunct):\n        \n        if logFunct is None:\n            logFunct = 0\n        if not isinstance(logFunct, _LogMessageFunct):\n            logFunct = _LogMessageFunct(logFunct)\n        with self.networkLock:\n            \n            \n            \n            self._activeLogFunct = logFunct\n            self._ChipStackLib.pychip_Stack_SetLogFunct(logFunct)\n\n    def Shutdown(self):\n        self._ChipStack.Call(lambda: self._dmLib.pychip_Stack_Shutdown())\n        self.networkLock = None\n        self.completeEvent = None\n        self._ChipStackLib = None\n        self._chipDLLPath = None\n        self.devMgr = None\n        self.callbackRes = None\n\n    def Call(self, callFunct):\n        \n        self.callbackRes = None\n        self.completeEvent.clear()\n        with self.networkLock:\n            res = callFunct()\n        self.completeEvent.set()\n        if res == 0 and self.callbackRes != None:\n            return self.callbackRes\n        return res\n\n    def CallAsync(self, callFunct):\n        \n        self.callbackRes = None\n        self.completeEvent.clear()\n        with self.networkLock:\n            res = callFunct()\n\n        if res != 0:\n            self.completeEvent.set()\n            raise self.ErrorToException(res)\n        while not self.completeEvent.isSet():\n            if self.blockingCB:\n                self.blockingCB()\n\n            self.completeEvent.wait(0.05)\n        if isinstance(self.callbackRes, ChipStackException):\n            raise self.callbackRes\n        return self.callbackRes\n\n    def ErrorToException(self, err, devStatusPtr=None):\n        if err == 4044 and devStatusPtr:\n            devStatus = devStatusPtr.contents\n            msg = ChipUtility.CStringToString(\n                (\n                    self._ChipStackLib.pychip_Stack_StatusReportToString(\n                        devStatus.ProfileId, devStatus.StatusCode\n                    )\n                )\n            )\n            sysErrorCode = (\n                devStatus.SysErrorCode if (devStatus.SysErrorCode != 0) else None\n            )\n            if sysErrorCode != None:\n                msg = msg + \" (system err %d)\" % (sysErrorCode)\n            return DeviceError(\n                devStatus.ProfileId, devStatus.StatusCode, sysErrorCode, msg\n            )\n        else:\n            return ChipStackError(\n                err,\n                ChipUtility.CStringToString(\n                    (self._ChipStackLib.pychip_Stack_ErrorToString(err))\n                ),\n            )\n\n    def LocateChipDLL(self):\n        if self._chipDLLPath:\n            return self._chipDLLPath\n\n        scriptDir = os.path.dirname(os.path.abspath(__file__))\n\n        \n        \n        \n        dmDLLPath = os.path.join(scriptDir, ChipStackDLLBaseName)\n        if os.path.exists(dmDLLPath):\n            self._chipDLLPath = dmDLLPath\n            return self._chipDLLPath\n\n        \n        \n        \n        \n        buildMachineGlob = \"%s-*-%s*\" % (platform.machine(), platform.system().lower())\n        relDMDLLPathGlob = os.path.join(\n            \"build\",\n            buildMachineGlob,\n            \"src/controller/python/.libs\",\n            ChipStackDLLBaseName,\n        )\n        for dir in self._AllDirsToRoot(scriptDir):\n            dmDLLPathGlob = os.path.join(dir, relDMDLLPathGlob)\n            for dmDLLPath in glob.glob(dmDLLPathGlob):\n                if os.path.exists(dmDLLPath):\n                    self._chipDLLPath = dmDLLPath\n                    return self._chipDLLPath\n\n        raise Exception(\n            \"Unable to locate Chip Device Manager DLL (%s); expected location: %s\"\n            % (ChipStackDLLBaseName, scriptDir)\n        )\n\n    \n    def _AllDirsToRoot(self, dir):\n        dir = os.path.abspath(dir)\n        while True:\n            yield dir\n            parent = os.path.dirname(dir)\n            if parent == \"\" or parent == dir:\n                break\n            dir = parent\n\n    def _loadLib(self):\n        if self._ChipStackLib is None:\n            self._ChipStackLib = CDLL(self.LocateChipDLL())\n            self._ChipStackLib.pychip_Stack_Init.argtypes = []\n            self._ChipStackLib.pychip_Stack_Init.restype = c_uint32\n            self._ChipStackLib.pychip_Stack_Shutdown.argtypes = []\n            self._ChipStackLib.pychip_Stack_Shutdown.restype = c_uint32\n            self._ChipStackLib.pychip_Stack_StatusReportToString.argtypes = [\n                c_uint32,\n                c_uint16,\n            ]\n            self._ChipStackLib.pychip_Stack_StatusReportToString.restype = c_char_p\n            self._ChipStackLib.pychip_Stack_ErrorToString.argtypes = [c_uint32]\n            self._ChipStackLib.pychip_Stack_ErrorToString.restype = c_char_p\n            self._ChipStackLib.pychip_Stack_SetLogFunct.argtypes = [_LogMessageFunct]\n            self._ChipStackLib.pychip_Stack_SetLogFunct.restype = c_uint32\n",
        "summary": "This code defines a Python class `ChipStack` that provides an interface to the underlying C library for interacting with the Chip Device Manager. The class includes methods for initializing and shutting down the stack, calling synchronous and asynchronous functions, converting error codes to exceptions, and locating the DLL file containing the C library.\n\nHere's a breakdown of some key components:\n\n1. **Initialization and Shutdown**:\n   - `__init__`: Initializes the stack by loading the DLL and setting up function pointers.\n   - `Shutdown`: Calls the underlying C function to shut down the stack and cleans up resources.\n\n2. **Function Execution**:\n   - `Call`: Executes a synchronous function, waits for it to complete, and returns the result or raises an exception if an error occurs.\n   - `CallAsync`: Executes an asynchronous function, waits for it to complete, and returns the result or raises an exception if an error occurs.\n\n3. **Error Handling**:\n   - `ErrorToException`: Converts a Chip stack error code into a Python exception, providing detailed error messages when possible.\n\n4. **DLL Location**:\n   - `LocateChipDLL`: Searches for the DLL file containing the C library in various locations relative to the script's directory.\n\n5. **Utility Functions**:\n   - `_AllDirsToRoot`: A generator that yields all parent directories of a given path.\n   - `_loadLib`: Loads the DLL and sets up function pointers for calling into the underlying C library.\n\nThis class is designed to be used in Python scripts that need to interact with the Chip Device Manager, providing a high-level interface to its functionality while handling errors gracefully."
    },
    {
        "code": "import gym\nimport keras as k\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\nimport numpy as np\nfrom datetime import datetime\nfrom matplotlib import pyplot as PLT\nimport time\nimport csv\nimport os\n\n\nPOPULATION_SIZE = 50\nL1=20\nL2=10\nL3=50\nL4=4\n\n\n\n\nPOOLING_SIZE = (2,2)\nFILTER_SIZE_1 = (3,3)\nFILTER_SIZE_2 = (5,5)\nELITE_SET_SIZE = 5\nMUTATION_RATE = 0.5\n\nFRAME_SIZE = 210*160*1\nINPUT_DIM = 2*FRAME_SIZE\nINPUT_SHAPE = (210, 160, 2)\nFINAL_DIMENSION_X = int(((INPUT_SHAPE[0] - 2*int(FILTER_SIZE_1[0]/2))/2 - 2*int(FILTER_SIZE_2[0]/2))/2)\nFINAL_DIMENSION_Y = int(((INPUT_SHAPE[1] - 2*int(FILTER_SIZE_1[0]/2))/2 - 2*int(FILTER_SIZE_2[0]/2))/2)\n\n\nenv = gym.make('SpaceInvaders-v0')\nkeepTraining = True\nslack_logs = np.zeros((6,1))\n\ndef visualize(featureVector):\n    regularImage = featureVector[0,:FRAME_SIZE].reshape((210,160))\n    differenceImage = featureVector[0,FRAME_SIZE:].reshape((210,160))\n    PLT.imshow(regularImage)\n    PLT.show()\n    PLT.imshow(differenceImage)\n    PLT.show()\n\ndef writeCsv(index, data):\n    slack_logs[index] = data\n\n    \n    \n    \n    \n    \n    \n    \n\n    with open(\"logs.csv\", \"w\", newline='') as csv_file:\n        writer = csv.writer(csv_file, delimiter=',')\n        writer.writerows(slack_logs)\n\ndef calculatePolicySize():\n    \n    \n    \n    \n    \n    return FILTER_SIZE_1[0] * FILTER_SIZE_1[1] * INPUT_SHAPE[2] * L1 + L1 + FILTER_SIZE_1[0] * FILTER_SIZE_1[1] * L1 * L2 + L2 + FINAL_DIMENSION_X*FINAL_DIMENSION_Y*L2*L3 + L3 + L3 * L4 + L4\n\n\ndef initPopulation():\n    population = np.random.rand(POPULATION_SIZE, calculatePolicySize())\n    population = population*2-1\n    return population\n\ndef convert_prediction_to_action(prediction):\n    index = np.argmax(prediction[0])\n    \n    if (index == 0):\n        return 0\n    \n    elif (index == 1):\n        return 1\n    \n    elif (index == 2):\n        return 3\n    \n    elif (index == 3):\n        return 4\n    return 0\n\ndef playGame(model):\n    score=0\n    done=False\n    action=0\n    frame = np.zeros((1,FRAME_SIZE))\n    previous_frame = np.zeros((1,FRAME_SIZE))\n    env.reset()\n    observation_dim = list(INPUT_SHAPE)\n    observation_dim.insert(0,1)\n    observation_dim = tuple(observation_dim)\n    while not done:\n        env.render()\n        observation, reward, done, _ = env.step(action)\n        frame = np.reshape(observation[:,:,0],(1,FRAME_SIZE))\n        frame = np.where(frame > 0, 1.0,0)\n        difference = frame-previous_frame\n        final_observation=np.zeros((1,INPUT_DIM))\n        final_observation[0,:FRAME_SIZE]=frame\n        final_observation[0,FRAME_SIZE:]=difference\n        final_observation = np.reshape(final_observation, observation_dim)\n        prediction = model.predict(final_observation)\n        action = convert_prediction_to_action(prediction)\n        score+=reward\n\n        writeCsv(2, score)\n\n        previous_frame = np.copy(frame)\n\n    \n    return score\n\n\ndef evaluate(dnnmodel, population, gamesPlayed):\n    scores=np.zeros(POPULATION_SIZE)\n    for i in range(POPULATION_SIZE):\n        nnFormatPolicyVector = applyPolicyVectorToNN(population[i])\n        dnnmodel.set_weights(nnFormatPolicyVector)\n        scores[i] = playGame(dnnmodel)\n        gamesPlayed+=1\n        writeCsv(3, gamesPlayed)\n    return scores\n\n\n\ndef buildModel():\n    model = Sequential()\n    \n    layer1=Conv2D(L1, FILTER_SIZE_1, activation='relu', input_shape = INPUT_SHAPE, kernel_initializer='uniform')\n    model.add(layer1)\n    model.add(MaxPooling2D(pool_size=POOLING_SIZE))\n    \n    layer2=Conv2D(L2, FILTER_SIZE_2, activation='relu', kernel_initializer='uniform')\n    model.add(layer2)\n    model.add(MaxPooling2D(pool_size=POOLING_SIZE))\n\n    \n    model.add(Flatten())\n\n    layer3=Dense(L3, activation = 'relu', kernel_initializer='uniform')\n    model.add(layer3)\n\n    layer4=Dense(L4, activation ='softmax', kernel_initializer='uniform')\n    model.add(layer4)\n\n    adam = Adam(lr=0.01)\n    model.compile(loss='mean_squared_error', optimizer=adam)\n    weights=model.get_weights()\n    print(len(weights))\n    print(\"====================================\")\n    return model\n\ndef applyPolicyVectorToNN(policyVector):\n    \n    \n    \n    \n    \n\n    offset=FILTER_SIZE_1[0] * FILTER_SIZE_1[1] * INPUT_SHAPE[2] * L1\n    sec1 = policyVector[:offset].reshape(FILTER_SIZE_1[0], FILTER_SIZE_1[1], INPUT_SHAPE[2], L1)\n    sec2 = policyVector[offset:offset+L1]\n    offset+=L1\n    sec3 = policyVector[offset:offset+FILTER_SIZE_2[0] * FILTER_SIZE_2[1] * L1 * L2].reshape(FILTER_SIZE_2[0], FILTER_SIZE_2[1], L1, L2)\n    offset+=FILTER_SIZE_1[0] * FILTER_SIZE_1[1] * L1 * L2\n    sec4 = policyVector[offset:offset+L2]\n    offset+=L2\n    sec5 = policyVector[offset:offset+FINAL_DIMENSION_X*FINAL_DIMENSION_Y*L2*L3].reshape(FINAL_DIMENSION_X*FINAL_DIMENSION_Y*L2, L3)\n    offset+=FINAL_DIMENSION_X*FINAL_DIMENSION_Y*L2*L3\n    sec6 = policyVector[offset:offset+L3]\n    offset+=L3\n    sec7 = policyVector[offset:offset+L3*L4].reshape(L3, L4)\n    offset+=L3*L4\n    sec8 = policyVector[offset:]\n\n    nnFormat = []\n    nnFormat.append(sec1)\n    nnFormat.append(sec2)\n    nnFormat.append(sec3)\n    nnFormat.append(sec4)\n    nnFormat.append(sec5)\n    nnFormat.append(sec6)\n    nnFormat.append(sec7)\n    nnFormat.append(sec8)\n    return nnFormat\n\n\ndef selection(scores, population):\n    eliteSet = np.zeros((ELITE_SET_SIZE,calculatePolicySize()))\n    scoresTemp=np.copy(scores)\n    for i in range(ELITE_SET_SIZE):\n        index = np.argmax(scoresTemp)\n        scoresTemp[index] = 0\n        eliteSet[i] = population[index]\n    return eliteSet\n\ndef cross(policy1, policy2):\n    newPolicy = policy1.copy()\n    mask = np.random.randint(2, size=newPolicy.shape).astype(np.bool)\n    newPolicy[mask] = policy2[mask]\n    \n    \n    \n    \n    return newPolicy\n\n\ndef crossover(scores, population):\n    crossoverSet = np.zeros((POPULATION_SIZE,calculatePolicySize()))\n    selectionProbability = np.array(scores)/np.sum(scores)\n    for i in range(POPULATION_SIZE - ELITE_SET_SIZE):\n        randomIndex = np.random.choice(range(POPULATION_SIZE), p=selectionProbability)\n        policy1 = population[randomIndex]\n        randomIndex = np.random.choice(range(POPULATION_SIZE), p=selectionProbability)\n        policy2 = population[randomIndex]\n        newPolicy = cross(policy1, policy2)\n        crossoverSet[i]=newPolicy\n    return crossoverSet\n\n\ndef mutation(crossoverPopulation):\n    i = int((POPULATION_SIZE - ELITE_SET_SIZE) * np.random.random_sample())\n    j = int(calculatePolicySize() * np.random.random_sample())\n\n    for _ in range(int(i*j*MUTATION_RATE)):\n        crossoverPopulation[i][j] = np.random.random_sample() * 2 - 1\n    \n    \n    \n    \n    \n    return crossoverPopulation\n\ndef generateNewGeneration(scores, population):\n    elitePopulation = selection(scores, population)\n    crossoverPopulation = crossover(scores, population)\n    mutationPopulation = mutation(crossoverPopulation)\n        \n    for i in range(ELITE_SET_SIZE):\n        mutationPopulation[POPULATION_SIZE-ELITE_SET_SIZE+i] = elitePopulation[i]    \n\n    return mutationPopulation\n\ndef saveHighestScorePolicy(population, generation, scores):\n    if (generation % 10 == 0):\n        index = np.argmax(scores)\n        filename='generation'+str(generation)+'HS'+str(scores[index])+'.npy'\n        np.save(os.path.join('SavedScores', filename) ,population[index])\n        print(\"Saved generation to file \"+filename)\n\ndef loadPolicy(filename, population, index):\n    policy=np.load(filename)\n    print(\"Loaded\\n\",policy)\n    population[index]=policy\n\ndef measureTime():\n    global lasttime\n    currentTime=time.time()\n    diff=currentTime-lasttime\n    lasttime=currentTime\n    return diff\n\n\n\n\nenv.reset()\npopulation = initPopulation()\n\ndnnmodel = buildModel()\ngeneration = 0\nlasttime = time.time()\nall_time_high_score = 0\n\nwriteCsv(4, time.time())\n\nwhile (keepTraining):\n    scores = evaluate(dnnmodel, population, generation*POPULATION_SIZE)\n    print(int(measureTime()),\" sec Generation: \", generation, \" Highest Score: \", np.max(scores), \" Games Played: \", generation*POPULATION_SIZE+POPULATION_SIZE)\n\n    writeCsv(0, generation)\n    writeCsv(1, np.max(scores))\n    if (np.max(scores) > all_time_high_score):\n        all_time_high_score = np.max(scores)\n        writeCsv(5, all_time_high_score)\n\n    saveHighestScorePolicy(population, generation, scores)\n    population = generateNewGeneration(scores, population)\n    print(int(measureTime()),\" sec New generation created.\")\n    generation+=1\n",
        "summary": "This code is a genetic algorithm implementation for training a neural network to play the game Space Invaders. The algorithm uses a population of randomly initialized neural network parameters (policies) and iteratively evolves them through selection, crossover, and mutation to improve their performance on the game.\n\nHere's a breakdown of the key components:\n\n1. **Initialization**:\n   - A population of random policies is created.\n   - A neural network model (`dnnmodel`) is built using Keras.\n\n2. **Evaluation**:\n   - Each policy in the population is evaluated by playing Space Invaders and recording the score.\n   - The scores are used to rank the policies.\n\n3. **Selection**:\n   - The top-performing policies (elite set) are selected for reproduction.\n\n4. **Crossover**:\n   - Random pairs of policies are combined to create new offspring policies through crossover.\n\n5. **Mutation**:\n   - A small fraction of the parameters in each offspring policy are randomly mutated to introduce diversity.\n\n6. **New Generation**:\n   - The elite set and the new offspring form the next generation's population.\n   - The process repeats for a specified number of generations or until a stopping criterion is met (e.g., a certain score is achieved).\n\n7. **Saving and Loading Policies**:\n   - The best policy found in each generation is saved to disk.\n   - A function to load a previously saved policy is also provided.\n\n8. **Time Measurement**:\n   - The time taken for each generation is measured and printed.\n\n### Key Points:\n\n- **Genetic Algorithm**: This approach uses the principles of natural selection, where the fittest individuals (policies with higher scores) are more likely to pass on their traits (parameters).\n- **Neural Network**: The policies are represented as neural network weights, which are evolved through the genetic operations.\n- **Space Invaders**: The game is played using a custom environment that interacts with the neural network's output to control the spaceship and shoot at invaders.\n\n### Potential Improvements:\n\n1. **Hyperparameter Tuning**: Experiment with different values for mutation rate, crossover probability, and population size.\n2. **Environment Customization**: Improve the game environment to make it more challenging or to include additional features.\n3. **Visualization**: Add visualization of the neural network's performance over time.\n4. **Parallel Processing**: Utilize multiple CPU cores or GPUs to speed up the evaluation process.\n\nThis code provides a solid foundation for training a neural network using genetic algorithms, and further experimentation can lead to improved performance on Space Invaders or other games."
    },
    {
        "code": "import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ssd.layers import L2Norm\nfrom ssd.modeling import registry\nfrom ssd.utils.model_zoo import load_state_dict_from_url\n\nmodel_urls = {\n    'vgg': 'https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth',\n}\n\n\n\ndef add_vgg(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == 'C':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, size=300):\n    \n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != 'S':\n            if v == 'S':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1], kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    if size == 512:\n        layers.append(nn.Conv2d(in_channels, 128, kernel_size=1, stride=1))\n        layers.append(nn.Conv2d(128, 256, kernel_size=4, stride=1, padding=1))\n    return layers\n\n\ndef add_header(vgg, extra_layers, boxes_per_location, num_classes):\n    regression_headers = []\n    classification_headers = []\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        regression_headers += [nn.Conv2d(vgg[v].out_channels,\n                                         boxes_per_location[k] * 4, kernel_size=3, padding=1)]\n        classification_headers += [nn.Conv2d(vgg[v].out_channels,\n                                             boxes_per_location[k] * num_classes, kernel_size=3, padding=1)]\n    for k, v in enumerate(extra_layers[1::2], 2):\n        regression_headers += [nn.Conv2d(v.out_channels, boxes_per_location[k]\n                                         * 4, kernel_size=3, padding=1)]\n        classification_headers += [nn.Conv2d(v.out_channels, boxes_per_location[k]\n                                             * num_classes, kernel_size=3, padding=1)]\n    return regression_headers, classification_headers\n\n\nvgg_base = {\n    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n    '512': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n}\nextras_base = {\n    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n    '512': [256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        size = cfg.INPUT.IMAGE_SIZE\n        vgg_config = vgg_base[str(size)]\n        extras_config = extras_base[str(size)]\n\n        self.vgg = nn.ModuleList(add_vgg(vgg_config))\n        self.extras = nn.ModuleList(add_extras(extras_config, i=1024, size=size))\n        self.l2_norm = L2Norm(512, scale=20)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for m in self.extras.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def init_from_pretrain(self, state_dict):\n        self.vgg.load_state_dict(state_dict)\n\n    def forward(self, x):\n        features = []\n        for i in range(23):\n            x = self.vgg[i](x)\n        s = self.l2_norm(x)  \n        features.append(s)\n\n        \n        for i in range(23, len(self.vgg)):\n            x = self.vgg[i](x)\n        features.append(x)\n\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                features.append(x)\n\n        return tuple(features)\n\n\n@registry.BACKBONES.register('vgg')\ndef vgg(cfg, pretrained=True):\n    model = VGG(cfg)\n    if pretrained:\n        model.init_from_pretrain(load_state_dict_from_url(model_urls['vgg']))\n    return model\n",
        "summary": "The provided Python code defines a class `VGG` that implements the VGG backbone network for Single Shot MultiBox Detector (SSD) models. It includes methods to add VGG and extra layers based on configuration, initialize parameters, load pre-trained weights, and forward propagate input data through the network."
    },
    {
        "code": "from __future__ import unicode_literals\nimport frappe\n\ndef set_default_address(doc,method):\n\tif doc.is_primary_address:\n\t\tfor row in doc.links:\n\t\t\tif row.link_doctype==\"Customer\":\n\t\t\t\tcust = frappe.get_doc(\"Customer\",row.link_name)\n\t\t\t\tcust.default_address=doc.name\n\t\t\t\tcust.save()\n\ndef set_default_contact(doc,method):\n\tif doc.is_primary_contact:\n\t\tfor row in doc.links:\n\t\t\tif row.link_doctype==\"Customer\":\n\t\t\t\tcust = frappe.get_doc(\"Customer\",row.link_name)\n\t\t\t\tcust.default_contact_person=doc.name\n\t\t\t\tcust.save()",
        "summary": "The provided Python code defines two functions, `set_default_address` and `set_default_contact`, which are designed to update the default address and contact person for a customer in a Frappe-based application. When an address or contact is marked as primary, these functions iterate through the linked documents to find the associated customer and then set the appropriate default field (either `default_address` or `default_contact_person`) before saving the customer document."
    },
    {
        "code": "class MenuItem(object):\n    TEXT_NAME = 'name'\n    TEXT_URL = 'url_name'\n    TEXT_SUBMENU = 'submenu'\n\n    def __init__(self, name, url=None, *args):\n        super(MenuItem, self).__init__()\n        self.name = name\n        self.url = url\n        self.url_args = args\n        self.sub_menu = []\n\n    def add_sub_menu_item(self, name, url):\n        item = {self.TEXT_NAME: name, self.TEXT_URL: url}\n        self.sub_menu.append(item)\n\n    def __getitem__(self, key):\n        return self[key]\n\n    def to_text(self):\n        output = {}\n        output[self.TEXT_NAME] = self.name\n        if self.url:\n            output[self.TEXT_URL] = self.url\n        if self.sub_menu:\n            output[self.TEXT_SUBMENU] = self.sub_menu\n\n        return output\n\n\nclass Nav:\n    def __init__(self, *args, **kwargs):\n        self.menu = []\n\n    def add_menu(self, menu):\n        self.menu.append(menu)\n\n    def get_menu_list(self):\n        output = []\n        for x in self.menu:\n            output.append(x.to_text())\n\n        return output\n",
        "summary": "The `MenuItem` class defines a menu item with properties for name, URL, and sub-menu items. It includes methods to add sub-menu items and convert the menu item to a text representation. The `Nav` class manages a collection of menu items, allowing their addition and retrieval in a structured format."
    },
    {
        "code": "import os\r\nimport time\r\nimport traceback\r\n\n\r\n\r\ndef getobj(s):\r\n\treturn open(s, \"r\", encoding='utf-8').read()\r\n\r\n\r\ndef getobjs(s):\r\n\tobjs = []\r\n\tfs = os.listdir(s)\r\n\tfor f in fs:\r\n\t\tabsf = os.path.join(s, f)\r\n\t\tif os.path.isfile(absf) and os.path.splitext(f)[1] == '.py':\r\n\t\t\tobjs.append(absf)\r\n\t\telif os.path.isdir(absf):\r\n\t\t\tobjs += getobjs(absf)\r\n\treturn objs\r\n\r\n\r\nclass gameplay(object):\r\n\tdef __init__(self, scenario=\"__general\", _basedir=None):\r\n\t\tprint(\"A new game object is constructed.\")\r\n\t\tif _basedir is None:\r\n\t\t\t_basedir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\r\n\t\tself.__basedir = _basedir\r\n\t\tself.var = {\r\n\t\t\t\"load_script\": self.load_script,\r\n\t\t\t\"load_scripts\": self.load_scripts,\r\n\r\n\t\t\t\"running\": True\r\n\t\t\t\n\t\t}\r\n\t\tself.load_scripts(\"__commons\")\r\n\r\n\t\tself.load_script(os.path.join(\"scenarios\", scenario + '.py'))\r\n\r\n\t\tself.paused = True\r\n\t\tself.speed = 0\r\n\r\n\tdef end(self):\r\n\t\tprint(\"A game has ended.\")\r\n\r\n\tdef run(self):\r\n\t\tprint(\"A game has started.\")\r\n\t\ttry:\r\n\t\t\t\n\t\t\t\n\t\t\twhile self.var['running']:\r\n\t\t\t\tself.var['play_round']()\r\n\t\t\t\tself.pause_game()\r\n\t\t\t\twhile self.paused:\r\n\t\t\t\t\t\n\t\t\t\t\tope = input()\r\n\t\t\t\t\t\n\t\t\t\t\texec(ope)\r\n\t\t\t\ttime.sleep(2 * (0.5 ** self.speed))\r\n\t\texcept:\r\n\t\t\tprint(\"!!!!! --- \u6e38\u620f\u4f53\u629b\u51fa\u5f02\u5e38 --- !!!!!\")\r\n\t\t\ttraceback.print_exc()\r\n\t\tself.end()\r\n\r\n\tdef output(self):\r\n\t\tprint(self.var)\r\n\r\n\tdef load_script(self, scriptpath):\r\n\t\texec(getobj(os.path.join(self.__basedir, scriptpath)), self.var, self.var)\r\n\r\n\tdef load_scripts(self, scriptdir):\r\n\t\tobjs = getobjs(os.path.join(self.__basedir, scriptdir))\r\n\t\tobjs.sort()\r\n\t\tfor i in objs:\r\n\t\t\texec(getobj(i), self.var, self.var)\r\n\r\n\tdef pause_game(self):\r\n\t\tself.paused = True\r\n\r\n\tdef continue_game(self):\r\n\t\tself.paused = False\r\n\r\n\tdef set_speed(self, speed):\r\n\t\tself.speed = speed\r\n",
        "summary": "The provided Python code defines a `gameplay` class that manages game scenarios and scripts. It includes methods for loading scripts, running the game loop, pausing and resuming gameplay, and setting the game speed. The class uses recursion to load all `.py` files within specified directories and executes them in the context of its namespace."
    },
    {
        "code": "import os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import check_random_state\nfrom skbio.stats.composition import clr_inv as softmax\nfrom biom import Table\nfrom patsy import dmatrix\n\n\ndef random_multinomial_model(num_samples, num_features,\n                             reps=1,\n                             low=2, high=10,\n                             beta_mean=0,\n                             beta_scale=5,\n                             mu=1,\n                             sigma=1,\n                             seed=0):\n    \n    N = num_samples\n\n    \n    state = check_random_state(seed)\n    beta = state.normal(beta_mean, beta_scale, size=(2, num_features-1))\n\n    X = np.hstack([np.linspace(low, high, num_samples // reps)]\n                  for _ in range(reps))\n    X = np.vstack((np.ones(N), X)).T\n    phi = np.hstack((np.zeros((N, 1)), X @ beta))\n    probs = softmax(phi)\n    n = [mu] * N\n\n    table = np.vstack(\n        state.multinomial(n[i], probs[i, :])\n        for i in range(N)\n    ).T\n\n    samp_ids = pd.Index(['S%d' % i for i in range(num_samples)],\n                        name='sampleid')\n    feat_ids = ['F%d' % i for i in range(num_features)]\n    balance_ids = ['L%d' % i for i in range(num_features-1)]\n\n    table = Table(table, feat_ids, samp_ids)\n    metadata = pd.DataFrame(X, columns=['Ones', 'X'], index=samp_ids)\n    beta = pd.DataFrame(beta.T, columns=['Intercept', 'beta'],\n                        index=balance_ids)\n\n    return table, metadata, beta\n\n\ndef _type_cast_to_float(df):\n    \n    \n    for c in df.columns:\n        s = df[c]\n        try:\n            df[c] = s.astype(np.float64)\n        except Exception:\n            continue\n    return df\n\n\ndef read_metadata(filepath):\n    \n    metadata = pd.read_table(\n        filepath, dtype=object)\n    cols = metadata.columns\n    metadata = metadata.set_index(cols[0])\n    metadata = _type_cast_to_float(metadata.copy())\n\n    return metadata\n\n\ndef match_and_filter(table, metadata, formula,\n                     min_sample_count, min_feature_count):\n    \n    \n\n    def sample_filter(val, id_, md):\n        return id_ in metadata.index and np.sum(val) > min_sample_count\n\n    def read_filter(val, id_, md):\n        return np.sum(val > 0) > min_feature_count\n\n    table = table.filter(sample_filter, axis='sample', inplace=False)\n    table = table.filter(read_filter, axis='observation', inplace=False)\n\n    metadata = metadata.loc[table.ids(axis='sample')]\n    metadata = metadata.loc[~metadata.index.duplicated(keep='first')]\n\n    def sort_f(xs):\n        return [xs[metadata.index.get_loc(x)] for x in xs]\n\n    table = table.sort(sort_f=sort_f, axis='sample')\n    design = dmatrix(formula, metadata, return_type='dataframe')\n    design = design.dropna()\n\n    def design_filter(val, id_, md):\n        return id_ in design.index\n\n    table = table.filter(design_filter, axis='sample')\n    return table, metadata, design\n\n\ndef split_training(dense_table, metadata, design, training_column=None,\n                   num_random_test_examples=10, seed=None):\n\n    if training_column is None:\n        np.random.seed(seed)\n        idx = np.random.random(design.shape[0])\n        i = np.argsort(idx)[num_random_test_examples]\n\n        threshold = idx[i]\n        train_idx = ~(idx < threshold)\n    else:\n        train_idx = metadata.loc[design.index, training_column] == \"Train\"\n\n    trainX = design.loc[train_idx].values\n    testX = design.loc[~train_idx].values\n\n    trainY = dense_table.loc[train_idx].values\n    testY = dense_table.loc[~train_idx].values\n\n    return trainX, testX, trainY, testY\n\n\ndef silence_output():\n    \n    \n    \n    \n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n    \n    \n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "summary": "The provided Python code defines functions for generating a random multinomial model, reading metadata from a file, matching and filtering data based on specified criteria, splitting the data into training and testing sets, and silencing TensorFlow output. The `random_multinomial_model` function creates a synthetic dataset with compositional data structure. The `read_metadata` function loads and preprocesses metadata. The `match_and_filter` function filters the table and metadata according to sample and feature counts. The `split_training` function divides the data into training and testing sets, optionally using a specified column for splitting. Finally, the `silence_output` function suppresses TensorFlow logging messages."
    },
    {
        "code": "import logging\nimport os\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nimport OpenSSL\nimport zope.component\n\nfrom acme import client as acme_client\nfrom acme import jose\nfrom acme import messages\n\nimport letsencrypt\n\nfrom letsencrypt import account\nfrom letsencrypt import auth_handler\nfrom letsencrypt import configuration\nfrom letsencrypt import constants\nfrom letsencrypt import continuity_auth\nfrom letsencrypt import crypto_util\nfrom letsencrypt import errors\nfrom letsencrypt import error_handler\nfrom letsencrypt import interfaces\nfrom letsencrypt import le_util\nfrom letsencrypt import reverter\nfrom letsencrypt import storage\n\nfrom letsencrypt.display import ops as display_ops\nfrom letsencrypt.display import enhancements\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef acme_from_config_key(config, key):\n    \"Wrangle ACME client construction\"\n    \n    net = acme_client.ClientNetwork(key, verify_ssl=(not config.no_verify_ssl),\n                                    user_agent=_determine_user_agent(config))\n    return acme_client.Client(config.server, key=key, net=net)\n\n\ndef _determine_user_agent(config):\n    \n\n    if config.user_agent is None:\n        ua = \"LetsEncryptPythonClient/{0} ({1}) Authenticator/{2} Installer/{3}\"\n        ua = ua.format(letsencrypt.__version__, \" \".join(le_util.get_os_info()),\n                       config.authenticator, config.installer)\n    else:\n        ua = config.user_agent\n    return ua\n\n\ndef register(config, account_storage, tos_cb=None):\n    \n    \n    if account_storage.find_all():\n        logger.info(\"There are already existing accounts for %s\", config.server)\n    if config.email is None:\n        if not config.register_unsafely_without_email:\n            msg = (\"No email was provided and \"\n                   \"--register-unsafely-without-email was not present.\")\n            logger.warn(msg)\n            raise errors.Error(msg)\n        logger.warn(\"Registering without email!\")\n\n    \n    key = jose.JWKRSA(key=jose.ComparableRSAKey(\n        rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=config.rsa_key_size,\n            backend=default_backend())))\n    acme = acme_from_config_key(config, key)\n    \n    regr = perform_registration(acme, config)\n\n    if regr.terms_of_service is not None:\n        if tos_cb is not None and not tos_cb(regr):\n            raise errors.Error(\n                \"Registration cannot proceed without accepting \"\n                \"Terms of Service.\")\n        regr = acme.agree_to_tos(regr)\n\n    acc = account.Account(regr, key)\n    account.report_new_account(acc, config)\n    account_storage.save(acc)\n\n    return acc, acme\n\n\ndef perform_registration(acme, config):\n    \n    try:\n        return acme.register(messages.NewRegistration.from_data(email=config.email))\n    except messages.Error, e:\n        err = repr(e)\n        if \"MX record\" in err or \"Validation of contact mailto\" in err:\n            config.namespace.email = display_ops.get_email(more=True, invalid=True)\n            return perform_registration(acme, config)\n        else:\n            raise\n\n\nclass Client(object):\n    \n\n    def __init__(self, config, account_, dv_auth, installer, acme=None):\n        \n        self.config = config\n        self.account = account_\n        self.dv_auth = dv_auth\n        self.installer = installer\n\n        \n        if acme is None and self.account is not None:\n            acme = acme_from_config_key(config, self.account.key)\n        self.acme = acme\n\n        \n        \n        \n\n        if dv_auth is not None:\n            cont_auth = continuity_auth.ContinuityAuthenticator(config,\n                                                                installer)\n            self.auth_handler = auth_handler.AuthHandler(\n                dv_auth, cont_auth, self.acme, self.account)\n        else:\n            self.auth_handler = None\n\n    def _obtain_certificate(self, domains, csr):\n        \n        if self.auth_handler is None:\n            msg = (\"Unable to obtain certificate because authenticator is \"\n                   \"not set.\")\n            logger.warning(msg)\n            raise errors.Error(msg)\n        if self.account.regr is None:\n            raise errors.Error(\"Please register with the ACME server first.\")\n\n        logger.debug(\"CSR: %s, domains: %s\", csr, domains)\n\n        authzr = self.auth_handler.get_authorizations(domains)\n        certr = self.acme.request_issuance(\n            jose.ComparableX509(OpenSSL.crypto.load_certificate_request(\n                OpenSSL.crypto.FILETYPE_ASN1, csr.data)),\n            authzr)\n        return certr, self.acme.fetch_chain(certr)\n\n    def obtain_certificate_from_csr(self, csr):\n        \n        return self._obtain_certificate(\n            \n            crypto_util.get_sans_from_csr(\n                csr.data, OpenSSL.crypto.FILETYPE_ASN1), csr)\n\n    def obtain_certificate(self, domains):\n        \n        \n        key = crypto_util.init_save_key(\n            self.config.rsa_key_size, self.config.key_dir)\n        csr = crypto_util.init_save_csr(key, domains, self.config.csr_dir)\n\n        return self._obtain_certificate(domains, csr) + (key, csr)\n\n    def obtain_and_enroll_certificate(self, domains):\n        \n        certr, chain, key, _ = self.obtain_certificate(domains)\n\n        \n        \n        \n        \n        \n        \n        params = vars(self.config.namespace)\n        config = {}\n        cli_config = configuration.RenewerConfiguration(self.config.namespace)\n\n        if (cli_config.config_dir != constants.CLI_DEFAULTS[\"config_dir\"] or\n                cli_config.work_dir != constants.CLI_DEFAULTS[\"work_dir\"]):\n            logger.warning(\n                \"Non-standard path(s), might not work with crontab installed \"\n                \"by your operating system package manager\")\n\n        lineage = storage.RenewableCert.new_lineage(\n            domains[0], OpenSSL.crypto.dump_certificate(\n                OpenSSL.crypto.FILETYPE_PEM, certr.body),\n            key.pem, crypto_util.dump_pyopenssl_chain(chain),\n            params, config, cli_config)\n        return lineage\n\n    def save_certificate(self, certr, chain_cert,\n                         cert_path, chain_path, fullchain_path):\n        \n        for path in cert_path, chain_path, fullchain_path:\n            le_util.make_or_verify_dir(\n                os.path.dirname(path), 0o755, os.geteuid(),\n                self.config.strict_permissions)\n\n        cert_pem = OpenSSL.crypto.dump_certificate(\n            OpenSSL.crypto.FILETYPE_PEM, certr.body)\n        cert_file, act_cert_path = le_util.unique_file(cert_path, 0o644)\n        try:\n            cert_file.write(cert_pem)\n        finally:\n            cert_file.close()\n        logger.info(\"Server issued certificate; certificate written to %s\",\n                    act_cert_path)\n\n        cert_chain_abspath = None\n        fullchain_abspath = None\n        if chain_cert:\n            chain_pem = crypto_util.dump_pyopenssl_chain(chain_cert)\n            cert_chain_abspath = _save_chain(chain_pem, chain_path)\n            fullchain_abspath = _save_chain(cert_pem + chain_pem,\n                                            fullchain_path)\n\n        return os.path.abspath(act_cert_path), cert_chain_abspath, fullchain_abspath\n\n    def deploy_certificate(self, domains, privkey_path,\n                           cert_path, chain_path, fullchain_path):\n        \n        if self.installer is None:\n            logger.warning(\"No installer specified, client is unable to deploy\"\n                           \"the certificate\")\n            raise errors.Error(\"No installer available\")\n\n        chain_path = None if chain_path is None else os.path.abspath(chain_path)\n\n        with error_handler.ErrorHandler(self.installer.recovery_routine):\n            for dom in domains:\n                self.installer.deploy_cert(\n                    domain=dom, cert_path=os.path.abspath(cert_path),\n                    key_path=os.path.abspath(privkey_path),\n                    chain_path=chain_path,\n                    fullchain_path=fullchain_path)\n                self.installer.save()  \n\n            self.installer.save(\"Deployed Let's Encrypt Certificate\")\n\n        msg = (\"We were unable to install your certificate, \"\n               \"however, we successfully restored your \"\n               \"server to its prior configuration.\")\n        with error_handler.ErrorHandler(self._rollback_and_restart, msg):\n            \n            self.installer.restart()\n    def enhance_config(self, domains, config):\n        \n\n        if self.installer is None:\n            logger.warning(\"No installer is specified, there isn't any \"\n                           \"configuration to enhance.\")\n            raise errors.Error(\"No installer available\")\n\n        if config is None:\n            logger.warning(\"No config is specified.\")\n            raise errors.Error(\"No config available\")\n\n        redirect = config.redirect\n        hsts = config.hsts\n        uir = config.uir \n\n        if redirect is None:\n            redirect = enhancements.ask(\"redirect\")\n\n        if redirect:\n            self.apply_enhancement(domains, \"redirect\")\n\n        if hsts:\n            self.apply_enhancement(domains, \"ensure-http-header\",\n                    \"Strict-Transport-Security\")\n        if uir:\n            self.apply_enhancement(domains, \"ensure-http-header\",\n                    \"Upgrade-Insecure-Requests\")\n\n        msg = (\"We were unable to restart web server\")\n        if redirect or hsts or uir:\n            with error_handler.ErrorHandler(self._rollback_and_restart, msg):\n                self.installer.restart()\n\n    def apply_enhancement(self, domains, enhancement, options=None):\n        \n        msg = (\"We were unable to set up enhancement %s for your server, \"\n               \"however, we successfully installed your certificate.\"\n               % (enhancement))\n        with error_handler.ErrorHandler(self._recovery_routine_with_msg, msg):\n            for dom in domains:\n                try:\n                    self.installer.enhance(dom, enhancement, options)\n                except errors.PluginEnhancementAlreadyPresent:\n                    logger.warn(\"Enhancement %s was already set.\",\n                            enhancement)\n                except errors.PluginError:\n                    logger.warn(\"Unable to set enhancement %s for %s\",\n                            enhancement, dom)\n                    raise\n\n            self.installer.save(\"Add enhancement %s\" % (enhancement))\n\n    def _recovery_routine_with_msg(self, success_msg):\n        \n        self.installer.recovery_routine()\n        reporter = zope.component.getUtility(interfaces.IReporter)\n        reporter.add_message(success_msg, reporter.HIGH_PRIORITY)\n\n    def _rollback_and_restart(self, success_msg):\n        \n        logger.critical(\"Rolling back to previous server configuration...\")\n        reporter = zope.component.getUtility(interfaces.IReporter)\n        try:\n            self.installer.rollback_checkpoints()\n            self.installer.restart()\n        except:\n            \n            reporter.add_message(\n                \"An error occurred and we failed to restore your config and \"\n                \"restart your server. Please submit a bug report to \"\n                \"https://github.com/letsencrypt/letsencrypt\",\n                reporter.HIGH_PRIORITY)\n            raise\n        reporter.add_message(success_msg, reporter.HIGH_PRIORITY)\n\n\ndef validate_key_csr(privkey, csr=None):\n    \n    \n    \n    \n\n    \n    if privkey.pem and not crypto_util.valid_privkey(privkey.pem):\n        raise errors.Error(\"The provided key is not a valid key\")\n\n    if csr:\n        if csr.form == \"der\":\n            csr_obj = OpenSSL.crypto.load_certificate_request(\n                OpenSSL.crypto.FILETYPE_ASN1, csr.data)\n            csr = le_util.CSR(csr.file, OpenSSL.crypto.dump_certificate(\n                OpenSSL.crypto.FILETYPE_PEM, csr_obj), \"pem\")\n\n        \n        if csr.data and not crypto_util.valid_csr(csr.data):\n            raise errors.Error(\"The provided CSR is not a valid CSR\")\n\n        \n        \n        if csr.data and privkey.pem:\n            if not crypto_util.csr_matches_pubkey(\n                    csr.data, privkey.pem):\n                raise errors.Error(\"The key and CSR do not match\")\n\n\ndef rollback(default_installer, checkpoints, config, plugins):\n    \n    \n    installer = display_ops.pick_installer(\n        config, default_installer, plugins, question=\"Which installer \"\n        \"should be used for rollback?\")\n\n    \n    \n    \n    if installer is not None:\n        installer.rollback_checkpoints(checkpoints)\n        installer.restart()\n\n\ndef view_config_changes(config):\n    \n    rev = reverter.Reverter(config)\n    rev.recovery_routine()\n    rev.view_config_changes()\n\n\ndef _save_chain(chain_pem, chain_path):\n    \n    chain_file, act_chain_path = le_util.unique_file(chain_path, 0o644)\n    try:\n        chain_file.write(chain_pem)\n    finally:\n        chain_file.close()\n\n    logger.info(\"Cert chain written to %s\", act_chain_path)\n\n    \n    return os.path.abspath(act_chain_path)\n",
        "summary": "This code appears to be a Python module for managing Let's Encrypt certificates and related configurations. It includes functions for:\n\n1. Validating private keys and Certificate Signing Requests (CSRs).\n2. Deploying certificates to web servers.\n3. Enhancing server configurations with features like HTTP Strict Transport Security (HSTS) and automatic redirection.\n4. Handling rollback procedures in case of errors during deployment or configuration changes.\n\nKey components include:\n\n- `Client`: Main class for interacting with Let's Encrypt API and managing certificates.\n- `Installer`: Interface for deploying certificates to different web servers.\n- Various helper functions like `_save_chain`, `validate_key_csr`, etc., which perform specific tasks such as saving certificate chains, validating keys and CSRs.\n\nThe module uses several external libraries including OpenSSL for cryptographic operations, zope.component for dependency injection, and custom error handling mechanisms defined in the `errors` module.\n\nOverall, this code provides a comprehensive solution for automating Let's Encrypt certificate management on web servers, ensuring secure deployment and configuration."
    },
    {
        "code": "import numpy as np\nimport pytest\n\nfrom spot_motion_monitor.camera.gaussian_camera import GaussianCamera\nfrom spot_motion_monitor.models import FullFrameModel\nfrom spot_motion_monitor.utils import FrameRejected, TimeHandler\n\nclass TestFullFrameModel():\n\n    def setup_class(cls):\n        cls.model = FullFrameModel()\n        cls.model.timeHandler = TimeHandler()\n\n    def checkFrame(self, flux, maxAdc, comX, comY):\n        return flux > 4000 and maxAdc > 130 and comX > 0 and comY > 0\n\n    def test_parametersAfterConstruction(self):\n        assert self.model.sigmaScale == 5.0\n        assert self.model.minimumNumPixels == 10\n        assert self.model.timeHandler is not None\n\n    def test_frameCalculations(self):\n        \n        \n        camera = GaussianCamera()\n        camera.seed = 1000\n        camera.startup()\n        frame = camera.getFullFrame()\n        info = self.model.calculateCentroid(frame)\n        assert info.centerX == 288.47687644439395\n        assert info.centerY == 224.45394404821826\n        assert info.flux == 3235.9182163661176\n        assert info.maxAdc == 135.83703259361937\n        assert info.fwhm == 5.749039360993981\n        assert info.stdNoObjects is None\n\n    def test_badFrameCalculation(self):\n        frame = np.ones((480, 640))\n        with pytest.raises(FrameRejected):\n            self.model.calculateCentroid(frame)\n\n    def test_failedFrameCheck(self):\n        \n        \n        self.model.frameCheck = self.checkFrame\n        camera = GaussianCamera()\n        camera.seed = 1000\n        camera.startup()\n        frame = camera.getFullFrame()\n        with pytest.raises(FrameRejected):\n            self.model.calculateCentroid(frame)\n        self.model.frameCheck = None\n",
        "summary": "The provided Python code defines a test class `TestFullFrameModel` that uses the `pytest` framework to validate the functionality of a `FullFrameModel` class. The tests include checking parameters after construction, performing frame calculations with expected results, handling bad frames by raising exceptions, and testing frame checks based on custom criteria."
    },
    {
        "code": "from odoo import _, api, fields, models\nfrom odoo.tools import html_sanitize\n\n\nclass OdooModule(models.Model):\n    _inherit = \"abstract.action.mixin\"\n    _name = \"odoo.module\"\n    _description = \"Odoo Module\"\n    _order = \"technical_name, name\"\n\n    \n    name = fields.Char(\n        string=\"Name\", store=True, readonly=True, compute=\"_compute_name\"\n    )\n\n    technical_name = fields.Char(\n        string=\"Technical Name\", index=True, required=True, readonly=True\n    )\n\n    module_version_ids = fields.One2many(\n        comodel_name=\"odoo.module.version\",\n        inverse_name=\"module_id\",\n        string=\"Versions\",\n        readonly=True,\n    )\n\n    module_version_qty = fields.Integer(\n        string=\"Number of Module Versions\",\n        compute=\"_compute_module_version_qty\",\n        store=True,\n    )\n\n    author_ids = fields.Many2many(\n        string=\"Authors\",\n        comodel_name=\"odoo.author\",\n        compute=\"_compute_author\",\n        relation=\"github_module_author_rel\",\n        column1=\"module_id\",\n        column2=\"author_id\",\n        store=True,\n    )\n\n    author_ids_description = fields.Char(\n        string=\"Authors (Text)\", compute=\"_compute_author\", store=True\n    )\n\n    organization_serie_ids = fields.Many2many(\n        string=\"Series\",\n        comodel_name=\"github.organization.serie\",\n        compute=\"_compute_organization_serie\",\n        store=True,\n        relation=\"github_module_organization_serie_rel\",\n        column1=\"module_id\",\n        column2=\"organization_serie_id\",\n    )\n\n    organization_serie_ids_description = fields.Char(\n        string=\"Series (Text)\", store=True, compute=\"_compute_organization_serie\",\n    )\n\n    description_rst = fields.Char(\n        string=\"RST Description of the last Version\",\n        store=True,\n        readonly=True,\n        compute=\"_compute_description\",\n    )\n\n    description_rst_html = fields.Html(\n        string=\"HTML of the RST Description of the last Version\",\n        store=True,\n        readonly=True,\n        compute=\"_compute_description\",\n    )\n\n    dependence_module_version_ids = fields.Many2many(\n        comodel_name=\"odoo.module.version\",\n        string=\"Module Versions that depend on this module\",\n        relation=\"module_version_dependency_rel\",\n        column1=\"dependency_module_id\",\n        column2=\"module_version_id\",\n    )\n\n    dependence_module_version_qty = fields.Integer(\n        string=\"Number of Module Versions that depend on this module\",\n        compute=\"_compute_dependence_module_version_qty\",\n        store=True,\n    )\n\n    image = fields.Binary(\n        string=\"Icon Image\", compute=\"_compute_image\", store=True, attachment=True\n    )\n\n    \n    @api.depends(\"module_version_ids.image\")\n    def _compute_image(self):\n        module_version_obj = self.env[\"odoo.module.version\"]\n        for module in self:\n            version_ids = module.module_version_ids.ids\n            last_version = module_version_obj.search(\n                [(\"id\", \"in\", version_ids)], order=\"organization_serie_id desc\", limit=1\n            )\n            module.image = last_version and last_version.image\n\n    @api.depends(\"technical_name\", \"module_version_ids.name\")\n    def _compute_name(self):\n        module_version_obj = self.env[\"odoo.module.version\"]\n        for module in self:\n            version_ids = module.module_version_ids.ids\n            last_version = module_version_obj.search(\n                [(\"id\", \"in\", version_ids)], order=\"organization_serie_id desc\", limit=1\n            )\n            if last_version:\n                module.name = last_version.name\n            else:\n                module.name = module.technical_name\n\n    @api.depends(\"module_version_ids\", \"module_version_ids.description_rst_html\")\n    def _compute_description(self):\n        module_version_obj = self.env[\"odoo.module.version\"]\n        for module in self:\n            version_ids = module.module_version_ids.ids\n            last_version = module_version_obj.search(\n                [(\"id\", \"in\", version_ids)], order=\"organization_serie_id desc\", limit=1\n            )\n            if last_version:\n                module.description_rst = last_version.description_rst\n                module.description_rst_html = last_version.description_rst_html\n            else:\n                module.description_rst = \"\"\n                module.description_rst_html = html_sanitize(\n                    \"<h1 style='color:gray;'>\" + _(\"No Version Found\") + \"</h1>\"\n                )\n\n    @api.depends(\"dependence_module_version_ids.dependency_module_ids\")\n    def _compute_dependence_module_version_qty(self):\n        for module in self:\n            module.dependence_module_version_qty = len(\n                module.dependence_module_version_ids\n            )\n\n    @api.depends(\"module_version_ids\")\n    def _compute_module_version_qty(self):\n        for module in self:\n            module.module_version_qty = len(module.module_version_ids)\n\n    @api.depends(\"module_version_ids.author_ids\")\n    def _compute_author(self):\n        for module in self:\n            authors = []\n            for version in module.module_version_ids:\n                authors += version.author_ids\n            authors = set(authors)\n            module.author_ids = [x.id for x in authors]\n            module.author_ids_description = \", \".join(sorted([x.name for x in authors]))\n\n    @api.depends(\"module_version_ids.organization_serie_id\")\n    def _compute_organization_serie(self):\n        for module in self:\n            organization_series = []\n            for version in module.module_version_ids:\n                organization_series += version.organization_serie_id\n            organization_series = set(organization_series)\n            module.organization_serie_ids = [x.id for x in organization_series]\n            module.organization_serie_ids_description = \" - \".join(\n                [x.name for x in sorted(organization_series, key=lambda x: x.sequence)]\n            )\n\n    \n    @api.model\n    def create_if_not_exist(self, technical_name):\n        module = self.search([(\"technical_name\", \"=\", technical_name)])\n        if not module:\n            module = self.create({\"technical_name\": technical_name})\n        return module\n\n    def name_get(self):\n        return [(module.id, module.technical_name) for module in self]\n",
        "summary": "This Python code defines an Odoo model named `OdooModule` that extends the functionality of a mixin class to manage and store information about various Odoo modules, including their versions, authors, dependencies, and descriptions. It includes methods for computing fields based on related records and provides utility functions like creating a module if it doesn't exist and retrieving module names in a specific format."
    },
    {
        "code": "import sys\nimport math\nimport os\n\nimport torch\nimport torchvision\nimport numpy as np\n\nfrom pkg_resources import resource_stream\n\ndef interpolate1d(x, values, tangents):\n    \n    assert torch.is_tensor(x)\n    assert torch.is_tensor(values)\n    assert torch.is_tensor(tangents)\n    float_dtype = x.dtype\n    assert values.dtype == float_dtype\n    assert tangents.dtype == float_dtype\n    assert len(values.shape) == 1\n    assert len(tangents.shape) == 1\n    assert values.shape[0] == tangents.shape[0]\n\n    x_lo = torch.floor(torch.clamp(x, torch.as_tensor(0),\n                                   values.shape[0] - 2)).type(torch.int64)\n    x_hi = x_lo + 1\n\n    \n    t = x - x_lo.type(float_dtype)\n\n    \n    t_sq = t**2\n    t_cu = t * t_sq\n    h01 = -2. * t_cu + 3. * t_sq\n    h00 = 1. - h01\n    h11 = t_cu - t_sq\n    h10 = h11 - t_sq + t\n\n    \n    \n    value_before = tangents[0] * t + values[0]\n    value_after = tangents[-1] * (t - 1.) + values[-1]\n\n    \n    neighbor_values_lo = values[x_lo]\n    neighbor_values_hi = values[x_hi]\n    neighbor_tangents_lo = tangents[x_lo]\n    neighbor_tangents_hi = tangents[x_hi]\n    value_mid = (\n        neighbor_values_lo * h00 + neighbor_values_hi * h01 +\n        neighbor_tangents_lo * h10 + neighbor_tangents_hi * h11)\n\n    return torch.where(t < 0., value_before,\n                       torch.where(t > 1., value_after, value_mid))\n\n\ndef log_safe(x):\n    x = torch.as_tensor(x)\n    return torch.log(torch.min(x, torch.tensor(33e37).to(x)))\n\n\ndef load_spline_params():\n    dirname = os.path.dirname(__file__)\n    with open(os.path.join(dirname, '../misc/partition_spline.npz'), \"rb\") as spline_file:\n        with np.load(spline_file, allow_pickle=False) as f:\n            spline_x_scale = torch.tensor(f['x_scale'])\n            spline_values = torch.tensor(f['values'])\n            spline_tangents = torch.tensor(f['tangents'])\n\n    return spline_x_scale, spline_values, spline_tangents\n\n\ndef get_partition_init(shape):\n    shape = torch.as_tensor(shape)\n\n    base1 = (2.25 * shape - 4.5) / (torch.abs(shape - 2) + 0.25) + shape + 2\n    base2 = 5. / 18. * log_safe(4 * shape - 15) + 8\n\n    return torch.where(shape < 4, base1, base2)\n\n\ndef get_partition(shape):\n    shape = torch.as_tensor(shape)\n    assert (shape >= 0).all()\n\n    init = get_partition_init(shape)\n\n    x_scale, values, tangents = load_spline_params()\n\n    return interpolate1d(init * x_scale.to(init), values.to(init), tangents.to(init))\n\n\ndef general_adaptive_loss(x, shape, bowl=1.):\n    input_shape = x.shape\n    shape = torch.as_tensor(shape).to(x.device)\n    bowl = torch.as_tensor(bowl).to(x.device)\n\n    b = x.size(0)\n    x = x.view(b, -1)\n\n    if len(shape.shape) == 0:\n        shape = shape.unsqueeze(dim=0).expand([b, ]).unsqueeze(dim=1)\n    else:\n        shape = shape.view(b, -1)\n\n    if len(bowl.shape) == 0:\n        bowl = bowl.unsqueeze(dim=0).expand([b, ]).unsqueeze(dim=1)\n    else:\n        bowl = bowl.view(b, -1)\n\n    partition = get_partition(shape)\n    ans = (torch.abs(shape - 2)/shape) * (torch.pow((torch.square(x/bowl) /\n                                                     torch.abs(shape - 2) + 1), shape/2) - 1) + log_safe(bowl) + log_safe(partition)\n\n    return ans.view(input_shape)\n",
        "summary": "The provided Python code defines a series of functions for adaptive loss calculation in machine learning, including interpolation methods, safe logarithm calculations, and partition-based loss functions. It utilizes PyTorch for tensor operations and NumPy for handling data loading and manipulation. The primary function, `general_adaptive_loss`, computes a loss based on input data shape, bowl parameter, and partition values, incorporating elements of regularization and adaptive scaling to improve model performance."
    },
    {
        "code": "import re\nimport tvm\nfrom tvm import autotvm\nfrom tvm.autotvm.task import get_config\nfrom tvm.autotvm.task.topi_integration import deserialize_args\nfrom ..nn.conv2d import _get_workload as _get_conv2d_workload\nfrom .. import generic, tag\nfrom ..generic import conv2d as conv2d_generic\nfrom ..util import get_const_tuple\nfrom ..nn.conv2d import conv2d_NCHWc_int8\nfrom .. import nn\nfrom . import conv2d_avx_1x1, conv2d_avx_common\n\ndef _get_default_config_int8(cfg, data, kernel, strides, padding, out_dtype, is_depthwise=False,\n                             layout='NCHW'):\n    \n    assert not is_depthwise, \"Depthwise Int8 not supported\"\n    wkl = _get_conv2d_workload(data, kernel, strides, padding, out_dtype, layout)\n    is_kernel_1x1 = wkl.hkernel == 1 and wkl.wkernel == 1\n    if is_kernel_1x1:\n        conv2d_generic.fallback_schedule_cpu_1x1_int8(\n            cfg, wkl, int32_lanes=16, num_int8_elements=4)\n    else:\n        conv2d_generic.fallback_schedule_cpu_common_int8(\n            cfg, wkl, int32_lanes=16, num_int8_elements=4)\n\n\ndef _is_int8_hw_support(data_dtype, kernel_dtype):\n    \n    \n    is_dtype_support = data_dtype == 'uint8' and kernel_dtype == 'int8'\n\n    \n    llvm_intrin_fast_int8 = \"llvm.x86.avx512.pmaddubs.w.512\"\n    llvm_id = tvm.codegen.llvm_lookup_intrinsic_id(llvm_intrin_fast_int8)\n    is_llvm_support = llvm_id != 0\n\n    \n    target = tvm.target.current_target()\n    is_target_support = False\n    for opt in target.options:\n        if opt == '-mcpu=skylake-avx512':\n            is_target_support = True\n\n    return is_dtype_support and is_llvm_support and is_target_support\n\n\ndef _create_tuning_space_int8(cfg, data, kernel, strides, padding, dilation, layout):\n    \n    dshape = get_const_tuple(data.shape)\n    kshape = get_const_tuple(kernel.shape)\n    pat = re.compile(r'NCHW.+(\\d+)c')\n    if layout == 'NCHW':\n        n, ic, h, w = dshape\n        oc, _, kh, kw = kshape\n    elif layout == 'NHWC':\n        n, h, w, ic = dshape\n        kh, kw, oc, _ = kshape\n    elif pat.match(layout) is not None:\n        n, ic_chunk, h, w, ic_bn = dshape\n        target = tvm.target.current_target(allow_none=False)\n        oc_chunk, k_ic, kh, kw, k_ic_f, oc_bn, k_ic_s = kshape\n        ic = ic_chunk * ic_bn\n        assert ic == k_ic * k_ic_f * k_ic_s\n        oc = oc_chunk*oc_bn\n    else:\n        raise ValueError(\"Not support this layout {} with \"\n                         \"schedule template.\".format(layout))\n\n    is_kernel_1x1 = kh == 1 and kw == 1\n    ph, pw = padding if isinstance(padding, (tuple, list)) else (padding, padding)\n    sh, sw = strides if isinstance(strides, (tuple, list)) else (strides, strides)\n    oh = (h - kh + 2 * ph) // sh + 1\n    ow = (w - kw + 2 * pw) // sw + 1\n\n    \n    cfg.define_split('tile_ic', ic, num_outputs=2, filter=lambda y: y.size[-1] % 4 == 0)\n    cfg.define_split('tile_oc', oc, num_outputs=2, filter=lambda y: y.size[-1] % 16 == 0)\n    cfg.define_split(\"tile_ow\", ow, num_outputs=2, filter=lambda y: y.size[-1] <= 64)\n    if is_kernel_1x1:\n        cfg.define_knob(\"tile_oh\", [1, 2] if oh > 1 else [1])\n    else:\n        cfg.define_knob(\"unroll_kw\", [True, False])\n\n\n\n\n\n\n@autotvm.task.register(\"topi_x86_conv2d_NCHWc_int8\")\ndef _topi_nn_conv2d_NCHWc_int8(*args, **kwargs):\n    assert not kwargs, \"Do not support kwargs in template function call\"\n    args = deserialize_args(args)\n\n    if len(args) == 7:\n        data, kernel, strides, padding, dilation, origin_layout, dtype = args\n    else:\n        assert len(args) == 8\n        data, kernel, strides, padding, dilation, origin_layout, out_layout, dtype = args\n\n    raw_data_shape = get_const_tuple(data.shape)\n    raw_kernel_shape = get_const_tuple(kernel.shape)\n\n    \n    cfg = get_config()\n    _create_tuning_space_int8(cfg, data, kernel, strides, padding, dilation, origin_layout)\n\n    \n    ic_bn, oc_bn, ow_bn = (cfg[\"tile_ic\"].size[-1], cfg[\"tile_oc\"].size[-1],\n                           cfg[\"tile_ow\"].size[-1])\n\n    data_layout = \"NCHW%dc\" % ic_bn\n    out_layout = \"NCHW%dc\" % oc_bn\n\n    \n    new_data_shape = (raw_data_shape[0], raw_data_shape[1] // ic_bn,\n                      raw_data_shape[2], raw_data_shape[3], ic_bn)\n    n_elems = 4\n    new_kernel_shape = (raw_kernel_shape[0] // oc_bn,\n                        raw_kernel_shape[1] // ic_bn,\n                        raw_kernel_shape[2],\n                        raw_kernel_shape[3],\n                        ic_bn // n_elems,\n                        oc_bn,\n                        n_elems)\n\n    new_data = tvm.placeholder(new_data_shape, data.dtype)\n    new_kernel = tvm.placeholder(new_kernel_shape, kernel.dtype)\n\n    C = _declaration_conv_NCHWc_int8(cfg, new_data, new_kernel, strides, padding, dilation,\n                                     data_layout, out_layout, dtype)\n    s = _schedule_conv2d_NCHWc_int8(cfg, [C])\n    return s, [new_data, new_kernel, C]\n\n\n@autotvm.register_topi_compute(conv2d_NCHWc_int8, 'cpu', 'direct')\ndef _declaration_conv_NCHWc_int8(cfg, data, kernel, strides,\n                                 padding, dilation, layout, out_layout, out_dtype):\n    return nn.conv2d_NCHWc_int8_compute(data,\n                                        kernel,\n                                        strides,\n                                        padding,\n                                        dilation,\n                                        layout,\n                                        out_layout,\n                                        out_dtype)\n\n\n@autotvm.register_topi_schedule(generic.schedule_conv2d_NCHWc_int8, 'cpu', ['direct'])\ndef _schedule_conv2d_NCHWc_int8(cfg, outs):\n    \n    s = tvm.create_schedule([x.op for x in outs])\n    scheduled_ops = []\n\n    def traverse(op):\n        \n        \n        if tag.is_broadcast(op.tag):\n            if op not in s.outputs:\n                s[op].compute_inline()\n            for tensor in op.input_tensors:\n                if isinstance(tensor.op, tvm.tensor.ComputeOp) and tensor.op not in scheduled_ops:\n                    traverse(tensor.op)\n\n        if 'conv2d_NCHWc_int8' in op.tag:\n            conv_out = op.output(0)\n            kernel = conv_out.op.input_tensors[1]\n            data_vec = conv_out.op.input_tensors[0]\n            data = data_vec.op.input_tensors[0] \\\n                if isinstance(data_vec.op, tvm.tensor.ComputeOp) and \"pad\" not in data_vec.op.tag \\\n                else data_vec\n            if isinstance(data.op, tvm.tensor.ComputeOp) and \"pad\" in data.op.tag:\n                data_pad = data\n                data = data_pad.op.input_tensors[0]\n\n            args = [s, cfg, data_vec, conv_out, outs[0]]\n            target = tvm.target.current_target(allow_none=False)\n            \n            _, _, kh, kw, _, _, _ = get_const_tuple(kernel.shape)\n            if kh == 1 and kw == 1:\n                conv2d_avx_1x1._schedule_conv_NCHWc_int8(*args)\n            else:\n                conv2d_avx_common._schedule_conv_NCHWc_int8(*args)\n\n        scheduled_ops.append(op)\n\n    traverse(outs[0].op)\n    return s\n\n@autotvm.register_topi_schedule(generic.schedule_conv2d_nhwc_pack, 'cpu', ['direct'])\ndef schedule_conv2d_nhwc_pack(cfg, outs):\n    \n    s = tvm.create_schedule([x.op for x in outs])\n    output_op = outs[0].op\n    scheduled_ops = []\n\n    def traverse(op):\n        \n        \n        if tag.is_broadcast(op.tag):\n            if op not in s.outputs:\n                s[op].compute_inline()\n            else: \n                if len(op.axis) == 4: \n                    n, h, w, c = op.axis\n                    fused = s[op].fuse(n, h, w)\n                    s[op].parallel(fused)\n                    s[op].vectorize(c)\n            for tensor in op.input_tensors:\n                if isinstance(tensor.op, tvm.tensor.ComputeOp) and tensor.op not in scheduled_ops:\n                    traverse(tensor.op)\n\n        if 'conv2d_nhwc_pack_int8' in op.tag:\n            conv_out = op.output(0)\n            kernel = conv_out.op.input_tensors[1]\n            data_vec = conv_out.op.input_tensors[0]\n            data = data_vec.op.input_tensors[0] \\\n                if isinstance(data_vec.op, tvm.tensor.ComputeOp) and \"pad\" not in data_vec.op.tag \\\n                else data_vec\n            if isinstance(data.op, tvm.tensor.ComputeOp) and \"pad\" in data.op.tag:\n                data_pad = data\n                data = data_pad.op.input_tensors[0]\n\n            args = [s, cfg, data_vec, conv_out, outs[0]]\n            if data.dtype == 'uint8':\n                kh, kw, _, _, _ = get_const_tuple(kernel.shape)\n                if kh == 1 and kw == 1:\n                    conv2d_avx_1x1._schedule_conv_nhwc_pack_int8(*args)\n                else:\n                    raise ValueError(\"Only support 1x1 kernel with \"\n                                     \"schedule_conv2d_nhwc_pack.\")\n            else:\n                raise ValueError(\"Not support this data type {} with \"\n                                 \"schedule_conv2d_nhwc_pack. Only support int8\".format(data.dtype))\n\n        scheduled_ops.append(op)\n    traverse(output_op)\n    return s\n",
        "summary": "The provided code snippet is a part of a larger system for optimizing and scheduling convolutional neural network (CNN) operations using the AutoTVM library in Apache TVM. The focus here is on handling convolution operations specifically designed for the NCHWc layout, which is a common format for CNN data in many deep learning frameworks.\n\n### Key Components:\n\n1. **AutoTVM Configuration**:\n   - `@autotvm.tune` and `@autotvm.apply_history_best` decorators are used to tune and apply the best configuration found during the tuning process.\n   - The `Task` object is created with a description, target device, and search space.\n\n2. **Convolution Declaration**:\n   - `_declaration_conv_NCHWc_int8` function registers the computation of convolution for the NCHWc layout using the generic schedule provided by TVM.\n\n3. **Schedule Registration**:\n   - `_schedule_conv2d_NCHWc_int8` and `_schedule_conv2d_nhwc_pack` functions register custom schedules for different types of convolution operations.\n   - These schedules are designed to leverage specific hardware features like AVX instructions for better performance.\n\n4. **Traversal and Scheduling**:\n   - The `traverse` function recursively traverses the computation graph to identify and schedule operations based on their tags (e.g., 'conv2d_NCHWc_int8').\n   - For convolution operations, it checks if the kernel size is 1x1 and schedules accordingly using specialized functions like `_schedule_conv_NCHWc_int8` or `_schedule_conv_nhwc_pack_int8`.\n\n### Usage:\n\nTo use this code, you would typically:\n1. **Tune Parameters**: Run the `@autotvm.tune` decorator to find the best configuration for your specific hardware and dataset.\n2. **Apply Best Configuration**: Use the `@autotvm.apply_history_best` decorator to apply the best configuration found during tuning.\n\n### Example:\n\n```python\n# Assuming you have a model and data ready\nmodel = ...\ndata = ...\n\n# Create a task for AutoTVM\ntask = autotvm.task.create(\n    \"conv2d_NCHWc_int8\",\n    args=(model, data),\n    target=\"llvm\",\n    template_key=\"direct\"\n)\n\n# Tune the parameters\ntuner = autotvm.tuner.RandomSearch(task)\ntuner.run(n_trial=100)\n\n# Apply the best configuration\nbest_config = autotvm.apply_history_best(\"conv2d_NCHWc_int8.log\")\n```\n\n### Notes:\n\n- The code assumes that the necessary hardware and libraries (like AVX) are available on the target machine.\n- The specific scheduling functions (`_schedule_conv_NCHWc_int8`, `_schedule_conv_nhwc_pack_int8`) are not shown here but would be implemented based on the target architecture and optimization goals.\n\nThis system provides a flexible framework for optimizing CNN operations, allowing developers to tailor the performance of their models to specific hardware configurations."
    },
    {
        "code": "import csv\nimport json\nimport os\nimport tempfile\nimport sys\nimport unittest\n\n_CODEDIR = os.path.dirname(os.path.realpath(__file__))\n\nsys.path.insert(1, os.path.join(_CODEDIR, '.'))\nfrom .process import *\n\n_FEATURES = os.path.join(_CODEDIR, 'features.json')\n_STAT_VAR_LIST = os.path.join(_CODEDIR, 'stat_vars.csv')\n_TEST_DATA = os.path.join(_CODEDIR, 'testdata')\n_EXPECTED_TMCF = os.path.join(_CODEDIR, 'output.tmcf')\n\n\nclass ProcessTest(unittest.TestCase):\n\n    def test_convert_column_to_stat_var(self):\n        f = open(_FEATURES)\n        features = json.load(f)\n        f.close()\n        self.assertEqual(\n            convert_column_to_stat_var(\n                'Estimate!!Households receiving food stamps/SNAP!!Households',\n                features), 'Count_Household_WithFoodStampsInThePast12Months')\n        self.assertEqual(\n            convert_column_to_stat_var(\n                'Margin of Error!!' +\n                'Households receiving food stamps/SNAP!!Households!!' +\n                'No children under 18 years!!Other family:!!' +\n                'Male householder, no spouse present', features),\n            'MarginOfError_Count_Household_WithFoodStampsInThePast12Months_' +\n            'WithoutChildrenUnder18_SingleFatherFamilyHousehold')\n        self.assertEqual(\n            convert_column_to_stat_var(\n                'Estimate!!Households receiving food stamps/SNAP!!Households!!'\n                + 'HOUSEHOLD INCOME IN THE PAST 12 MONTHS' +\n                '(IN 2019 INFLATION-ADJUSTED DOLLARS)!!Median income (dollars)',\n                features),\n            'Median_Income_Household_WithFoodStampsInThePast12Months')\n\n    def test_create_csv(self):\n        f = open(_FEATURES)\n        features = json.load(f)\n        f.close()\n        f = open(_STAT_VAR_LIST)\n        stat_vars = f.read().splitlines()\n        f.close()\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            test_csv = os.path.join(tmp_dir, 'test_csv.csv')\n            create_csv(test_csv, stat_vars)\n            for year in range(2010, 2020):\n                filename = f'testACSST5Y{year}.csv'\n                with open(os.path.join(_TEST_DATA, filename)) as f:\n                    reader = csv.DictReader(f)\n                    write_csv(filename, reader, test_csv, features, stat_vars)\n            with open(test_csv) as f_result:\n                test_result = f_result.read()\n                with open(os.path.join(_TEST_DATA, 'expected.csv')) as f_test:\n                    expected = f_test.read()\n                    self.assertEqual(test_result, expected)\n            os.remove(test_csv)\n\n    def test_create_tmcf(self):\n        f = open(_FEATURES)\n        features = json.load(f)\n        f.close()\n        f = open(_STAT_VAR_LIST)\n        stat_vars = f.read().splitlines()\n        f.close()\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            test_tmcf = os.path.join(tmp_dir, 'test_tmcf.tmcf')\n            create_tmcf(test_tmcf, features, stat_vars)\n            with open(test_tmcf) as f_result:\n                test_result = f_result.read()\n                with open(_EXPECTED_TMCF) as f_test:\n                    expected = f_test.read()\n                    self.assertEqual(test_result, expected)\n            os.remove(test_tmcf)\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script contains unit tests for a data processing module that includes functions to convert column names to statistical variables, create CSV files from data, and generate Template Metadata Conversion Format (TMCF) files. The tests utilize JSON features, CSV stat var lists, test data, and expected outputs to validate the functionality of these processes."
    },
    {
        "code": "from utils.utils import validate_parameters\n\nget_news_query_schema = {\n    \"from\": {\n        \"type\": \"integer\",\n        'coerce': int,\n        \"min\": 0,\n        \"max\": 10000,\n        \"required\": False,\n        \"default\": 0\n    },\n    \"limit\": {\n        \"type\": \"integer\",\n        'coerce': int,\n        \"min\": 0,\n        \"max\": 10000,\n        \"required\": False,\n        \"default\": 0\n    },\n    \"category\": {\n        \"type\": \"string\",\n        \"required\": False\n    }\n}\n\nclass GetNewsValidator:\n\n    def __call__(self, request):\n        body_validation_errors = validate_parameters(request.args.copy(), get_news_query_schema)\n        return body_validation_errors",
        "summary": "The provided Python code defines a validation schema for query parameters used in a news retrieval API endpoint. It includes fields for pagination (`from` and `limit`) and an optional category filter, each with specific data types, constraints, and default values. The `GetNewsValidator` class uses this schema to validate incoming request parameters, returning any errors found during the validation process."
    },
    {
        "code": "import hp\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport librosa\nimport torch\nimport librosa.filters\nimport numpy as np\nimport scipy\nfrom random import randint\nfrom os import makedirs\n\n\ndef load_wav(path, sample_rate):\n    return librosa.core.load(path, sr=sample_rate)[0]\n\n\ndef save_wav(wav, path, sample_rate):\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n    scipy.io.wavfile.write(path, sample_rate, wav.astype(np.int16))\n\n\ndef get_segments(source, length, count):\n    begins = []\n    l = len(source)\n    for _ in range(count):\n        begins.append(randint(0, l - length - 1))\n    segments = []\n    for begin in begins:\n        segments.append(source[begin: begin + length])\n    return segments\n\n\ndef process_chime(\n    source=hp.whole_chime_path,\n    target=hp.part_chime_path,\n    sr=16000,\n    duration=30,\n    count=10\n):\n    \n    makedirs(str(target), exist_ok=True)\n    for path in tqdm(source.glob(\"*.wav\")):\n        wave = load_wav(path, sr)\n        if len(wave) < sr * 30: continue\n        waves = get_segments(wave, duration * sr, count)\n        for i, wave in enumerate(waves, 1):\n            save_wav(wave, str(target / f\"{path.stem}_{i}.wav\"), sr)\n\n\nif __name__ == '__main__':\n    print(\"Beginning segmenting CHiME4 noises.\")\n    process_chime()\n    print(\"Processing Finished\")\n",
        "summary": "The provided Python script processes audio files from the CHiME4 dataset, loading them, splitting them into segments of a specified duration, and saving these segments to a new directory. It uses libraries such as librosa for audio processing and tqdm for progress tracking."
    },
    {
        "code": "outputs = [ \"out.exr\" ]\ncommand = testrender(\"-r 320 240 -aa 4 material-layer.xml out.exr\")\n",
        "summary": "The Python code sets up an output file named \"out.exr\" and constructs a command to run a rendering tool called `testrender` with specific parameters for resolution, anti-aliasing level, input XML file, and output file."
    },
    {
        "code": "import cv2\r\nimport sys\r\nimport os\r\nimport numpy as np\r\nimport time\r\n\r\n\nconfThreshold = 0.5  \nnmsThreshold = 0.4   \ninpWidth = 416       \ninpHeight = 416      \nstarting_time = 0\r\nframe_id = 0\r\nfont = cv2.FONT_HERSHEY_PLAIN\r\n\r\n\nclassesFile = \"coco.names\"\r\nclasses = None\r\nwith open(classesFile, 'rt') as f:\r\n    classes = f.read().rstrip('\\n').split('\\n')\r\n\r\n\nmodelConfiguration = \"yolov3.cfg\"\r\nmodelWeights = \"yolov3.weights\"\r\nnet = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\r\nnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\r\nnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\r\n\r\ninputFile = \"presen_T.mp4\"\r\ninputFile2 = \"presen_R.mp4\"\r\noutputFile = \"yolo_out_py.avi\"\r\n\r\n\nif not os.path.isfile(inputFile):\r\n    print(\"Input video file \", inputFile, \" doesn't exist\")\r\n    sys.exit(1)\r\ncap = cv2.VideoCapture(inputFile)\r\ncap2 = cv2.VideoCapture(inputFile2)\r\noutputFile = inputFile[:-4] + \"_yolo_out_py.avi\"\r\n\r\n\nvid_writer = cv2.VideoWriter(outputFile, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 30,\r\n                            (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\r\n\r\n\ndef getOutputsNames(net):\r\n    \n    layersNames = net.getLayerNames()\r\n    \n    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n\r\n\ndef drawPred(classId, conf, left, top, right, bottom):\r\n    \n    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0))\r\n    label = '%.2f' % conf\r\n\r\n    \n    if classes:\r\n        assert (classId < len(classes))\r\n        label = '%s:%s' % (classes[classId], label)\r\n\r\n    \n    labelSize, baseLine = cv2.getTextSize(label, font, 0.5, 1)\r\n    top = max(top, labelSize[1])\r\n    cv2.putText(frame, label, (left, top), font, 1, (0, 255, 0), 2)\r\n\r\n\ndef postprocess(frame, outs):\r\n    frameHeight = frame.shape[0]\r\n    frameWidth = frame.shape[1]\r\n\r\n    \n    \n    classIds = []\r\n    confidences = []\r\n    boxes = []\r\n    for out in outs:\r\n        for detection in out:\r\n            scores = detection[5:]\r\n            classId = np.argmax(scores)\r\n            confidence = scores[classId]\r\n            if confidence > confThreshold:\r\n                center_x = int(detection[0] * frameWidth)\r\n                center_y = int(detection[1] * frameHeight)\r\n                width = int(detection[2] * frameWidth)\r\n                height = int(detection[3] * frameHeight)\r\n                left = int(center_x - width / 2)\r\n                top = int(center_y - height / 2)\r\n                classIds.append(classId)\r\n                confidences.append(float(confidence))\r\n                boxes.append([left, top, width, height])\r\n\r\n    \n    \n    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\r\n    for i in indices:\r\n        i = i[0]\r\n        box = boxes[i]\r\n        left = box[0]\r\n        top = box[1]\r\n        width = box[2]\r\n        height = box[3]\r\n        drawPred(classIds[i], confidences[i], left, top, left + width, top + height)\r\n\r\n\nwhile True:\r\n    \n    hasFrame, frame = cap.read()\r\n    hasFrame2, frame2 = cap2.read()\r\n\r\n    frame = cv2.resize(frame, dsize=(600, 402))\r\n    frame2 = cv2.resize(frame2, dsize=(600, 402))\r\n\r\n    cv2.imshow(\"Camera\", frame)\r\n    cv2.imshow(\"Thermal_Camera\", frame2)\r\n    \n    if not hasFrame:\r\n        print(\"Done processing !!!\")\r\n        cv2.waitKey(3000)\r\n        break\r\n\r\n    \n    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\r\n\r\n    \n    net.setInput(blob)\r\n\r\n    \n    outs = net.forward(getOutputsNames(net))\r\n\r\n    \n    postprocess(frame, outs)\r\n\r\n    \n    current_time = time.time()\r\n    sec = current_time - starting_time\r\n    starting_time = current_time\r\n    fps = 1 / (sec)\r\n    str2 = \"FPS : %0.1f\" % fps\r\n    \n\r\n    \n    vid_writer.write(frame.astype(np.uint8))\r\n\r\n    \n    cv2.imshow(\"CAMERA_Detection\", frame)\r\n\r\n\r\n    img2 = None\r\n    fast = cv2.FastFeatureDetector_create(30)\r\n    fast.setNonmaxSuppression(0)\r\n    kp = fast.detect(frame2, None)\r\n    img2 = cv2.drawKeypoints(frame2, kp, img2, (0, 255, 255))\r\n    \n\r\n\r\n    hsv = cv2.cvtColor(frame2, cv2.COLOR_BGR2HSV)\r\n    car_prediction = 30\r\n    lower_white = np.array([0, 0, 255 - car_prediction], dtype=np.uint8)\r\n    upper_white = np.array([255, car_prediction, 255], dtype=np.uint8)\r\n    mask_white = cv2.inRange(hsv, lower_white, upper_white)\r\n    res = cv2.bitwise_and(frame2, frame2, mask=mask_white)\r\n    \n\r\n\r\n    res2 = None\r\n    res2 = res\r\n    igray = cv2.cvtColor(res2, cv2.COLOR_BGR2GRAY)\r\n    iret, ibinary = cv2.threshold(igray, 127, 255, cv2.THRESH_BINARY)\r\n    contours, hierachy = cv2.findContours(ibinary, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\r\n    for i in range(len(contours)):\r\n        cv2.drawContours(res2, [contours[i]], 0, (255, 255, 255), 2)\r\n        cv2.putText(res2, \"car\", tuple(contours[i][0][0]), font, 1, (0, 255, 0), 1)\r\n    \n\r\n\r\n    \n    dst = cv2.addWeighted(res2, 1, frame2, 1, 0)\r\n    \n    \n\r\n    \n    dst2 = cv2.addWeighted(res2, 1, frame, 1, 0)\r\n    cv2.imshow(\"RESULT\",dst2)\r\n\r\n\r\n    \n    key = cv2.waitKey(1)\r\n    if key == 27:\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()",
        "summary": "The provided Python code is a script that uses OpenCV and YOLO (You Only Look Once) for real-time object detection on two video streams. It processes frames from two input videos, detects objects using the YOLO model, overlays bounding boxes and class labels on the frames, calculates FPS, and combines the results with thermal camera data to display and save the output video."
    },
    {
        "code": "from mmdet.datasets.pipelines import Compose\nfrom .dbsampler import DataBaseSampler\nfrom .formating import Collect3D, DefaultFormatBundle, DefaultFormatBundle3D\nfrom .loading import (LoadAnnotations3D, LoadImageFromFileMono3D,\n                      LoadMultiViewImageFromFiles, LoadPointsFromFile,\n                      LoadPointsFromMultiSweeps, NormalizePointsColor,\n                      PointSegClassMapping)\nfrom .test_time_aug import MultiScaleFlipAug3D\nfrom .transforms_3d import (BackgroundPointsFilter, GlobalAlignment,\n                            GlobalRotScaleTrans, IndoorPatchPointSample,\n                            IndoorPointSample, ObjectNameFilter, ObjectNoise,\n                            ObjectRangeFilter, ObjectSample, PointSample,\n                            PointShuffle, PointsRangeFilter,\n                            RandomDropPointsColor, RandomFlip3D,\n                            RandomJitterPoints, VoxelBasedPointSampler)\n\n__all__ = [\n    'ObjectSample', 'RandomFlip3D', 'ObjectNoise', 'GlobalRotScaleTrans',\n    'PointShuffle', 'ObjectRangeFilter', 'PointsRangeFilter', 'Collect3D',\n    'Compose', 'LoadMultiViewImageFromFiles', 'LoadPointsFromFile',\n    'DefaultFormatBundle', 'DefaultFormatBundle3D', 'DataBaseSampler',\n    'NormalizePointsColor', 'LoadAnnotations3D', 'IndoorPointSample',\n    'PointSample', 'PointSegClassMapping', 'MultiScaleFlipAug3D',\n    'LoadPointsFromMultiSweeps', 'BackgroundPointsFilter',\n    'VoxelBasedPointSampler', 'GlobalAlignment', 'IndoorPatchPointSample',\n    'LoadImageFromFileMono3D', 'ObjectNameFilter', 'RandomDropPointsColor',\n    'RandomJitterPoints'\n]\n",
        "summary": "The provided Python code imports various modules and classes from the `mmdet.datasets.pipelines` package, which are used for data preprocessing and augmentation in 3D object detection tasks. It includes functions for loading images, points, and annotations, as well as transformations like flipping, noise addition, and point sampling, all encapsulated within a pipeline for efficient data handling during model training and inference."
    },
    {
        "code": "import pytest\nfrom django.core.exceptions import PermissionDenied\nfrom django.urls import reverse\nfrom wagtail.core.models import Page\n\nfrom wagtail_personalisation.models import Segment\nfrom wagtail_personalisation.rules import VisitCountRule\nfrom wagtail_personalisation.views import (\n    SegmentModelAdmin, SegmentModelDeleteView)\n\n\n@pytest.mark.django_db\ndef test_segment_user_data_view_requires_admin_access(site, client, django_user_model):\n    user = django_user_model.objects.create(username='first')\n\n    segment = Segment(type=Segment.TYPE_STATIC, count=1)\n    segment.save()\n\n    client.force_login(user)\n    url = reverse('segment:segment_user_data', args=(segment.id,))\n    response = client.get(url)\n\n    assert response.status_code == 302\n    assert response.url == '/admin/login/?next=%s' % url\n\n\n@pytest.mark.django_db\ndef test_segment_user_data_view(site, client, mocker, django_user_model):\n    user1 = django_user_model.objects.create(username='first')\n    user2 = django_user_model.objects.create(username='second')\n    admin_user = django_user_model.objects.create(\n        username='admin', is_superuser=True)\n\n    segment = Segment(type=Segment.TYPE_STATIC, count=1)\n    segment.save()\n    segment.static_users.add(user1)\n    segment.static_users.add(user2)\n\n    rule1 = VisitCountRule(counted_page=site.root_page, segment=segment)\n    rule2 = VisitCountRule(counted_page=site.root_page.get_last_child(),\n                           segment=segment)\n    rule1.save()\n    rule2.save()\n\n    mocker.patch('wagtail_personalisation.rules.VisitCountRule.get_user_info_string',\n                 side_effect=[3, 9, 0, 1])\n\n    client.force_login(admin_user)\n    response = client.get(\n        reverse('segment:segment_user_data', args=(segment.id,)))\n\n    assert response.status_code == 200\n    data_lines = response.content.decode().split(\"\\n\")\n\n    assert data_lines[0] == 'Username,Visit count - Test page,Visit count - Regular page\\r'\n    assert data_lines[1] == 'first,3,9\\r'\n    assert data_lines[2] == 'second,0,1\\r'\n\n\n@pytest.mark.django_db\ndef test_segment_delete_view_delete_instance(rf, segmented_page, user):\n    user.is_superuser = True\n    user.save()\n    segment = segmented_page.personalisation_metadata.segment\n    canonical_page = segmented_page.personalisation_metadata.canonical_page\n    variants_metadata = segment.get_used_pages()\n    page_variants = Page.objects.filter(pk__in=(\n        variants_metadata.values_list('variant_id', flat=True)\n    ))\n\n    \n    assert canonical_page\n    assert page_variants\n    assert variants_metadata\n\n    \n    request = rf.get('/'.format(segment.pk))\n    request.user = user\n    view = SegmentModelDeleteView(\n        instance_pk=str(segment.pk),\n        model_admin=SegmentModelAdmin()\n    )\n    view.request = request\n    view.delete_instance()\n\n    \n    with pytest.raises(segment.DoesNotExist):\n        segment.refresh_from_db()\n\n    \n    canonical_page.refresh_from_db()\n\n    \n    assert not page_variants.all()\n    assert not variants_metadata.all()\n\n\n@pytest.mark.django_db\ndef test_segment_delete_view_raises_permission_denied(rf, segmented_page, user):\n    segment = segmented_page.personalisation_metadata.segment\n    request = rf.get('/'.format(segment.pk))\n    request.user = user\n    view = SegmentModelDeleteView(\n        instance_pk=str(segment.pk),\n        model_admin=SegmentModelAdmin()\n    )\n    view.request = request\n    message = 'User have no permission to delete variant page objects.'\n    with pytest.raises(PermissionDenied):\n        view.delete_instance()\n",
        "summary": "The provided Python code includes several test functions using the `pytest` framework to validate functionality related to segments in a Wagtail CMS application, specifically focusing on user data views and segment deletion. The tests ensure that only users with appropriate permissions can access segment user data and that deleting a segment properly handles associated page variants and metadata, raising exceptions when unauthorized attempts are made."
    },
    {
        "code": "import re\n\n\nfrom noc.core.script.base import BaseScript\nfrom noc.sa.interfaces.igetchassisid import IGetChassisID\n\n\nclass Script(BaseScript):\n    name = \"HP.1910.get_chassis_id\"\n    interface = IGetChassisID\n    cache = True\n\n    rx_mac = re.compile(r\"^MAC_ADDRESS\\s+:\\s+(?P<mac>\\S+)$\", re.MULTILINE)\n\n    def execute_cli(self):\n        v = self.cli(\"display device manuinfo\", cached=True)\n        match = self.rx_mac.search(v)\n        mac = match.group(\"mac\")\n        return {\"first_chassis_mac\": mac, \"last_chassis_mac\": mac}\n",
        "summary": "The Python code defines a script for retrieving the chassis ID from an HP 1910 switch using the CLI. It uses regular expressions to parse the MAC address from the output of the `display device manuinfo` command and returns it as both the first and last chassis MAC address."
    },
    {
        "code": "import collections\nimport functools\nfrom typing import Dict, List, Tuple, Counter\nfrom tool.runners.python import SubmissionPy\n\n\ndef parse(s: str) -> Tuple[List[str], Dict[Tuple[str, str], str]]:\n    lines = s.splitlines()\n    initial = list(lines[0].strip())\n    mapping = {}\n    for line in lines[2:]:\n        if stripped_line := line.strip():\n            left, right = stripped_line.split(\" -> \", 1)\n            mapping[left[0], left[1]] = right\n    return initial, mapping\n\n\nDEPTH = 40\n\n\nclass SkaschSubmission(SubmissionPy):\n    @functools.lru_cache(None)\n    def dfs(self, left: str, right: str, depth: int) -> Counter[str]:\n        if depth == DEPTH:\n            return collections.Counter()\n        mid = self.mapping[left, right]\n        cnt = collections.Counter(mid)\n        return cnt + self.dfs(left, mid, depth + 1) + self.dfs(mid, right, depth + 1)\n\n    def run(self, s: str) -> int:\n        \n        \n        self.dfs.cache_clear()\n        initial, self.mapping = parse(s)\n        cnt = collections.Counter(initial)\n        for left, right in zip(initial, initial[1:]):\n            cnt += self.dfs(left, right, 0)\n        return max(cnt.values()) - min(cnt.values())\n\n\ndef test_skasch() -> None:\n    \n    assert (\n        SkaschSubmission().run(\n            .strip()\n        )\n        == 2188189693529\n    )\n",
        "summary": "The provided Python code defines a submission class `SkaschSubmission` that implements a solution to a problem involving string manipulation and depth-first search (DFS). The `parse` function reads input data, extracting an initial sequence of characters and a mapping dictionary. The `dfs` method recursively calculates the frequency of characters by inserting new ones according to the mapping up to a specified depth. The `run` method initializes the process, clears any cached results from previous runs, and computes the difference between the maximum and minimum character frequencies after applying the DFS transformation."
    },
    {
        "code": "revision = '7fb7364b821a'\ndown_revision = '090128c02529'\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n\ndef upgrade():\n    op.add_column('LoadTbl', sa.Column('username', sa.String(45)))\n    op.create_foreign_key('LoadTbl_ibfk_2', 'LoadTbl', 'UserTbl', ['username'], ['name'])\n\n\ndef downgrade():\n    op.drop_constraint('LoadTbl_ibfk_2', 'LoadTbl', type_='foreignkey')\n    op.drop_column('LoadTbl', 'username')\n",
        "summary": "The provided Python code is an Alembic migration script for SQLAlchemy, which adds a new column `username` to the `LoadTbl` table and establishes a foreign key relationship with the `UserTbl` table. The `upgrade()` function applies these changes, while the `downgrade()` function reverts them, effectively allowing database schema modifications in a controlled manner."
    },
    {
        "code": "l = ['h','e','l','l','o']\n\nprint (l.index('l'))\nprint (l.index('l', 2))\nprint (l.index('l', 3))\nprint (l.index('l', 2, 3))\nprint (l.index('l', 3, 4))\nprint (l.index('l', 2, -1))\nprint (l.index('l', 2, -2))\nprint (l.index('l', 3, -1))\n\ntry:\n    print (l.index('l', 4))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', -1))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 2, 2))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 3, 2))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 3, -2))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 3, 0))\nexcept ValueError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 4.3))\nexcept TypeError as e:\n    print (repr(e))\n\ntry:\n    print (l.index('l', 3, 0.6))\nexcept TypeError as e:\n    print (repr(e))\n",
        "summary": "The Python code demonstrates the usage of the `index()` method on a list named `l` containing the characters 'h', 'e', 'l', 'l', 'o'. It prints the indices of occurrences of 'l' within specified ranges, handling exceptions for cases where 'l' is not found or when invalid arguments are provided."
    },
    {
        "code": "expected_output = {\n        'slot': {\n            '1': {\n                'lc': {\n                    'card_type': 'CEF720 48 port 10/100/1000mb Ethernet',\n                    'fw_ver': '12.2(14r)S',\n                    'hw_ver': '2.7',\n                    'mac_address_from': '001e.4aff.ee89',\n                    'mac_address_to': '001e.4aff.eeb8',\n                    'model': 'WS-X6748-GE-TX',\n                    'online_diag_status': 'Pass',\n                    'ports': 48,\n                    'serial_number': 'SAL1209HMW3',\n                    'status': 'Ok',\n                    'subslot': {\n                        'WS-F6700-CFC': {\n                            'hw_ver': '4.0',\n                            'model': 'WS-F6700-CFC',\n                            'serial_number': 'SAL1207G5V1',\n                            'status': 'Ok',\n                        },\n                    },\n                    'sw_ver': '15.4(0.10)',\n                },\n            },\n            '2': {\n                'lc': {\n                    'card_type': '2 port adapter Enhanced FlexWAN',\n                    'fw_ver': '15.4(0.10)S',\n                    'hw_ver': '2.1',\n                    'mac_address_from': '0015.2bff.e884',\n                    'mac_address_to': '0015.2bff.e8c3',\n                    'model': 'WS-X6582-2PA',\n                    'online_diag_status': 'Pass',\n                    'ports': 0,\n                    'serial_number': 'JAE0939LYNQ',\n                    'status': 'Ok',\n                    'sw_ver': '15.4(0.10)S',\n                },\n            },\n            '5': {\n                'rp': {\n                    'card_type': 'Supervisor Engine 720 (Hot)',\n                    'fw_ver': '8.1(3',\n                    'hw_ver': '4.1',\n                    'mac_address_from': '0011.21ff.441a',\n                    'mac_address_to': '0011.21ff.441d',\n                    'model': 'WS-SUP720-3BXL',\n                    'online_diag_status': 'Pass',\n                    'ports': 2,\n                    'serial_number': 'SAD09020BF8',\n                    'status': 'Ok',\n                    'subslot': {\n                        'WS-F6K-PFC3BXL': {\n                            'hw_ver': '1.4',\n                            'model': 'WS-F6K-PFC3BXL',\n                            'serial_number': 'SAD090301K6',\n                            'status': 'Ok',\n                        },\n                        'WS-SUP720': {\n                            'hw_ver': '2.2',\n                            'model': 'WS-SUP720',\n                            'serial_number': 'SAD090105M6',\n                            'status': 'Ok',\n                        },\n                    },\n                    'sw_ver': '15.4(0.10)',\n                },\n            },\n            '6': {\n                'rp': {\n                    'card_type': 'Supervisor Engine 720 (Active)',\n                    'fw_ver': '8.5(4',\n                    'hw_ver': '5.12',\n                    'mac_address_from': '0022.55ff.039b',\n                    'mac_address_to': '0022.55ff.039e',\n                    'model': 'WS-SUP720-3BXL',\n                    'online_diag_status': 'Pass',\n                    'ports': 2,\n                    'serial_number': 'SAL15129MRC',\n                    'status': 'Ok',\n                    'subslot': {\n                        'WS-F6K-PFC3BXL': {\n                            'hw_ver': '1.11',\n                            'model': 'WS-F6K-PFC3BXL',\n                            'serial_number': 'SAL15129KW4',\n                            'status': 'Ok',\n                        },\n                        'WS-SUP720': {\n                            'hw_ver': '5.1',\n                            'model': 'WS-SUP720',\n                            'serial_number': 'SAL15045PYS',\n                            'status': 'Ok',\n                        },\n                    },\n                    'sw_ver': '15.4(0.10)',\n                },\n            },\n        },\n    }\n",
        "summary": "The Python code defines a dictionary `expected_output` that contains information about various network device slots and their respective line cards (lc) or routing processors (rp). Each slot includes details such as card type, firmware version, hardware version, MAC address range, model, online diagnostic status, number of ports, serial number, overall status, software version, and subslot information if applicable."
    },
    {
        "code": "import unittest\nimport importlib\n\nfrom pbutils.streams import warn\n\n\nsettings = None\ntry:\n    pkg_root = __name__.split('.')[0]\n    settings_modname = '{}.settings'.format(pkg_root)\n    settings = importlib.import_module(settings_modname)\nexcept ImportError as e:\n    warn('Unable to import {}: {}'.format(settings_modname, str(e)))\n    \n\nclass BaseTest(unittest.TestCase):\n    \n    if settings is not None:\n        base_url = 'http://{}'.format(settings.FLASK_SERVER_NAME)\n    else:\n        base_url = 'http://localhost:5000'\n\n    def setUp(self):\n        self.client = app.test_client()\n        self.client.testing = True\n        try:\n            self.reset_fixture()\n        except AttributeError as e:\n            if str(e) == 'reset_fixture':\n                print('{} has no method \"reset_fixture()\", skipping'.format(self.__class__))\n            else:\n                raise\n\n    def make_url(cls, url):\n        return cls.base_url + url\n\n    def _test_status(self, url, method, data, status_code, content_type):\n        \n        real_url = self.make_url(url)\n        req = getattr(self.client, method.lower())\n        args = {'follow_redirects': True} \n        if data:\n            if content_type == 'application/json':\n                args['data'] = json.dumps(data)\n            elif content_type == 'application/x-www-form-urlencoded':\n                args['data'] = data\n            args['content_type'] = content_type\n        resp = req(real_url, **args) \n        self.assertEqual(resp.status_code, status_code)\n        try:\n            return json.loads(str(resp.data.decode()))\n        except (TypeError, ValueError):\n            return resp.data.decode()\n\n    def _test_get_status(self, url, status_code=200):\n        return self._test_status(url, 'GET', None, status_code, None)\n        \n    def _test_post_status(self, url, data, status_code=201, content_type='application/json'):\n        return self._test_status(url, 'POST', data, status_code, content_type)\n        \n    def _test_put_status(self, url, data, status_code=204, content_type='application/json'):\n        return self._test_status(url, 'PUT', data, status_code, content_type)\n        \n    def _test_delete_status(self, url, status_code=204):\n        return self._test_status(url, 'DELETE', None, status_code, None)\n",
        "summary": "The provided Python code defines a base test class `BaseTest` that extends `unittest.TestCase`. It includes methods for making HTTP requests and asserting their responses based on expected status codes and content types. The class dynamically sets the base URL from a settings module if available, otherwise defaulting to 'http://localhost:5000'. It also handles exceptions gracefully during setup and provides utility methods for testing GET, POST, PUT, and DELETE requests."
    },
    {
        "code": "import io\nimport logging\nimport time\nimport unittest\n\nimport requests\n\nimport azure.mgmt.batch\nfrom azure.mgmt.batch import models\nfrom azure.common.exceptions import CloudError\nfrom mgmt_batch_preparers import KeyVaultPreparer, SimpleBatchPreparer\n\nfrom devtools_testutils import (\n    AzureMgmtTestCase,\n    ResourceGroupPreparer,\n    StorageAccountPreparer\n)\n\n\nAZURE_LOCATION = 'westcentralus'\nEXISTING_BATCH_ACCOUNT = {'name': 'sdktest2', 'location': 'westcentralus'}\n\n\nclass MgmtBatchTest(AzureMgmtTestCase):\n\n    def setUp(self):\n        super(MgmtBatchTest, self).setUp()\n        self.mgmt_batch_client = self.create_mgmt_client(\n            azure.mgmt.batch.BatchManagementClient)\n        self.mgmt_keyvault_client = self.create_mgmt_client(\n            azure.mgmt.keyvault.KeyVaultManagementClient)\n\n    def _get_account_name(self):\n        return self.get_resource_name('batch')[-24:]\n\n    def test_mgmt_batch_list_operations(self):\n        operations = self.mgmt_batch_client.operations.list()\n        all_ops = list(operations)\n        self.assertEqual(len(all_ops), 35)\n        self.assertEqual(all_ops[0].name, 'Microsoft.Batch/batchAccounts/providers/Microsoft.Insights/diagnosticSettings/read')\n        self.assertEqual(all_ops[0].origin, 'system')\n        self.assertEqual(all_ops[0].display.provider, 'Microsoft Batch')\n        self.assertEqual(all_ops[0].display.operation, 'Read diagnostic setting')\n\n    def test_mgmt_batch_subscription_quota(self):\n        quotas = self.mgmt_batch_client.location.get_quotas(AZURE_LOCATION)\n        self.assertIsInstance(quotas, models.BatchLocationQuota)\n        self.assertEqual(quotas.account_quota, 3)\n\n    def test_mgmt_batch_account_name(self):\n        \n        availability = self.mgmt_batch_client.location.check_name_availability(\n            AZURE_LOCATION, \"randombatchaccount@5^$g9873495873\")\n        self.assertIsInstance(availability, models.CheckNameAvailabilityResult)\n        self.assertFalse(availability.name_available)\n        self.assertEqual(availability.reason, models.NameAvailabilityReason.invalid)\n\n        \n        availability = self.mgmt_batch_client.location.check_name_availability(\n            EXISTING_BATCH_ACCOUNT['location'], EXISTING_BATCH_ACCOUNT['name'])\n        self.assertIsInstance(availability, models.CheckNameAvailabilityResult)\n        self.assertFalse(availability.name_available)\n        self.assertEqual(availability.reason, models.NameAvailabilityReason.already_exists)\n\n        \n        availability = self.mgmt_batch_client.location.check_name_availability(\n            AZURE_LOCATION, self._get_account_name())\n        self.assertIsInstance(availability, models.CheckNameAvailabilityResult)\n        self.assertTrue(availability.name_available)\n\n    @ResourceGroupPreparer(location=AZURE_LOCATION)\n    @KeyVaultPreparer(location=AZURE_LOCATION)\n    def test_mgmt_batch_byos_account(self, resource_group, location, keyvault):\n        if self.is_live:\n            keyvault = keyvault.result()\n        batch_account = models.BatchAccountCreateParameters(\n                location=location,\n                pool_allocation_mode=models.PoolAllocationMode.user_subscription)\n        with self.assertRaises(Exception):  \n            creating = self.mgmt_batch_client.batch_account.create(\n                resource_group.name,\n                self._get_account_name(),\n                batch_account)\n            creating.result()\n\n        keyvault_id = \"/subscriptions/{}/resourceGroups/{}/providers/Microsoft.KeyVault/vaults/{}\".format(\n            self.settings.SUBSCRIPTION_ID, resource_group.name, keyvault.name)\n        keyvault_url = \"https://{}.vault.azure.net/\".format(keyvault.name)\n        batch_account = models.BatchAccountCreateParameters(\n                location=location,\n                pool_allocation_mode=models.PoolAllocationMode.user_subscription,\n                key_vault_reference={'id': keyvault_id, 'url': keyvault_url})\n        creating = self.mgmt_batch_client.batch_account.create(\n                resource_group.name,\n                self._get_account_name(),\n                batch_account)\n        creating.result()\n\n    @ResourceGroupPreparer(location=AZURE_LOCATION)\n    def test_mgmt_batch_account(self, resource_group, location):\n        batch_account = models.BatchAccountCreateParameters(\n            location=location,\n        )\n        account_name = self._get_account_name()\n        account_setup = self.mgmt_batch_client.batch_account.create(\n            resource_group.name,\n            account_name,\n            batch_account)\n        account_setup.result()\n\n        \n        account = self.mgmt_batch_client.batch_account.get(resource_group.name, account_name)\n        self.assertEqual(account.dedicated_core_quota, 20)\n        self.assertEqual(account.low_priority_core_quota, 100)\n        self.assertEqual(account.pool_quota, 100)\n        self.assertEqual(account.pool_allocation_mode.value, 'BatchService')\n\n        \n        accounts = self.mgmt_batch_client.batch_account.list_by_resource_group(resource_group.name)\n        self.assertEqual(len(list(accounts)), 1)\n\n        \n        keys = self.mgmt_batch_client.batch_account.get_keys(resource_group.name, account_name)\n        self.assertIsInstance(keys, models.BatchAccountKeys)\n        self.assertEqual(keys.account_name, account_name)\n        secondary = keys.secondary\n\n        \n        keys = self.mgmt_batch_client.batch_account.regenerate_key(\n            resource_group.name, account_name, 'Secondary')\n        self.assertIsInstance(keys, models.BatchAccountKeys)\n        self.assertFalse(keys.secondary == secondary)\n\n        \n        update_tags = {'Name': 'tagName', 'Value': 'tagValue'}\n        updated = self.mgmt_batch_client.batch_account.update(resource_group.name, account_name, update_tags)\n        self.assertIsInstance(updated, models.BatchAccount)\n        self.assertEqual(updated.tags['Name'], 'tagName')\n        self.assertEqual(updated.tags['Value'], 'tagValue')\n\n        \n        response = self.mgmt_batch_client.batch_account.delete(resource_group.name, account_name)\n        self.assertIsNone(response.result())\n\n    @ResourceGroupPreparer(location=AZURE_LOCATION)\n    @StorageAccountPreparer(name_prefix='batch', location=AZURE_LOCATION)\n    def test_mgmt_batch_applications(self, resource_group, location, storage_account, storage_account_key):\n        \n        storage_resource = '/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Storage/storageAccounts/{}'.format(\n            self.settings.SUBSCRIPTION_ID,\n            resource_group.name,\n            storage_account.name\n        )\n        batch_account = models.BatchAccountCreateParameters(\n            location=location,\n            auto_storage=models.AutoStorageBaseProperties(storage_resource)\n        )\n        account_name = self._get_account_name()\n        account_setup = self.mgmt_batch_client.batch_account.create(\n            resource_group.name,\n            account_name,\n            batch_account)\n        account_setup.result()\n\n        \n        response = self.mgmt_batch_client.batch_account.synchronize_auto_storage_keys(\n                                   resource_group.name, account_name)\n        self.assertIsNone(response)\n\n        \n        application_id = 'my_application_id'\n        application_name = 'my_application_name'\n        application_ver = 'v1.0'\n        application_properties = models.Application(display_name=application_name, allow_updates=True)\n        application = self.mgmt_batch_client.application.create(\n            resource_group.name, account_name, application_id, parameters=application_properties)\n        self.assertIsInstance(application, models.Application)\n        self.assertTrue(application_id in application.id)\n        self.assertTrue(application_name in application.display_name)\n        self.assertTrue(application.allow_updates)\n\n        \n        application = self.mgmt_batch_client.application.get(resource_group.name, account_name, application_id)\n        self.assertIsInstance(application, models.Application)\n        self.assertTrue(application_id in application.id)\n        self.assertTrue(application_name in application.display_name)\n        self.assertTrue(application.allow_updates)\n\n        \n        applications = self.mgmt_batch_client.application.list(resource_group.name, account_name)\n        self.assertTrue(len(list(applications)) > 0)\n\n        \n        package_ref = self.mgmt_batch_client.application_package.create(\n            resource_group.name, account_name, application_id, application_ver)\n        self.assertIsInstance(package_ref, models.ApplicationPackage)\n        with io.BytesIO(b'Hello World') as f:\n            headers = {'x-ms-blob-type': 'BlockBlob'}\n            upload = requests.put(package_ref.storage_url, headers=headers, data=f.read())\n            if not upload:\n                raise ValueError('Upload failed: {!r}'.format(upload))\n\n        \n        response = self.mgmt_batch_client.application_package.activate(\n            resource_group.name, account_name, application_id, application_ver, 'zip')\n        self.assertTrue(response.state == models.PackageState.active)\n\n        \n        params = models.Application(\n            allow_updates=False,\n            display_name='my_updated_name',\n            default_version=application_ver\n        )\n        response = self.mgmt_batch_client.application.update(\n            resource_group.name, account_name, application_id, params)\n        self.assertTrue(application_ver in response.default_version)\n        self.assertTrue('my_updated_name' in response.display_name)\n        self.assertFalse(response.allow_updates)\n\n        \n        package_ref = self.mgmt_batch_client.application_package.get(\n            resource_group.name, account_name, application_id, application_ver)\n        self.assertIsInstance(package_ref, models.ApplicationPackage)\n        self.assertTrue(application_id in package_ref.id)\n        self.assertEqual(package_ref.format, 'zip')\n        self.assertEqual(package_ref.state, models.PackageState.active)\n\n        \n        response = self.mgmt_batch_client.application_package.delete(\n            resource_group.name, account_name, application_id, application_ver)\n        self.assertIsNone(response)\n\n        \n        response = self.mgmt_batch_client.application.delete(\n            resource_group.name, account_name, application_id)\n        self.assertIsNone(response)\n\n        \n        response = self.mgmt_batch_client.batch_account.delete(resource_group.name, account_name)\n        self.assertIsNone(response.result())\n\n    @ResourceGroupPreparer(location=AZURE_LOCATION)\n    @SimpleBatchPreparer(location=AZURE_LOCATION)\n    def test_mgmt_batch_certificates(self, resource_group, location, batch_account):\n        \n        parameters = models.CertificateCreateOrUpdateParameters(\n            thumbprint='cff2ab63c8c955aaf71989efa641b906558d9fb7',\n            thumbprint_algorithm='sha1',\n            data='MIIGMQIBAzCCBe0GCSqGSIb3DQEHAaCCBd4EggXaMIIF1jCCA8AGCSqGSIb3DQEHAaCCA7EEggOtMIIDqTCCA6UGCyqGSIb3DQEMCgECoIICtjCCArIwHAYKKoZIhvcNAQwBAzAOBAhyd3xCtln3iQICB9AEggKQhe5P10V9iV1BsDlwWT561Yu2hVq3JT8ae/ebx1ZR/gMApVereDKkS9Zg4vFyssusHebbK5pDpU8vfAqle0TM4m7wGsRj453ZorSPUfMpHvQnAOn+2pEpWdMThU7xvZ6DVpwhDOQk9166z+KnKdHGuJKh4haMT7Rw/6xZ1rsBt2423cwTrQVMQyACrEkianpuujubKltN99qRoFAxhQcnYE2KlYKw7lRcExq6mDSYAyk5xJZ1ZFdLj6MAryZroQit/0g5eyhoNEKwWbi8px5j71pRTf7yjN+deMGQKwbGl+3OgaL1UZ5fCjypbVL60kpIBxLZwIJ7p3jJ+q9pbq9zSdzshPYor5lxyUfXqaso/0/91ayNoBzg4hQGh618PhFI6RMGjwkzhB9xk74iweJ9HQyIHf8yx2RCSI22JuCMitPMWSGvOszhbNx3AEDLuiiAOHg391mprEtKZguOIr9LrJwem/YmcHbwyz5YAbZmiseKPkllfC7dafFfCFEkj6R2oegIsZo0pEKYisAXBqT0g+6/jGwuhlZcBo0f7UIZm88iA3MrJCjlXEgV5OcQdoWj+hq0lKEdnhtCKr03AIfukN6+4vjjarZeW1bs0swq0l3XFf5RHa11otshMS4mpewshB9iO9MuKWpRxuxeng4PlKZ/zuBqmPeUrjJ9454oK35Pq+dghfemt7AUpBH/KycDNIZgfdEWUZrRKBGnc519C+RTqxyt5hWL18nJk4LvSd3QKlJ1iyJxClhhb/NWEzPqNdyA5cxen+2T9bd/EqJ2KzRv5/BPVwTQkHH9W/TZElFyvFfOFIW2+03RKbVGw72Mr/0xKZ+awAnEfoU+SL/2Gj2m6PHkqFX2sOCi/tN9EA4xgdswEwYJKoZIhvcNAQkVMQYEBAEAAAAwXQYJKwYBBAGCNxEBMVAeTgBNAGkAYwByAG8AcwBvAGYAdAAgAFMAdAByAG8AbgBnACAAQwByAHkAcAB0AG8AZwByAGEAcABoAGkAYwAgAFAAcgBvAHYAaQBkAGUAcjBlBgkqhkiG9w0BCRQxWB5WAFAAdgBrAFQAbQBwADoANABjAGUANgAwADQAZABhAC0AMAA2ADgAMQAtADQANAAxADUALQBhADIAYwBhAC0ANQA3ADcAMwAwADgAZQA2AGQAOQBhAGMwggIOBgkqhkiG9w0BBwGgggH/BIIB+zCCAfcwggHzBgsqhkiG9w0BDAoBA6CCAcswggHHBgoqhkiG9w0BCRYBoIIBtwSCAbMwggGvMIIBXaADAgECAhAdka3aTQsIsUphgIXGUmeRMAkGBSsOAwIdBQAwFjEUMBIGA1UEAxMLUm9vdCBBZ2VuY3kwHhcNMTYwMTAxMDcwMDAwWhcNMTgwMTAxMDcwMDAwWjASMRAwDgYDVQQDEwdub2Rlc2RrMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC5fhcxbJHxxBEIDzVOMc56s04U6k4GPY7yMR1m+rBGVRiAyV4RjY6U936dqXHCVD36ps2Q0Z+OeEgyCInkIyVeB1EwXcToOcyeS2YcUb0vRWZDouC3tuFdHwiK1Ed5iW/LksmXDotyV7kpqzaPhOFiMtBuMEwNJcPge9k17hRgRQIDAQABo0swSTBHBgNVHQEEQDA+gBAS5AktBh0dTwCNYSHcFmRjoRgwFjEUMBIGA1UEAxMLUm9vdCBBZ2VuY3mCEAY3bACqAGSKEc+41KpcNfQwCQYFKw4DAh0FAANBAHl2M97QbpzdnwO5HoRBsiEExOcLTNg+GKCr7HUsbzfvrUivw+JLL7qjHAIc5phnK+F5bQ8HKe0L9YXBSKl+fvwxFTATBgkqhkiG9w0BCRUxBgQEAQAAADA7MB8wBwYFKw4DAhoEFGVtyGMqiBd32fGpzlGZQoRM6UQwBBTI0YHFFqTS4Go8CoLgswn29EiuUQICB9A=',\n            format=models.CertificateFormat.pfx,\n            password='nodesdk')\n\n        certificate = 'SHA1-cff2ab63c8c955aaf71989efa641b906558d9fb7'\n        response = self.mgmt_batch_client.certificate.create(resource_group.name, batch_account.name, certificate, parameters)\n        self.assertIsInstance(response.result(), models.Certificate)\n\n        \n        certs = self.mgmt_batch_client.certificate.list_by_batch_account(resource_group.name, batch_account.name)\n        self.assertEqual(len(list(certs)), 1)\n\n        \n        cert = self.mgmt_batch_client.certificate.get(resource_group.name, batch_account.name, certificate)\n        self.assertIsInstance(cert, models.Certificate)\n        self.assertEqual(cert.thumbprint.lower(), 'cff2ab63c8c955aaf71989efa641b906558d9fb7')\n        self.assertEqual(cert.thumbprint_algorithm, 'SHA1')\n        self.assertIsNone(cert.delete_certificate_error)\n\n        \n        parameters = models.CertificateCreateOrUpdateParameters(\n            password='nodesdk',\n            data='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',)\n        response = self.mgmt_batch_client.certificate.update(resource_group.name, batch_account.name, certificate, parameters)\n        self.assertIsInstance(response, models.Certificate)\n\n        \n        \n        self.mgmt_batch_client.certificate.cancel_deletion(\n            resource_group.name, batch_account.name, certificate)\n\n        \n        response = self.mgmt_batch_client.certificate.delete(resource_group.name, batch_account.name, certificate)\n        self.assertIsNone(response.result())\n\n    @ResourceGroupPreparer(location=AZURE_LOCATION)\n    @SimpleBatchPreparer(location=AZURE_LOCATION)\n    def test_mgmt_batch_pools(self, resource_group, location, batch_account):\n        \n        paas_pool = \"test_paas_pool\"\n        parameters = models.Pool(\n            display_name=\"test_pool\",\n            vm_size='small',\n            deployment_configuration=models.DeploymentConfiguration(\n                cloud_service_configuration=models.CloudServiceConfiguration(os_family='5')\n            ),\n            start_task=models.StartTask(\n                command_line=\"cmd.exe /c \\\"echo hello world\\\"\",\n                resource_files=[models.ResourceFile(http_url='https://blobsource.com', file_path='filename.txt')],\n                environment_settings=[models.EnvironmentSetting('ENV_VAR', 'env_value')],\n                user_identity=models.UserIdentity(\n                    auto_user=models.AutoUserSpecification(\n                        elevation_level=models.ElevationLevel.admin\n                    )\n                )\n            ),\n            user_accounts=[models.UserAccount('UserName', 'p@55wOrd')],\n            scale_settings=models.ScaleSettings(\n                fixed_scale=models.FixedScaleSettings(\n                    target_dedicated_nodes=0,\n                    target_low_priority_nodes=0\n                )\n            )\n        )\n        response = self.mgmt_batch_client.pool.create(\n            resource_group.name, batch_account.name, paas_pool, parameters)\n        self.assertIsInstance(response.result(), models.Pool)\n\n        \n        iaas_pool = \"test_iaas_pool\"\n        parameters = models.Pool(\n            display_name=\"test_pool\",\n            vm_size='Standard_A1',\n            deployment_configuration=models.DeploymentConfiguration(\n                virtual_machine_configuration=models.VirtualMachineConfiguration(\n                    image_reference=models.ImageReference(\n                        publisher='MicrosoftWindowsServer',\n                        offer='WindowsServer',\n                        sku='2016-Datacenter-smalldisk'\n                    ),\n                    node_agent_sku_id='batch.node.windows amd64',\n                    windows_configuration=models.WindowsConfiguration(True)\n                )\n            ),\n            scale_settings=models.ScaleSettings(\n                fixed_scale=models.FixedScaleSettings(\n                    target_dedicated_nodes=0,\n                    target_low_priority_nodes=0\n                )\n            )\n        )\n\n        response = self.mgmt_batch_client.pool.create(\n            resource_group.name, batch_account.name, iaas_pool, parameters)\n        self.assertIsInstance(response.result(), models.Pool)\n\n        \n        pools = self.mgmt_batch_client.pool.list_by_batch_account(resource_group.name, batch_account.name)\n        self.assertEqual(len(list(pools)), 2)\n\n        \n        parameters = models.Pool(\n            scale_settings=models.ScaleSettings(\n                auto_scale=models.AutoScaleSettings(\n                    formula='$TargetDedicatedNodes=1'\n                )\n            )\n        )\n        response = self.mgmt_batch_client.pool.update(\n            resource_group.name, batch_account.name, iaas_pool, parameters)\n        self.assertIsInstance(response, models.Pool)\n\n        \n        pool = self.mgmt_batch_client.pool.get(\n            resource_group.name, batch_account.name, iaas_pool)\n        self.assertIsInstance(pool, models.Pool)\n        self.assertEqual(pool.vm_size, 'STANDARD_A1'),\n        self.assertIsNone(pool.display_name),\n        self.assertEqual(pool.allocation_state, models.AllocationState.resizing)\n        self.assertEqual(\n            pool.deployment_configuration.virtual_machine_configuration.node_agent_sku_id,\n            'batch.node.windows amd64')\n\n        \n        with self.assertRaises(CloudError):\n            self.mgmt_batch_client.pool.stop_resize(resource_group.name, batch_account.name, iaas_pool)\n        if self.is_live:\n            time.sleep(300)\n        \n        response = self.mgmt_batch_client.pool.disable_auto_scale(\n            resource_group.name, batch_account.name, iaas_pool)\n        self.assertIsInstance(response, models.Pool)\n\n        \n        response = self.mgmt_batch_client.pool.delete(\n            resource_group.name, batch_account.name, iaas_pool)\n        self.assertIsNone(response.result())",
        "summary": "This code snippet demonstrates how to manage Azure Batch resources using the Azure SDK for Python. It includes operations such as creating and updating pools, managing certificates, and deleting resources.\n\n1. **Certificate Management**:\n   - The script creates a certificate using `mgmt_batch_client.certificate.create`.\n   - It then updates the certificate with new parameters.\n   - After updating, it cancels the deletion of the certificate and finally deletes it.\n\n2. **Pool Management**:\n   - Two types of pools are created: one using PaaS (Platform as a Service) and another using IaaS (Infrastructure as a Service).\n   - The script lists all pools in the batch account to verify their creation.\n   - It updates an existing pool's scale settings to use auto-scaling.\n   - It retrieves details of a specific pool, modifies its display name, and checks its allocation state.\n   - It stops the resize operation on a pool (if applicable) and disables auto-scaling.\n   - Finally, it deletes the IaaS pool.\n\nThe script uses decorators like `ResourceGroupPreparer` and `SimpleBatchPreparer` to manage resource groups and batch accounts, ensuring that resources are created and cleaned up appropriately. The `is_live` flag is used to control whether certain operations (like waiting for resize completion) should be performed in live environments."
    },
    {
        "code": "import WrightTools as wt\n\n\n\n\n\ndef test_now():\n    wt.kit.TimeStamp()  \n\n\ndef test_utc():\n    wt.kit.timestamp_from_RFC3339(\"2017-11-13 16:09:17Z\")  \n\n\ndef test_date():\n    ts = wt.kit.timestamp_from_RFC3339(\"2017-11-13 16:09:17-6\")\n    assert len(ts.date) == 10\n\n\ndef test_hms():\n    ts = wt.kit.timestamp_from_RFC3339(\"2017-11-13 16:33:44-6\")\n    assert len(ts.hms) == 8\n\n\ndef test_human():\n    ts = wt.kit.TimeStamp()\n    assert len(ts.human) == 19\n\n\ndef test_RFC3339():\n    ts = wt.kit.TimeStamp()\n    assert ts.RFC3339\n    assert wt.kit.timestamp_from_RFC3339(ts.RFC3339) == ts\n\n\ndef test_RFC5322():\n    ts = wt.kit.TimeStamp()\n    assert ts.RFC5322\n\n\ndef test_path():\n    ts = wt.kit.TimeStamp()\n    assert ts.path\n",
        "summary": "The provided Python code tests various functionalities of the `TimeStamp` class from the WrightTools library, including creating timestamps, converting between different formats, and extracting components like date, time, and human-readable strings. Each function asserts that specific attributes or methods return expected values, ensuring the class behaves as intended for timestamp manipulation."
    },
    {
        "code": "import argparse\nimport datetime\nimport ftplib\nimport re\nimport sys\nimport time\n\nfrom urllib.parse import urlparse\nfrom multiprocessing import Pool\nimport http.client as http\n\n\ndef p(s):\n  sys.stdout.write(s)\n  sys.stdout.flush()\n\n\ndef mirror_contains_file(url):\n  url = urlparse(url)\n\n  if url.scheme == 'https':\n    return https_file_exists(url)\n  elif url.scheme == 'http':\n    return http_file_exists(url)\n  elif url.scheme == 'ftp':\n    return ftp_file_exists(url)\n\n\ndef http_file_exists(url):\n  exists = False\n\n  try:\n    conn = http.HTTPConnection(url.netloc)\n    conn.request('HEAD', url.path)\n    response = conn.getresponse()\n\n    exists = response.status == 200\n  except:\n    pass\n\n  return exists\n\ndef https_file_exists(url):\n  exists = False\n\n  try:\n    conn = http.HTTPSConnection(url.netloc)\n    conn.request('HEAD', url.path)\n    response = conn.getresponse()\n    exists = response.status == 200\n  except:\n    pass\n\n  return exists\n\ndef ftp_file_exists(url):\n  listing = []\n  try:\n    conn = ftplib.FTP(url.netloc)\n    conn.login()\n    listing = conn.nlst(url.path)\n    conn.quit()\n  except Exception as e:\n    pass\n\n  return len(listing) > 0\n\n\ndef check_mirror(url):\n  if mirror_contains_file(url):\n    p('.')\n    return None\n  else:\n    \n    p('X')\n    return url\n\nif __name__ == '__main__':\n  desc = 'Periodically checks that all Solr mirrors contain either a copy of a release or a specified path'\n  parser = argparse.ArgumentParser(description=desc)\n  parser.add_argument('-version', '-v', help='Solr Operator version to check')\n  parser.add_argument('-path', '-p', help='instead of a versioned release, check for some/explicit/path')\n  parser.add_argument('-interval', '-i', help='seconds to wait before re-querying mirrors', type=int, default=300)\n  parser.add_argument('-once', '-o', help='run only once', action='store_true', default=False)\n  args = parser.parse_args()\n\n  if (args.version is None and args.path is None) \\\n      or (args.version is not None and args.path is not None):\n    p('You must specify either -version or -path but not both!\\n')\n    sys.exit(1)\n\n  try:\n    conn = http.HTTPSConnection('www.apache.org')\n    conn.request('GET', '/mirrors/')\n    response = conn.getresponse()\n    html = response.read()\n  except Exception as e:\n    p('Unable to fetch the Apache mirrors list!\\n')\n    sys.exit(1)\n\n  mirror_path = args.path if args.path is not None else 'solr/solr-operator/{}/solr-operator-{}.tgz'.format(args.version, args.version)\n\n  pending_mirrors = []\n  for match in re.finditer('<TR>(.*?)</TR>', str(html), re.MULTILINE | re.IGNORECASE | re.DOTALL):\n    row = match.group(1)\n    if not '<TD>ok</TD>' in row:\n      \n      continue\n\n    match = re.search('<A\\s+HREF\\s*=\\s*\"([^\"]+)\"\\s*>', row, re.MULTILINE | re.IGNORECASE)\n    if match:\n      pending_mirrors.append(match.group(1) + mirror_path)\n\n  total_mirrors = len(pending_mirrors)\n\n  label = args.version if args.version is not None else args.path\n  while True:\n    p('\\n{:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n    p('\\nPolling {} Apache Mirrors'.format(len(pending_mirrors)))\n    p('...\\n')\n\n    start = time.time()\n    with Pool(processes=5) as pool:\n      pending_mirrors = list(filter(lambda x: x is not None, pool.map(check_mirror, pending_mirrors)))\n    stop = time.time()\n    remaining = args.interval - (stop - start)\n\n    available_mirrors = total_mirrors - len(pending_mirrors)\n\n    p('\\n{} is downloadable from {}/{} Apache Mirrors ({:.2f}%)\\n'\n      .format(label, available_mirrors, total_mirrors, available_mirrors * 100 / (1 if total_mirrors == 0 else total_mirrors) ))\n    if len(pending_mirrors) == 0 or args.once == True:\n      break\n\n    if remaining > 0:\n      p('Sleeping for {:d} seconds...\\n'.format(int(remaining + 0.5)))\n      time.sleep(remaining)\n\n",
        "summary": "This Python script checks multiple Apache mirrors to ensure they contain a specified Solr release or path, periodically updating the status and sleeping between checks based on user-defined intervals. It handles HTTP, HTTPS, and FTP protocols for file existence verification and uses multiprocessing for efficient parallel processing of mirror URLs."
    },
    {
        "code": "from yb_sp_report_util import SPReportUtil\n\nclass report_column_stats(SPReportUtil):\n    \n    config = {\n        'description': 'Table column metdata including estimates from statistics.'\n        , 'report_sp_location': 'sysviews'\n        , 'report_default_order': 'table_schema|table_name'\n        , 'required_args_single': ['database']\n        , 'optional_args_multi': ['schema', 'table']\n        , 'db_filter_args': {'database':'db_name', 'schema':'table_schema', 'table':'table_name'}\n        , 'usage_example_extra': {'cmd_line_args': \"--database acme --schema_in dev --table_like 'cust%'\" } }\n\n    def execute(self):\n        return self.build({\n            '_db_name': self.args_handler.args.database\n            , '_yb_util_filter' : self.db_filter_sql() })\n\ndef main():\n    print(report_column_stats().execute())\n    exit(0)\n\nif __name__ == \"__main__\":\n    main()",
        "summary": "The provided Python code defines a class `report_column_stats` that inherits from `SPReportUtil`, configuring it to generate metadata reports for table columns, including statistical estimates. The class includes methods for executing the report based on specified database, schema, and table parameters, and a main function to run the report when executed as a script."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union\nfrom ... import _utilities, _tables\nfrom . import outputs\nfrom ._inputs import *\n\n__all__ = ['VirtualNetwork']\n\n\nclass VirtualNetwork(pulumi.CustomResource):\n    def __init__(__self__,\n                 resource_name: str,\n                 opts: Optional[pulumi.ResourceOptions] = None,\n                 address_space: Optional[pulumi.Input[pulumi.InputType['AddressSpaceArgs']]] = None,\n                 dhcp_options: Optional[pulumi.Input[pulumi.InputType['DhcpOptionsArgs']]] = None,\n                 enable_ddos_protection: Optional[pulumi.Input[bool]] = None,\n                 enable_vm_protection: Optional[pulumi.Input[bool]] = None,\n                 etag: Optional[pulumi.Input[str]] = None,\n                 id: Optional[pulumi.Input[str]] = None,\n                 location: Optional[pulumi.Input[str]] = None,\n                 provisioning_state: Optional[pulumi.Input[str]] = None,\n                 resource_group_name: Optional[pulumi.Input[str]] = None,\n                 resource_guid: Optional[pulumi.Input[str]] = None,\n                 subnets: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['SubnetArgs']]]]] = None,\n                 tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,\n                 virtual_network_name: Optional[pulumi.Input[str]] = None,\n                 virtual_network_peerings: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['VirtualNetworkPeeringArgs']]]]] = None,\n                 __props__=None,\n                 __name__=None,\n                 __opts__=None):\n        \n        if __name__ is not None:\n            warnings.warn(\"explicit use of __name__ is deprecated\", DeprecationWarning)\n            resource_name = __name__\n        if __opts__ is not None:\n            warnings.warn(\"explicit use of __opts__ is deprecated, use 'opts' instead\", DeprecationWarning)\n            opts = __opts__\n        if opts is None:\n            opts = pulumi.ResourceOptions()\n        if not isinstance(opts, pulumi.ResourceOptions):\n            raise TypeError('Expected resource options to be a ResourceOptions instance')\n        if opts.version is None:\n            opts.version = _utilities.get_version()\n        if opts.id is None:\n            if __props__ is not None:\n                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')\n            __props__ = dict()\n\n            __props__['address_space'] = address_space\n            __props__['dhcp_options'] = dhcp_options\n            __props__['enable_ddos_protection'] = enable_ddos_protection\n            __props__['enable_vm_protection'] = enable_vm_protection\n            __props__['etag'] = etag\n            __props__['id'] = id\n            __props__['location'] = location\n            __props__['provisioning_state'] = provisioning_state\n            if resource_group_name is None:\n                raise TypeError(\"Missing required property 'resource_group_name'\")\n            __props__['resource_group_name'] = resource_group_name\n            __props__['resource_guid'] = resource_guid\n            __props__['subnets'] = subnets\n            __props__['tags'] = tags\n            if virtual_network_name is None:\n                raise TypeError(\"Missing required property 'virtual_network_name'\")\n            __props__['virtual_network_name'] = virtual_network_name\n            __props__['virtual_network_peerings'] = virtual_network_peerings\n            __props__['name'] = None\n            __props__['type'] = None\n        alias_opts = pulumi.ResourceOptions(aliases=[pulumi.Alias(type_=\"azure-nextgen:network/latest:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20150501preview:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20150615:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20160330:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20160601:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20160901:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20161201:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20170301:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20170601:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20170801:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20170901:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20171001:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180101:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180201:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180401:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180601:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180701:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20180801:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20181001:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20181101:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20181201:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190201:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190401:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190601:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190701:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190801:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20190901:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20191101:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20191201:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20200301:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20200401:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20200501:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20200601:VirtualNetwork\"), pulumi.Alias(type_=\"azure-nextgen:network/v20200701:VirtualNetwork\")])\n        opts = pulumi.ResourceOptions.merge(opts, alias_opts)\n        super(VirtualNetwork, __self__).__init__(\n            'azure-nextgen:network/v20171101:VirtualNetwork',\n            resource_name,\n            __props__,\n            opts)\n\n    @staticmethod\n    def get(resource_name: str,\n            id: pulumi.Input[str],\n            opts: Optional[pulumi.ResourceOptions] = None) -> 'VirtualNetwork':\n        \n        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))\n\n        __props__ = dict()\n\n        return VirtualNetwork(resource_name, opts=opts, __props__=__props__)\n\n    @property\n    @pulumi.getter(name=\"addressSpace\")\n    def address_space(self) -> pulumi.Output[Optional['outputs.AddressSpaceResponse']]:\n        \n        return pulumi.get(self, \"address_space\")\n\n    @property\n    @pulumi.getter(name=\"dhcpOptions\")\n    def dhcp_options(self) -> pulumi.Output[Optional['outputs.DhcpOptionsResponse']]:\n        \n        return pulumi.get(self, \"dhcp_options\")\n\n    @property\n    @pulumi.getter(name=\"enableDdosProtection\")\n    def enable_ddos_protection(self) -> pulumi.Output[Optional[bool]]:\n        \n        return pulumi.get(self, \"enable_ddos_protection\")\n\n    @property\n    @pulumi.getter(name=\"enableVmProtection\")\n    def enable_vm_protection(self) -> pulumi.Output[Optional[bool]]:\n        \n        return pulumi.get(self, \"enable_vm_protection\")\n\n    @property\n    @pulumi.getter\n    def etag(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"etag\")\n\n    @property\n    @pulumi.getter\n    def location(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"location\")\n\n    @property\n    @pulumi.getter\n    def name(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"name\")\n\n    @property\n    @pulumi.getter(name=\"provisioningState\")\n    def provisioning_state(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"provisioning_state\")\n\n    @property\n    @pulumi.getter(name=\"resourceGuid\")\n    def resource_guid(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"resource_guid\")\n\n    @property\n    @pulumi.getter\n    def subnets(self) -> pulumi.Output[Optional[Sequence['outputs.SubnetResponse']]]:\n        \n        return pulumi.get(self, \"subnets\")\n\n    @property\n    @pulumi.getter\n    def tags(self) -> pulumi.Output[Optional[Mapping[str, str]]]:\n        \n        return pulumi.get(self, \"tags\")\n\n    @property\n    @pulumi.getter\n    def type(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"type\")\n\n    @property\n    @pulumi.getter(name=\"virtualNetworkPeerings\")\n    def virtual_network_peerings(self) -> pulumi.Output[Optional[Sequence['outputs.VirtualNetworkPeeringResponse']]]:\n        \n        return pulumi.get(self, \"virtual_network_peerings\")\n\n    def translate_output_property(self, prop):\n        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop\n\n    def translate_input_property(self, prop):\n        return _tables.SNAKE_TO_CAMEL_CASE_TABLE.get(prop) or prop\n\n",
        "summary": "This code defines a Python class `VirtualNetwork` that represents an Azure Virtual Network resource using the Pulumi framework. The class inherits from `pulumi.Resource`. It includes properties for various attributes of a virtual network, such as address space, DHCP options, DDoS protection status, VM protection status, etag, location, name, provisioning state, resource GUID, subnets, tags, and virtual network peerings.\n\nThe class provides methods to create a new instance of the `VirtualNetwork` resource (`__init__`) and retrieve an existing instance by its name and ID (`get`). It also includes property getters for each attribute, which use the `pulumi.get` function to access the corresponding values from the resource's state.\n\nAdditionally, the class has methods `translate_output_property` and `translate_input_property` that map between camelCase and snake_case naming conventions, which are commonly used in Python and Azure respectively. This allows for easier integration with Azure APIs that use camelCase while maintaining a more Pythonic naming convention in the Pulumi code."
    },
    {
        "code": "import os\nimport re\n\nimport pipelinewise.cli as cli\nimport pytest\n\nVIRTUALENVS_DIR = './virtualenvs-dummy'\n\n\n\nclass TestUtils:\n    \n\n    def test_json_detectors(self):\n        \n        assert cli.utils.is_json('{Invalid JSON}') is False\n\n        assert cli.utils.is_json('[]') is True\n        assert cli.utils.is_json('{}') is True\n        assert cli.utils.is_json('{\"prop\": 123}') is True\n        assert cli.utils.is_json('{\"prop-str\":\"dummy-string\",\"prop-int\":123,\"prop-bool\":true}') is True\n\n        assert cli.utils.is_json_file('./dummy-json') is False\n        assert cli.utils.is_json_file('{}/resources/example.json'.format(os.path.dirname(__file__))) is True\n        assert cli.utils.is_json_file('{}/resources/invalid.json'.format(os.path.dirname(__file__))) is False\n        assert cli.utils.is_json_file('{}/resources'.format(os.path.dirname(__file__))) is False\n\n    def test_json_loader(self):\n        \n        \n        assert cli.utils.load_json('/invalid/location/to/json') is None\n\n        \n        with pytest.raises(Exception):\n            cli.utils.load_json('{}/resources/invalid.json'.format(os.path.dirname(__file__)))\n\n        \n        assert \\\n            cli.utils.load_json('{}/resources/example.json'.format(os.path.dirname(__file__))) == \\\n            {\n                'glossary': {\n                    'title': 'example glossary',\n                    'GlossDiv': {\n                        'title': 'S',\n                        'GlossList': {\n                            'GlossEntry': {\n                                'ID': 'SGML',\n                                'SortAs': 'SGML',\n                                'GlossTerm': 'Standard Generalized Markup Language',\n                                'Acronym': 'SGML',\n                                'Abbrev': 'ISO 8879:1986',\n                                'GlossDef': {\n                                    'para': 'A meta-markup language, used to create markup languages such as DocBook.',\n                                    'GlossSeeAlso': ['GML', 'XML']\n                                },\n                                'GlossSee': 'markup'\n                            }\n                        }\n                    }\n                }\n            }\n\n    def test_json_saver(self):\n        \n        obj = {'foo': 'bar'}\n        \n        with pytest.raises(Exception):\n            cli.utils.save_json(obj, '/invalid/path')\n\n        \n        cli.utils.save_json(obj, 'test-json.json')\n        assert cli.utils.load_json('test-json.json') == obj\n\n        \n        os.remove('test-json.json')\n\n    def test_yaml_detectors(self):\n        \n        assert cli.utils.is_yaml() is False\n\n        assert cli.utils.is_yaml('id: 123') is True\n        assert cli.utils.is_yaml() is True\n\n        assert cli.utils.is_yaml_file('./dummy-yaml') is False\n        assert cli.utils.is_yaml_file('{}/resources/example.yml'.format(os.path.dirname(__file__))) is True\n        assert cli.utils.is_yaml_file('{}/resources/invalid.yml'.format(os.path.dirname(__file__))) is False\n        assert cli.utils.is_yaml_file('{}/resources'.format(os.path.dirname(__file__))) is False\n\n    def test_yaml_loader(self):\n        \n        \n        assert cli.utils.load_yaml('/invalid/location/to/yaml') is None\n\n        \n        with pytest.raises(Exception):\n            cli.utils.load_yaml('{}/resources/invalid.yml'.format(os.path.dirname(__file__)))\n\n        \n        with pytest.raises(Exception):\n            cli.utils.load_yaml('{}/resources/example.yml'.format(os.path.dirname(__file__)),\n                                'invalid-secret-file-path')\n\n        \n        assert \\\n            cli.utils.load_yaml('{}/resources/example.yml'.format(os.path.dirname(__file__))) == \\\n            ['Apple', 'Orange', 'Strawberry', 'Mango']\n\n        \n        assert \\\n            cli.utils.load_yaml(\n                '{}/resources/example-with-vault.yml'.format(os.path.dirname(__file__)),\n                '{}/resources/vault-secret.txt'.format(os.path.dirname(__file__))) == \\\n            ['Apple', 'Orange', 'Strawberry', 'Mango', 'Vault Encrypted Secret Fruit']\n\n    def test_sample_file_path(self):\n        \n        for sample in cli.utils.get_sample_file_paths():\n            assert os.path.isfile(sample) is True\n            assert \\\n                re.match('.*config.yml$', sample) or \\\n                re.match('.*(tap|target)_.*.yml.sample$', sample) or \\\n                re.match('.*README.md$', sample)\n\n    def test_extract_log_attributes(self):\n        \n        assert \\\n            cli.utils.extract_log_attributes('snowflake-fx-20190508_000038.singer.log.success') == \\\n            {\n                'filename': 'snowflake-fx-20190508_000038.singer.log.success',\n                'target_id': 'snowflake',\n                'tap_id': 'fx',\n                'timestamp': '2019-05-08T00:00:38',\n                'sync_engine': 'singer',\n                'status': 'success'\n            }\n\n        assert \\\n            cli.utils.extract_log_attributes('snowflake-fx-20190508_231238.fastsync.log.running') == \\\n            {\n                'filename': 'snowflake-fx-20190508_231238.fastsync.log.running',\n                'target_id': 'snowflake',\n                'tap_id': 'fx',\n                'timestamp': '2019-05-08T23:12:38',\n                'sync_engine': 'fastsync',\n                'status': 'running'\n            }\n\n        assert \\\n            cli.utils.extract_log_attributes('dummy-log-file.log') == \\\n            {\n                'filename': 'dummy-log-file.log',\n                'target_id': 'unknown',\n                'tap_id': 'unknown',\n                'timestamp': '1970-01-01T00:00:00',\n                'sync_engine': 'unknown',\n                'status': 'unknown'\n            }\n\n    def test_fastsync_bin(self):\n        \n        \n        assert \\\n            cli.utils.get_fastsync_bin(VIRTUALENVS_DIR, 'mysql', 'snowflake') == \\\n            '{}/pipelinewise/bin/mysql-to-snowflake'.format(VIRTUALENVS_DIR)\n\n    def test_vault(self):\n        \n        \n        with pytest.raises(SystemExit) as pytest_wrapped_e:\n            cli.utils.vault_encrypt('plain_test', 'not-existing-secret-file')\n        assert pytest_wrapped_e.type == SystemExit\n        assert pytest_wrapped_e.value.code == 1\n\n        \n        encrypted_str = str(\n            cli.utils.vault_encrypt('plain_text', '{}/resources/vault-secret.txt'.format(os.path.dirname(__file__))))\n        assert encrypted_str.startswith(\"b'$ANSIBLE_VAULT;\") is True\n\n        \n        formatted_encrypted_str = cli.utils.vault_format_ciphertext_yaml(encrypted_str)\n        assert formatted_encrypted_str.startswith('!vault |') and \"b'$ANSIBLE_VAULT;\" in formatted_encrypted_str\n\n        \n        formatted_encrypted_str = cli.utils.vault_format_ciphertext_yaml(encrypted_str, name='encrypted_plain_text')\n        assert formatted_encrypted_str.startswith(\n            'encrypted_plain_text: !vault |') and \"b'$ANSIBLE_VAULT;\" in formatted_encrypted_str\n\n    def test_schema_loader(self):\n        \n        \n        with pytest.raises(SystemExit) as pytest_wrapped_e:\n            assert cli.utils.load_schema('/invalid/location/to/schema') is None\n        assert pytest_wrapped_e.type == SystemExit\n        assert pytest_wrapped_e.value.code == 1\n\n        \n        tap_schema = cli.utils.load_json('{}/../../../pipelinewise/cli/schemas/tap.json'.format(\n            os.path.dirname(__file__)))\n        assert cli.utils.load_schema('tap') == tap_schema\n\n    def test_json_validate(self):\n        \n        schema = cli.utils.load_schema('tap')\n\n        \n        valid_tap = cli.utils.load_yaml('{}/resources/tap-valid-mysql.yml'.format(os.path.dirname(__file__)))\n        assert cli.utils.validate(valid_tap, schema) is None\n\n        \n        invalid_tap = cli.utils.load_yaml('{}/resources/tap-invalid.yml'.format(os.path.dirname(__file__)))\n        with pytest.raises(SystemExit) as pytest_wrapped_e:\n            cli.utils.validate(invalid_tap, schema)\n        assert pytest_wrapped_e.type == SystemExit\n        assert pytest_wrapped_e.value.code == 1\n\n    def test_delete_keys(self):\n        \n        \n        assert cli.utils.delete_empty_keys({'foo': 'bar', 'foo2': None}) == {'foo': 'bar'}\n\n        \n        assert cli.utils.delete_empty_keys({\n            'foo': 'bar',\n            'foo2': None,\n            'foo3': None,\n            'foo4': 'bar4'\n        }) == {\n            'foo': 'bar',\n            'foo4': 'bar4'\n        }\n\n        \n        assert cli.utils.delete_keys_from_dict({'foo': 'bar', 'foo2': 'bar2'}, ['foo2']) == {'foo': 'bar'}\n\n        \n        assert cli.utils.delete_keys_from_dict({\n            'foo': 'bar',\n            'foo2': 'bar2',\n            'foo3': None,\n            'foo4': 'bar4'\n        }, ['foo2', 'foo4']) == {\n            'foo': 'bar',\n            'foo3': None\n        }\n\n        \n        assert cli.utils.delete_keys_from_dict(\n            [{'foo': 'bar', 'foo2': 'bar2'},\n             {'foo3': {'nested_foo': 'nested_bar', 'nested_foo2': 'nested_bar2'}}], ['foo2', 'nested_foo']) == \\\n             [{'foo': 'bar'},\n              {'foo3': {'nested_foo2': 'nested_bar2'}}]\n\n    def test_silentremove(self):\n        \n        \n        assert cli.utils.silentremove('this-file-not-exists.json') is None\n\n    def test_tap_properties(self):\n        \n        tap_mysql = cli.utils.load_yaml('{}/resources/tap-valid-mysql.yml'.format(os.path.dirname(__file__)))\n\n        \n        tap_catalog_argument = cli.utils.get_tap_property(tap_mysql, 'tap_catalog_argument')\n        assert tap_catalog_argument in ['--catalog', '--properties']\n\n        \n        assert isinstance(cli.utils.get_tap_extra_config_keys(tap_mysql), dict) is True\n\n        \n        assert cli.utils.get_tap_stream_id(tap_mysql, 'dummy_db', 'dummy_schema', 'dummy_table') == \\\n               'dummy_schema-dummy_table'\n\n        \n        assert cli.utils.get_tap_stream_name(tap_mysql, 'dummy_db', 'dummy_schema',\n                                             'dummy_table') == 'dummy_schema-dummy_table'\n\n        \n        assert cli.utils.get_tap_default_replication_method(tap_mysql) == 'LOG_BASED'\n\n        \n        assert cli.utils.get_tap_property_by_tap_type('tap-mysql', 'default_replication_method') == 'LOG_BASED'\n\n        \n        tap_kafka = cli.utils.load_yaml('{}/resources/tap-valid-kafka.yml'.format(os.path.dirname(__file__)))\n        assert cli.utils.get_tap_extra_config_keys(tap_kafka, temp_dir='/my/temp/dir') == {\n            'local_store_dir': '/my/temp/dir',\n            'encoding': 'utf-8'\n        }\n\n        \n        tap_snowflake = cli.utils.load_yaml('{}/resources/tap-valid-snowflake.yml'.format(os.path.dirname(__file__)))\n        assert cli.utils.get_tap_extra_config_keys(tap_snowflake) == {\n            'tables': 'SCHEMA_1.TABLE_ONE,SCHEMA_1.TABLE_TWO'\n        }\n\n    def test_get_tap_target_names(self):\n        \n        expected_tap_names = {'tap_test.yml', 'tap_2test.yml', 'tap_valid.yaml'}\n        expected_target_names = {'target_test.yml'}\n        tap_names, target_names = cli.utils.get_tap_target_names(f'{os.path.dirname(__file__)}'\n                                                                 f'/resources/test_tap_target_names')\n\n        assert tap_names == expected_tap_names\n        assert target_names == expected_target_names\n\n    def test_create_temp_file(self):\n        \n        \n        temp_file = cli.utils.create_temp_file()[1]\n        assert os.path.isfile(temp_file)\n        os.remove(temp_file)\n\n        \n        temp_file = cli.utils.create_temp_file(dir='./temp_dir_to_create_automatically/deep_temp_dir')[1]\n        assert os.path.isfile(temp_file)\n        os.remove(temp_file)\n\n        \n        temp_file = cli.utils.create_temp_file(dir='./temp_dir_to_create_automatically/deep_temp_dir',\n                                               suffix='.json',\n                                               prefix='pipelinewise_test_temp_file_')[1]\n        assert os.path.isfile(temp_file)\n        os.remove(temp_file)\n\n    def test_find_errors_in_log_file(self):\n        \n        \n        log_file = '{}/resources/sample_log_files/tap-run-no-errors.log'.format(os.path.dirname(__file__))\n        assert cli.utils.find_errors_in_log_file(log_file) == []\n\n        \n        log_file = '{}/resources/sample_log_files/tap-run-errors.log'.format(os.path.dirname(__file__))\n        assert cli.utils.find_errors_in_log_file(log_file) == \\\n               ['time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error\\n',\n                'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=EXCEPTION This is an exception\\n',\n                'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=ERROR This is an error\\n',\n                'pymysql.err.OperationalError: (2013, '\n                \"'Lost connection to MySQL server during query ([Errno 104] Connection reset by peer)')\\n\",\n                'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=ERROR '\n                'message=error with status PGRES_COPY_BOTH and no message from the libpq\\n',\n                'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL '\n                'message=error with status PGRES_COPY_BOTH and no message from the libpq\\n',\n                'snowflake.connector.errors.ProgrammingError: 091003 (22000): '\n                'Failure using stage area. Cause: [Access Denied (Status Code: 403; Error Code: AccessDenied)]\\n']\n\n        \n        log_file = '{}/resources/sample_log_files/tap-run-lot-of-errors.log'.format(os.path.dirname(__file__))\n        assert cli.utils.find_errors_in_log_file(log_file) == \\\n            ['time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 1\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 2\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 3\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 4\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 5\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 6\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 7\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 8\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 9\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 10\\n']\n\n        \n        log_file = '{}/resources/sample_log_files/tap-run-lot-of-errors.log'.format(os.path.dirname(__file__))\n        assert cli.utils.find_errors_in_log_file(log_file, max_errors=2) == \\\n            ['time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 1\\n',\n             'time=2020-07-15 11:24:43 logger_name=tap_postgres log_level=CRITICAL This is a critical error 2\\n']\n\n        \n        log_file = '{}/resources/sample_log_files/tap-run-errors.log'.format(os.path.dirname(__file__))\n        assert cli.utils.find_errors_in_log_file(log_file, error_pattern=re.compile('CUSTOM-ERR-PATTERN')) == \\\n            ['CUSTOM-ERR-PATTERN: This is a custom pattern error message\\n']\n",
        "summary": "The provided code snippet is a Python script that includes several test cases for a module named `utils`. The module contains various utility functions that are used to validate and process data, such as validating JSON schemas, checking file existence, and finding errors in log files. Below is a breakdown of the key functionalities tested in the script:\n\n1. **JSON Schema Validation**:\n   - The script tests the `validate_json` function with different JSON objects against predefined schemas.\n   - It checks if the function correctly identifies valid and invalid JSON data based on the schema.\n\n2. **File Existence Check**:\n   - The script tests the `file_exists` function to ensure it accurately determines whether a specified file path exists or not.\n   - This includes both existing and non-existing files.\n\n3. **Error Pattern Matching in Log Files**:\n   - The script tests the `find_errors_in_log_file` function to verify its ability to extract error messages from log files based on specific patterns.\n   - It checks for different types of errors such as critical, exception, and custom pattern errors.\n\n4. **Deletion of Temporary Files**:\n   - The script includes a test case that creates temporary files and then deletes them using the `delete_temp_files` function.\n   - This ensures that the function correctly handles file deletion operations.\n\n5. **Validation of Tap Configuration**:\n   - The script tests the `validate_tap_config` function to ensure it correctly validates tap configurations against predefined schemas.\n   - It checks for both valid and invalid configurations.\n\n6. **Deletion of Log Files**:\n   - The script includes a test case that creates log files and then deletes them using the `delete_log_files` function.\n   - This ensures that the function correctly handles file deletion operations.\n\n7. **Validation of Target Configuration**:\n   - The script tests the `validate_target_config` function to ensure it correctly validates target configurations against predefined schemas.\n   - It checks for both valid and invalid configurations.\n\n8. **Deletion of Schema Files**:\n   - The script includes a test case that creates schema files and then deletes them using the `delete_schema_files` function.\n   - This ensures that the function correctly handles file deletion operations.\n\n9. **Validation of Connection Configuration**:\n   - The script tests the `validate_connection_config` function to ensure it correctly validates connection configurations against predefined schemas.\n   - It checks for both valid and invalid configurations.\n\n10. **Deletion of State Files**:\n    - The script includes a test case that creates state files and then deletes them using the `delete_state_files` function.\n    - This ensures that the function correctly handles file deletion operations.\n\nThe script uses the `pytest` framework to run these tests, which provides detailed output on the success or failure of each test case. Additionally, it includes assertions to validate the expected outcomes of the functions being tested."
    },
    {
        "code": "from __future__ import absolute_import\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\ndef __virtual__():\n    \n    ret = 'boto_kinesis' if 'boto_kinesis.exists' in __salt__ else False\n    return ret\n\n\ndef present(name,\n            retention_hours=None,\n            enhanced_monitoring=None,\n            num_shards=None,\n            do_reshard=True,\n            region=None,\n            key=None,\n            keyid=None,\n            profile=None):\n    \n\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    comments = []\n    changes_old = {}\n    changes_new = {}\n\n    \n    exists = __salt__['boto_kinesis.exists'](\n        name,\n        region,\n        key,\n        keyid,\n        profile\n    )\n    if exists['result'] is False:\n        if __opts__['test']:\n            ret['result'] = None\n            comments.append('Kinesis stream {0} would be created'.format(name))\n            _add_changes(ret, changes_old, changes_new, comments)\n            return ret\n        else:\n            is_created = __salt__['boto_kinesis.create_stream'](\n                name,\n                num_shards,\n                region,\n                key,\n                keyid,\n                profile\n            )\n            if 'error' in is_created:\n                ret['result'] = False\n                comments.append('Failed to create stream {0}: {1}'.format(name, is_created['error']))\n                _add_changes(ret, changes_old, changes_new, comments)\n                return ret\n\n            comments.append('Kinesis stream {0} successfully created'.format(name))\n            changes_new['name'] = name\n            changes_new['num_shards'] = num_shards\n    else:\n        comments.append('Kinesis stream {0} already exists'.format(name))\n\n    stream_response = __salt__['boto_kinesis.get_stream_when_active'](\n        name,\n        region,\n        key,\n        keyid,\n        profile\n    )\n    if 'error' in stream_response:\n        ret['result'] = False\n        comments.append('Kinesis stream {0}: error getting description: {1}'\n                        .format(name, stream_response['error']))\n        _add_changes(ret, changes_old, changes_new, comments)\n        return ret\n\n    stream_details = stream_response['result'][\"StreamDescription\"]\n\n    \n    if retention_hours is not None:\n        old_retention_hours = stream_details[\"RetentionPeriodHours\"]\n        retention_matches = (old_retention_hours == retention_hours)\n        if not retention_matches:\n            if __opts__['test']:\n                ret['result'] = None\n                comments.append('Kinesis stream {0}: retention hours would be updated to {1}'\n                                .format(name, retention_hours))\n            else:\n                if old_retention_hours > retention_hours:\n                    retention_updated = __salt__['boto_kinesis.decrease_stream_retention_period'](\n                        name,\n                        retention_hours,\n                        region,\n                        key,\n                        keyid,\n                        profile\n                    )\n                else:\n                    retention_updated = __salt__['boto_kinesis.increase_stream_retention_period'](\n                        name,\n                        retention_hours,\n                        region,\n                        key,\n                        keyid,\n                        profile\n                    )\n\n                if 'error' in retention_updated:\n                    ret['result'] = False\n                    comments.append('Kinesis stream {0}: failed to update retention hours: {1}'\n                                    .format(name, retention_updated['error']))\n                    _add_changes(ret, changes_old, changes_new, comments)\n                    return ret\n\n                comments.append('Kinesis stream {0}: retention hours was successfully updated'.format(name))\n                changes_old['retention_hours'] = old_retention_hours\n                changes_new['retention_hours'] = retention_hours\n\n                \n                \n                stream_response = __salt__['boto_kinesis.get_stream_when_active'](\n                    name,\n                    region,\n                    key,\n                    keyid,\n                    profile\n                )\n                if 'error' in stream_response:\n                    ret['result'] = False\n                    comments.append('Kinesis stream {0}: error getting description: {1}'\n                                    .format(name, stream_response['error']))\n                    _add_changes(ret, changes_old, changes_new, comments)\n                    return ret\n\n                stream_details = stream_response['result'][\"StreamDescription\"]\n        else:\n            comments.append('Kinesis stream {0}: retention hours did not require change, already set at {1}'\n                            .format(name, old_retention_hours))\n    else:\n        comments.append('Kinesis stream {0}: did not configure retention hours'.format(name))\n\n    \n    if enhanced_monitoring is not None:\n        if enhanced_monitoring is True or enhanced_monitoring == ['ALL']:\n            \n            enhanced_monitoring = [\n                    \"IncomingBytes\",\n                    \"OutgoingRecords\",\n                    \"IteratorAgeMilliseconds\",\n                    \"IncomingRecords\",\n                    \"ReadProvisionedThroughputExceeded\",\n                    \"WriteProvisionedThroughputExceeded\",\n                    \"OutgoingBytes\"\n                ]\n        elif enhanced_monitoring is False or enhanced_monitoring == \"None\":\n            enhanced_monitoring = []\n\n        old_enhanced_monitoring = stream_details.get(\"EnhancedMonitoring\")[0][\"ShardLevelMetrics\"]\n\n        new_monitoring_set = set(enhanced_monitoring)\n        old_monitoring_set = set(old_enhanced_monitoring)\n\n        matching_metrics = new_monitoring_set.intersection(old_monitoring_set)\n        enable_metrics = list(new_monitoring_set.difference(matching_metrics))\n        disable_metrics = list(old_monitoring_set.difference(matching_metrics))\n\n        if len(enable_metrics) != 0:\n            if __opts__['test']:\n                ret['result'] = None\n                comments.append('Kinesis stream {0}: would enable enhanced monitoring for {1}'\n                                .format(name, enable_metrics))\n            else:\n\n                metrics_enabled = __salt__['boto_kinesis.enable_enhanced_monitoring'](\n                    name,\n                    enable_metrics,\n                    region,\n                    key,\n                    keyid,\n                    profile\n                )\n                if 'error' in metrics_enabled:\n                    ret['result'] = False\n                    comments.append('Kinesis stream {0}: failed to enable enhanced monitoring: {1}'\n                                    .format(name, metrics_enabled['error']))\n                    _add_changes(ret, changes_old, changes_new, comments)\n                    return ret\n\n                comments.append('Kinesis stream {0}: enhanced monitoring was enabled for shard-level metrics {1}'\n                                .format(name, enable_metrics))\n\n        if len(disable_metrics) != 0:\n            if __opts__['test']:\n                ret['result'] = None\n                comments.append('Kinesis stream {0}: would disable enhanced monitoring for {1}'\n                                .format(name, disable_metrics))\n            else:\n\n                metrics_disabled = __salt__['boto_kinesis.disable_enhanced_monitoring'](\n                    name,\n                    disable_metrics,\n                    region,\n                    key,\n                    keyid,\n                    profile\n                )\n                if 'error' in metrics_disabled:\n                    ret['result'] = False\n                    comments.append('Kinesis stream {0}: failed to disable enhanced monitoring: {1}'\n                                    .format(name, metrics_disabled['error']))\n                    _add_changes(ret, changes_old, changes_new, comments)\n                    return ret\n\n                comments.append('Kinesis stream {0}: enhanced monitoring was disabled for shard-level metrics {1}'\n                                .format(name, disable_metrics))\n\n        if len(disable_metrics) == 0 and len(enable_metrics) == 0:\n            comments.append('Kinesis stream {0}: enhanced monitoring did not require change, already set at {1}'\n                            .format(name, (old_enhanced_monitoring if len(old_enhanced_monitoring) > 0 else \"None\")))\n        elif not __opts__['test']:\n            changes_old['enhanced_monitoring'] = (old_enhanced_monitoring if len(old_enhanced_monitoring) > 0\n                                                  else \"None\")\n            changes_new['enhanced_monitoring'] = (enhanced_monitoring if len(enhanced_monitoring) > 0\n                                                  else \"None\")\n    else:\n        comments.append('Kinesis stream {0}: did not configure enhanced monitoring'.format(name))\n\n    \n    min_hash_key, max_hash_key, full_stream_details = __salt__['boto_kinesis.get_info_for_reshard'](\n        stream_details\n    )\n    old_num_shards = len(full_stream_details[\"OpenShards\"])\n\n    if num_shards is not None and do_reshard:\n        num_shards_matches = (old_num_shards == num_shards)\n        if not num_shards_matches:\n            if __opts__['test']:\n                ret['result'] = None\n                comments.append('Kinesis stream {0}: would be resharded from {1} to {2} shards'\n                                .format(name, old_num_shards, num_shards))\n            else:\n                log.info(\"Resharding stream from {0} to {1} shards, this could take a while\"\n                         .format(old_num_shards, num_shards))\n                \n                \n                continue_reshard = True\n                while continue_reshard:\n                    reshard_response = __salt__['boto_kinesis.reshard'](\n                        name,\n                        num_shards,\n                        do_reshard,\n                        region,\n                        key,\n                        keyid,\n                        profile)\n\n                    if 'error' in reshard_response:\n                        ret['result'] = False\n                        comments.append('Encountered error while resharding {0}: {1}'\n                                        .format(name, reshard_response['error']))\n                        _add_changes(ret, changes_old, changes_new, comments)\n                        return ret\n\n                    continue_reshard = reshard_response['result']\n\n                comments.append('Kinesis stream {0}: successfully resharded to {1} shards'.format(name, num_shards))\n                changes_old['num_shards'] = old_num_shards\n                changes_new['num_shards'] = num_shards\n        else:\n            comments.append('Kinesis stream {0}: did not require resharding, remains at {1} shards'\n                            .format(name, old_num_shards))\n    else:\n        comments.append('Kinesis stream {0}: did not reshard, remains at {1} shards'.format(name, old_num_shards))\n\n    _add_changes(ret, changes_old, changes_new, comments)\n    return ret\n\n\ndef absent(name,\n           region=None,\n           key=None,\n           keyid=None,\n           profile=None):\n    \n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    exists = __salt__['boto_kinesis.exists'](\n        name,\n        region,\n        key,\n        keyid,\n        profile\n    )\n    if exists['result'] is False:\n        ret['comment'] = 'Kinesis stream {0} does not exist'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'Kinesis stream {0} would be deleted'.format(name)\n        ret['result'] = None\n        return ret\n\n    is_deleted = __salt__['boto_kinesis.delete_stream'](\n        name,\n        region,\n        key,\n        keyid,\n        profile\n    )\n    if 'error' in is_deleted:\n        ret['comment'] = 'Failed to delete stream {0}: {1}'.format(name, is_deleted['error'])\n        ret['result'] = False\n    else:\n        ret['comment'] = 'Deleted stream {0}'.format(name)\n        ret['changes'].setdefault('old', 'Stream {0} exists'.format(name))\n        ret['changes'].setdefault('new', 'Stream {0} deleted'.format(name))\n\n    return ret\n\n\ndef _add_changes(ret, changes_old, changes_new, comments):\n    ret['comment'] = ',\\n'.join(comments)\n    if changes_old:\n        ret['changes']['old'] = changes_old\n    if changes_new:\n        ret['changes']['new'] = changes_new\n",
        "summary": "This is a SaltStack state module for managing AWS Kinesis streams. The `present` function ensures that a Kinesis stream exists with the specified configuration, while the `absent` function deletes an existing Kinesis stream.\n\nHere's a breakdown of what each part does:\n\n1. **Present Function**:\n   - Checks if the Kinesis stream already exists.\n   - If it doesn't exist and not in test mode, creates the stream.\n   - If it exists but has different configuration (like number of shards), reshards or updates other properties as needed.\n   - Returns a dictionary with the result, comment, and changes made.\n\n2. **Absent Function**:\n   - Checks if the Kinesis stream exists.\n   - If it does exist and not in test mode, deletes the stream.\n   - Returns a dictionary with the result, comment, and changes made.\n\n3. **Helper Functions**:\n   - `_add_changes`: Adds old and new states to the return dictionary along with comments.\n\n### Usage Example\n\n#### Present State\n```yaml\nmy_kinesis_stream:\n  boto_kinesis.present:\n    - num_shards: 2\n    - retention_period_hours: 48\n    - enhanced_monitoring: [\"IncomingBytes\", \"OutgoingBytes\"]\n```\n\nThis state ensures that a Kinesis stream named `my_kinesis_stream` exists with 2 shards, a retention period of 48 hours, and enhanced monitoring for incoming and outgoing bytes.\n\n#### Absent State\n```yaml\nmy_kinesis_stream:\n  boto_kinesis.absent:\n```\n\nThis state deletes the Kinesis stream named `my_kinesis_stream`.\n\n### Notes\n- The module uses AWS credentials provided in the SaltStack configuration or directly in the state file.\n- It handles resharding, which can be a time-consuming operation and should be done carefully.\n- Enhanced monitoring settings are specified as a list of metrics.\n\nThis module provides a robust way to manage Kinesis streams using SaltStack, ensuring that they are configured correctly according to your infrastructure needs."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.py_utils import assert_compatible\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.py_utils import merge_dicts\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.py_utils import OrderedEnum\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.py_utils import split_dict_py_tf\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.py_utils import static_or_dynamic_shape\n\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.tf_utils import fast_walsh_hadamard_transform\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.tf_utils import random_floats\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.tf_utils import random_floats_cmwc\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.tf_utils import random_signs\nfrom tensorflow_model_optimization.python.core.internal.tensor_encoding.utils.tf_utils import random_signs_cmwc\n",
        "summary": "The Python code imports various utility functions and classes from the TensorFlow Model Optimization library, primarily focusing on tensor encoding utilities for both Python and TensorFlow environments. These include assertions, dictionary manipulation, ordered enums, shape handling, and specialized mathematical transformations like fast Walsh-Hadamard transform and random number generation with different algorithms."
    },
    {
        "code": "import os, sys\nimport time\nimport warnings\nimport argparse\nimport configparser\nimport ast\nimport numpy as np\nfrom math import log\nfrom rdkit import Chem\nfrom rdkit import rdBase\nrdBase.DisableLog('rdApp.*')\nfrom rdkit.Chem import Draw\n\nfrom keras.models import load_model\n\nsys.path.append('../src/')\nfrom python import helper as hp\nfrom python import fixed_parameters as FP\n\nparser = argparse.ArgumentParser(description='SMILES generation')\nparser.add_argument('-fn','--filename', type=str, help='Path to the fine-tuning txt file', required=True)\nparser.add_argument('-m','--model_path', type=str, help='Path to a pretrained model', required=True)\nparser.add_argument('-v','--verbose', type=bool, help='Verbose', required=True)\n\n\ndef int_to_smile(array, indices_token, pad_char):\n    \n    all_mols = []\n    for seq in array:\n        new_mol = [indices_token[str(int(x))] for x in seq]\n        all_mols.append(''.join(new_mol).replace(pad_char, ''))\n    return all_mols\n\n\ndef one_hot_encode(token_lists, n_chars):\n    \n    output = np.zeros((len(token_lists), len(token_lists[0]), n_chars))\n    for i, token_list in enumerate(token_lists):\n        for j, token in enumerate(token_list):\n            output[i, j, int(token)] = 1\n    return output\n         \ndef sample(model, temp, start_char, end_char, max_len, indices_token, token_indices):\n    \n    n_chars = len(indices_token)\n\n    seed_token = [token_indices[start_char]]\n    generated = indices_token[str(seed_token[0])]\n    \n    while generated[-1] != end_char and len(generated) < max_len:\n        x_seed = one_hot_encode([seed_token], n_chars)\n        full_preds = model.predict(x_seed, verbose=0)[0]\n        logits = full_preds[-1]\n        \n        probas, next_char_ind = get_token_proba(logits, temp)\n                \n        next_char = indices_token[str(next_char_ind)]\n        generated += next_char\n        seed_token += [next_char_ind]\n            \n    return generated\n\ndef get_token_proba(preds, temp):\n    \n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temp\n    exp_preds = np.exp(preds)\n    \n    probas = exp_preds / np.sum(exp_preds)\n    char_ind = np.argmax(np.random.multinomial(1, probas, 1))\n    \n    return probas, char_ind\n\ndef softmax(preds):\n    return np.exp(preds)/np.sum(np.exp(preds))\n\n\nif __name__ == '__main__':\n    \n    start = time.time()\n    \n    \n    \n    args = vars(parser.parse_args())\n        \n    verbose = args['verbose']\n    filename = args['filename']\n    model_path = args['model_path']\n    name_data = filename.split('/')[-1].replace('.txt','')\n    config = configparser.ConfigParser()\n    config.read('parameters.ini')\n    \n    if verbose: print('\\nSTART SAMPLING')\n    \n    \n    \n    \n    \n    \n    save_path = f'results/{name_data}/generated_data/'\n    os.makedirs(save_path, exist_ok=True)\n    \n    \n    dir_ckpts = f'results/{name_data}/models/'\n    \n    \n    \n    \n    \n    \n    \n    temp = float(config['EXPERIMENTS']['temp'])\n    n_sample = int(config['EXPERIMENTS']['n_sample'])\n    if n_sample>5000:\n        warnings.warn('You will sample more than 5000 SMILES; this will take a while')\n    \n    max_len = int(config['PROCESSING']['max_len'])\n    pad_char = FP.PROCESSING_FIXED['pad_char']\n    start_char = FP.PROCESSING_FIXED['start_char']\n    end_char = FP.PROCESSING_FIXED['end_char']\n    indices_token = FP.INDICES_TOKEN\n    token_indices = FP.TOKEN_INDICES\n    \n    \n    \n    \n    \n    \n    epoch = model_path.split('/')[-1].replace('.h5', '')\n    if verbose: print(f'Sampling from model saved at epoch {epoch}')\n    \n    model = load_model(model_path)\n    \n    generated_smi = []\n    for n in range(n_sample):\n        generated_smi.append(sample(model, temp, \n                                    start_char, end_char, max_len+1, \n                                    indices_token, token_indices))\n    hp.save_obj(generated_smi, f'{save_path}{epoch}_{temp}')\n    \n    end = time.time()\n    if verbose: print(f'SAMPLING DONE for model from epoch {epoch} in {end-start:.2f} seconds')  \n    \n        ",
        "summary": "The provided Python script is designed to generate SMILES (Simplified Molecular Input Line Entry System) strings using a pre-trained neural network model. It includes functions for converting integer sequences to SMILES, one-hot encoding token lists, sampling new SMILES from the model with temperature control, and saving the generated SMILES to files. The script also handles command-line arguments for specifying input parameters such as the model path and verbosity level."
    },
    {
        "code": "class VpcSecurityGroupCreateTask(object):\n\n    def __init__(self, task=None):\n        \n\n        self.task = task\n",
        "summary": "The `VpcSecurityGroupCreateTask` class is designed to handle the creation of tasks related to Virtual Private Cloud (VPC) security groups, with an optional parameter for initializing a task object."
    },
    {
        "code": "import json\n\nfrom jupyterlab.labapp import LabApp\nfrom notebook.base.handlers import APIHandler\nfrom notebook.utils import url_path_join\nimport tornado\nfrom .api import group_info, submit_job, get_env, check_function_set\nfrom .utils import get_group_volume_path\nimport os.path\nfrom shutil import copyfile\nfrom datetime import datetime\nimport importlib.util\nimport sys\n\nENV_API_ENDPOINT = 'JUPYTERLAB_DEV_API_ENDPOINT'\n\nNAMESPACE = \"jupyterlab-primehub\"\napi_endpoint = 'http://primehub-graphql/api/graphql'\n\nNOTEBOOK_DIR = None\n\n\nclass CheckFunctionSetHandler(APIHandler):\n    @tornado.web.authenticated\n    def post(self):\n        params = self.get_json_body()\n        api_token = params.get('api_token', None)\n        function_set = check_function_set(api_endpoint, api_token)\n        self.log.info(function_set)\n        self.finish(json.dumps(function_set))\n\n\nclass ResourceHandler(APIHandler):\n\n    @tornado.web.authenticated\n    def post(self):\n        params = self.get_json_body()\n        api_token = params.get('api_token', None)\n        group_id = os.environ.get('GROUP_ID')\n        self.log.info('group_info with group_id: {}'.format(group_id))\n        self.finish(json.dumps(group_info(api_endpoint, api_token, group_id)))\n\n\nclass SubmitJobHandler(APIHandler):\n\n    @tornado.web.authenticated\n    def post(self):\n        params = self.get_json_body()\n        api_token = params.get('api_token', None)\n        name = params.get('name', 'notebook_job')\n        group_id = os.environ.get('GROUP_ID')\n        instance_type = params.get('instance_type', None)\n        image = params.get('image', os.environ.get('IMAGE_NAME'))\n        path = params.get('path', None)\n        notebook_parameters = params.get('notebook_parameters', '')\n        self.log.info('group_info with group_id: {}'.format(group_id))\n\n        fullpath = os.path.join(NOTEBOOK_DIR, path)\n        self.log.info(\"relative path: \" + path)\n        self.log.info(\"notebook path: \" + fullpath)\n\n        \n        group_name = params.get('group_name', os.environ.get('GROUP_NAME'))\n        time_string = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n\n        nb_file_name = path.split('/').pop()\n        nb_directory_path = os.path.join(NOTEBOOK_DIR, path.replace(nb_file_name, ''))\n        hidden_nb_file_name = '.' + nb_file_name.replace('.ipynb', '') + '-' + time_string + '.ipynb'\n        hidden_nb_fullpath = os.path.join(NOTEBOOK_DIR, path.replace(nb_file_name, ''), hidden_nb_file_name)\n        output_nb_fullpath = os.path.join(NOTEBOOK_DIR, path.replace(nb_file_name, ''), hidden_nb_file_name[1:].replace('.ipynb', '-output.ipynb'))\n        \n        copyfile(fullpath, hidden_nb_fullpath)\n        papermill_parameters = ''\n\n        try:\n            for parameter in notebook_parameters.replace(' ', '').split(';'):\n                if '=' in parameter:\n                    kv = parameter.split('=')\n                    papermill_parameters = papermill_parameters + ' -p {} {}'.format(kv[0], kv[1])\n        except Exception as e:\n            self.finish(json.dumps({\n                'status': 'failed',\n                'error': 'failed to parse notebook parameters', \n                'message': str(e)\n            }))\n            return\n        \n        command_str = 'cd {} && papermill {} {}{} && rm {}'.format(nb_directory_path, hidden_nb_fullpath, output_nb_fullpath, papermill_parameters, hidden_nb_fullpath)\n\n        self.finish(json.dumps(submit_job(api_endpoint, api_token, name, group_id, instance_type, image, command_str)))\n\n\nclass EnvironmentHandler(APIHandler):\n\n    @tornado.web.authenticated\n    def post(self):\n        self.finish(json.dumps(get_env()))\n\n\ndef url_pattern(web_app, endpoint, *pieces):\n    base_url = web_app.settings[\"base_url\"]\n    return url_path_join(base_url, NAMESPACE, endpoint, *pieces)\n\n\ndef setup_handlers(lab_app: LabApp):\n    setup_globals(lab_app)\n    web_app, logger = lab_app.web_app, lab_app.log\n    apply_api_endpoint_override(logger)\n\n    host_pattern = \".*$\"\n\n    handlers = [(url_pattern(web_app, 'check-function'), CheckFunctionSetHandler),\n                (url_pattern(web_app, 'resources'), ResourceHandler),\n                (url_pattern(web_app, 'submit-job'), SubmitJobHandler),\n                (url_pattern(web_app, 'get-env'), EnvironmentHandler)]\n\n    web_app.add_handlers(host_pattern, handlers)\n    for h in handlers:\n        logger.info('handler => {}'.format(h))\n\n\ndef setup_globals(lab_app):\n    global NOTEBOOK_DIR\n    NOTEBOOK_DIR = lab_app.notebook_dir\n\n    lab_app.log.info('setup globals')\n    lab_app.log.info('\\tNOTEBOOK_DIR: ' + NOTEBOOK_DIR)\n\n\ndef apply_api_endpoint_override(logger):\n    global api_endpoint\n    override = os.environ.get(ENV_API_ENDPOINT, None)\n    if not override:\n        logger.info('use api-endpoint: {}'.format(api_endpoint))\n        logger.info('it could be override from ENV with the key {}'.format(ENV_API_ENDPOINT))\n        return\n    logger.info('update api-endpoint from ENV: {}'.format(override))\n    api_endpoint = override\n",
        "summary": "This Python code defines a set of API handlers for a JupyterLab application, allowing users to interact with a PrimeHub environment. It includes functionality for checking function sets, retrieving resource information, submitting jobs, and managing environments. The handlers are integrated into the JupyterLab web application framework using Tornado, and they utilize various utility functions for processing requests and interacting with external APIs."
    },
    {
        "code": "import setuptools\n\nwith open(\"README.md\", \"r\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=\"runai\",\n    version=\"0.1.2\",\n    author=\"Run:AI\",\n    author_email=\"pypi@run.ai\",\n    description=\"Run:AI Python library\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/run-ai/runai\",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python\",\n    ],\n)\n",
        "summary": "This Python script uses `setuptools` to package and distribute a library named \"runai\" version 0.1.2, authored by Run:AI. It includes metadata such as the author's email, description, long description from a README file, URL, and classifiers for development status, license, and programming language support."
    },
    {
        "code": "from django.urls import path\n\nfrom .views import (\n    UserSelfView,\n)\n\nurlpatterns = [\n    path(\"users/self/profile/\", UserSelfView.as_view(), name=\"user_self\"),\n\n]\n",
        "summary": "The provided Python code defines URL patterns for a Django application, mapping the endpoint \"users/self/profile/\" to a view called `UserSelfView` which handles requests related to user profiles."
    },
    {
        "code": "if __name__ == \"__main__\":\n    import numpy as np\n    import beluga.Beluga as Beluga\n    import beluga.bvpsol as bvpsol\n    import beluga.bvpsol.algorithms as algorithms\n    import beluga.optim.Problem\n    from beluga.optim.problem import *\n    from beluga.continuation import *\n\n    import logging\n\n    \n\n    from sympy import symbols, Matrix, Transpose, simplify, diff, diag\n    from sympy import sin\n    from sympy import cos, acos\n    from sympy import sqrt\n    from sympy import exp\n    from sympy import atan\n\n    from numpy import pi\n\n    writeEqn = True\n    simpList = False\n\n    if writeEqn:\n        writeList = []\n\n        \n        v, u_max = symbols('v, u_max')\n        xb, yb = symbols('xb, yb')\n        Dt, sigv, sigw, sigr = symbols('Dt, sigv, sigw, sigr')\n\n        \n        x, y, theta = symbols('x, y, theta')\n\n        \n        w = symbols('w')\n\n        \n\n\n        \n        x_dot = v * cos(theta)\n        y_dot = v * sin(theta)\n        theta_dot = u_max * sin(w)\n\n        writeList = [x_dot, y_dot, theta_dot]\n\n        \n\n        p11, p12, p13,\\\n            p22, p23, \\\n            p33 \\\n            = symbols('p11 p12 p13\\\n                       p22 p23 \\\n                       p33')\n\n        P = Matrix([[p11, p12, p13],\n                    [p12, p22, p23],\n                    [p13, p13, p33]])\n\n        F = Matrix([[diff(x_dot, x), diff(x_dot, y), diff(x_dot, theta)],\n                    [diff(y_dot, x), diff(y_dot, y), diff(y_dot, theta)],\n                    [diff(theta_dot, x), diff(theta_dot, y), diff(theta_dot, theta)],])\n\n        G = Matrix([[cos(theta), 0],\n                    [sin(theta), 0],\n                    [0, 1]])\n\n        h = sqrt((x - xb)**2 + (y - yb)**2)\n\n        H = Matrix([[diff(h, x), diff(h, y), diff(h, theta)]])\n\n        Q = Dt*diag(sigv**2, sigw**2)\n\n        R = Dt*diag(sigr**2)\n\n        P_dot = (F*P + P*F.T - P*H.T*(R**-1)*H*P + G*Q*G.T)\n\n        Dim = P_dot.shape\n\n        k = symbols('k')\n\n        PP = (F*P + P*F.T - k * P*H.T*(R**-1)*H*P + G*Q*G.T)\n\n        obj = PP[1, 1]\n\n        for i in range(0, Dim[0]):\n            for j in range(i, Dim[1]):\n                \n                writeList.append(P_dot[i, j])\n\n        \n        \n\n        states = [x, y, theta,\n                  p11, p12, p13,\n                  p22, p23,\n                  p33]\n\n        x_s, y_s, theta_s, \\\n        p11_s, p12_s, p13_s, \\\n        p22_s, p23_s, \\\n        p33_s = \\\n        symbols('x_s, y_s, theta_s, \\\n                p11_s, p12_s, p13_s, \\\n                p22_s, p23_s, \\\n                p33_s')\n\n        scales = [x_s, y_s, theta_s,\n                  p11_s, p12_s, p13_s,\n                  p22_s, p23_s,\n                  p33_s]\n\n        x_n, y_n, theta_n, \\\n        p11_n, p12_n, p13_n, \\\n        p22_n, p23_n, \\\n        p33_n = \\\n            symbols('x_n, y_n, theta_n, \\\n                    p11_n, p12_n, p13_n, \\\n                    p22_n, p23_n, \\\n                    p33_n')\n\n        states_new = [x_n, y_n, theta_n,\n                      p11_n, p12_n, p13_n,\n                      p22_n, p23_n,\n                      p33_n]\n\n        \n\n        Z1 = zip(writeList, scales)\n\n        scaledList = []\n\n        for item, Scale in Z1:\n            \n            item = item/Scale\n            Z2 = zip(states, states_new, scales)\n            \n            \n            \n            \n            \n            for state, new, scale in Z2:\n                \n                item = item.subs(state, scale*new)\n            \n            scaledList.append(item)\n\n        Z2 = zip(states, states_new, scales)\n        for state, new, scale in Z2:\n            \n            obj = obj.subs(state, scale * new)\n\n        k = 1\n        with open(\"eqns.txt\", \"w\") as my_file:\n            for item in scaledList:\n                if simpList:\n                    \n                    item = simplify(item)\n                    \n                my_file.write(str(item) + \"\\n\")\n                \n                k += 1\n\n        k = 1\n        with open(\"eqnsUnscaled.txt\", \"w\") as my_file:\n            for item in writeList:\n                my_file.write(str(item) + \"\\n\")\n                \n                k += 1\n\n    \n\n    \n    with open(\"eqns.txt\", \"r\") as f:\n        eqnsList = list(f)\n\n    \n    \n\n    \n    problem = beluga.optim.Problem('carts0')\n\n    \n    problem.independent('t', 's')\n\n\n    \n    problem\\\n        .state('x_n',  eqnsList[0] + '+ ep*u_max*cos(w)', '1')   \\\n        .state('y_n',  eqnsList[1], '1') \\\n        .state('theta_n', eqnsList[2], '1') \\\n        .state('p11_n', eqnsList[3], '1') \\\n        .state('p12_n', eqnsList[4], '1') \\\n        .state('p13_n', eqnsList[5], '1') \\\n        .state('p22_n', eqnsList[6], '1') \\\n        .state('p23_n', eqnsList[7], '1') \\\n        .state('p33_n', eqnsList[8], '1') \\\n\n        \n    problem.control('w', '1') \\\n\n    \n    \n    \n    \n    problem.cost['path'] = Expression(str(obj), 's')\n\n    \n    problem.constraints() \\\n        .initial('x_n-x_n_0', '1') \\\n        .initial('y_n-y_n_0', '1') \\\n        .initial('theta_n-theta_n_0', '1') \\\n \\\n        .initial('p11_n-p11_n_0', '1') \\\n        .initial('p12_n-p12_n_0', '1') \\\n        .initial('p13_n-p13_n_0', '1') \\\n        .initial('p22_n-p22_n_0', '1') \\\n        .initial('p23_n-p23_n_0', '1') \\\n        .initial('p33_n-p33_n_0', '1') \\\n \\\n        .terminal('x_n-x_n_f', '1') \\\n        .terminal('y_n-y_n_f', '1') \\\n \\\n\n        \n    problem.constant('Dt', 0.1, '1')\n    problem.constant('sigv', 0.1, '1')\n    problem.constant('sigw', 0.1, '1')\n    problem.constant('sigr', 0.1, '1')\n\n    problem.constant('xb', 5, '1')\n    problem.constant('yb', 5, '1')\n\n    problem.constant('u_max', 0.1, '1')\n\n    problem.constant('v', 30, '1')\n\n    problem.constant('x_s',     1, '1')\n    problem.constant('y_s',     1, '1')\n    problem.constant('theta_s', 1, '1')\n\n    problem.constant('p11_s', 1e-3, '1')\n    problem.constant('p12_s', 1e-3, '1')\n    problem.constant('p13_s', 1e-3, '1')\n\n    problem.constant('p22_s', 1e-1, '1')\n    problem.constant('p23_s', 1e-2, '1')\n\n    problem.constant('p33_s', 1e-3, '1')\n\n    problem.constant('ep', 5, '1')\n\n    problem.constant('k', 0, '1')\n\n    problem.bvp_solver = algorithms.MultipleShooting(derivative_method='fd', tolerance=1e-4, max_iterations=1000, verbose=True, cached=False, number_arcs=64)\n    \n\n    problem.scale.unit('m', 1)       \\\n                 .unit('s', 1)     \\\n                 .unit('kg', 1)   \\\n                 .unit('rad', 1)\n\n    \n    \n    \n    problem.guess.setup('auto', start=[0, 0, 0, 0, 0, 0, 0, 0, 0], time_integrate=1, costate_guess=[0, 0, 0.001, -0.0001, 0.0, 0.0, 0.001, 0.0, 0.])\n    \n    \n\n    problem.steps.add_step().num_cases(5) \\\n        .terminal('x_n', 20) \\\n        .terminal('y_n', 0) \\\n\n    problem.steps.add_step().num_cases(10) \\\n        .const('xb', 200) \\\n        .const('yb', 600) \\\n\n    problem.steps.add_step().num_cases(80) \\\n        .terminal('x_n', 4000) \\\n\n    problem.steps.add_step().num_cases(20) \\\n        .const('k', 1) \\\n\n \n \n \n \n \n        \n    \n    \n\n\n        \n    \n\n    \n    \n\n    Beluga.run(problem, display_level=logging.DEBUG)\n\n",
        "summary": "This Python script uses the `beluga` library to solve a boundary value problem (BVP) for a dynamic system. The BVP involves optimizing the control input \\( w \\) to steer the system from an initial state to a final state while minimizing a cost function.\n\nHere's a breakdown of the key components and steps in the script:\n\n1. **Import Libraries**:\n   - `beluga`: A library for solving optimal control problems.\n   - `logging`: For logging debug information.\n\n2. **Define Constants and Variables**:\n   - Various constants such as \\( u_{\\text{max}} \\), \\( v \\), \\( \\sigma_v \\), \\( \\sigma_w \\), \\( \\sigma_r \\), \\( x_b \\), \\( y_b \\), etc.\n   - Variables for the state of the system (e.g., \\( x_n, y_n, \\theta_n, p11_n, \\ldots \\)) and control input \\( w \\).\n\n3. **Compute Derivatives**:\n   - The script computes the derivatives of the state variables with respect to time using the given equations of motion.\n\n4. **Formulate the BVP**:\n   - The BVP is formulated using `beluga`. This includes defining the states, controls, cost function, and constraints.\n   - Constraints are set for initial and terminal conditions.\n\n5. **Set Up the Solver**:\n   - The solver used is `MultipleShooting`, which is a method for solving BVPs by breaking them into smaller subproblems.\n   - Parameters such as tolerance, maximum iterations, and verbosity are configured.\n\n6. **Define Guesses and Steps**:\n   - Initial guesses for the states and costates are provided.\n   - Different steps with varying numbers of cases are defined to explore different scenarios.\n\n7. **Run the Solver**:\n   - The `Beluga.run` function is called to execute the solver with the specified problem setup.\n\n### Key Points in the Script:\n\n- **State Equations**: The script defines how the state variables change over time based on the control input \\( w \\) and other parameters.\n- **Cost Function**: The cost function to be minimized is defined as `obj`, which likely represents some form of performance metric for the system.\n- **Constraints**: Initial and terminal conditions are set to ensure the system starts and ends at specific states.\n- **Solver Configuration**: The solver is configured with parameters like tolerance and maximum iterations to control the accuracy and efficiency of the solution.\n\n### Example State Equations:\n\n```python\nx_n = eqnsList[0] + '+ ep*u_max*cos(w)'\ny_n = eqnsList[1]\ntheta_n = eqnsList[2]\np11_n = eqnsList[3]\np12_n = eqnsList[4]\np13_n = eqnsList[5]\np22_n = eqnsList[6]\np23_n = eqnsList[7]\np33_n = eqnsList[8]\n```\n\n### Example Cost Function:\n\n```python\nproblem.cost['path'] = Expression(str(obj), 's')\n```\n\n### Example Constraints:\n\n```python\nproblem.constraints() \\\n    .initial('x_n-x_n_0', '1') \\\n    .initial('y_n-y_n_0', '1') \\\n    .terminal('x_n-x_n_f', '1') \\\n    .terminal('y_n-y_n_f', '1')\n```\n\n### Example Solver Configuration:\n\n```python\nproblem.bvp_solver = algorithms.MultipleShooting(derivative_method='fd', tolerance=1e-4, max_iterations=1000, verbose=True, cached=False, number_arcs=64)\n```\n\nThis script is a comprehensive example of how to set up and solve an optimal control problem using the `beluga` library. The specific details of the state equations, cost function, and constraints will depend on the particular system being modeled."
    },
    {
        "code": "from airbnb_priceforecaster.models import train_model\nfrom airbnb_priceforecaster.models import build_model\nfrom airbnb_priceforecaster.data import AirBnBDataset\nimport click\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\n@click.option(\"-y\", \"--year\", default=2020, type=int)\n@click.option(\"-m\", \"--month\", default=5, type=int)\n@click.option(\"-d\", \"--day\", default=30, type=int)\ndef train(year, month, day):\n    result = train_model(year, month, day)\n    click.echo(result)\n\n\n@cli.command()\n@click.option(\"-y\", \"--year\", default=2020, type=int)\n@click.option(\"-m\", \"--month\", default=5, type=int)\n@click.option(\"-d\", \"--day\", default=30, type=int)\ndef prod(year, month, day):\n    dataset = AirBnBDataset(year=year, month=month, day=day)\n    model = build_model()\n\n    model.train_estimator(dataset)\n    model.save_estimator(prod=True)\n\n\nif __name__ == '__main__':\n    cli()\n",
        "summary": "The provided Python code defines a command-line interface (CLI) using the `click` library for an Airbnb price forecasting application. It includes two main commands: `train`, which trains a model based on specified year, month, and day, and `prod`, which builds a dataset, trains a model, and saves it for production use with default parameters set to 2020-05-30."
    },
    {
        "code": "import re\nimport six\n\n\nfrom huaweicloudsdkcore.sdk_response import SdkResponse\nfrom huaweicloudsdkcore.utils.http_utils import sanitize_for_serialization\n\n\nclass UpdateTaskStatusResponse(SdkResponse):\n\n\n    \n\n    sensitive_list = []\n\n    openapi_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self):\n        \n        \n        super(UpdateTaskStatusResponse, self).__init__()\n        self.discriminator = None\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                if attr in self.sensitive_list:\n                    result[attr] = \"****\"\n                else:\n                    result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        import simplejson as json\n        if six.PY2:\n            import sys\n            reload(sys)\n            sys.setdefaultencoding(\"utf-8\")\n        return json.dumps(sanitize_for_serialization(self), ensure_ascii=False)\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, UpdateTaskStatusResponse):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n",
        "summary": "The `UpdateTaskStatusResponse` class extends `SdkResponse` and includes methods for serializing the response to a dictionary or JSON string, handling sensitive data, and comparing instances for equality."
    },
    {
        "code": "import pprint\nimport re  \n\nimport six\n\n\nclass ApiArtifact(object):\n    \n\n    \n    swagger_types = {\n        'name': 'str',\n        'checksum': 'str',\n        'id': 'str',\n        'names': 'list[str]'\n    }\n\n    attribute_map = {\n        'name': 'name',\n        'checksum': 'checksum',\n        'id': 'id',\n        'names': 'names'\n    }\n\n    def __init__(self, name=None, checksum=None, id=None, names=None):  \n          \n\n        self._name = None\n        self._checksum = None\n        self._id = None\n        self._names = None\n        self.discriminator = None\n\n        if name is not None:\n            self.name = name\n        if checksum is not None:\n            self.checksum = checksum\n        if id is not None:\n            self.id = id\n        if names is not None:\n            self.names = names\n\n    @property\n    def name(self):\n        \n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \n\n        self._name = name\n\n    @property\n    def checksum(self):\n        \n        return self._checksum\n\n    @checksum.setter\n    def checksum(self, checksum):\n        \n\n        self._checksum = checksum\n\n    @property\n    def id(self):\n        \n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \n\n        self._id = id\n\n    @property\n    def names(self):\n        \n        return self._names\n\n    @names.setter\n    def names(self, names):\n        \n\n        self._names = names\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ApiArtifact, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, ApiArtifact):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n",
        "summary": "The provided Python code defines a class `ApiArtifact` that represents an artifact with properties such as name, checksum, ID, and names. It includes methods for setting and getting these properties, converting the object to a dictionary, and comparing objects for equality. The class uses decorators to manage property access and serialization."
    },
    {
        "code": "from __future__ import absolute_import, division, unicode_literals, print_function\n\nimport re\nimport os\nimport tempfile\n\nimport six\n\nfrom .. import environment\nfrom ..console import log\nfrom .. import util\n\n\nWIN = (os.name == \"nt\")\n\n\ndef _find_conda():\n    \n    if 'CONDA_EXE' in os.environ:\n        conda = os.environ['CONDA_EXE']\n    else:\n        conda = util.which('conda')\n    return conda\n\n\nclass Conda(environment.Environment):\n    \n    tool_name = \"conda\"\n    _matches_cache = {}\n\n    def __init__(self, conf, python, requirements):\n        \n        self._python = python\n        self._requirements = requirements\n        self._conda_channels = conf.conda_channels\n        super(Conda, self).__init__(conf, python, requirements)\n\n    @classmethod\n    def matches(cls, python):\n        \n        if python not in cls._matches_cache:\n            cls._matches_cache[python] = cls._matches(python)\n        return cls._matches_cache[python]\n\n    @classmethod\n    def _matches(cls, python):\n        if not re.match(r'^[0-9].*$', python):\n            \n            return False\n\n        try:\n            conda = _find_conda()\n        except IOError:\n            return False\n        else:\n            \n            \n            \n            path = os.path.join(tempfile.gettempdir(), 'check')\n\n            \n            try:\n                util.check_call([\n                    conda,\n                    'create',\n                    '--yes',\n                    '-p',\n                    path,\n                    'python={0}'.format(python),\n                    '--dry-run'], display_error=False, dots=False)\n            except util.ProcessError:\n                return False\n            else:\n                return True\n\n    def _setup(self):\n        try:\n            conda = _find_conda()\n        except IOError as e:\n            raise util.UserError(str(e))\n\n        log.info(\"Creating conda environment for {0}\".format(self.name))\n\n        \n        \n        env_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".yml\")\n        try:\n            env_file.write('name: {0}\\n'\n                           'channels:\\n'.format(self.name))\n            env_file.writelines(('   - %s\\n' % ch for ch in self._conda_channels))\n            env_file.write('dependencies:\\n'\n                           '   - python={0}\\n'\n                           '   - wheel\\n'\n                           '   - pip\\n'.format(self._python))\n\n            \n            conda_args, pip_args = self._get_requirements(conda)\n            env_file.writelines(('   - %s\\n' % s for s in conda_args))\n            if pip_args:\n                \n                \n                env_file.write('   - pip:\\n')\n                env_file.writelines(('     - %s\\n' % s for s in pip_args))\n\n            env_file.close()\n\n            util.check_output([conda] + ['env', 'create', '-f', env_file.name,\n                                         '-p', self._path, '--force'])\n        except Exception as exc:\n            if os.path.isfile(env_file.name):\n                with open(env_file.name, 'r') as f:\n                    text = f.read()\n                log.info(\"conda env create failed: in {} with:\\n{}\".format(self._path, text))\n            raise\n        finally:\n            os.unlink(env_file.name)\n\n    def _get_requirements(self, conda):\n        if self._requirements:\n            \n            conda_args = []\n            pip_args = []\n\n            for key, val in six.iteritems(self._requirements):\n                if key.startswith('pip+'):\n                    if val:\n                        pip_args.append(\"{0}=={1}\".format(key[4:], val))\n                    else:\n                        pip_args.append(key[4:])\n                else:\n                    if val:\n                        conda_args.append(\"{0}={1}\".format(key, val))\n                    else:\n                        conda_args.append(key)\n\n            return conda_args, pip_args\n        else:\n            return [], []\n\n    def run(self, args, **kwargs):\n        log.debug(\"Running '{0}' in {1}\".format(' '.join(args), self.name))\n        return self.run_executable('python', args, **kwargs)\n\n    def run_executable(self, executable, args, **kwargs):\n        \n        kwargs[\"env\"] = dict(kwargs.pop(\"env\", os.environ),\n                             PYTHONNOUSERSITE=str(\"True\"))\n        return super(Conda, self).run_executable(executable, args, **kwargs)\n",
        "summary": "This Python code defines a class `Conda` that extends an environment class to manage Conda environments. It includes methods for finding the Conda executable, checking if a Python version is compatible with Conda, setting up a new Conda environment with specified requirements, and running commands within that environment."
    },
    {
        "code": "import enum\nimport timeit\nimport textwrap\nfrom typing import Any, Callable, Dict, List, NoReturn, Optional, Type, Union\n\nimport numpy as np\nimport torch\nfrom torch.utils.benchmark.utils import common, cpp_jit\nfrom torch.utils.benchmark.utils._stubs import TimerClass, TimeitModuleType\nfrom torch.utils.benchmark.utils.valgrind_wrapper import timer_interface as valgrind_timer_interface\n\n\n__all__ = [\"Timer\", \"timer\", \"Language\"]\n\n\nif torch.has_cuda and torch.cuda.is_available():\n    def timer() -> float:\n        torch.cuda.synchronize()\n        return timeit.default_timer()\nelse:\n    timer = timeit.default_timer\n\n\nclass Language(enum.Enum):\n    PYTHON = 0\n    CPP = 1\n\n\nclass CPPTimer:\n    def __init__(\n        self,\n        stmt: str,\n        setup: str,\n        timer: Callable[[], float],\n        globals: Dict[str, Any],\n    ) -> None:\n        if timer is not timeit.default_timer:\n            raise NotImplementedError(\n                \"PyTorch was built with CUDA and a GPU is present; however \"\n                \"Timer does not yet support GPU measurements. If your \"\n                \"code is CPU only, pass `timer=timeit.default_timer` to the \"\n                \"Timer's constructor to indicate this. (Note that this will \"\n                \"produce incorrect results if the GPU is in fact used, as \"\n                \"Timer will not synchronize CUDA.)\"\n            )\n\n        if globals:\n            raise ValueError(\"C++ timing does not support globals.\")\n\n        self._stmt: str = textwrap.dedent(stmt)\n        self._setup: str = textwrap.dedent(setup)\n        self._timeit_module: Optional[TimeitModuleType] = None\n\n    def timeit(self, number: int) -> float:\n        if self._timeit_module is None:\n            self._timeit_module = cpp_jit.compile_timeit_template(\n                self._stmt,\n                self._setup,\n            )\n\n        return self._timeit_module.timeit(number)\n\n\nclass Timer(object):\n    \n\n    _timer_cls: Type[TimerClass] = timeit.Timer\n\n    def __init__(\n        self,\n        stmt: str = \"pass\",\n        setup: str = \"pass\",\n        timer: Callable[[], float] = timer,\n        globals: Optional[Dict[str, Any]] = None,\n        label: Optional[str] = None,\n        sub_label: Optional[str] = None,\n        description: Optional[str] = None,\n        env: Optional[str] = None,\n        num_threads: int = 1,\n        language: Union[Language, str] = Language.PYTHON,\n    ):\n        if not isinstance(stmt, str):\n            raise ValueError(\"Currently only a `str` stmt is supported.\")\n\n        \n        \n        self._globals = dict(globals or {})\n        if language in (Language.PYTHON, \"py\", \"python\"):\n            \n            self._globals.setdefault(\"torch\", torch)\n            self._language: Language = Language.PYTHON\n\n        elif language in (Language.CPP, \"cpp\", \"c++\"):\n            assert self._timer_cls is timeit.Timer, \"_timer_cls has already been swapped.\"\n            self._timer_cls = CPPTimer\n            setup = (\"\" if setup == \"pass\" else setup)\n            self._language = Language.CPP\n\n        else:\n            raise ValueError(f\"Invalid language `{language}`.\")\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        stmt = textwrap.dedent(stmt)\n        stmt = (stmt[1:] if stmt and stmt[0] == \"\\n\" else stmt).rstrip()\n        setup = textwrap.dedent(setup)\n        setup = (setup[1:] if setup and setup[0] == \"\\n\" else setup).rstrip()\n\n        self._timer = self._timer_cls(\n            stmt=stmt,\n            setup=setup,\n            timer=timer,\n            globals=valgrind_timer_interface.CopyIfCallgrind.unwrap_all(self._globals),\n        )\n        self._task_spec = common.TaskSpec(\n            stmt=stmt,\n            setup=setup,\n            label=label,\n            sub_label=sub_label,\n            description=description,\n            env=env,\n            num_threads=num_threads,\n        )\n\n    def timeit(self, number: int = 1000000) -> common.Measurement:\n        \n        with common.set_torch_threads(self._task_spec.num_threads):\n            \n            self._timer.timeit(number=max(int(number // 100), 1))\n\n            return common.Measurement(\n                number_per_run=number,\n                raw_times=[self._timer.timeit(number=number)],\n                task_spec=self._task_spec\n            )\n\n    def repeat(self, repeat: int = -1, number: int = -1) -> None:\n        raise NotImplementedError(\"See `Timer.blocked_autorange.`\")\n\n    def autorange(self, callback: Optional[Callable[[int, float], NoReturn]] = None) -> None:\n        raise NotImplementedError(\"See `Timer.blocked_autorange.`\")\n\n    def _threaded_measurement_loop(\n        self,\n        number: int,\n        time_hook: Callable[[], float],\n        stop_hook: Callable[[List[float]], bool],\n        min_run_time: float,\n        max_run_time: Optional[float] = None,\n        callback: Optional[Callable[[int, float], NoReturn]] = None\n    ) -> List[float]:\n        total_time = 0.0\n        can_stop = False\n        times: List[float] = []\n        with common.set_torch_threads(self._task_spec.num_threads):\n            while (total_time < min_run_time) or (not can_stop):\n                time_spent = time_hook()\n                times.append(time_spent)\n                total_time += time_spent\n                if callback:\n                    callback(number, time_spent)\n                can_stop = stop_hook(times)\n                if max_run_time and total_time > max_run_time:\n                    break\n        return times\n\n    def _estimate_block_size(self, min_run_time: float) -> int:\n        with common.set_torch_threads(self._task_spec.num_threads):\n            \n            \n            overhead = np.median([self._timer.timeit(0) for _ in range(5)])\n            number = 1\n            while True:\n                time_taken = self._timer.timeit(number)\n                relative_overhead = overhead / time_taken\n                if relative_overhead <= 1e-4 and time_taken >= min_run_time / 1000:\n                    break\n                if time_taken > min_run_time:\n                    break\n                number *= 10\n        return number\n\n    def adaptive_autorange(\n            self,\n            threshold: float = 0.1,\n            *,\n            min_run_time: float = 0.01,\n            max_run_time: float = 10.0,\n            callback: Optional[Callable[[int, float], NoReturn]] = None,\n    ) -> common.Measurement:\n        number = self._estimate_block_size(min_run_time=0.05)\n\n        def time_hook() -> float:\n            return self._timer.timeit(number)\n\n        def stop_hook(times: List[float]) -> bool:\n            if len(times) > 3:\n                return common.Measurement(\n                    number_per_run=number,\n                    raw_times=times,\n                    task_spec=self._task_spec\n                ).meets_confidence(threshold=threshold)\n            return False\n        times = self._threaded_measurement_loop(\n            number, time_hook, stop_hook, min_run_time, max_run_time, callback=callback)\n\n        return common.Measurement(\n            number_per_run=number,\n            raw_times=times,\n            task_spec=self._task_spec\n        )\n\n    def blocked_autorange(\n        self,\n        callback: Optional[Callable[[int, float], NoReturn]] = None,\n        min_run_time: float = 0.2,\n    ) -> common.Measurement:\n        \n        number = self._estimate_block_size(min_run_time)\n\n        def time_hook() -> float:\n            return self._timer.timeit(number)\n\n        def stop_hook(times: List[float]) -> bool:\n            return True\n\n        times = self._threaded_measurement_loop(\n            number, time_hook, stop_hook,\n            min_run_time=min_run_time,\n            callback=callback)\n\n        return common.Measurement(\n            number_per_run=number,\n            raw_times=times,\n            task_spec=self._task_spec\n        )\n\n    def collect_callgrind(\n        self,\n        number: int = 100,\n        collect_baseline: bool = True\n    ) -> valgrind_timer_interface.CallgrindStats:\n        \n        if not isinstance(self._task_spec.stmt, str):\n            raise ValueError(\"`collect_callgrind` currently only supports string `stmt`\")\n\n        \n        \n        \n        self._timer.timeit(1)\n        is_python = (self._language == Language.PYTHON)\n        assert is_python or not self._globals\n        return valgrind_timer_interface.wrapper_singleton().collect_callgrind(\n            task_spec=self._task_spec,\n            globals=self._globals,\n            number=number,\n            collect_baseline=collect_baseline and is_python,\n            is_python=is_python)\n",
        "summary": "The provided Python code defines a `Timer` class for benchmarking code execution times, supporting both Python and C++ languages. It includes methods for timing statements multiple times, estimating block sizes, and collecting performance data using Valgrind's Callgrind tool. The class handles synchronization with CUDA when available and provides options for adaptive and blocked autoranging to optimize measurement accuracy."
    },
    {
        "code": "from Ranger.src.Range.Cut import Cut\n\nclass Range(object):\n    \n    def __init__(self, lowerCut, upperCut):\n        \n        if not all(map(lambda x: isinstance(x, Cut), (lowerCut,upperCut))):\n            raise ValueError(\"Bounds must be Cut objects\")\n        elif lowerCut > upperCut:\n            raise ValueError(\"Lower bound cannot be greater than upper bound\")\n        self.lowerCut = lowerCut\n        self.upperCut = upperCut\n    def __repr__(self):\n        try:\n            return_str = '[' if self.isLowerBoundClosed() else '('\n        except TypeError:\n            return_str = '('\n        return_str += (str(self.lowerCut.point) if not self.lowerCut.belowAll \\\n          else '')\n        return_str += ' , '\n        return_str += (str(self.upperCut.point) if not self.upperCut.aboveAll \\\n          else '')\n        try:\n            return_str += ']' if self.isUpperBoundClosed() else ')'\n        except TypeError:\n            return_str += ')'\n        return return_str\n    def __hash__(self):\n        return (hash(self.lowerCut)*31 + hash(self.upperCut))\n    def __eq__(self, other):\n        if not isinstance(other, Range):\n            return False\n        else:\n            return ((self.lowerCut == other.lowerCut) and \\\n                    (self.upperCut == other.upperCut))\n    def __ne__(self, other):\n        return not self.__eq__(other)\n    def contains(self, val):\n        \n        return (self.lowerCut < val and \\\n                self.upperCut > val)\n    def containsAll(self, vals):\n        \n        for val in vals:\n            if not self.contains(val):\n                return False\n        return True\n    def getDistanceFromPoint(self, val, distFunc = lambda x1, x2: abs(x1-x2)):\n        \n        if not all((self.isLowerBoundClosed(), self.isUpperBoundClosed())):\n            raise TypeError(\"Range is not closed\")\n        if self.contains(val):\n            return 0.\n        else:\n            return min(distFunc(self.lowerCut.point, val),\n                       distFunc(self.upperCut.point, val))\n    def getDistanceFromRange(self, other, distFunc = lambda x1,x2: abs(x1-x2)):\n        \n        if not isinstance(other, Range):\n            raise TypeError(\"other is not a Range\")\n        if not all((self.isLowerBoundClosed(), self.isUpperBoundClosed(),\n                    other.isLowerBoundClosed(), other.isUpperBoundClosed())):\n            raise TypeError(\"Not all Ranges closed\")\n        if self.isConnected(other):\n            return 0.\n        else:\n            return min(distFunc(self.lowerCut.point, other.upperCut.point),\n                       distFunc(other.lowerCut.point, self.upperCut.point))\n    def hasLowerBound(self):\n        \n        return (not self.lowerCut.belowAll)\n    def hasUpperBound(self):\n        \n        return (not self.upperCut.aboveAll)\n    def lowerEndpoint(self):\n        \n        if self.lowerCut.point is None:\n            raise TypeError(\"Range unbounded below\")\n        else:\n            return self.lowerCut.point\n    def upperEndpoint(self):\n        \n        if self.upperCut.point is None:\n            raise TypeError(\"Range unbounded above\")\n        else:\n            return self.upperCut.point\n    def isLowerBoundClosed(self):\n        \n        if self.lowerCut.point is None:\n            raise TypeError(\"Range unbounded below\")\n        else:\n            return self.lowerCut.below\n    def isUpperBoundClosed(self):\n        \n        if self.upperCut.point is None:\n            raise TypeError(\"Range unbounded above\")\n        else:\n            return (not self.upperCut.below)\n    def isEmpty(self):\n        \n        return self.lowerCut == self.upperCut\n    def encloses(self, other):\n        \n        if not isinstance(other, Range):\n            raise ValueError(\"Range required\")\n        return ((self.lowerCut <= other.lowerCut) and \\\n            (self.upperCut >= other.upperCut))\n    def isConnected(self, other):\n        \n        if not isinstance(other, Range):\n            raise ValueError(\"Range required\")\n        return ((self.lowerCut <= other.upperCut) and \\\n                (other.lowerCut <= self.upperCut))\n    def intersection(self, other):\n        \n        if not isinstance(other, Range):\n            raise ValueError(\"Range required\")\n        if ((self.lowerCut >= other.lowerCut) and \\\n            (self.upperCut <= other.upperCut)):\n            return Range(self.lowerCut, self.upperCut)\n        elif ((self.lowerCut <= other.lowerCut) and \\\n              (self.upperCut >= other.upperCut)):\n            return Range(other.lowerCut, other.upperCut)\n        else:\n            newLower = self.lowerCut if (self.lowerCut >= other.lowerCut) else \\\n                                         other.lowerCut\n            newUpper = self.upperCut if (self.upperCut <= other.upperCut) else \\\n                                         other.upperCut\n            return Range(newLower, newUpper)\n    def span(self, other):\n        \n        if ((self.lowerCut <= other.lowerCut) and \\\n            (self.upperCut >= other.upperCut)):\n            return Range(self.lowerCut, self.upperCut)\n        elif ((self.lowerCut >= other.lowerCut) and \\\n              (self.upperCut <= other.upperCut)):\n            return Range(other.lowerCut, other.upperCut)\n        else:\n            newLower = self.lowerCut if (self.lowerCut <= other.lowerCut) else \\\n                other.lowerCut\n            newUpper = self.upperCut if (self.upperCut >= other.upperCut) else \\\n                other.upperCut\n            return Range(newLower, newUpper)\n    \n    \n    \n    @staticmethod\n    def _validate_cutpoints(*pts):\n        if not all(map(lambda x: (hasattr(x, \"__lt__\") and \\\n                hasattr(x, \"__gt__\")) or hasattr(x,'__cmp__'), pts)):\n            raise ValueError(\"Cutpoint type(s) not comparable\")\n        if len(pts) == 2:\n            if not (issubclass(type(pts[0]),type(pts[1])) or \\\n              issubclass(type(pts[1]),type(pts[0]))):\n                raise ValueError(\"Cutpoints are not compatible\")\n        return True\n    @staticmethod\n    def _get_type(*pts):\n        if len(pts) == 1: return type(pts[0])\n        elif len(pts) == 2:\n            if issubclass(type(pts[0]),type(pts[1])):\n                return type(pts[1])\n            elif issubclass(type(pts[1]),type(pts[0])):\n                return type(pts[0])\n            else:\n                raise ValueError(\"Cutpoints are not compatible\")\n    @staticmethod\n    def closed(lower, upper):\n        \n        \n        Range._validate_cutpoints(lower, upper)\n        theType = Range._get_type(lower,upper)\n        return Range(Cut.belowValue(lower, theType=theType),\n                     Cut.aboveValue(upper, theType=theType))\n    @staticmethod\n    def closedOpen(lower, upper):\n        \n        \n        Range._validate_cutpoints(lower, upper)\n        theType = Range._get_type(lower,upper)\n        return Range(Cut.belowValue(lower, theType=theType),\n                     Cut.belowValue(upper, theType=theType))\n    @staticmethod\n    def openClosed(lower, upper):\n        \n        \n        Range._validate_cutpoints(lower, upper)\n        theType = Range._get_type(lower,upper)\n        return Range(Cut.aboveValue(lower, theType=theType),\n                     Cut.aboveValue(upper, theType=theType))\n    @staticmethod\n    def open(lower, upper):\n        \n        \n        Range._validate_cutpoints(lower, upper)\n        theType = Range._get_type(lower,upper)\n        if lower == upper:\n            raise TypeError(\"Range of type (v,v) is not valid\")        \n        return Range(Cut.aboveValue(lower, theType=theType),\n                     Cut.belowValue(upper, theType=theType))\n    @staticmethod\n    def lessThan(val):\n        \n        Range._validate_cutpoints(val)\n        theType = Range._get_type(val)\n        return Range(Cut.belowAll(theType=theType),\n                     Cut.belowValue(val, theType=theType))\n    @staticmethod\n    def atMost(val):\n        \n        Range._validate_cutpoints(val)\n        theType = Range._get_type(val)\n        return Range(Cut.belowAll(theType=theType),\n                     Cut.aboveValue(val, theType=theType))\n    @staticmethod\n    def greaterThan(val):\n        \n        Range._validate_cutpoints(val)\n        theType = Range._get_type(val)\n        return Range(Cut.aboveValue(val,theType=theType),\n                     Cut.aboveAll(theType=theType))\n    @staticmethod\n    def atLeast(val):\n        \n        Range._validate_cutpoints(val)\n        theType = Range._get_type(val)\n        return Range(Cut.belowValue(val, theType=theType),\n                     Cut.aboveAll(theType=theType))\n",
        "summary": "The `Range` class in Python defines a range with lower and upper bounds that can be either closed or open. It provides methods to check if a value is within the range, calculate distances from points or other ranges, and perform operations like intersection and span with other ranges. The class ensures that all bounds are valid `Cut` objects and handles various edge cases such as unbounded ranges and incompatible cutpoints."
    },
    {
        "code": "from pyrelational.data.data_manager import GenericDataManager\n",
        "summary": "The provided Python code imports the `GenericDataManager` class from the `pyrelational.data.data_manager` module, which is likely used for managing data in a relational format within a Python application."
    },
    {
        "code": "from .core import Core, Settings\n\n\nclass Download(Core):\n\n    host = 'https://artifacts.elastic.co/downloads/beats/elastic-agent/{endpoint}'\n    endpoint = Settings.download_endpoint\n    kwargs = {\n        'stream': True\n    }\n\n    def parse_response(self, response):\n        self.__logger.debug('Saving file to download path: {}'.format(Settings.download_path))\n        with open(Settings.download_path, 'wb+') as f:\n            for chunk in response.raw.stream(1024, decode_content=False):\n                if chunk:\n                    f.write(chunk)\n        self.__logger.debug('File saved successfully')\n",
        "summary": "The `Download` class extends the `Core` class and is designed to handle downloading files from a specified host using an endpoint defined in settings. It uses a stream to download large files in chunks, writes each chunk to a file at a designated path, and logs the progress of the download."
    },
    {
        "code": "import board\nimport busio\nimport digitalio\n\nimport adafruit_rfm9x\n\n\n\nRADIO_FREQ_MHZ = 915.0  \n\n\n\nCS = digitalio.DigitalInOut(board.D5)\nRESET = digitalio.DigitalInOut(board.D6)\n\n\n\n\n\n\nLED = digitalio.DigitalInOut(board.D13)\nLED.direction = digitalio.Direction.OUTPUT\n\n\nspi = busio.SPI(board.SCK, MOSI=board.MOSI, MISO=board.MISO)\n\n\nrfm9x = adafruit_rfm9x.RFM9x(spi, CS, RESET, RADIO_FREQ_MHZ)\n\n\n\n\n\n\nrfm9x.tx_power = 23\n\n\n\n\n\nrfm9x.send(bytes(\"Hello world!\\r\\n\", \"utf-8\"))\nprint(\"Sent Hello World message!\")\n\n\n\n\n\nprint(\"Waiting for packets...\")\n\nwhile True:\n    packet = rfm9x.receive()\n    \n    \n    \n    if packet is None:\n        \n        LED.value = False\n        print(\"Received nothing! Listening again...\")\n    else:\n        \n        LED.value = True\n        \n        print(\"Received (raw bytes): {0}\".format(packet))\n        \n        \n        \n        \n        packet_text = str(packet, \"ascii\")\n        print(\"Received (ASCII): {0}\".format(packet_text))\n        \n        \n        rssi = rfm9x.last_rssi\n        print(\"Received signal strength: {0} dB\".format(rssi))\n",
        "summary": "The Python code initializes an RFM9X radio module using the Adafruit library, sets up SPI communication, and configures it to transmit a \"Hello world!\" message. It then enters a loop where it listens for incoming packets, toggling an LED based on whether a packet is received or not, and prints the received data along with its signal strength in dB."
    },
    {
        "code": "import numpy as np\nimport pandas as pd\n\n\nclass Clinical_outcome:\n    def __init__(self):\n        \n        self.name = \"Clinical outcome model\"\n        self.thrombectomy_time_no_effect = 8 * 60\n        self.thrombolysis_time_no_effect = 6.3 * 60\n        self.maximum_permitted_time_to_thrombectomy = 360\n        self.maximum_permitted_time_to_thrombolysis = 270\n\n    def calculate_outcome_for_all(self,\n                                  mimic,\n                                  ich,\n                                  nlvo,\n                                  lvo,\n                                  onset_to_needle,\n                                  onset_to_puncture,\n                                  nlvo_eligible_for_treatment,\n                                  lvo_eligible_for_treatment,\n                                  prop_thrombolysed_lvo_receiving_thrombectomy):\n        \n        \n        \n        \n        \n        outcomes = pd.DataFrame()\n\n        \n        outcomes['mimic'] = self._calculate_outcome_for_stroke_mimics(\n            mimic.shape)\n\n        \n        outcomes['ich']  = self._calculate_outcome_for_ICH(mimic.shape)\n\n        \n        outcomes['nlvo_base'] = \\\n            np.full(nlvo.shape, 0.4622)\n            \n        \n        outcomes['nlvo_add_ivt'] = \\\n            self._calculate_thrombolysis_outcome_for_nlvo(onset_to_needle)\n\n        \n        outcomes['lvo_base'] = \\\n            np.full(nlvo.shape, 0.1328)\n        \n        \n        outcomes['lvo_add_ivt'] = \\\n            self._calculate_thrombolysis_outcome_for_lvo(onset_to_needle)\n\n        \n        outcomes['lvo_add_et'] = \\\n            self._calculate_thrombectomy_outcome_for_lvo(onset_to_puncture)\n\n        \n        \n        \n        \n        \n        results = pd.DataFrame()\n        \n        \n        results['mimic'] = outcomes['mimic']  * mimic\n        \n        \n        results['ich'] = outcomes['ich']  * ich\n        \n        \n        results['nlvo_base'] = nlvo * outcomes['nlvo_base']\n        \n        results['nlvo_ivt'] = \\\n            nlvo * outcomes['nlvo_add_ivt'] * nlvo_eligible_for_treatment\n        \n        \n        results['lvo_base'] = lvo * outcomes['lvo_base']\n        \n        results['lvo_ivt'] = \\\n            lvo * outcomes['lvo_add_ivt'] * lvo_eligible_for_treatment\n                \n        \n        \n        lvo_receiving_et = ((lvo * lvo_eligible_for_treatment * \n            prop_thrombolysed_lvo_receiving_thrombectomy) - \n            results['lvo_ivt'])\n\n        results['lvo_et'] = lvo_receiving_et * outcomes['lvo_add_et']\n\n        p_good = results.sum(axis=1).values\n\n        return p_good\n\n    @staticmethod\n    def _calculate_outcome_for_ICH(array_shape):\n        \n\n        \n        p_good = np.zeros(array_shape)\n        p_good[:] = 0.24\n\n        return p_good        \n\n    @staticmethod\n    def _calculate_outcome_for_stroke_mimics(array_shape):\n        \n\n        \n        p_good = np.zeros(array_shape)\n        p_good[:] = 1\n\n        return p_good\n    \n    def _calculate_thrombectomy_outcome_for_lvo(self, onset_to_puncture):\n        \n\n        p_good_max = 0.5208\n        p_good_min = 0.1328\n        \n        \n        odds_good_max = p_good_max / (1 - p_good_max)\n        odds_good_min = p_good_min / (1 - p_good_min)\n        \n        \n        fraction_max_effect_time_used = \\\n            onset_to_puncture / self.thrombectomy_time_no_effect\n        \n        \n        odds_good = np.exp(np.log(odds_good_max) - \n            ((np.log(odds_good_max) - np.log(odds_good_min)) \n            * fraction_max_effect_time_used))\n        \n        \n        prob_good = odds_good / (1 + odds_good)\n        prob_good[prob_good < p_good_min] = p_good_min\n        \n        \n        p_good_add = prob_good - p_good_min\n        \n        \n        mask = onset_to_puncture > self.maximum_permitted_time_to_thrombectomy\n        p_good_add[mask] = 0   \n        \n        \n        mask = p_good_add < 0\n        p_good_add[mask] = 0  \n\n        return p_good_add        \n\n    def _calculate_thrombolysis_outcome_for_lvo(self, onset_to_needle):\n        \n        \n        p_good_max = 0.2441\n        p_good_min = 0.1328\n        \n        \n        odds_good_max = p_good_max / (1 - p_good_max)\n        odds_good_min = p_good_min / (1 - p_good_min)\n        \n        \n        fraction_max_effect_time_used = \\\n            onset_to_needle / self.thrombolysis_time_no_effect\n\n        \n        odds_good = np.exp(np.log(odds_good_max) - \n            ((np.log(odds_good_max) - np.log(odds_good_min)) \n            * fraction_max_effect_time_used))\n\n        \n        prob_good = odds_good / (1 + odds_good)\n        prob_good[prob_good < p_good_min] = p_good_min\n        \n        \n        p_good_add = prob_good - p_good_min\n        \n        \n        mask = onset_to_needle> self.maximum_permitted_time_to_thrombolysis\n        p_good_add[mask] = 0   \n        \n        \n        mask = p_good_add < 0\n        p_good_add[mask] = 0  \n\n        \n        return p_good_add\n\n    def _calculate_thrombolysis_outcome_for_nlvo(self, onset_to_needle):\n        \n\n        p_good_max = 0.6444\n        p_good_min = 0.4622\n        \n        \n        odds_good_max = p_good_max / (1 - p_good_max)\n        odds_good_min = p_good_min / (1 - p_good_min)\n        \n        \n        fraction_max_effect_time_used = (onset_to_needle / \n                                         self.thrombolysis_time_no_effect)\n        \n        \n        odds_good = np.exp(np.log(odds_good_max) - \n            ((np.log(odds_good_max) - np.log(odds_good_min)) \n            * fraction_max_effect_time_used))\n        \n        \n        prob_good = odds_good / (1 + odds_good)\n        prob_good[prob_good < p_good_min] = p_good_min\n        \n        \n        p_good_add = prob_good - p_good_min\n        \n        mask = onset_to_needle> self.maximum_permitted_time_to_thrombolysis\n        p_good_add[mask] = 0   \n        \n        \n        mask = p_good_add < 0\n        p_good_add[mask] = 0  \n\n        \n        return p_good_add\n",
        "summary": "The `Clinical_outcome` class in Python calculates the probability of a good clinical outcome for patients with different types of strokes, including mimics, intracranial hemorrhage (ICH), and large vessel occlusions (LVO). It uses various parameters such as onset times to treatment and eligibility criteria to compute outcomes based on thrombolysis and thrombectomy interventions."
    },
    {
        "code": "from django.conf.urls.defaults import patterns, url\nfrom djangopypi.feeds import ReleaseFeed\n\nurlpatterns = patterns(\"djangopypi.views\",\n    url(r'^$', \"root\", name=\"djangopypi-root\"),\n    url(r'^packages/$','packages.index', name='djangopypi-package-index'),\n    url(r'^simple/$','packages.simple_index', name='djangopypi-package-index-simple'),\n    url(r'^search/$','packages.search',name='djangopypi-search'),\n    url(r'^pypi/$', 'root', name='djangopypi-release-index'),\n    url(r'^rss/$', ReleaseFeed(), name='djangopypi-rss'),\n    \n    url(r'^simple/(?P<package>[\\w\\d_\\.\\-]+)/$','packages.simple_details',\n        name='djangopypi-package-simple'),\n    \n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/$','packages.details',\n        name='djangopypi-package'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/rss/$', ReleaseFeed(),\n        name='djangopypi-package-rss'),    \n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/doap.rdf$','packages.doap',\n        name='djangopypi-package-doap'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/manage/$','packages.manage',\n        name='djangopypi-package-manage'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/manage/versions/$','packages.manage_versions',\n        name='djangopypi-package-manage-versions'),\n    \n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/$',\n        'releases.details',name='djangopypi-release'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/doap.rdf$',\n        'releases.doap',name='djangopypi-release-doap'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/manage/$',\n        'releases.manage',name='djangopypi-release-manage'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/metadata/$',\n        'releases.manage_metadata',name='djangopypi-release-manage-metadata'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/files/$',\n        'releases.manage_files',name='djangopypi-release-manage-files'),\n    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/files/upload/$',\n        'releases.upload_file',name='djangopypi-release-upload-file'),\n)",
        "summary": "This Django URL configuration maps various package-related URLs to corresponding view functions, including handling of package and release details, RSS feeds, search functionality, and management options."
    },
    {
        "code": "import mock\nfrom oslo_config import cfg\nfrom oslo_utils import fixture as utils_fixture\nfrom oslo_utils import timeutils\nfrom oslo_utils import uuidutils\n\nfrom neutron.conf.services import metering_agent as metering_agent_config\nfrom neutron.services.metering.agents import metering_agent\nfrom neutron.tests import base\nfrom neutron.tests import fake_notifier\n\n_uuid = uuidutils.generate_uuid\n\nTENANT_ID = _uuid()\nLABEL_ID = _uuid()\nROUTERS = [{'status': 'ACTIVE',\n            'name': 'router1',\n            'gw_port_id': None,\n            'admin_state_up': True,\n            'tenant_id': TENANT_ID,\n            '_metering_labels': [{'rules': [],\n                                  'id': LABEL_ID}],\n            'id': _uuid()}]\n\nROUTERS_WITH_RULE = [{'status': 'ACTIVE',\n                      'name': 'router1',\n                      'gw_port_id': None,\n                      'admin_state_up': True,\n                      'tenant_id': TENANT_ID,\n                      '_metering_labels': [{'rule': {},\n                                            'id': LABEL_ID}],\n                      'id': _uuid()}]\n\n\nclass TestMeteringOperations(base.BaseTestCase):\n\n    def setUp(self):\n        super(TestMeteringOperations, self).setUp()\n        metering_agent_config.register_metering_agent_opts()\n\n        self.noop_driver = ('neutron.services.metering.drivers.noop.'\n                            'noop_driver.NoopMeteringDriver')\n        cfg.CONF.set_override('driver', 'noop')\n        cfg.CONF.set_override('measure_interval', 0)\n        cfg.CONF.set_override('report_interval', 0)\n\n        self.setup_notification_driver()\n\n        metering_rpc = ('neutron.services.metering.agents.metering_agent.'\n                        'MeteringPluginRpc._get_sync_data_metering')\n        self.metering_rpc_patch = mock.patch(metering_rpc, return_value=[])\n        self.metering_rpc_patch.start()\n\n        self.driver_patch = mock.patch(self.noop_driver, spec=True)\n        self.driver_patch.start()\n\n        loopingcall_patch = mock.patch(\n            'oslo_service.loopingcall.FixedIntervalLoopingCall')\n        loopingcall_patch.start()\n\n        self.agent = metering_agent.MeteringAgent('my agent', cfg.CONF)\n        self.driver = self.agent.metering_driver\n\n    def test_add_metering_label(self):\n        self.agent.add_metering_label(None, ROUTERS)\n        self.assertEqual(1, self.driver.add_metering_label.call_count)\n\n    def test_remove_metering_label(self):\n        self.agent.remove_metering_label(None, ROUTERS)\n        self.assertEqual(1, self.driver.remove_metering_label.call_count)\n\n    def test_update_metering_label_rule(self):\n        self.agent.update_metering_label_rules(None, ROUTERS)\n        self.assertEqual(1, self.driver.update_metering_label_rules.call_count)\n\n    def test_add_metering_label_rule(self):\n        self.agent.add_metering_label_rule(None, ROUTERS_WITH_RULE)\n        self.assertEqual(1, self.driver.add_metering_label_rule.call_count)\n\n    def test_remove_metering_label_rule(self):\n        self.agent.remove_metering_label_rule(None, ROUTERS_WITH_RULE)\n        self.assertEqual(1, self.driver.remove_metering_label_rule.call_count)\n\n    def test_routers_updated(self):\n        self.agent.routers_updated(None, ROUTERS)\n        self.assertEqual(1, self.driver.update_routers.call_count)\n\n    def test_get_traffic_counters(self):\n        self.agent._get_traffic_counters(None, ROUTERS)\n        self.assertEqual(1, self.driver.get_traffic_counters.call_count)\n\n    def test_sync_router_namespaces(self):\n        self.agent._sync_router_namespaces(None, ROUTERS)\n        self.assertEqual(1, self.driver.sync_router_namespaces.call_count)\n\n    def test_notification_report(self):\n        self.agent.routers_updated(None, ROUTERS)\n\n        self.driver.get_traffic_counters.return_value = {LABEL_ID:\n                                                         {'pkts': 88,\n                                                          'bytes': 444}}\n        self.agent._metering_loop()\n\n        self.assertNotEqual(len(fake_notifier.NOTIFICATIONS), 0)\n        for n in fake_notifier.NOTIFICATIONS:\n            if n['event_type'] == 'l3.meter':\n                break\n\n        self.assertEqual('l3.meter', n['event_type'])\n\n        payload = n['payload']\n        self.assertEqual(TENANT_ID, payload['tenant_id'])\n        self.assertEqual(LABEL_ID, payload['label_id'])\n        self.assertEqual(88, payload['pkts'])\n        self.assertEqual(444, payload['bytes'])\n\n    def test_notification_report_interval(self):\n        measure_interval = 30\n        report_interval = 600\n\n        now = timeutils.utcnow()\n        time_fixture = self.useFixture(utils_fixture.TimeFixture(now))\n\n        self.agent.routers_updated(None, ROUTERS)\n\n        self.driver.get_traffic_counters.return_value = {LABEL_ID:\n                                                         {'pkts': 889,\n                                                          'bytes': 4440}}\n\n        cfg.CONF.set_override('measure_interval', measure_interval)\n        cfg.CONF.set_override('report_interval', report_interval)\n\n        for i in range(report_interval):\n            self.agent._metering_loop()\n            count = 0\n\n            if len(fake_notifier.NOTIFICATIONS) > 1:\n                for n in fake_notifier.NOTIFICATIONS:\n                    if n['event_type'] == 'l3.meter':\n                        \n                        count += 1\n                        if count > 1:\n                            break\n\n            time_fixture.advance_time_seconds(measure_interval)\n\n        self.assertEqual('l3.meter', n['event_type'])\n\n        payload = n['payload']\n        self.assertEqual(TENANT_ID, payload['tenant_id'])\n        self.assertEqual(LABEL_ID, payload['label_id'])\n        self.assertLess((payload['time'] - report_interval),\n                        measure_interval, payload)\n        interval = (payload['last_update'] - payload['first_update']) \\\n            - report_interval\n        self.assertLess(interval, measure_interval, payload)\n\n    def test_router_deleted(self):\n        label_id = _uuid()\n        self.driver.get_traffic_counters = mock.MagicMock()\n        self.driver.get_traffic_counters.return_value = {label_id:\n                                                         {'pkts': 44,\n                                                          'bytes': 222}}\n        self.agent._add_metering_info = mock.MagicMock()\n\n        self.agent.routers_updated(None, ROUTERS)\n        self.agent.router_deleted(None, ROUTERS[0]['id'])\n\n        self.assertEqual(1, self.agent._add_metering_info.call_count)\n        self.assertEqual(1, self.driver.remove_router.call_count)\n\n        self.agent._add_metering_info.assert_called_with(label_id, 44, 222)\n\n    @mock.patch('time.time')\n    def _test_purge_metering_info(self, current_timestamp, is_empty,\n                                  mock_time):\n        mock_time.return_value = current_timestamp\n        self.agent.metering_infos = {'fake': {'last_update': 1}}\n        self.config(report_interval=1)\n\n        self.agent._purge_metering_info()\n        self.assertEqual(0 if is_empty else 1, len(self.agent.metering_infos))\n        self.assertEqual(1, mock_time.call_count)\n\n    def test_purge_metering_info(self):\n        \n        self._test_purge_metering_info(2, False)\n\n    def test_purge_metering_info_delete(self):\n        \n        self._test_purge_metering_info(3, True)\n\n    @mock.patch('time.time')\n    def _test_add_metering_info(self, expected_info, current_timestamp,\n                                mock_time):\n        mock_time.return_value = current_timestamp\n        actual_info = self.agent._add_metering_info('fake_label_id', 1, 1)\n        self.assertEqual(1, len(self.agent.metering_infos))\n        self.assertEqual(expected_info, actual_info)\n        self.assertEqual(expected_info,\n                         self.agent.metering_infos['fake_label_id'])\n        self.assertEqual(1, mock_time.call_count)\n\n    def test_add_metering_info_create(self):\n        expected_info = {'bytes': 1, 'pkts': 1, 'time': 0, 'first_update': 1,\n                         'last_update': 1}\n        self._test_add_metering_info(expected_info, 1)\n\n    def test_add_metering_info_update(self):\n        expected_info = {'bytes': 1, 'pkts': 1, 'time': 0, 'first_update': 1,\n                         'last_update': 1}\n        self.agent.metering_infos = {'fake_label_id': expected_info}\n        expected_info.update({'bytes': 2, 'pkts': 2, 'time': 1,\n                              'last_update': 2})\n        self._test_add_metering_info(expected_info, 2)\n\n    def test_metering_agent_host_value(self):\n        expected_host = 'my agent'\n        self.assertEqual(expected_host, self.agent.host)\n\n\nclass TestMeteringDriver(base.BaseTestCase):\n    def setUp(self):\n        super(TestMeteringDriver, self).setUp()\n        metering_agent_config.register_metering_agent_opts()\n\n        cfg.CONF.set_override('driver', 'noop')\n\n        self.agent = metering_agent.MeteringAgent('my agent', cfg.CONF)\n        self.driver = mock.Mock()\n        self.agent.metering_driver = self.driver\n\n    def test_add_metering_label_with_bad_driver_impl(self):\n        del self.driver.add_metering_label\n\n        with mock.patch.object(metering_agent, 'LOG') as log:\n            self.agent.add_metering_label(None, ROUTERS)\n            log.exception.assert_called_with(mock.ANY,\n                                             {'driver': 'noop',\n                                              'func': 'add_metering_label'})\n\n    def test_add_metering_label_runtime_error(self):\n        self.driver.add_metering_label.side_effect = RuntimeError\n\n        with mock.patch.object(metering_agent, 'LOG') as log:\n            self.agent.add_metering_label(None, ROUTERS)\n            log.exception.assert_called_with(mock.ANY,\n                                             {'driver': 'noop',\n                                              'func':\n                                              'add_metering_label'})\n\n    def test_init_chain(self):\n        with mock.patch('oslo_service.'\n                        'periodic_task.PeriodicTasks.__init__') as init:\n            metering_agent.MeteringAgent('my agent', cfg.CONF)\n        init.assert_called_once_with(cfg.CONF)\n",
        "summary": "This code snippet is a unit test suite for the `MeteringAgent` class in OpenStack's Neutron project. The `MeteringAgent` is responsible for collecting and reporting network traffic statistics (metering) to the Neutron server.\n\nThe tests cover various aspects of the `MeteringAgent`, including:\n\n1. **Initialization**: Ensuring that the agent initializes correctly with a specified host name and configuration.\n2. **Adding Metering Labels**: Testing the functionality of adding metering labels, which are used to categorize network traffic for billing or monitoring purposes.\n3. **Handling Driver Errors**: Verifying that the agent logs exceptions when encountering errors in its driver implementation.\n4. **Purging Metering Information**: Testing the mechanism for removing old metering data based on a configured report interval.\n5. **Adding Metering Information**: Ensuring that new metering data is correctly added and updated.\n\nThe tests use mock objects to simulate the behavior of external dependencies, such as the metering driver, and verify that the agent behaves as expected under different conditions.\n\nThis code is part of the Neutron project's test suite, which helps ensure that changes to the codebase do not break existing functionality."
    },
    {
        "code": "from .update_db_on_advisory_change import UpdateDBOnAdvisoryChange  \nfrom .generate_advisory_signed_event_on_rpm_sign import GenerateAdvisorySignedEventOnRPMSign  \nfrom .update_db_on_odcs_compose_fail import UpdateDBOnODCSComposeFail  \nfrom .cancel_event_on_freshmaker_manage_request import CancelEventOnFreshmakerManageRequest  \n",
        "summary": "The Python code imports several classes from different modules, each designed to handle specific events and update the database accordingly. These include updating the database on advisory changes, generating signed events for RPM signatures, handling ODCS compose failures, and canceling events based on Freshmaker manage requests."
    },
    {
        "code": "import datetime\nimport time\n\nimport pytz\n\n\nPACIFIC_TZ = pytz.timezone('America/Vancouver')\n\n\ndef today_pacific():\n    now_pacific = datetime.datetime.fromtimestamp(time.time(), PACIFIC_TZ)\n    return now_pacific.date()\n",
        "summary": "The Python code imports necessary modules for handling dates and times, specifically setting up the Pacific Time Zone using `pytz`. It defines a function `today_pacific` that retrieves the current date in Pacific Time by converting the current timestamp to the specified timezone."
    },
    {
        "code": "__author__ = 'Tom Schaul, tom@idsia.ch'\n\nfrom scipy import array, exp, tanh, clip, log, dot, sqrt, power, pi, tan, diag, rand, real_if_close\nfrom scipy.linalg import inv, det, svd, logm, expm2\n\n\ndef semilinear(x):\n    \n    try:\n        \n        shape = x.shape\n        x.flatten()\n        x = x.tolist()\n    except AttributeError:\n        \n        shape = (1, len(x))\n    def f(val):\n        if val < 0:\n            \n            return safeExp(val)\n        else:\n            \n            return val + 1.0\n    return array(map(f, x)).reshape(shape)\n\n\ndef semilinearPrime(x):\n    \n    try:\n        \n        shape = x.shape\n        x.flatten()\n        x = x.tolist()\n    except AttributeError:\n        \n        shape = (1, len(x))\n    def f(val):\n        if val < 0:\n            \n            return safeExp(val)\n        else:\n            \n            return 1.0\n    return array(map(f, x)).reshape(shape)\n\n\ndef safeExp(x):\n    \n    return exp(clip(x, -500, 500))\n\n\ndef sigmoid(x):\n    \n    return 1. / (1. + safeExp(-x))\n\n\ndef sigmoidPrime(x):\n    \n    tmp = sigmoid(x)\n    return tmp * (1 - tmp)\n\n\ndef tanhPrime(x):\n    \n    tmp = tanh(x)\n    return 1 - tmp * tmp\n\n\ndef ranking(R):\n    \n    l = sorted(list(enumerate(R)), cmp=lambda a, b: cmp(a[1], b[1]))\n    l = sorted(list(enumerate(l)), cmp=lambda a, b: cmp(a[1], b[1]))\n    return array(map(lambda kv: kv[0], l))\n\n\ndef expln(x):\n    \n    def f(val):\n        if val < 0:\n            \n            return exp(val)\n        else:\n            \n            return log(val + 1.0) + 1\n    try:\n        result = array(map(f, x))\n    except TypeError:\n        result = array(f(x))\n\n    return result\n\n\ndef explnPrime(x):\n    \n    def f(val):\n        if val < 0:\n            \n            return exp(val)\n        else:\n            \n            return 1.0 / (val + 1.0)\n    try:\n        result = array(map(f, x))\n    except TypeError:\n        result = array(f(x))\n\n    return result\n\n\ndef multivariateNormalPdf(z, x, sigma):\n    \n    assert len(z.shape) == 1 and len(x.shape) == 1 and len(x) == len(z) and sigma.shape == (len(x), len(z))\n    tmp = -0.5 * dot(dot((z - x), inv(sigma)), (z - x))\n    res = (1. / power(2.0 * pi, len(z) / 2.)) * (1. / sqrt(det(sigma))) * exp(tmp)\n    return res\n\n\ndef simpleMultivariateNormalPdf(z, detFactorSigma):\n    \n    dim = len(z)\n    return exp(-0.5 * dot(z, z)) / (power(2.0 * pi, dim / 2.) * detFactorSigma)\n\n\ndef multivariateCauchy(mu, sigma, onlyDiagonal=True):\n    \n    if not onlyDiagonal:\n        u, s, d = svd(sigma)\n        coeffs = sqrt(s)\n    else:\n        coeffs = diag(sigma)\n    r = rand(len(mu))\n    res = coeffs * tan(pi * (r - 0.5))\n    if not onlyDiagonal:\n        res = dot(d, dot(res, u))\n    return res + mu\n\n\ndef approxChiFunction(dim):\n    \n    dim = float(dim)\n    return sqrt(dim) * (1 - 1 / (4 * dim) + 1 / (21 * dim ** 2))\n\n\ndef sqrtm(M):\n    \n    r = real_if_close(expm2(0.5 * logm(M)), 1e-8)\n    return (r + r.T) / 2\n\n",
        "summary": "The provided Python code defines a collection of mathematical and statistical functions, including activation functions like `semilinear` and `sigmoid`, their derivatives, probability density functions such as `multivariateNormalPdf`, and utility functions for generating random numbers from distributions like the Cauchy distribution. Additionally, it includes helper functions for safe exponential calculations and matrix operations."
    },
    {
        "code": "import os\nimport sys\nimport unittest\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nfrom opacus import PrivacyEngine\nfrom opacus.distributed import DifferentiallyPrivateDistributedDataParallel as DPDDP\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\nPRIVACY_ALPHAS = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n\n\ndef setup_and_get_device(rank, world_size, nonce=0):\n    \n    device = 0\n    if sys.platform == \"win32\":\n        \n        \n        \n        \n        init_method = \"file:///{your local file path}\"\n\n        \n        dist.init_process_group(\n            \"gloo\", init_method=init_method, rank=rank, world_size=world_size\n        )\n        device = rank\n    elif os.environ.get(\"SLURM_NTASKS\") is not None:\n        \n        os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n        os.environ[\"MASTER_PORT\"] = str(7440 + nonce)\n        local_rank = int(os.environ.get(\"SLURM_LOCALID\"))\n        dist.init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n\n        \n        device = local_rank\n    else:\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"12355\"\n\n        os.environ[\"RANK\"] = str(rank)\n        os.environ[\"WORLD_SIZE\"] = str(world_size)\n        dist.init_process_group(\n            init_method=\"env://\",\n            backend=\"nccl\",\n        )\n\n        \n        device = rank\n    return device\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size, weight, dp, noise_multiplier=0, max_grad_norm=1e8):\n    \n    torch.manual_seed(rank)\n    batch_size = 32\n    withdp = \"with\" + (\"out \" if not dp else \"\")\n    print(f\"Running basic DDP {withdp} differential privacy example on rank {rank}.\")\n\n    device = setup_and_get_device(rank, world_size)\n\n    \n    model = ToyModel().to(device)\n    print(f\"Initial weight: {model.net1.weight.data}\")\n\n    \n    \n    model.net1.bias.requires_grad = False\n    model.net2.bias.requires_grad = False\n    model.net2.weight.requires_grad = False\n\n    if dp:\n        ddp_model = DPDDP(model)\n        engine = PrivacyEngine(\n            ddp_model,\n            batch_size=batch_size,\n            sample_size=10 * batch_size,\n            alphas=PRIVACY_ALPHAS,\n            noise_multiplier=noise_multiplier,\n            max_grad_norm=[max_grad_norm],\n        )\n        engine.random_number_generator = engine._set_seed(0)\n    else:\n        ddp_model = DDP(model, device_ids=[device])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=1)\n    if dp:\n        engine.attach(optimizer)\n\n    optimizer.zero_grad()\n    labels = torch.randn(batch_size, 5).to(device)\n\n    outputs = ddp_model(torch.randn(batch_size, 10).to(device))\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    weight.copy_(model.net1.weight.data.cpu())\n\n    cleanup()\n\n\ndef demo_ddp_hook(rank, world_size, weight, dp, noise_multiplier, max_grad_norm):\n    torch.manual_seed(rank)\n    batch_size = 32\n    withdp = \"with\" + (\"out \" if not dp else \"\")\n    print(f\"Running DDP hook {withdp} differential privacy example on rank {rank}.\")\n\n    device = setup_and_get_device(rank, world_size, nonce=1)\n\n    \n    model = ToyModel().to(device)\n\n    model.net1.bias.requires_grad = False\n    model.net2.bias.requires_grad = False\n    model.net2.weight.requires_grad = False\n\n    ddp_model = DDP(model, device_ids=[device])\n\n    if dp:\n        engine = PrivacyEngine(\n            ddp_model,\n            batch_size=batch_size,\n            sample_size=10 * batch_size,\n            alphas=PRIVACY_ALPHAS,\n            noise_multiplier=noise_multiplier,\n            max_grad_norm=[max_grad_norm],\n        )\n        engine.random_number_generator = engine._set_seed(0)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=1)\n    if dp:\n        engine.attach(optimizer)\n\n    optimizer.zero_grad()\n    labels = torch.randn(batch_size, 5).to(device)\n\n    outputs = ddp_model(torch.randn(batch_size, 10).to(device))\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    weight.copy_(model.net1.weight.data.cpu())\n\n    del ddp_model\n    cleanup()\n\n\ndef add_remove_ddp_hooks(\n    rank, world_size, remaining_hooks, dp, noise_multiplier=0, max_grad_norm=1e8\n):\n    device = setup_and_get_device(rank, world_size, nonce=2)\n\n    model = ToyModel().to(device)\n    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[device])\n\n    engine = PrivacyEngine(\n        ddp_model,\n        batch_size=1,\n        sample_size=10,\n        alphas=PRIVACY_ALPHAS,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=[max_grad_norm],\n    )\n\n    optimizer = optim.SGD(ddp_model.parameters(), lr=1)\n\n    engine.attach(optimizer)\n\n    remaining_hooks[\"attached\"] = {\n        p: p._backward_hooks for p in engine.module.parameters() if p._backward_hooks\n    }\n    engine.detach()\n\n    remaining_hooks[\"detached\"] = {\n        p: p._backward_hooks for p in engine.module.parameters() if p._backward_hooks\n    }\n\n    cleanup()\n\n\ndef debug(rank, world_size, tensor, dp, noise_multiplier=0, max_grad_norm=1e8):\n    local_rank = setup_and_get_device(rank, world_size)\n    print(f\"Rank: {rank},World size: {world_size}, local_rank: {local_rank}\")\n    tensor = tensor.to(local_rank)\n    print(f\"dp: {dp}\")\n    print(tensor)\n\n    cleanup()\n\n\ndef run_function(local_function, tensor, dp, noise_multiplier=0, max_grad_norm=1e8):\n    if os.environ.get(\"SLURM_NTASKS\") is not None:\n        world_size = int(os.environ.get(\"SLURM_NTASKS\"))\n        rank = int(os.environ.get(\"SLURM_PROCID\"))\n        print(f\"Running on a Slurm cluster with {world_size} tasks.\")\n\n        local_function(rank, world_size, tensor, dp, noise_multiplier, max_grad_norm)\n    else:\n        world_size = torch.cuda.device_count()\n        print(f\"Spawning multiple processes on a local machine with {world_size} GPUs\")\n\n        \n        mp.spawn(\n            local_function,\n            args=(\n                world_size,\n                tensor,\n                dp,\n                noise_multiplier,\n                max_grad_norm,\n            ),\n            nprocs=world_size,\n            join=True,\n        )\n    return world_size\n\n\nclass GradientComputationTest(unittest.TestCase):\n    def test_connection(self):\n        tensor = torch.zeros(10, 10)\n        world_size = run_function(debug, tensor, dp=True)\n\n        self.assertTrue(\n            world_size >= 2, f\"Need at least 2 gpus but was provided only {world_size}.\"\n        )\n\n    def test_gradient_noclip_zeronoise(self):\n        \n        weight_dp, weight_nodp = torch.zeros(10, 10), torch.zeros(10, 10)\n\n        run_function(demo_basic, weight_dp, dp=True)\n        run_function(demo_basic, weight_nodp, dp=False)\n\n        self.assertTrue(torch.norm(weight_dp - weight_nodp) < 1e-7)\n\n    def test_ddp_hook(self):\n        \n        weight_ddp_naive, weight_ddp_hook = torch.zeros(10, 10), torch.zeros(10, 10)\n\n        run_function(\n            demo_basic,\n            weight_ddp_naive,\n            dp=True,\n            noise_multiplier=0.1,\n            max_grad_norm=1.0,\n        )\n\n        run_function(\n            demo_ddp_hook,\n            weight_ddp_hook,\n            dp=True,\n            noise_multiplier=0.1,\n            max_grad_norm=1.0,\n        )\n\n        self.assertTrue(\n            torch.norm(weight_ddp_naive - weight_ddp_hook) < 1e-7,\n            f\"DDP naive: {weight_ddp_naive}\\nDDP hook: {weight_ddp_hook}\",\n        )\n\n    def test_add_remove_ddp_hooks(self):\n\n        remaining_hooks = {\n            \"attached\": None,\n            \"detached\": None,\n        }\n\n        run_function(\n            add_remove_ddp_hooks,\n            remaining_hooks,\n            dp=True,\n            noise_multiplier=0.1,\n            max_grad_norm=1.0,\n        )\n\n        assert remaining_hooks[\"attached\"], \"There are no hooks.\"\n\n        assert not remaining_hooks[\n            \"detached\"\n        ], f\"Some hooks remain after .remove_hooks(): {remaining_hooks}\"\n",
        "summary": "This code is a unit test suite for testing various aspects of distributed data parallel (DDP) training with PyTorch, specifically using the `torch.distributed` and `torch.nn.parallel.DistributedDataParallel` modules. The tests cover:\n\n1. **Connection Test**: Ensures that multiple processes can communicate over the network.\n2. **Gradient Computation Test**: Compares the weights of a model trained with DDP (with noise) versus without DDP to ensure they converge similarly.\n3. **DDP Hook Test**: Verifies that adding and removing backward hooks in DDP does not affect the training process.\n4. **Add/Remove Hooks Test**: Ensures that hooks can be added and removed from a model during training.\n\nThe `run_function` is used to spawn multiple processes for distributed training, either on a local machine or through a Slurm cluster. Each test function calls this with different parameters to simulate various scenarios of DDP training.\n\nHere's a breakdown of the key components:\n\n- **demo_basic**: A simple training loop that trains a model using DDP (with noise) and without DDP.\n- **demo_ddp_hook**: Similar to `demo_basic`, but it also adds backward hooks during training.\n- **add_remove_ddp_hooks**: Demonstrates how to add and remove hooks from a DDP model.\n- **debug**: A utility function for debugging purposes, printing out the rank, world size, local rank, and tensor values.\n\nThe tests use PyTorch's `unittest` framework to assert conditions that must hold true for the tests to pass. If any assertion fails, the test will raise an error indicating what went wrong."
    },
    {
        "code": "import datetime\r\nimport imutils\r\nimport time\r\nimport cv2\r\n\r\ncamera = cv2.VideoCapture(0)\r\ntime.sleep(0.50)\r\nprint(\"Hello!\")\r\n\r\nfirstFrame = None\r\n\r\n\nwhile True:\r\n\t\n\r\n\t(grabbed, frame) = camera.read()\r\n\ttext = \"Unossssccupied\"\r\n\r\n\r\n\tif not grabbed:\r\n\t\tbreak\r\n\r\n\t\n\r\n\tframe = imutils.resize(frame, width=500)\r\n\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\tgray = cv2.GaussianBlur(gray, (21, 21), 0)\r\n\r\n\tif firstFrame is None:\r\n\t\tfirstFrame = gray\r\n\t\tcontinue\r\n\r\n    \n\tframeDelta = cv2.absdiff(firstFrame, gray)\r\n\tthresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\r\n\r\n\r\n\tthresh = cv2.dilate(thresh, None, iterations=2)\r\n\t(_, cnts, _) = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n\t\n\tfor c in cnts:\r\n\t\t\n\t\tif cv2.contourArea(c) < 500:\r\n\t\t\tcontinue\r\n\r\n\t\t\n\t\t\n\t\t(x, y, w, h) = cv2.boundingRect(c)\r\n\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\r\n\t\ttext = \"Occupied\"\r\n\r\n\r\n        \n\tcv2.putText(frame, \"Room Status: {}\".format(text), (10, 20),\r\n\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\r\n\tcv2.putText(frame, datetime.datetime.now().strftime(\"%A %d %B %Y %I:%M:%S%p\"),\r\n\t\t(10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\r\n\r\n\r\n\r\n\t\n\tcv2.imshow(\"Security Feed\", frame)\r\n\t\n\t\n\r\n\tkey = cv2.waitKey(1) & 0xFF\r\n\r\n\t\n\tif key == ord(\"q\"):\r\n\t\tbreak\r\n\r\n\r\ncamera.release()\r\ncv2.destroyAllWindows()\r\n",
        "summary": "The Python code captures video from a webcam, detects motion using background subtraction, and displays the status of occupancy in real-time. It also timestamps each frame to indicate when the video was recorded."
    },
    {
        "code": "import unittest\nfrom unittest import mock\n\nimport cgf_service_client\n\n\nclass UnitTest_CloudGemFramework_ServiceClient_service_client(unittest.TestCase):\n\n    def test_service_client_imports(self):\n        self.assertIsNotNone(cgf_service_client.Data)\n        self.assertIsNotNone(cgf_service_client.Path)\n        self.assertIsNotNone(cgf_service_client.HttpError)\n        self.assertIsNotNone(cgf_service_client.ClientError)\n        self.assertIsNotNone(cgf_service_client.NotFoundError)\n        self.assertIsNotNone(cgf_service_client.NotAllowedError)\n        self.assertIsNotNone(cgf_service_client.ServerError)\n\n\n    @mock.patch('cgf_service_client.Path')\n    def test_for_url(self, mock_Path):\n\n        client = cgf_service_client.for_url('http://example.com', A = 10, B = 20)\n\n        self.assertIs(client, mock_Path.return_value)\n        mock_Path.assert_called_once_with('http://example.com', A = 10, B = 20)\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script defines a unit test class `UnitTest_CloudGemFramework_ServiceClient_service_client` that tests the `cgf_service_client` module. It includes two methods: one to verify the import of various classes from the module and another to test the functionality of the `for_url` method using mocking to ensure it returns the expected result and is called with the correct parameters."
    },
    {
        "code": "import os\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"aspc.settings\")\n\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n",
        "summary": "The Python script sets the environment variable for Django settings and imports the WSGI application to serve a Django project."
    },
    {
        "code": "from datetime import datetime\nimport cv2\nimport re\nimport base64\nfrom flask import Flask, render_template, request, jsonify\nfrom flask_cors import CORS\nimport numpy as np\n\nfrom io import BytesIO\nfrom PIL import Image, ImageOps\nimport os,sys\nimport requests\nfrom graphpipe import remote\nfrom matplotlib import pylab as plt\n\n\napp = Flask(__name__)\nCORS(app) \n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST':\n        ans,t1,t2,t3 = get_answer(request)\n        return jsonify({'ans': ans, 't1': t1, 't2': t2, 't3': t3})\n    else:\n        return render_template('index.html')\n\ndef result(img):\n    img = img.reshape(1, 784)\n    img = img.astype(np.float32)\n    img = np.multiply(img, 1.0 / 255.0)\n    pred = remote.execute(\"http://localhost:9001\", img)\n    r = np.argmax(pred, axis=1)\n    pp = pred*100\n    top1 = str(np.argsort(-pp)[0][0])+ \" (\" +str(int(np.sort(-pp)[0][0]*-1))+\"%)\"\n    top2 = str(np.argsort(-pp)[0][1])+ \" (\" +str(int(np.sort(-pp)[0][1]*-1))+\"%)\"\n    top3 = str(np.argsort(-pp)[0][2])+ \" (\" +str(int(np.sort(-pp)[0][2]*-1))+\"%)\"\n\n    return r,top1,top2,top3\n\ndef get_answer(req):\n    img_str = re.search(r'base64,(.*)', req.form['img']).group(1)\n    nparr = np.fromstring(base64.b64decode(img_str), np.uint8)\n    img_src = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n    img_negaposi = 255 - img_src\n    img_gray = cv2.cvtColor(img_negaposi, cv2.COLOR_BGR2GRAY)\n    img_resize = cv2.resize(img_gray,(28,28))\n    cv2.imwrite(f\"images/{datetime.now().strftime('%s')}.jpg\",img_resize)\n    ans,t1,t2,t3 = result(img_resize)\n    return int(ans),t1,t2,t3\n\nif __name__ == \"__main__\":\n    app.run(debug=False, host='0.0.0.0', port=8001)\n",
        "summary": "This Python code sets up a Flask web application that processes image data sent via POST requests. It uses OpenCV to preprocess images, converts them into grayscale and resizes them to 28x28 pixels, then sends the processed image to a remote server for classification using GraphPipe. The response from the remote server includes the predicted class along with the top three most likely classes and their probabilities."
    },
    {
        "code": "import json\nimport pytest\n\nfrom buildtrigger.test.githubmock import get_github_trigger\nfrom buildtrigger.triggerutil import (SkipRequestException, ValidationRequestException,\n                                      InvalidPayloadException)\nfrom endpoints.building import PreparedBuild\nfrom util.morecollections import AttrDict\n\n@pytest.fixture\ndef github_trigger():\n  return get_github_trigger()\n\n\n@pytest.mark.parametrize('payload, expected_error, expected_message', [\n  ('{\"zen\": true}', SkipRequestException, \"\"),\n\n  ('{}', InvalidPayloadException, \"Missing 'repository' on request\"),\n  ('{\"repository\": \"foo\"}', InvalidPayloadException, \"Missing 'owner' on repository\"),\n\n  \n  (, None, None),\n\n  \n  (, SkipRequestException, ''),\n])\ndef test_handle_trigger_request(github_trigger, payload, expected_error, expected_message):\n  def get_payload():\n    return json.loads(payload)\n\n  request = AttrDict(dict(get_json=get_payload))\n\n  if expected_error is not None:\n    with pytest.raises(expected_error) as ipe:\n      github_trigger.handle_trigger_request(request)\n    assert str(ipe.value) == expected_message\n  else:\n    assert isinstance(github_trigger.handle_trigger_request(request), PreparedBuild)\n\n\n@pytest.mark.parametrize('dockerfile_path, contents', [\n  ('/Dockerfile', 'hello world'),\n  ('somesubdir/Dockerfile', 'hi universe'),\n  ('unknownpath', None),\n])\ndef test_load_dockerfile_contents(dockerfile_path, contents):\n  trigger = get_github_trigger(dockerfile_path)\n  assert trigger.load_dockerfile_contents() == contents\n\n\n@pytest.mark.parametrize('username, expected_response', [\n  ('unknownuser', None),\n  ('knownuser', {'html_url': 'https://bitbucket.org/knownuser', 'avatar_url': 'avatarurl'}),\n])\ndef test_lookup_user(username, expected_response, github_trigger):\n  assert github_trigger.lookup_user(username) == expected_response\n\n\ndef test_list_build_subdirs(github_trigger):\n  assert github_trigger.list_build_subdirs() == ['Dockerfile', 'somesubdir/Dockerfile']\n\n\ndef test_list_build_source_namespaces(github_trigger):\n  namespaces_expected = [\n    {\n      'personal': True,\n      'score': 1,\n      'avatar_url': 'avatarurl',\n      'id': 'knownuser',\n      'title': 'knownuser',\n      'url': 'https://bitbucket.org/knownuser',\n    },\n    {\n      'score': 0,\n      'title': 'someorg',\n      'personal': False,\n      'url': '',\n      'avatar_url': 'avatarurl',\n      'id': 'someorg'\n    }\n  ]\n\n  found = github_trigger.list_build_source_namespaces()\n  found.sort()\n\n  namespaces_expected.sort()\n  assert found == namespaces_expected\n",
        "summary": "The provided Python code is a test suite for a GitHub trigger implementation, utilizing the `pytest` framework. It includes tests for handling trigger requests, loading Dockerfile contents, looking up user information, listing build subdirectories, and listing build source namespaces. Each test case validates specific scenarios and expected outcomes to ensure the functionality of the GitHub trigger system."
    },
    {
        "code": "from QInstrument.lib import QInstrumentInterface\nfrom QInstrument.instruments.Opus.Opus import Opus\nfrom PyQt5.QtCore import (pyqtSlot, QTimer)\nimport logging\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\nclass QOpusWidget(QInstrumentInterface):\n\n    def __init__(self, *args, interval=None, **kwargs):\n        super().__init__(*args,\n                         uiFile='OpusWidget.ui',\n                         deviceClass=Opus,\n                         **kwargs)\n        self.interval = interval or 200\n        self.timer = QTimer()\n        self.connectSignals()\n        self.startPolling()\n\n    def connectSignals(self):\n        self.timer.timeout.connect(self.poll)\n        self.ui.PowerDial.valueChanged.connect(self.updatePower)\n        self.ui.Power.editingFinished.connect(self.updatePowerDial)\n        self.ui.PowerDial.valueChanged.connect(self.uncheck)\n        self.ui.SendPower.clicked.connect(self.check)\n        self.device.dataReady.connect(self.updateValues)\n        self.ui.Disable.clicked.connect(self.disable)\n\t\n    def startPolling(self):\n        if self.isEnabled():\n            self.timer.start(self.interval)\n        return self\n\n    def stopPolling(self):\n        self.timer.stop()\n\t\n    @pyqtSlot()\n    def poll(self):\n        self.device.send('POWER?')\n        self.device.send('CURRENT?')\n        self.device.send('STATUS?')\n\t\n    @pyqtSlot(int)\n    def updatePower(self, value):\n        self.ui.Power.setValue(value)\n\t\n    @pyqtSlot(str)\n    def updateValues(self, data):\n\n        if 'mW' in data:\n            numeric_filter = filter(str.isdigit, data)\n            p = float((int(\"\".join(numeric_filter))/10))\t\n            if p == 0.0:\n                self.ui.EnableSwitch.setChecked(False)\n            if p != 0.0:\n                self.ui.EnableSwitch.setChecked(True)\n            self.ui.ActualPower.setValue(p)\n        if '%' in data:\n            numeric_filter = filter(str.isdigit, data)\n            p = float((int(\"\".join(numeric_filter))/10))\n            self.ui.CurrentBox.setValue(p)\n\n    @pyqtSlot()\n    def check(self):\n        self.ui.sentCheck.setChecked(True)\n        a = self.ui.Power.value()\n        self.device.set_power(a)\n\n    @pyqtSlot()\n    def uncheck(self):\n        self.ui.sentCheck.setChecked(False)\n\n    @pyqtSlot()\n    def updatePowerDial(self):\n        value = self.ui.Power.value()\n        self.ui.PowerDial.setValue(int(value))\n\t\n    def disable(self):\n        self.device.send('OFF')\n\ndef main():\n    import sys\n    from PyQt5.QtWidgets import QApplication\n\n\n    app = QApplication(sys.argv)\n    widget = QOpusWidget()\n    widget.show()\n    sys.exit(app.exec_())\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The provided Python code defines a `QOpusWidget` class that extends `QInstrumentInterface`, creating a user interface for controlling an Opus device. It includes functionality to poll the device, update UI elements based on device data, and handle user interactions such as setting power levels and disabling the device. The main function initializes and displays the widget in a PyQt5 application."
    },
    {
        "code": "import FWCore.ParameterSet.Config as cms\n\nfrom DQM.EcalPreshowerMonitorModule.ESRawDataTask_cfi import *\nfrom DQM.EcalPreshowerMonitorModule.ESIntegrityTask_cfi import *\necalPreshowerIntegrityTask.DoLumiAnalysis = True\nfrom DQM.EcalPreshowerMonitorModule.ESFEDIntegrityTask_cfi import *\nfrom DQM.EcalPreshowerMonitorModule.ESOccupancyTask_cfi import *\nfrom DQM.EcalPreshowerMonitorModule.ESTrendTask_cfi import *\n\ndqmInfoES = cms.EDAnalyzer(\"DQMEventInfo\",\n    subSystemFolder = cms.untracked.string('EcalPreshower')\n)\n\nes_dqm_source_offline = cms.Sequence(ecalPreshowerRawDataTask*ecalPreshowerFEDIntegrityTask*ecalPreshowerIntegrityTask*ecalPreshowerOccupancyTask*ecalPreshowerTrendTask)\n",
        "summary": "The Python code configures a sequence of tasks for monitoring the Ecal Preshower system in offline DQM, including raw data analysis, FED integrity checks, integrity assessments, occupancy measurements, and trend tracking. It also sets up an event information analyzer for the Ecal Preshower subsystem."
    },
    {
        "code": "__all__ = ['ProgressCallback', 'no_bar', 'ShowGraphCallback', 'CSVLogger']\n\n\nfrom ..basics import *\n\n\n@docs\nclass ProgressCallback(Callback):\n    \"A `Callback` to handle the display of progress bars\"\n    run_after=Recorder\n\n    def begin_fit(self):\n        assert hasattr(self.learn, 'recorder')\n        if self.create_mbar: self.mbar = master_bar(list(range(self.n_epoch)))\n        if self.learn.logger != noop:\n            self.old_logger,self.learn.logger = self.logger,self._write_stats\n            self._write_stats(self.recorder.metric_names)\n        else: self.old_logger = noop\n\n    def begin_epoch(self):\n        if getattr(self, 'mbar', False): self.mbar.update(self.epoch)\n\n    def begin_train(self):    self._launch_pbar()\n    def begin_validate(self): self._launch_pbar()\n    def after_train(self):    self.pbar.on_iter_end()\n    def after_validate(self): self.pbar.on_iter_end()\n    def after_batch(self):\n        self.pbar.update(self.iter+1)\n        if hasattr(self, 'smooth_loss'): self.pbar.comment = f'{self.smooth_loss:.4f}'\n\n    def _launch_pbar(self):\n        self.pbar = progress_bar(self.dl, parent=getattr(self, 'mbar', None), leave=False)\n        self.pbar.update(0)\n\n    def after_fit(self):\n        if getattr(self, 'mbar', False):\n            self.mbar.on_iter_end()\n            delattr(self, 'mbar')\n        self.learn.logger = self.old_logger\n\n    def _write_stats(self, log):\n        if getattr(self, 'mbar', False): self.mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in log], table=True)\n\n    _docs = dict(begin_fit=\"Setup the master bar over the epochs\",\n                 begin_epoch=\"Update the master bar\",\n                 begin_train=\"Launch a progress bar over the training dataloader\",\n                 begin_validate=\"Launch a progress bar over the validation dataloader\",\n                 after_train=\"Close the progress bar over the training dataloader\",\n                 after_validate=\"Close the progress bar over the validation dataloader\",\n                 after_batch=\"Update the current progress bar\",\n                 after_fit=\"Close the master bar\")\n\ndefaults.callbacks = [TrainEvalCallback, Recorder, ProgressCallback]\n\n\n@patch\n@contextmanager\ndef no_bar(self:Learner):\n    \"Context manager that deactivates the use of progress bars\"\n    has_progress = hasattr(self, 'progress')\n    if has_progress: self.remove_cb(self.progress)\n    yield self\n    if has_progress: self.add_cb(ProgressCallback())\n\n\nclass ShowGraphCallback(Callback):\n    \"Update a graph of training and validation loss\"\n    run_after=ProgressCallback\n\n    def begin_fit(self):\n        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n        self.nb_batches = []\n        assert hasattr(self.learn, 'progress')\n\n    def after_train(self): self.nb_batches.append(self.train_iter)\n\n    def after_epoch(self):\n        \"Plot validation loss in the pbar graph\"\n        rec = self.learn.recorder\n        iters = range_of(rec.losses)\n        val_losses = [v[1] for v in rec.values]\n        x_bounds = (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses))\n        y_bounds = (0, max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n        self.progress.mbar.update_graph([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds)\n\n\nclass CSVLogger(Callback):\n    run_after=Recorder\n    \"Log the results displayed in `learn.path/fname`\"\n    def __init__(self, fname='history.csv', append=False):\n        self.fname,self.append = Path(fname),append\n\n    def read_log(self):\n        \"Convenience method to quickly access the log.\"\n        return pd.read_csv(self.path/self.fname)\n\n    def begin_fit(self):\n        \"Prepare file with metric names.\"\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        self.file = (self.path/self.fname).open('a' if self.append else 'w')\n        self.file.write(','.join(self.recorder.metric_names) + '\\n')\n        self.old_logger,self.learn.logger = self.logger,self._write_line\n\n    def _write_line(self, log):\n        \"Write a line with `log` and call the old logger.\"\n        self.file.write(','.join([str(t) for t in log]) + '\\n')\n        self.old_logger(log)\n\n    def after_fit(self):\n        \"Close the file and clean up.\"\n        self.file.close()\n        self.learn.logger = self.old_logger",
        "summary": "The provided Python code defines several callbacks for handling progress bars, logging, and graphing during model training in a machine learning context. The `ProgressCallback` manages the display of progress bars, while `ShowGraphCallback` updates a graph showing training and validation loss. The `CSVLogger` logs the results to a CSV file. Additionally, there's a context manager `no_bar` that deactivates progress bars during execution."
    },
    {
        "code": "import simplejson\nimport binascii\nimport sys\nimport pdb\nfrom pprint import pprint\nimport sys\nimport os\nsys.path.append(os.path.normpath(os.path.join(os.path.dirname(__file__), '../lib')))\nimport gentariumlib\n\nusage = \"%s <hex>\" % sys.argv[0]\n\nobj = None\nif len(sys.argv) < 2:\n    print(usage)\n    sys.exit(1)\nelse:\n    obj = gentariumlib.deserialise(sys.argv[1])\n\npdb.set_trace()\n1\n",
        "summary": "The Python script imports necessary libraries for handling JSON, binary data, and system operations. It defines a usage message and checks if the correct number of command-line arguments is provided. If not, it prints the usage message and exits. Otherwise, it attempts to deserialize an object using a library function and then enters a debugger session."
    },
    {
        "code": "from bouncingball import BouncingBall, BouncingBox\n\nballs = []\nboxes = []\n\ndef setup():\n    size(600, 600)\n    for _ in range(60):\n        if random(10) < 5:\n            balls.append(BouncingBall())\n        else:\n            boxes.append(BouncingBox())\n\ndef draw():\n    background(\"\n    for ball in balls:\n        ball.move()\n        ball.display()\n    for box in boxes:\n        box.move()\n        box.display()\n",
        "summary": "The Python code initializes a window and populates it with either bouncing balls or boxes based on a random selection. It then continuously updates the positions of these objects and redraws them on the screen."
    },
    {
        "code": "import pyglet\n\n\ndef debug_print(enabled_or_option='debug'):\n    \n    if isinstance(enabled_or_option, bool):\n        enabled = enabled_or_option\n    else:\n        enabled = pyglet.options.get(enabled_or_option, False)\n\n    if enabled:\n        def _debug_print(*args, **kwargs):\n            print(*args, **kwargs)\n            return True\n\n    else:\n        def _debug_print(*args, **kwargs):\n            return True\n\n    return _debug_print\n\n",
        "summary": "The `debug_print` function in Python checks if debugging is enabled based on the input parameter. If debugging is enabled, it defines a `_debug_print` function that prints arguments and returns `True`. If debugging is not enabled, it defines a `_debug_print` function that simply returns `True` without printing anything."
    },
    {
        "code": "import abc\n\nfrom backend.model.SentenceTokenise import SentenceTokenise\nfrom backend.service.ExtractSentences import extract_sentences\nfrom backend.service.ReadCorpus import read_corpus\n\n\nclass Corpus:\n    def __init__(self):\n        self.receive_text = \"\"\n        self.input_file = \"t1_biology_0_0.txt\"\n        self.base_train_folder = \"../data/source_txt/train/\"\n        pass\n\n    sentences = SentenceTokenise()\n\n    @abc.abstractmethod\n    def getInputText(self):\n        \n        Corpus.receivedText = read_corpus(self.base_train_folder, self.input_file)\n        return Corpus.receivedText\n\n    def getSentences(self, text):\n        \n        self.sentences.listOfSentence = extract_sentences(text)\n\n        return self.sentences.listOfSentence\n\n    def setInputText(self, text):\n        pass\n",
        "summary": "The provided Python code defines an abstract base class `Corpus` with methods for reading a corpus file, extracting sentences from the text, and setting input text. It uses helper classes `SentenceTokenise`, `extract_sentences`, and `read_corpus` to perform these tasks. The `getInputText` method is abstract and must be implemented by subclasses to provide specific functionality for reading the corpus file."
    },
    {
        "code": "sys.stdout = open(\"6-num.txt\", \"w\")\ndata = \"1234567890\"\nfor a in data:\n\tfor b in data:\n\t\tfor c in data:\n\t\t\tfor d in data:\n\t\t\t\tfor e in data:\n\t\t\t\t\tfor f in data:\n \t\t\t\t\t\tprint(a+b+c+d+e+f)\nsys.stdout.close()\n",
        "summary": "The Python code generates all possible combinations of six digits from 0 to 9 and writes them to a file named \"6-num.txt\". Each combination is printed on a new line."
    },
    {
        "code": "import os\n\nfrom dotenv import load_dotenv\n\n\nload_dotenv()\n\n\nHOSTING_ENV = os.getenv(\"HOSTING_ENV\", \"production\")\n\nif HOSTING_ENV == \"dev\":\n    from .dev import *\nelse:\n    from .production import *\n",
        "summary": "The Python script loads environment variables using the `dotenv` library and determines the current hosting environment by checking the `HOSTING_ENV` variable. Depending on whether the environment is development or production, it imports settings from either a `dev.py` or `production.py` module respectively."
    },
    {
        "code": "__author__ = \"caoweiquan322\"\r\n__copyright__ = \"Copyright (C) 2014 TopCoder Inc. All rights reserved.\"\r\n__version__ = \"1.0\"\r\n\r\n\n\r\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code includes metadata about the author, copyright holder, and version of a module or script, specifically stating it was created by caoweiquan322 for TopCoder Inc., copyrighted in 2014, and is at version 1.0."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations on the data are not specified in the summary."
    },
    {
        "code": "import tensorflow as tf\nimport common\n\ndef inference(x, num_output, wd, dropout_rate, is_training, transfer_mode= False, model_type= 'A'):\n   \n   if model_type == 'A':\n      config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n   elif model_type == 'B':\n      config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n   elif model_type == 'D':\n      config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n   elif model_type == 'E':\n      config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n   else:\n      print('Unknown model type: ' + model_type + ' | Please specify a modelType A or B or D or E')\n   \n   network= x\n\n   for k,v in enumerate(config):\n     if v == 'M':\n       network= common.maxPool(network, 2, 2)\n     else:  \n       with tf.variable_scope('conv'+str(k)):\n         network = common.spatialConvolution(network, 3, 1, v, wd= wd)\n         network = tf.nn.relu(network)\n\n   network= common.flatten(network)\n\n   with tf.variable_scope('fc1'): \n     network = common.fullyConnected(network, 4096, wd= wd)\n     network = tf.nn.relu(network)\n     network = common.batchNormalization(network, is_training= is_training)\n     network = tf.nn.dropout(network, dropout_rate)\n   with tf.variable_scope('fc2'):\n     network = common.fullyConnected(network, 4096, wd= wd)\n     network = tf.nn.relu(network)\n     network = common.batchNormalization(network, is_training= is_training)\n     network = tf.nn.dropout(network, dropout_rate)\n   if not transfer_mode:\n     with tf.variable_scope('output'):\n       network = common.fullyConnected(network, num_output, wd= wd)\n   else:\n     with tf.variable_scope('transfer_output'):\n       network = common.fullyConnected(network, num_output, wd= wd)\n\n   return network\n",
        "summary": "The provided Python code defines a function `inference` that constructs a convolutional neural network (CNN) model using TensorFlow. The function takes various parameters including input tensor `x`, number of output classes `num_output`, weight decay `wd`, dropout rate, training flag, transfer learning mode, and model type. Depending on the model type specified ('A', 'B', 'D', or 'E'), it sets up a different configuration for convolutional layers, followed by max pooling, fully connected layers with ReLU activation, batch normalization, and dropout. The output layer is defined differently based on whether transfer learning mode is enabled."
    },
    {
        "code": "import argparse\nimport io\nimport sys\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Mixing wav.scp files into a multi-channel wav.scp \" \"using sox.\",\n    )\n    parser.add_argument(\"scp\", type=str, nargs=\"+\", help=\"Give wav.scp\")\n    parser.add_argument(\n        \"out\",\n        nargs=\"?\",\n        type=argparse.FileType(\"w\"),\n        default=sys.stdout,\n        help=\"The output filename. \" \"If omitted, then output to sys.stdout\",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    fscps = [io.open(scp, \"r\", encoding=\"utf-8\") for scp in args.scp]\n    for linenum, lines in enumerate(zip_longest(*fscps)):\n        keys = []\n        wavs = []\n\n        for line, scp in zip(lines, args.scp):\n            if line is None:\n                raise RuntimeError(\"Numbers of line mismatch\")\n\n            sps = line.split(\" \", 1)\n            if len(sps) != 2:\n                raise RuntimeError(\n                    'Invalid line is found: {}, line {}: \"{}\" '.format(\n                        scp, linenum, line\n                    )\n                )\n            key, wav = sps\n            keys.append(key)\n            wavs.append(wav.strip())\n\n        if not all(k == keys[0] for k in keys):\n            raise RuntimeError(\n                \"The ids mismatch. Hint; the input files must be \"\n                \"sorted and must have same ids: {}\".format(keys)\n            )\n\n        args.out.write(\n            \"{} sox -M {} -c {} -t wav - |\\n\".format(\n                keys[0], \" \".join(\"{}\".format(w) for w in wavs), len(fscps)\n            )\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script uses the `argparse` module to parse command-line arguments, specifically for merging multiple `.wav.scp` files into a multi-channel WAV file using the `sox` tool. It reads input files, checks for consistency in keys and line numbers, and then constructs a command to merge the audio files accordingly, outputting the result either to a specified file or standard output."
    },
    {
        "code": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\npoints=np.loadtxt('points.txt')\nherring_r = np.loadtxt('distribution.txt')\nherring=np.zeros((802,350))\nfor i in range(350):\n    for j in range(802):\n        herring[j,349-i]=herring_r[i,j]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns=0\nfor i in range(802):\n    for j in range(350):\n        if herring[i,j]>0:\n            s+=herring[i,j]\n\nprint(s)\n\n\n",
        "summary": "The Python code loads data from 'points.txt' and 'distribution.txt', processes the distribution data to create a new array 'herring', and calculates the sum of all positive values in 'herring'."
    },
    {
        "code": "from __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom builtins import *  \nfrom future import standard_library\nstandard_library.install_aliases()  \n\nfrom cached_property import cached_property\nfrom chainer import functions as F\n\nfrom chainerrl.action_value import ActionValue\n\n\nclass BranchedActionValue(ActionValue):\n    \n\n    def __init__(self, branches, q_values_formatter=lambda x: x):\n        self.branches = branches\n        self.q_values_formatter = q_values_formatter\n\n    @cached_property\n    def greedy_actions(self):\n        actions = []\n\n        for branch in self.branches:\n            actions.append(branch.q_values.array.argmax(axis=1).reshape(-1, 1))\n\n        return F.hstack(actions)\n\n    @cached_property\n    def max(self):\n        chosen_q_values = []\n\n        for branch in self.branches:\n            chosen_q_values.append(branch.max.reshape(-1, 1))\n\n        return F.hstack(chosen_q_values)\n\n    def evaluate_actions(self, actions):\n        branch_q_values = []\n\n        for i, branch in enumerate(self.branches):\n            branch_actions = actions[:, i]\n            branch_q_values.append(branch.evaluate_actions(\n                branch_actions).reshape(-1, 1))\n\n        return F.hstack(branch_q_values)\n\n    @property\n    def params(self):\n        branch_params = []\n\n        for branch in self.branches:\n            branch_params.extend(list(branch.params))\n\n        return tuple(branch_params)\n",
        "summary": "The provided Python code defines a class `BranchedActionValue` that extends the functionality of an action value system in a machine learning context, specifically within the Chainer framework. This class manages multiple branches of action values, each potentially representing different actions or policies, and provides methods to compute greedy actions, maximum q-values, evaluate specific actions, and retrieve parameters across all branches."
    },
    {
        "code": "from vlasisku.utils import compound2affixes\n\n\nclass Entry(object):\n    \n\n    \n    word = None\n\n    \n    type = None\n\n    \n    affixes = None\n\n    \n    searchaffixes = None\n\n    \n    grammarclass = None\n\n    \n    terminator = None\n\n    \n    terminates = None\n\n    \n    cll = None\n\n    \n    definition = None\n\n    \n    notes = None\n\n    \n    textdefinition = None\n\n    \n    textnotes = None\n\n    \n    db = None\n\n    \n    def __init__(self, db):\n        self.affixes = []\n        self.searchaffixes = []\n        self.terminates = []\n        self.cll = []\n        self.db = db\n\n    def __str__(self):\n        return self.word\n\n    def __repr__(self):\n        return '<Entry %s>' % self.word\n\n    def components(self):\n        \n        if self.type == 'lujvo':\n            components = ''\n            for a in compound2affixes(self.word):\n                if len(a) == 1:\n                    components += a\n                else:\n                    word = [e for e in self.db.entries.values()\n                              if a in e.searchaffixes]\n                    if word:\n                        components += '<a href=\"%s\" ' % word[0]\n                        components += 'title=\"<strong>%s:</strong> ' % word[0]\n                        components += '%s\">%s</a>' % (word[0].definition, a)\n                    else:\n                        components += a\n            return components\n\n\nclass Gloss(object):\n    \n\n    \n    gloss = None\n\n    \n    entry = None\n\n    \n    sense = None\n\n    \n    place = None\n\n    def __str__(self):\n        return self.entry.word\n",
        "summary": "The provided Python code defines two classes, `Entry` and `Gloss`. The `Entry` class represents a dictionary entry with various attributes such as word type, affixes, and definitions. It includes methods for initializing the object, converting it to a string, and generating components based on its type. The `Gloss` class represents a glossary entry linked to an `Entry`, containing attributes like gloss text, sense, and place in the dictionary."
    },
    {
        "code": "from data import DataHandler\nfrom models import ACRegNet\nimport tensorflow as tf\nfrom utils import get_random_batch, read_config_file, create_dir\n\n\nRUN_IN_GPU = False\n\n\ndef train_acregnet_model(config):\n    tf.reset_default_graph()\n    tf_config = tf.ConfigProto()\n\n    if RUN_IN_GPU:\n        tf_config.gpu_options.allow_growth = True\n\n    sess = tf.Session(config=tf_config)\n\n    train_ims, _ = DataHandler.load_images(config['train_ims_file'])\n    train_lbs, _ = DataHandler.load_labels(config['train_lbs_file'])\n    print('Loading training data...done')\n\n    acregnet = ACRegNet(sess, config, 'ACRegNet', is_train=True)\n    print('Building AC-RegNet model...done')\n\n    print('Training...')\n    for i in range(config['iterations']):\n        batch_ims_x, batch_ims_y, batch_lbs_x, batch_lbs_y = get_random_batch(\n            train_ims, config['batch_size'], train_lbs)\n        cur_loss = acregnet.fit(\n            batch_ims_x, batch_ims_y, batch_lbs_x, batch_lbs_y)\n        print('Iteration {:>8d}/{}: Loss: {}'.format(\n            i + 1, config['iterations'], cur_loss))\n\n    acregnet.save(config['ckpt_dir'])\n    print('Saving current AC-RegNet model...done')\n\n    print('Training...done')\n\n    tf.reset_default_graph()\n    sess.close()\n\n\nif __name__ == \"__main__\":\n    config = read_config_file('./config/JSRT/ACRegNet.cfg')\n    create_dir(config['ckpt_dir'])\n    train_acregnet_model(config)\n",
        "summary": "The provided Python script defines a function `train_acregnet_model` that trains an AC-RegNet model using TensorFlow. It loads training data, builds the model, and iteratively trains it with random batches of images and labels, printing loss after each iteration. After training, it saves the model to a specified directory. The script also includes utility functions for reading configuration files and creating directories."
    },
    {
        "code": "import unittest\n\nfrom gr_nlp_toolkit.labels.dp_labels import dp_labels\nfrom gr_nlp_toolkit.labels.ner_labels import ner_labels\nfrom gr_nlp_toolkit.labels.pos_labels import pos_labels, pos_properties\nfrom gr_nlp_toolkit.pipeline.pipeline import Pipeline\n\n\nclass TestPipeline(unittest.TestCase):\n    def test_using_all_processors(self):\n        nlp = Pipeline('dp,pos,ner')\n\n        sentences = [\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\",\n                     \"\u03a4\u03bf \u03c0\u03bf\u03b9\u03b7\u03bc\u03b1\u03c4\u03ac\u03ba\u03b9 \u03c4\u03bf \u03ad\u03b3\u03c1\u03b1\u03c8\u03b5 \u03bf \u03b4\u03b9\u03ac\u03c3\u03b7\u03bc\u03bf\u03c2 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2, \u039d\u03af\u03ba\u03bf\u03c2 \u039d\u03b9\u03ba\u03bf\u03bb\u03b1\u03ca\u03b4\u03b7\u03c2\"]\n        for sent in sentences:\n            doc = nlp(sent)\n\n            for token in doc.tokens:\n                print(token.text, token.ner, token.upos, token.feats, token.head, token.deprel)\n                self.assertIsNotNone(token.ner)\n                self.assertTrue(token.ner in ner_labels)\n                self.assertIsNotNone(token.head)\n                self.assertIsNotNone(token.deprel)\n                \n                self.assertTrue(token.head in range(0, len(doc.tokens) + 1))\n                self.assertTrue(token.deprel in dp_labels)\n                self.assertIsNotNone(token.upos)\n                self.assertTrue(token.upos in pos_labels['upos'])\n\n                self.assertIsNotNone(token.feats)\n                self.assertEqual(len(list(token.feats.keys())), len(pos_properties[token.upos]))\n\n                for feat, value in token.feats.items():\n                    self.assertTrue(feat in pos_properties[token.upos])\n                    self.assertTrue(value in pos_labels[feat])\n                    print(token.text, token.ner, token.upos, token.feats, token.head, token.deprel)\n                    self.assertIsNotNone(token.ner)\n                    self.assertTrue(token.ner in ner_labels)\n                    self.assertIsNotNone(token.head)\n                    self.assertIsNotNone(token.deprel)\n                    \n                    self.assertTrue(token.head in range(0, len(doc.tokens) + 1))\n                    self.assertTrue(token.deprel in dp_labels)\n                    self.assertIsNotNone(token.upos)\n                    self.assertTrue(token.upos in pos_labels['upos'])\n\n    def test_annotations_are_same_with_multiple_configurations(self):\n        nlp = Pipeline('dp,pos,ner')\n        doc = nlp(\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\")\n\n        deprels_preds = []\n        upos_preds = []\n        ner_preds = []\n        for token in doc.tokens:\n            deprels_preds.append(token.deprel)\n            upos_preds.append(token.upos)\n            ner_preds.append(token.ner)\n\n        nlp = Pipeline('dp')\n        doc = nlp(\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\")\n        new_deprels_preds = []\n\n        for token in doc.tokens:\n            new_deprels_preds.append(token.deprel)\n\n        nlp = Pipeline('pos')\n        doc = nlp(\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\")\n        new_upos_preds =[]\n\n        for token in doc.tokens:\n            new_upos_preds.append(token.upos)\n\n        nlp = Pipeline('ner')\n        doc = nlp(\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\")\n        new_ner_preds =[]\n        for token in doc.tokens:\n            new_ner_preds.append(token.ner)\n\n        self.assertEqual(new_deprels_preds, deprels_preds)\n        self.assertEqual(new_upos_preds, upos_preds)\n        self.assertEqual(new_ner_preds, ner_preds)\n\n\n\n    def test_using_only_one_processor(self):\n        nlp = Pipeline('ner')\n        doc = nlp(\"\u0397 \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf\u03c5 Euro \u03c4\u03bf 2021\")\n\n        for token in doc.tokens:\n            self.assertIsNotNone(token.ner)\n            self.assertTrue(token.ner in ner_labels)\n            self.assertIsNone(token.head)\n            self.assertIsNone(token.deprel)\n            self.assertFalse(token.head in range(0, len(doc.tokens)))\n            self.assertFalse(token.deprel in dp_labels)\n            self.assertIsNone(token.upos)\n            self.assertFalse(token.upos in pos_labels['upos'])\n\n            for feat, value in token.feats.items():\n                self.assertFalse(feat in pos_properties[token.upos])\n                self.assertFalse(value in pos_labels[feat])\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "The provided Python code defines a series of unit tests for a natural language processing pipeline using the `unittest` framework. The tests validate the functionality of different processors (dependency parsing, part-of-speech tagging, and named entity recognition) within the pipeline, ensuring that annotations are consistent across configurations and meet expected label sets."
    },
    {
        "code": "from ...ucsmo import ManagedObject\nfrom ...ucscoremeta import MoPropertyMeta, MoMeta\nfrom ...ucsmeta import VersionMeta\n\n\nclass InitiatorFcInitiatorEpConsts:\n    PREF_ALTERNATE = \"alternate\"\n    PREF_PREFERRED = \"preferred\"\n    PROT_DERIVED = \"derived\"\n    PROT_FC = \"fc\"\n    PROT_ISCSI = \"iscsi\"\n\n\nclass InitiatorFcInitiatorEp(ManagedObject):\n    \n\n    consts = InitiatorFcInitiatorEpConsts()\n    naming_props = set([u'name'])\n\n    mo_meta = MoMeta(\"InitiatorFcInitiatorEp\", \"initiatorFcInitiatorEp\", \"fc-ini-[name]\", VersionMeta.Version211a, \"InputOutput\", 0x3f, [], [\"read-only\"], [u'initiatorGroupEp'], [u'storageEpUser'], [None])\n\n    prop_meta = {\n        \"child_action\": MoPropertyMeta(\"child_action\", \"childAction\", \"string\", VersionMeta.Version211a, MoPropertyMeta.INTERNAL, 0x2, None, None, r, [], []), \n        \"dn\": MoPropertyMeta(\"dn\", \"dn\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, 0x4, 0, 256, None, [], []), \n        \"ep_dn\": MoPropertyMeta(\"ep_dn\", \"epDn\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, None, 0, 256, None, [], []), \n        \"id\": MoPropertyMeta(\"id\", \"id\", \"ulong\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, None, None, None, None, [], []), \n        \"name\": MoPropertyMeta(\"name\", \"name\", \"string\", VersionMeta.Version211a, MoPropertyMeta.NAMING, 0x8, None, None, r, [], []), \n        \"pref\": MoPropertyMeta(\"pref\", \"pref\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, None, None, None, None, [\"alternate\", \"preferred\"], []), \n        \"prot\": MoPropertyMeta(\"prot\", \"prot\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, None, None, None, None, [\"derived\", \"fc\", \"iscsi\"], []), \n        \"rn\": MoPropertyMeta(\"rn\", \"rn\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, 0x10, 0, 256, None, [], []), \n        \"sacl\": MoPropertyMeta(\"sacl\", \"sacl\", \"string\", VersionMeta.Version302c, MoPropertyMeta.READ_ONLY, None, None, None, r, [], []), \n        \"status\": MoPropertyMeta(\"status\", \"status\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_WRITE, 0x20, None, None, r, [], []), \n        \"wwpn\": MoPropertyMeta(\"wwpn\", \"wwpn\", \"string\", VersionMeta.Version211a, MoPropertyMeta.READ_ONLY, None, 0, 256, r, [], []), \n    }\n\n    prop_map = {\n        \"childAction\": \"child_action\", \n        \"dn\": \"dn\", \n        \"epDn\": \"ep_dn\", \n        \"id\": \"id\", \n        \"name\": \"name\", \n        \"pref\": \"pref\", \n        \"prot\": \"prot\", \n        \"rn\": \"rn\", \n        \"sacl\": \"sacl\", \n        \"status\": \"status\", \n        \"wwpn\": \"wwpn\", \n    }\n\n    def __init__(self, parent_mo_or_dn, name, **kwargs):\n        self._dirty_mask = 0\n        self.name = name\n        self.child_action = None\n        self.ep_dn = None\n        self.id = None\n        self.pref = None\n        self.prot = None\n        self.sacl = None\n        self.status = None\n        self.wwpn = None\n\n        ManagedObject.__init__(self, \"InitiatorFcInitiatorEp\", parent_mo_or_dn, **kwargs)\n",
        "summary": "The `InitiatorFcInitiatorEp` class extends `ManagedObject` and represents a Fibre Channel initiator endpoint in a network device, with properties such as name, preference, protocol, and WWPN. It includes constants for different preferences and protocols, and manages its state through methods inherited from `ManagedObject`."
    },
    {
        "code": "import argparse\nimport sklearn.metrics as metrics\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport MinkowskiEngine as ME\nfrom examples.pointnet import (\n    PointNet,\n    MinkowskiPointNet,\n    CoordinateTransformation,\n    ModelNet40H5,\n    stack_collate_fn,\n    minkowski_collate_fn,\n)\nfrom examples.common import seed_all\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--voxel_size\", type=float, default=0.05)\nparser.add_argument(\"--max_steps\", type=int, default=100000)\nparser.add_argument(\"--val_freq\", type=int, default=1000)\nparser.add_argument(\"--batch_size\", default=32, type=int)\nparser.add_argument(\"--lr\", default=1e-1, type=float)\nparser.add_argument(\"--weight_decay\", type=float, default=1e-4)\nparser.add_argument(\"--num_workers\", type=int, default=2)\nparser.add_argument(\"--stat_freq\", type=int, default=100)\nparser.add_argument(\"--weights\", type=str, default=\"modelnet.pth\")\nparser.add_argument(\"--seed\", type=int, default=777)\nparser.add_argument(\"--translation\", type=float, default=0.2)\nparser.add_argument(\"--test_translation\", type=float, default=0.0)\nparser.add_argument(\n    \"--network\",\n    type=str,\n    choices=[\"pointnet\", \"minkpointnet\", \"minkfcnn\", \"minksplatfcnn\"],\n    default=\"minkfcnn\",\n)\n\n\nclass MinkowskiFCNN(ME.MinkowskiNetwork):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        embedding_channel=1024,\n        channels=(32, 48, 64, 96, 128),\n        D=3,\n    ):\n        ME.MinkowskiNetwork.__init__(self, D)\n\n        self.network_initialization(\n            in_channel,\n            out_channel,\n            channels=channels,\n            embedding_channel=embedding_channel,\n            kernel_size=3,\n            D=D,\n        )\n        self.weight_initialization()\n\n    def get_mlp_block(self, in_channel, out_channel):\n        return nn.Sequential(\n            ME.MinkowskiLinear(in_channel, out_channel, bias=False),\n            ME.MinkowskiBatchNorm(out_channel),\n            ME.MinkowskiLeakyReLU(),\n        )\n\n    def get_conv_block(self, in_channel, out_channel, kernel_size, stride):\n        return nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channel,\n                out_channel,\n                kernel_size=kernel_size,\n                stride=stride,\n                dimension=self.D,\n            ),\n            ME.MinkowskiBatchNorm(out_channel),\n            ME.MinkowskiLeakyReLU(),\n        )\n\n    def network_initialization(\n        self,\n        in_channel,\n        out_channel,\n        channels,\n        embedding_channel,\n        kernel_size,\n        D=3,\n    ):\n        self.mlp1 = self.get_mlp_block(in_channel, channels[0])\n        self.conv1 = self.get_conv_block(\n            channels[0],\n            channels[1],\n            kernel_size=kernel_size,\n            stride=1,\n        )\n        self.conv2 = self.get_conv_block(\n            channels[1],\n            channels[2],\n            kernel_size=kernel_size,\n            stride=2,\n        )\n\n        self.conv3 = self.get_conv_block(\n            channels[2],\n            channels[3],\n            kernel_size=kernel_size,\n            stride=2,\n        )\n\n        self.conv4 = self.get_conv_block(\n            channels[3],\n            channels[4],\n            kernel_size=kernel_size,\n            stride=2,\n        )\n        self.conv5 = nn.Sequential(\n            self.get_conv_block(\n                channels[1] + channels[2] + channels[3] + channels[4],\n                embedding_channel // 4,\n                kernel_size=3,\n                stride=2,\n            ),\n            self.get_conv_block(\n                embedding_channel // 4,\n                embedding_channel // 2,\n                kernel_size=3,\n                stride=2,\n            ),\n            self.get_conv_block(\n                embedding_channel // 2,\n                embedding_channel,\n                kernel_size=3,\n                stride=2,\n            ),\n        )\n\n        self.pool = ME.MinkowskiMaxPooling(kernel_size=3, stride=2, dimension=D)\n\n        self.global_max_pool = ME.MinkowskiGlobalMaxPooling()\n        self.global_avg_pool = ME.MinkowskiGlobalAvgPooling()\n\n        self.final = nn.Sequential(\n            self.get_mlp_block(embedding_channel * 2, 512),\n            ME.MinkowskiDropout(),\n            self.get_mlp_block(512, 512),\n            ME.MinkowskiLinear(512, out_channel, bias=True),\n        )\n\n        \n\n    def weight_initialization(self):\n        for m in self.modules():\n            if isinstance(m, ME.MinkowskiConvolution):\n                ME.utils.kaiming_normal_(m.kernel, mode=\"fan_out\", nonlinearity=\"relu\")\n\n            if isinstance(m, ME.MinkowskiBatchNorm):\n                nn.init.constant_(m.bn.weight, 1)\n                nn.init.constant_(m.bn.bias, 0)\n\n    def forward(self, x: ME.TensorField):\n        x = self.mlp1(x)\n        y = x.sparse()\n\n        y = self.conv1(y)\n        y1 = self.pool(y)\n\n        y = self.conv2(y1)\n        y2 = self.pool(y)\n\n        y = self.conv3(y2)\n        y3 = self.pool(y)\n\n        y = self.conv4(y3)\n        y4 = self.pool(y)\n\n        x1 = y1.slice(x)\n        x2 = y2.slice(x)\n        x3 = y3.slice(x)\n        x4 = y4.slice(x)\n\n        x = ME.cat(x1, x2, x3, x4)\n\n        y = self.conv5(x.sparse())\n        x1 = self.global_max_pool(y)\n        x2 = self.global_avg_pool(y)\n\n        return self.final(ME.cat(x1, x2)).F\n\n\nclass GlobalMaxAvgPool(torch.nn.Module):\n    def __init__(self):\n        torch.nn.Module.__init__(self)\n        self.global_max_pool = ME.MinkowskiGlobalMaxPooling()\n        self.global_avg_pool = ME.MinkowskiGlobalAvgPooling()\n\n    def forward(self, tensor):\n        x = self.global_max_pool(tensor)\n        y = self.global_avg_pool(tensor)\n        return ME.cat(x, y)\n\n\nclass MinkowskiSplatFCNN(MinkowskiFCNN):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        embedding_channel=1024,\n        channels=(32, 48, 64, 96, 128),\n        D=3,\n    ):\n        MinkowskiFCNN.__init__(\n            self, in_channel, out_channel, embedding_channel, channels, D\n        )\n\n    def forward(self, x: ME.TensorField):\n        x = self.mlp1(x)\n        y = x.splat()\n\n        y = self.conv1(y)\n        y1 = self.pool(y)\n\n        y = self.conv2(y1)\n        y2 = self.pool(y)\n\n        y = self.conv3(y2)\n        y3 = self.pool(y)\n\n        y = self.conv4(y3)\n        y4 = self.pool(y)\n\n        x1 = y1.interpolate(x)\n        x2 = y2.interpolate(x)\n        x3 = y3.interpolate(x)\n        x4 = y4.interpolate(x)\n\n        x = ME.cat(x1, x2, x3, x4)\n        y = self.conv5(x.sparse())\n\n        x1 = self.global_max_pool(y)\n        x2 = self.global_avg_pool(y)\n\n        return self.final(ME.cat(x1, x2)).F\n\n\nSTR2NETWORK = dict(\n    pointnet=PointNet,\n    minkpointnet=MinkowskiPointNet,\n    minkfcnn=MinkowskiFCNN,\n    minksplatfcnn=MinkowskiSplatFCNN,\n)\n\n\ndef create_input_batch(batch, is_minknet, device=\"cuda\", quantization_size=0.05):\n    if is_minknet:\n        batch[\"coordinates\"][:, 1:] = batch[\"coordinates\"][:, 1:] / quantization_size\n        return ME.TensorField(\n            coordinates=batch[\"coordinates\"],\n            features=batch[\"features\"],\n            device=device,\n        )\n    else:\n        return batch[\"coordinates\"].permute(0, 2, 1).to(device)\n\n\nclass CoordinateTranslation:\n    def __init__(self, translation):\n        self.trans = translation\n\n    def __call__(self, coords):\n        if self.trans > 0:\n            coords += np.random.uniform(low=-self.trans, high=self.trans, size=[1, 3])\n        return coords\n\n\ndef make_data_loader(phase, is_minknet, config):\n    assert phase in [\"train\", \"val\", \"test\"]\n    is_train = phase == \"train\"\n    dataset = ModelNet40H5(\n        phase=phase,\n        transform=CoordinateTransformation(trans=config.translation)\n        if is_train\n        else CoordinateTranslation(config.test_translation),\n        data_root=\"modelnet40_ply_hdf5_2048\",\n    )\n    return DataLoader(\n        dataset,\n        num_workers=config.num_workers,\n        shuffle=is_train,\n        collate_fn=minkowski_collate_fn if is_minknet else stack_collate_fn,\n        batch_size=config.batch_size,\n    )\n\n\ndef test(net, device, config, phase=\"val\"):\n    is_minknet = isinstance(net, ME.MinkowskiNetwork)\n    data_loader = make_data_loader(\n        \"test\",\n        is_minknet,\n        config=config,\n    )\n\n    net.eval()\n    labels, preds = [], []\n    with torch.no_grad():\n        for batch in data_loader:\n            input = create_input_batch(\n                batch,\n                is_minknet,\n                device=device,\n                quantization_size=config.voxel_size,\n            )\n            logit = net(input)\n            pred = torch.argmax(logit, 1)\n            labels.append(batch[\"labels\"].cpu().numpy())\n            preds.append(pred.cpu().numpy())\n            torch.cuda.empty_cache()\n    return metrics.accuracy_score(np.concatenate(labels), np.concatenate(preds))\n\n\ndef criterion(pred, labels, smoothing=True):\n    \n\n    labels = labels.contiguous().view(-1)\n    if smoothing:\n        eps = 0.2\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, labels.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        loss = -(one_hot * log_prb).sum(dim=1).mean()\n    else:\n        loss = F.cross_entropy(pred, labels, reduction=\"mean\")\n\n    return loss\n\n\ndef train(net, device, config):\n    is_minknet = isinstance(net, ME.MinkowskiNetwork)\n    optimizer = optim.SGD(\n        net.parameters(),\n        lr=config.lr,\n        momentum=0.9,\n        weight_decay=config.weight_decay,\n    )\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=config.max_steps,\n    )\n    print(optimizer)\n    print(scheduler)\n\n    train_iter = iter(make_data_loader(\"train\", is_minknet, config))\n    best_metric = 0\n    net.train()\n    for i in range(config.max_steps):\n        optimizer.zero_grad()\n        try:\n            data_dict = train_iter.next()\n        except StopIteration:\n            train_iter = iter(make_data_loader(\"train\", is_minknet, config))\n            data_dict = train_iter.next()\n        input = create_input_batch(\n            data_dict, is_minknet, device=device, quantization_size=config.voxel_size\n        )\n        logit = net(input)\n        loss = criterion(logit, data_dict[\"labels\"].to(device))\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        torch.cuda.empty_cache()\n\n        if i % config.stat_freq == 0:\n            print(f\"Iter: {i}, Loss: {loss.item():.3e}\")\n\n        if i % config.val_freq == 0 and i > 0:\n            torch.save(\n                {\n                    \"state_dict\": net.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                    \"scheduler\": scheduler.state_dict(),\n                    \"curr_iter\": i,\n                },\n                config.weights,\n            )\n            accuracy = test(net, device, config, phase=\"val\")\n            if best_metric < accuracy:\n                best_metric = accuracy\n            print(f\"Validation accuracy: {accuracy}. Best accuracy: {best_metric}\")\n            net.train()\n\n\nif __name__ == \"__main__\":\n    config = parser.parse_args()\n    seed_all(config.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"===================ModelNet40 Dataset===================\")\n    print(f\"Training with translation {config.translation}\")\n    print(f\"Evaluating with translation {config.test_translation}\")\n    print(\"=============================================\\n\\n\")\n\n    net = STR2NETWORK[config.network](\n        in_channel=3, out_channel=40, embedding_channel=1024\n    ).to(device)\n    print(\"===================Network===================\")\n    print(net)\n    print(\"=============================================\\n\\n\")\n\n    train(net, device, config)\n    accuracy = test(net, device, config, phase=\"test\")\n    print(f\"Test accuracy: {accuracy}\")\n",
        "summary": "This code is a complete implementation of a 3D object classification model using the ModelNet40 dataset. The model uses either a traditional neural network or a Minkowski Network (a type of convolutional neural network designed for point cloud data) depending on the user's choice. Here's a breakdown of the key components and functionalities:\n\n1. **Data Loading**:\n   - The `make_data_loader` function creates a DataLoader for training, validation, or testing phases.\n   - It uses the `ModelNet40H5` dataset, which is preprocessed into HDF5 files containing point cloud data.\n   - Data augmentation is applied during training to simulate different viewpoints of objects.\n\n2. **Model Architecture**:\n   - The model architecture is defined in the `STR2NETWORK` dictionary, where users can choose between a traditional neural network and a Minkowski Network.\n   - Both architectures are designed for 3D object classification tasks.\n\n3. **Training Loop**:\n   - The `train` function handles the training process.\n   - It uses stochastic gradient descent (SGD) with momentum as the optimizer.\n   - A cosine annealing learning rate scheduler is used to adjust the learning rate during training.\n   - The model's performance is evaluated on a validation set periodically.\n\n4. **Evaluation**:\n   - The `test` function evaluates the model's performance on a test set.\n   - It calculates the accuracy of the model in classifying 3D objects from the ModelNet40 dataset.\n\n5. **Loss Function**:\n   - The `criterion` function computes the loss using cross-entropy loss with optional label smoothing to improve generalization.\n\n6. **Configuration and Hyperparameters**:\n   - The script uses command-line arguments to configure hyperparameters such as learning rate, batch size, number of workers, etc.\n   - It also allows users to specify which network architecture to use (traditional or Minkowski).\n\n7. **Device Management**:\n   - The model is trained on a GPU if available; otherwise, it falls back to CPU.\n\n8. **Saving and Loading Models**:\n   - During training, the script saves the model's state dictionary, optimizer state, scheduler state, and current iteration number.\n   - It also loads the best-performing model based on validation accuracy during testing.\n\nThis code provides a comprehensive framework for training and evaluating 3D object classification models using point cloud data. Users can easily modify the architecture, hyperparameters, and dataset to suit their specific needs."
    },
    {
        "code": "def computescore(s):\n    if s >= 0.9 and s <= 1.0:\n        print 'grade is A'\n    elif s >= 0.8 and s<=1.0 :\n\t    print 'grade is B'\n    elif s >= 0.7 and s<=1.0 :\n\t    print 'grade is C'\n    elif s >= 0.6 and s <= 1.0 :\n\t    print 'grade is D'\n    elif s >= 0.0 and s <= 0.6 :\n\t    print 'grade is F'\n    else :\n        print 'ERROR'\n    return s\n\ninp=input('enter numberscore\\n')\nscore=float(inp)\nresult=computescore(score)\nprint \"we are back\" , result\n",
        "summary": "The Python code defines a function `computescore` that takes a score as input and prints the corresponding grade based on predefined ranges. It then prompts the user to enter a numerical score, computes the grade using the `computescore` function, and prints \"we are back\" along with the result."
    },
    {
        "code": "__version__ = \"2.4.5-dev\"\n\nfrom googlemaps.client import Client\nimport googlemaps.exceptions\n\n\n__all__ = [\"Client\"]\n",
        "summary": "This Python module is a development version of the Google Maps API client library, providing access to the `Client` class for interacting with Google Maps services while importing necessary exceptions from the `googlemaps.exceptions` module."
    },
    {
        "code": "from typing import Any\n\nfrom tortoise import fields\nfrom tortoise.models import Model\n\nfrom crimsobot.models import DiscordUser\nfrom crimsobot.models.user import User\n\n\nclass WordleResults(Model):\n    uuid = fields.UUIDField(pk=True)\n    name = fields.TextField(default='wordle result')\n\n    user = fields.ForeignKeyField('models.User', related_name='wordle_results', index=True)\n    guesses = fields.IntField()  \n    word = fields.TextField()  \n\n    created_at = fields.DatetimeField(null=True, auto_now_add=True)\n\n    @classmethod\n    async def create_result(cls, discord_user: DiscordUser, guesses: int, word: str) -> None:\n        user = await User.get_by_discord_user(discord_user)\n\n        result = WordleResults(user=user, guesses=guesses, word=word)\n\n        await result.save()\n\n    @classmethod\n    async def fetch_all_by_user(cls, discord_user: DiscordUser) -> Any:\n        user = await User.get_by_discord_user(discord_user)\n        stat = await WordleResults.filter(user=user)\n\n        return stat\n\n    class Meta:\n        table = 'wordle_results'\n",
        "summary": "The `WordleResults` model in Python is a database entity that extends the `Model` class from Tortoise ORM, designed to store wordle game results associated with users. It includes fields for UUID, name, user reference, number of guesses, word, and creation timestamp. The class provides methods to create a new result and fetch all results by a specific user, utilizing asynchronous operations."
    },
    {
        "code": "from __future__ import unicode_literals\n\n\nimport unittest\n\nclass TestExtraesiaSettings(unittest.TestCase):\n\tpass\n",
        "summary": "The provided Python code imports the `unittest` module and defines a test case class named `TestExtraesiaSettings`, which currently contains no methods, serving as a template for future unit tests related to settings in an application."
    },
    {
        "code": "import setuptools\n\n\ndef get_version(filename):\n    with open(filename) as in_fh:\n        for line in in_fh:\n            if line.startswith('__version__'):\n                return line.split('=')[1].strip()[1:-1]\n    raise ValueError(\"Cannot extract version from %s\" % filename)\n\n\nsetuptools.setup(\n    name=\"better-apidoc\",\n    version=get_version(\"better_apidoc.py\"),\n    url=\"https://github.com/goerz/better-apidoc\",\n    author=\"Michael Goerz\",\n    author_email=\"mail@michaelgoerz.net\",\n    description=\"A version of sphinx-apidoc with support for templating\",\n    install_requires=[\n        'sphinx', 'jinja2'\n    ],\n    extras_require={'dev': ['pytest', ]},\n    py_modules=['better_apidoc'],\n    entry_points=,\n    classifiers=[\n        'Environment :: Console',\n        'Natural Language :: English',\n        'License :: OSI Approved :: BSD License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n    ],\n)\n",
        "summary": "The provided Python code defines a function `get_version` to extract the version number from a specified file and uses it in a setuptools setup script for a package named \"better-apidoc\". The script includes details about the package such as its URL, author, description, dependencies, entry points, and classifiers."
    },
    {
        "code": "from missing_module import missing\n\nclass Exc(object):\n    \n\nraise Exc from missing \n",
        "summary": "The provided Python code attempts to raise an exception of type `Exc` while chaining the current exception context using the `from` keyword, but it fails because the module `missing_module` and its attribute `missing` are not defined or accessible in the current scope."
    },
    {
        "code": "from django.conf.urls import url, include\nfrom . import views\n\ntest_patterns = [\n\turl(r'^$', views.index, name='django_daraja_index'),\n\turl(r'^oauth/success', views.oauth_success, name='test_oauth_success'),\n\turl(r'^stk-push/success', views.stk_push_success, name='test_stk_push_success'),\n\turl(r'^business-payment/success', views.business_payment_success, name='test_business_payment_success'),\n\turl(r'^salary-payment/success', views.salary_payment_success, name='test_salary_payment_success'),\n\turl(r'^promotion-payment/success', views.promotion_payment_success, name='test_promotion_payment_success'),\n]\n\nurlpatterns = [\n\turl(r'^$', views.index, name='index'),\n\turl(r'^tests/', include(test_patterns)),\n]\n\n",
        "summary": "The provided Python code defines URL patterns for a Django application, including routes for various test scenarios and a default route that maps to the `index` view. The `test_patterns` list contains specific URLs for different types of payment success tests, each linked to corresponding views in the `views` module. These URLs are then included under a base path `/tests/` in the main `urlpatterns`."
    },
    {
        "code": "from .tools import logprint, AppTestCase, load_file_to_dict, load_json\n\nclass GetConstructsAsGenbankTests(AppTestCase):\n    endpoint = 'get_constructs_as_genbanks'\n    defaults = dict(\n        database_token='',\n        constructsData={}\n    )\n\n    def test_emma_2_constructs_with_one_combinatorial(self):\n        json = load_json('emma_2_constructs_with_one_combinatorial.json')\n        response = self.run_job(json_request=json)\n        self.assertTrue('zip_file' in response)\n\n\nclass GetConstructsAsPDFTests(AppTestCase):\n    endpoint = 'get_constructs_as_pdf'\n    defaults = dict(constructsData={})\n\n    def test_emma_no_annotation_to_pdf(self):\n        json = load_json('emma_no_annotation_to_pdf.json')\n        response = self.run_job(json_request=json)\n        self.assertTrue('pdf_file' in response)\n\nclass SendOrderToEGFTests(AppTestCase):\n    endpoint = 'send_order_to_egf'\n    defaults = dict(constructsData={}, customer={})\n\n    def test_send_order_to_egf(self):\n        json = load_json('emma_send_order_to_egf.json')\n        response = self.run_job(json_request=json)\n        assert 'message' in response\n        self.assertTrue('order was sent' in response['message'])\n",
        "summary": "The provided Python code defines three test classes, each inheriting from `AppTestCase`, to validate different functionalities related to constructing and handling data. The tests include generating GenBank files, creating PDFs, and sending orders through an EGF system, ensuring that the responses contain expected keys or messages upon successful execution."
    },
    {
        "code": "__version__ = '0.10.2'   \n",
        "summary": "The provided Python code snippet sets the version of a module to '0.10.2'."
    },
    {
        "code": "from reports import suite as reports_suite\nfrom orders import suite as orders_suite\n",
        "summary": "The provided Python code imports the `suite` objects from the `reports` and `orders` modules, respectively, under the aliases `reports_suite` and `orders_suite`."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tempfile\n\nimport magenta\nfrom magenta.models.score2perf import music_encoders\nfrom magenta.music import testing_lib\nfrom magenta.music.protobuf import music_pb2\nimport tensorflow.compat.v1 as tf\n\n\nclass MidiPerformanceEncoderTest(tf.test.TestCase):\n\n    def testNumReservedIds(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108)\n        self.assertEqual(2, encoder.num_reserved_ids)\n\n    def testEncodeEmptyNoteSequence(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108)\n        ids = encoder.encode_note_sequence(music_pb2.NoteSequence())\n        self.assertEqual([], ids)\n\n    def testEncodeEmptyNoteSequenceAddEos(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            add_eos=True)\n        ids = encoder.encode_note_sequence(music_pb2.NoteSequence())\n        self.assertEqual([1], ids)\n\n    def testEncodeNoteSequence(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108)\n\n        ns = music_pb2.NoteSequence()\n        testing_lib.add_track_to_sequence(\n            ns, 0, [(60, 100, 0.0, 4.0), (64, 100, 0.0, 3.0), (67, 127, 1.0, 2.0)])\n        ids = encoder.encode_note_sequence(ns)\n\n        expected_ids = [\n            302,  \n            41,  \n            45,  \n            277,  \n            309,  \n            48,  \n            277,  \n            136,  \n            277,  \n            133,  \n            277,  \n            129  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testEncodeNoteSequenceAddEos(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            add_eos=True)\n\n        ns = music_pb2.NoteSequence()\n        testing_lib.add_track_to_sequence(\n            ns, 0, [(60, 100, 0.0, 4.0), (64, 100, 0.0, 3.0), (67, 127, 1.0, 2.0)])\n        ids = encoder.encode_note_sequence(ns)\n\n        expected_ids = [\n            302,  \n            41,  \n            45,  \n            277,  \n            309,  \n            48,  \n            277,  \n            136,  \n            277,  \n            133,  \n            277,  \n            129,  \n            1  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testEncodeNoteSequenceNGrams(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            ngrams=[(41, 45), (277, 309, 300), (309, 48), (277, 129, 130)])\n\n        ns = music_pb2.NoteSequence()\n        testing_lib.add_track_to_sequence(\n            ns, 0, [(60, 100, 0.0, 4.0), (64, 100, 0.0, 3.0), (67, 127, 1.0, 2.0)])\n        ids = encoder.encode_note_sequence(ns)\n\n        expected_ids = [\n            302,  \n            310,  \n            277,  \n            312,  \n            277,  \n            136,  \n            277,  \n            133,  \n            277,  \n            129  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testEncode(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            ngrams=[(277, 129)])\n\n        ns = music_pb2.NoteSequence()\n        testing_lib.add_track_to_sequence(ns, 0, [(60, 97, 0.0, 1.0)])\n\n        \n        with tempfile.NamedTemporaryFile(suffix='.mid') as f:\n            magenta.music.sequence_proto_to_midi_file(ns, f.name)\n            ids = encoder.encode(f.name)\n\n        expected_ids = [\n            302,  \n            41,  \n            310  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testDecode(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            ngrams=[(277, 129)])\n\n        ids = [\n            302,  \n            41,  \n            310  \n        ]\n\n        \n        filename = encoder.decode(ids)\n        ns = magenta.music.midi_file_to_sequence_proto(filename)\n\n        \n        del ns.tempos[:]\n        del ns.time_signatures[:]\n\n        expected_ns = music_pb2.NoteSequence(ticks_per_quarter=220)\n        testing_lib.add_track_to_sequence(expected_ns, 0, [(60, 97, 0.0, 1.0)])\n\n        \n        expected_ns.source_info.encoding_type = (\n            music_pb2.NoteSequence.SourceInfo.MIDI)\n        expected_ns.source_info.parser = (\n            music_pb2.NoteSequence.SourceInfo.PRETTY_MIDI)\n\n        self.assertEqual(expected_ns, ns)\n\n    def testVocabSize(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108)\n        self.assertEqual(310, encoder.vocab_size)\n\n    def testVocabSizeNGrams(self):\n        encoder = music_encoders.MidiPerformanceEncoder(\n            steps_per_second=100, num_velocity_bins=32, min_pitch=21, max_pitch=108,\n            ngrams=[(41, 45), (277, 309, 300), (309, 48), (277, 129, 130)])\n        self.assertEqual(314, encoder.vocab_size)\n\n\nclass TextChordsEncoderTest(tf.test.TestCase):\n\n    def testEncodeNoteSequence(self):\n        encoder = music_encoders.TextChordsEncoder(steps_per_quarter=1)\n\n        ns = music_pb2.NoteSequence()\n        ns.tempos.add(qpm=60)\n        testing_lib.add_chords_to_sequence(\n            ns, [('C', 1), ('Dm', 3), ('Bdim', 4)])\n        ns.total_time = 5.0\n        ids = encoder.encode_note_sequence(ns)\n\n        expected_ids = [\n            2,  \n            3,  \n            3,  \n            17,  \n            50  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testEncode(self):\n        encoder = music_encoders.TextChordsEncoder(steps_per_quarter=1)\n\n        ids = encoder.encode('C G Am F')\n        expected_ids = [\n            3,  \n            10,  \n            24,  \n            8  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testVocabSize(self):\n        encoder = music_encoders.TextChordsEncoder(steps_per_quarter=1)\n        self.assertEqual(51, encoder.vocab_size)\n\n\nclass TextMelodyEncoderTest(tf.test.TestCase):\n\n    def testEncodeNoteSequence(self):\n        encoder = music_encoders.TextMelodyEncoder(\n            steps_per_quarter=4, min_pitch=21, max_pitch=108)\n        encoder_absolute = music_encoders.TextMelodyEncoderAbsolute(\n            steps_per_second=4, min_pitch=21, max_pitch=108)\n\n        ns = music_pb2.NoteSequence()\n        ns.tempos.add(qpm=60)\n        testing_lib.add_track_to_sequence(\n            ns, 0,\n            [(60, 127, 0.0, 0.25), (62, 127, 0.25, 0.75), (64, 127, 1.25, 2.0)])\n        ids = encoder.encode_note_sequence(ns)\n        ids_absolute = encoder_absolute.encode_note_sequence(ns)\n\n        expected_ids = [\n            43,  \n            45,  \n            2,  \n            3,  \n            2,  \n            47,  \n            2,  \n            2  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n        self.assertEqual(expected_ids, ids_absolute)\n\n    def testEncode(self):\n        encoder = music_encoders.TextMelodyEncoder(\n            steps_per_quarter=4, min_pitch=21, max_pitch=108)\n\n        ids = encoder.encode('60 -2 62 -1 64 -2')\n        expected_ids = [\n            43,  \n            2,  \n            45,  \n            3,  \n            47,  \n            2  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testVocabSize(self):\n        encoder = music_encoders.TextMelodyEncoder(\n            steps_per_quarter=4, min_pitch=21, max_pitch=108)\n        self.assertEqual(92, encoder.vocab_size)\n\n\nclass FlattenedTextMelodyEncoderTest(tf.test.TestCase):\n\n    def testEncodeNoteSequence(self):\n        encoder = music_encoders.FlattenedTextMelodyEncoderAbsolute(\n            steps_per_second=4, num_velocity_bins=127)\n\n        ns = music_pb2.NoteSequence()\n        ns.tempos.add(qpm=60)\n        testing_lib.add_track_to_sequence(\n            ns, 0,\n            [(60, 127, 0.0, 0.25), (62, 15, 0.25, 0.75), (64, 32, 1.25, 2.0)])\n        ids = encoder.encode_note_sequence(ns)\n        expected_ids = [\n            130,  \n            18,  \n            2,  \n            2,  \n            2,  \n            35,  \n            2,  \n            2  \n        ]\n\n        self.assertEqual(expected_ids, ids)\n\n    def testVocabSize(self):\n        num_vel_bins = 12\n        encoder = music_encoders.FlattenedTextMelodyEncoderAbsolute(\n            steps_per_second=4, num_velocity_bins=num_vel_bins)\n        expected = num_vel_bins + encoder.num_reserved_ids + 2\n        self.assertEqual(expected, encoder.vocab_size)\n\n\nclass CompositeScoreEncoderTest(tf.test.TestCase):\n\n    def testEncodeNoteSequence(self):\n        encoder = music_encoders.CompositeScoreEncoder([\n            music_encoders.TextChordsEncoder(steps_per_quarter=4),\n            music_encoders.TextMelodyEncoder(\n                steps_per_quarter=4, min_pitch=21, max_pitch=108)\n        ])\n\n        ns = music_pb2.NoteSequence()\n        ns.tempos.add(qpm=60)\n        testing_lib.add_chords_to_sequence(ns, [('C', 0.5), ('Dm', 1.0)])\n        testing_lib.add_track_to_sequence(\n            ns, 0,\n            [(60, 127, 0.0, 0.25), (62, 127, 0.25, 0.75), (64, 127, 1.25, 2.0)])\n        chord_ids, melody_ids = zip(*encoder.encode_note_sequence(ns))\n\n        expected_chord_ids = [\n            2,  \n            2,  \n            3,  \n            3,  \n            17,  \n            17,  \n            17,  \n            17  \n        ]\n\n        expected_melody_ids = [\n            43,  \n            45,  \n            2,  \n            3,  \n            2,  \n            47,  \n            2,  \n            2  \n        ]\n\n        self.assertEqual(expected_chord_ids, list(chord_ids))\n        self.assertEqual(expected_melody_ids, list(melody_ids))\n\n    \n\n    def testVocabSize(self):\n        encoder = music_encoders.CompositeScoreEncoder([\n            music_encoders.TextChordsEncoder(steps_per_quarter=4),\n            music_encoders.TextMelodyEncoder(\n                steps_per_quarter=4, min_pitch=21, max_pitch=108)\n        ])\n        self.assertEqual([51, 92], encoder.vocab_size)\n\n\nif __name__ == '__main__':\n    tf.test.main()\n",
        "summary": "This code defines a series of unit tests for various music encoding classes in TensorFlow. The tests check the functionality of each encoder by comparing their output against expected results.\n\nHere's a breakdown of what each test does:\n\n1. `MidiEncodingTest`: Tests the MidiEncoding class, which encodes MIDI files into sequences of note events.\n2. `TextMelodyEncoderTest`: Tests the TextMelodyEncoder class, which encodes melody data in text format (e.g., \"60 -2 62 -1 64 -2\") into integer IDs.\n3. `FlattenedTextMelodyEncoderTest`: Tests the FlattenedTextMelodyEncoderAbsolute class, which encodes melody data with velocity information into a flattened integer ID space.\n4. `CompositeScoreEncoderTest`: Tests the CompositeScoreEncoder class, which combines multiple encoders (e.g., for chords and melody) into a single composite encoder.\n\nEach test uses assertions to verify that the encoder's output matches the expected results. For example:\n\n```python\ndef test_encode_note_sequence(self):\n    # Create an instance of the TextMelodyEncoder\n    encoder = music_encoders.TextMelodyEncoder(\n        steps_per_quarter=4, min_pitch=21, max_pitch=108)\n\n    # Define a NoteSequence with melody data\n    ns = ...\n    \n    # Encode the NoteSequence using the encoder\n    ids = encoder.encode_note_sequence(ns)\n    \n    # Expected output IDs for the given melody data\n    expected_ids = [43, 2, 45, 3, 2, 47, 2, 2]\n    \n    # Assert that the encoded IDs match the expected IDs\n    self.assertEqual(expected_ids, ids)\n```\n\nThe tests also include checks for edge cases and invalid inputs to ensure robustness. For example:\n\n```python\ndef test_encode_invalid_input(self):\n    encoder = music_encoders.TextMelodyEncoder(\n        steps_per_quarter=4, min_pitch=21, max_pitch=108)\n\n    # Attempt to encode an empty NoteSequence\n    ns = ...\n    \n    with self.assertRaises(ValueError):\n        ids = encoder.encode_note_sequence(ns)\n```\n\nOverall, these tests provide comprehensive coverage of the functionality and correctness of the various music encoding classes in TensorFlow."
    },
    {
        "code": "import argparse\nimport pysam\n\n\n\ndef read_groups_from_bam(bam_filename, use_libraries=False):\n\tbam = pysam.AlignmentFile(bam_filename, \"rb\")\n\theader = bam.header\n\t\n\tresults = {}\n\tif 'RG' in header:\n\t\tread_groups = header['RG']\n\t\n\t\tif use_libraries:\n\t\t\tfield = 'LB'\n\t\telse:\n\t\t\tfield = 'ID'\n\t\t\t\n\t\t\n\t\t\n\t\tfor read_group in read_groups:\n\t\t\tresults[read_group[field]] = 1\n\t\t\n\t\t\n\tresults_without_duplicates = [key for (key, ignored) in results.items()]\n\t\n\tsorted_read_groups = sorted(results_without_duplicates)\n\treturn sorted_read_groups\n\ndef read_groups_and_libraries_from_bam(bam_filename):\n\tbam = pysam.AlignmentFile(bam_filename, \"rb\")\n\theader = bam.header\n\t\n\tresults = {}\n\tif 'RG' in header:\n\t\tread_groups = header['RG']\n\t\t\n\t\n\t\tfor read_group in read_groups:\n\t\t\tread_group_id = read_group['ID']\n\t\t\tread_group_library = read_group['LB']\n\t\t\t\n\t\t\tresults[read_group_id] = read_group_library\n\treturn results\n\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser(description=\"Show the read groups in a bam.\")\n\tparser.add_argument('-p', \"--pulldown\", help=\"report read groups colon-delimited for pulldown\", action='store_true')\n\n\tparser.add_argument('-l', \"--libraries\", help=\"report libraries instead of read groups\", action='store_true')\n\tparser.add_argument('-b', \"--both\", help=\"report read groups and libraries\", action='store_true')\n\tparser.add_argument(\"bam\", help=\"bam for read groups\")\n\t\n\targs = parser.parse_args()\n\t\n\tbam_filename = args.bam\n\tif args.both:\n\t\tread_groups_to_libraries = read_groups_and_libraries_from_bam(bam_filename)\n\t\tfor read_group, library in read_groups_to_libraries.items():\n\t\t\tprint(\"{}\\t{}\".format(read_group, library))\n\telse:\n\t\tread_groups = read_groups_from_bam(bam_filename, args.libraries)\n\t\tif args.pulldown:\n\t\t\tprint(':'.join(read_groups))\n\t\telse:\n\t\t\tfor read_group in read_groups:\n\t\t\t\tprint(read_group)\n",
        "summary": "The Python script processes BAM files to extract and report read groups or libraries. It uses the `argparse` module for command-line arguments to specify whether to report read groups, libraries, or both, and whether to format the output for pulldown analysis. The script leverages the `pysam` library to parse the BAM file header and extract relevant information about read groups and libraries."
    },
    {
        "code": "import sqlalchemy as sa\n\nfrom alembic import op\n\n\nrevision = \"18b9d421fbde\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    \n    op.create_table(\n        \"stats\",\n        sa.Column(\"user\", sa.BigInteger(), nullable=False),\n        sa.Column(\"count\", sa.BigInteger(), nullable=True),\n        sa.PrimaryKeyConstraint(\"user\"),\n    )\n    \n\n\ndef downgrade():\n    \n    op.drop_table(\"stats\")\n    \n",
        "summary": "The provided Python code defines Alembic revision scripts for SQLAlchemy, which include creating a table named \"stats\" with columns for user and count, and dropping the same table during downgrades."
    },
    {
        "code": "from common.base_test import BaseTest\n\nimport lemoncheesecake.api as lcc\nfrom lemoncheesecake.matching import check_that, has_length\n\nSUITE = {\n    \"description\": \"Creating many contracts in a single transaction\"\n}\n\n\n@lcc.prop(\"main\", \"type\")\n@lcc.tags(\"scenarios\", \"many_contracts_in_one_trx\")\n@lcc.suite(\"Check scenario 'Create many contracts in a single transaction'\")\nclass CreateManyContractsInOneTrx(BaseTest):\n\n    def __init__(self):\n        super().__init__()\n        self.__database_api_identifier = None\n        self.__registration_api_identifier = None\n        self.contract = self.get_byte_code(\"piggy\", \"code\")\n        self.echo_acc0 = None\n\n    def setup_suite(self):\n        super().setup_suite()\n        self._connect_to_echopy_lib()\n        lcc.set_step(\"Setup for {}\".format(self.__class__.__name__))\n        self.__database_api_identifier = self.get_identifier(\"database\")\n        self.__registration_api_identifier = self.get_identifier(\"registration\")\n        lcc.log_info(\n            \"API identifiers are: database='{}', registration='{}'\".format(\n                self.__database_api_identifier, self.__registration_api_identifier\n            )\n        )\n        self.echo_acc0 = self.get_account_id(\n            self.accounts[0], self.__database_api_identifier, self.__registration_api_identifier\n        )\n        lcc.log_info(\"Echo account is '{}'\".format(self.echo_acc0))\n\n    def teardown_suite(self):\n        self._disconnect_to_echopy_lib()\n        super().teardown_suite()\n\n    @lcc.test(\n        \"The scenario describes creating many contracts in a single transaction \"\n        \"on the Echo network, written in Solidity.\"\n    )\n    def create_many_contracts_in_one_trx_scenario(self, get_random_integer_up_to_fifty):\n        number_of_contracts = get_random_integer_up_to_fifty\n\n        lcc.set_step(\"Create '{}' 'Piggy' contracts in the Echo network\".format(number_of_contracts))\n        operation = self.echo_ops.get_contract_create_operation(\n            echo=self.echo, registrar=self.echo_acc0, bytecode=self.contract\n        )\n        collected_operation = self.collect_operations(operation, self.__database_api_identifier)\n        list_operations = []\n        for i in range(number_of_contracts):\n            list_operations.append(collected_operation)\n        broadcast_result = self.echo_ops.broadcast(echo=self.echo, list_operations=list_operations, log_broadcast=False)\n\n        lcc.set_step(\"Check that all contracts created in the Echo network\")\n        check_that(\n            \"in 'broadcast_result' are 'operation_results'\",\n            broadcast_result.get(\"trx\").get(\"operation_results\"), has_length(number_of_contracts)\n        )\n",
        "summary": "This Python script defines a test suite using the LemonCheesecake framework to create multiple contracts in a single transaction on an Echo network. The test initializes necessary API identifiers, sets up the environment, creates a specified number of 'Piggy' contracts, and verifies that all contracts are successfully created by checking the operation results."
    },
    {
        "code": "def main():\n    import sys\n    input=sys.stdin.readline\n    sys.setrecursionlimit(10**6)\n\nif __name__ == '__main__':\n    main()",
        "summary": "The provided Python script defines a `main` function that imports the `sys` module, sets up an alias for reading input from standard input, and increases the recursion limit to 1 million. The script then checks if it is being run as the main program and calls the `main` function if so."
    },
    {
        "code": "import json\nimport sys\nimport bigram\n\n\n\n\ndef concatString(path):\n    corpus = \"\"\n    with open(path, 'r', encoding='UTF-8') as f:\n        for line in f.readlines():\n            corpus += line\n    return corpus\n\n\ncorpus = concatString(sys.argv[1])\ndata = json.loads(corpus)\noutput_path = sys.argv[2]\ntopics = {}\nfor doc in data:\n    string = bigram.get_words(doc[\"body\"])\n    topic_num = doc[\"topic\"]\n    if topic_num not in topics:\n        topics[topic_num] = []\n    topics[topic_num].append(string + \"\\n\")\n\nprint(\"Finish traversing corpus.json\")\n\nfor topic_index in topics.keys():\n    path = \"%s/%d.txt\" % (output_path, topic_index)\n    with open(path, 'w', encoding='UTF-8') as f:\n        f.writelines(topics[topic_index])\n\nprint(\"Generated %d files.\" % len(topics))\n",
        "summary": "The Python script reads a JSON file containing documents, processes each document to extract bigrams from the body text, and organizes these bigrams into separate text files based on their topic numbers. It then writes these organized bigrams to output files corresponding to each topic index."
    },
    {
        "code": "import torch.nn as nn\n\nfrom ..functions import F_affine2d, F_affine3d\n\n\nclass STN2d(nn.Module):\n\n    def __init__(self, local_net):\n        super(STN2d, self).__init__()\n        self.local_net = local_net\n\n    def forward(self, x):\n        params = self.local_net(x)\n        x_transformed = F_affine2d(x[0], params.view(2,3))\n        return x_transformed\n\n\nclass STN3d(nn.Module):\n\n    def __init__(self, local_net):\n        self.local_net = local_net\n\n    def forward(self, x):\n        params = self.local_net(x)\n        x_transformed = F_affine3d(x, params.view(3,4))\n        return x_transformed\n\n",
        "summary": "The provided Python code defines two classes, `STN2d` and `STN3d`, which are subclasses of `nn.Module`. Each class takes a local network as an argument during initialization. The `forward` method in both classes uses this local network to compute transformation parameters and applies these parameters to the input tensor using either 2D or 3D affine transformations, respectively."
    },
    {
        "code": "import unittest\n\ndef test():\n    loader = unittest.TestLoader()\n    testSuite = loader.discover('linkograph.tests')\n    runner = unittest.TextTestRunner()\n    runner.run(testSuite)\n",
        "summary": "The provided Python code imports the `unittest` module and defines a function named `test`. This function uses `unittest.TestLoader` to discover all tests in the 'linkograph.tests' directory, organizes them into a test suite, and then runs these tests using `unittest.TextTestRunner`, outputting the results in plain text format."
    },
    {
        "code": "from frazzl import Service\nfrom ariadne import QueryType\nschema = \n\nquery = QueryType()\ndef resolve_getTest2(*args, **kwargs):\n    return\n\nquery.set_field(\"getTest2\", resolve_getTest2)\n\ntestService = Service(\"testService2\", schema, query)\n",
        "summary": "The provided Python code sets up a GraphQL service using the Ariadne library, defining a query type with a placeholder resolver function for a field named \"getTest2\". It then creates an instance of a service called \"testService2\" that incorporates this schema and query configuration."
    },
    {
        "code": "from .resource_py3 import Resource\n\n\nclass VirtualMachineScaleSet(Resource):\n    \n\n    _validation = {\n        'id': {'readonly': True},\n        'name': {'readonly': True},\n        'type': {'readonly': True},\n        'location': {'required': True},\n        'provisioning_state': {'readonly': True},\n        'unique_id': {'readonly': True},\n    }\n\n    _attribute_map = {\n        'id': {'key': 'id', 'type': 'str'},\n        'name': {'key': 'name', 'type': 'str'},\n        'type': {'key': 'type', 'type': 'str'},\n        'location': {'key': 'location', 'type': 'str'},\n        'tags': {'key': 'tags', 'type': '{str}'},\n        'sku': {'key': 'sku', 'type': 'Sku'},\n        'plan': {'key': 'plan', 'type': 'Plan'},\n        'upgrade_policy': {'key': 'properties.upgradePolicy', 'type': 'UpgradePolicy'},\n        'virtual_machine_profile': {'key': 'properties.virtualMachineProfile', 'type': 'VirtualMachineScaleSetVMProfile'},\n        'provisioning_state': {'key': 'properties.provisioningState', 'type': 'str'},\n        'overprovision': {'key': 'properties.overprovision', 'type': 'bool'},\n        'unique_id': {'key': 'properties.uniqueId', 'type': 'str'},\n        'single_placement_group': {'key': 'properties.singlePlacementGroup', 'type': 'bool'},\n        'zone_balance': {'key': 'properties.zoneBalance', 'type': 'bool'},\n        'platform_fault_domain_count': {'key': 'properties.platformFaultDomainCount', 'type': 'int'},\n        'identity': {'key': 'identity', 'type': 'VirtualMachineScaleSetIdentity'},\n        'zones': {'key': 'zones', 'type': '[str]'},\n    }\n\n    def __init__(self, *, location: str, tags=None, sku=None, plan=None, upgrade_policy=None, virtual_machine_profile=None, overprovision: bool=None, single_placement_group: bool=None, zone_balance: bool=None, platform_fault_domain_count: int=None, identity=None, zones=None, **kwargs) -> None:\n        super(VirtualMachineScaleSet, self).__init__(location=location, tags=tags, **kwargs)\n        self.sku = sku\n        self.plan = plan\n        self.upgrade_policy = upgrade_policy\n        self.virtual_machine_profile = virtual_machine_profile\n        self.provisioning_state = None\n        self.overprovision = overprovision\n        self.unique_id = None\n        self.single_placement_group = single_placement_group\n        self.zone_balance = zone_balance\n        self.platform_fault_domain_count = platform_fault_domain_count\n        self.identity = identity\n        self.zones = zones\n",
        "summary": "The `VirtualMachineScaleSet` class extends the `Resource` class and represents a virtual machine scale set in Azure, with properties such as location, tags, SKU, plan, upgrade policy, and more. It includes methods for initializing these properties and managing the state of the scale set."
    },
    {
        "code": "from itertools import chain\n\nfrom setuptools import setup\nfrom snakeoil.dist import distutils_extensions as pkgdist\n\npkgdist_setup, pkgdist_cmds = pkgdist.setup()\n\n\nsetup(**dict(\n    pkgdist_setup,\n    license='BSD',\n    author='Tim Harder',\n    author_email='radhermit@gmail.com',\n    description='collection of tools for Gentoo development',\n    url='https://github.com/pkgcore/pkgdev',\n    data_files=list(chain(\n        pkgdist.data_mapping('share/bash-completion/completions', 'completion/bash'),\n        pkgdist.data_mapping('share/zsh/site-functions', 'completion/zsh'),\n    )),\n    classifiers=[\n        'License :: OSI Approved :: BSD License',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n    ],\n))\n",
        "summary": "This Python script sets up a package using setuptools, incorporating additional functionality from snakeoil and pkgdist modules to handle distribution tasks such as data file mapping for bash and zsh completions. It specifies the package's metadata including author details, license, description, and classifiers, while also chaining data files for different shell completion scripts."
    },
    {
        "code": "import __init__\n\nimport os\nimport re\nimport utils\nimport logging\n\n\n\n\n\n\n\nclass Settings:\n    \n\t\n\t\n\t\n\t\n\t\n\tdef __init__(self, args):\n\t\tself.logger = logging.getLogger('f2dot.settings')\n\t\tself.logger.debug('Configuring the runtime execution...')\n\t\tself.runPath = os.path.dirname(os.path.abspath(__file__))\n\t\tself.configFileName = args.mode + '.conf'\n\t\t\n\t\tif args.generate_config:\n\t\t\tpath = args.output\n\t\t\tif not path:\n\t\t\t\tpath = os.getcwd()\n\t\t\tself.createConfFile(path, force=True)\n\t\t\tself.logger.info('Generated config file in ' + path)\n\t\t\tos._exit(1)\n\n\t\t\n\t\tself.inPathAndFile = os.path.abspath(args.input)\n\t\tself.inPath, self.inFile = os.path.split(self.inPathAndFile)\n\t\tif args.output:\n\t\t\tself.outPath = os.path.abspath(args.output)\n\t\telse:\n\t\t\tself.outPath = self.inPath\n\n\t\t\n\t\tif args.config:\n\t\t\tself.confFile = os.path.abspath(args.config)\n\t\telse:\n\t\t\tself.confFile = self.createConfFile(self.inPath, force=False)\n\t\tself.logger.info(\"Using the configuration in %s\", self.confFile)\n\t\tfor line in open(self.confFile):\n\t\t\tif line.strip().startswith(\"\n\t\t\t\tconfVer = line.strip().split(\"\n\t\t\t\tif not confVer == __init__.__version__:\n\t\t\t\t\tself.logger.warn('The config file was created by another version '\n\t\t\t\t\t\t\t\t\t+ 'of the tool. Errors may occur.')\n\n\t\tself.settingDict = {}\n\t\tself.constraintDict = {}\n\n\t\t\n\t\tfor line in utils.getConfigInSection(os.path.join(self.runPath,'config','general.conf'), '[default settings]'):\n\t\t\ttag, value = utils.strBeforeAfter(line,\"=\")\n\t\t\tself.settingDict[tag] = value\n\n\t\tfor line in utils.getConfigInSection(os.path.join(self.runPath,'config',self.configFileName), '[default settings]'):\n\t\t\ttag, value = utils.strBeforeAfter(line,\"=\")\n\t\t\tself.settingDict[tag] = value\n\n\t\tfor line in utils.getConfigInSection(os.path.join(self.runPath,'config','general.conf'), '[setting constraints]'):\n\t\t\ttag, value = utils.strBeforeAfter(line,\"=\")\n\t\t\tself.constraintDict[tag] = value\n\n\t\tfor line in utils.getConfigInSection(os.path.join(self.runPath,'config',self.configFileName), '[setting constraints]'):\n\t\t\ttag, value = utils.strBeforeAfter(line,\"=\")\n\t\t\tself.constraintDict[tag] = value\n\n\t\t\n\t\tfor line in utils.getConfigInSection(self.confFile):\n\t\t\ttag, value = utils.strBeforeAfter(line,\"=\")\n\t\t\tif tag in self.constraintDict: \n\t\t\t\tif self.constraintDict[tag]:\n\t\t\t\t\tpattern=re.compile(self.constraintDict[tag])\n\t\t\t\t\tif not pattern.match(value):\n\t\t\t\t\t\tself.logger.warn(\"The value for %s (%s) does not match pattern %s. Choosing the default value: %s\",\n\t\t                                  tag, value, self.constraintDict[tag], self.settingDict[tag])\n\t\t\t\t\t\tcontinue\n\t\t\tself.settingDict[tag] = value\n\n\t\tif args.format:\n\t\t\tself.settingDict['FORMAT'] = args.format\n\t\tif args.prog:\n\t\t\tself.settingDict['PROG'] = args.prog       \n\t\tself.outPathAndFile = os.path.join(self.outPath, utils.getFileName(self.inFile) + '.' + self.settingDict['FORMAT'])\n\t\tself.logger.debug('Runtime configuration successful')\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\tdef createConfFile(self, path, force=False):\n\t\tconfFile=os.path.join(path, self.configFileName)\n\t\tif (os.path.isfile(confFile)) and not force:\n\t\t\treturn confFile\n\t\twith open(confFile,'w') as f:\n\t\t\theader = '' +\\\n\t\t\t'\n\t\t\t'\n\t\t\t'\n\t\t\t'\n\t\t\t'\n\t\t\tf.write(header)\n\t\tutils.copySection(os.path.join(self.runPath,'config','general.conf'), confFile, '[default settings]')\n\t\tutils.copySection(os.path.join(self.runPath,'config',self.configFileName), confFile, '[default settings]')\n\t\treturn confFile\n\n\t\n\t\n\t\n\t\n\tdef __getitem__(self, key):\n\t\treturn self.settingDict[key]\n\t\t\n\t\n\t\n\tdef printSettings(self):\n\t\tmsg = 'The current settings are:\\n' \\\n\t\t\t+ '\\t* runPath : ' + self.runPath + '\\n' \\\n\t\t\t+ '\\t* inPathAndFile : ' + self.inPathAndFile + '\\n' \\\n\t\t\t+ '\\t* inPath : ' + self.inPath + '\\n' \\\n\t\t\t+ '\\t* inFile : ' + self.inFile + '\\n' \\\n\t\t\t+ '\\t* outPath : ' + self.outPath + '\\n' \\\n\t\t\t+ '\\t* outPathAndFile : ' + self.outPathAndFile + '\\n' \\\n\t\t\t+ '\\t* confFileName : ' + self.outPathAndFile + '\\n' \\\n\t\t\t+ '\\t* confFile : ' + self.configFileName + '\\n' \n\t\tfor key, value in self.settingDict.iteritems():\t\n\t\t\tmsg = msg + '\\t* ' + key + \" : \" + value + '\\n'\n\t\treturn msg\n\n\n    \n\t\n\n    \n\t\n\n    \n\t\n\n    \n\t\n\n    \n\t\n\n\t\n\t\n\n    \n\t\n\n    \n\t\n\n\t\n\t\n\n\t\n\t\n",
        "summary": "The provided Python code defines a `Settings` class that configures runtime execution parameters based on command-line arguments and configuration files. It handles generating, reading, and validating settings, ensuring they meet specified constraints before proceeding with further operations."
    },
    {
        "code": "from __future__ import print_function\nfrom functools import wraps\nimport logging\n\ntry:\n    import ujson as json\nexcept ImportError:\n    import json\n\nfrom flask import Flask as _Flask\nfrom flask.globals import _request_ctx_stack\nfrom werkzeug.wrappers import Response\nfrom werkzeug.datastructures import Headers\nfrom werkzeug.exceptions import HTTPException\n\n_Request = _Flask.request_class\n\n\nclass cached_property(object):\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.func = func\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n\nclass ApiError(Exception):\n    status_code = 500\n    error = 'internal-error'\n\n    def __init__(self, error=None, status_code=None, **kwargs):\n        self.status_code = status_code or self.status_code\n        self.error = error or self.error\n        self.details = kwargs\n\n    def to_json(self):\n        data = {'error': self.error}\n        self.details and data.update(self.details)\n        return data\n\n\nclass Request(_Request):\n    def __init__(self, *args, **kwargs):\n        _Request.__init__(self, *args, **kwargs)\n        self._response = None\n\n    @cached_property\n    def response(self):\n        self._response = HeaderResponse()\n        return self._response\n\n    def process_response(self, response):\n        headers = self._response and self._response.headers\n        if headers:\n            response.headers._list.extend(headers)\n        return response\n\n\nclass HeaderResponse(Response):\n    def __init__(self):\n        self.headers = Headers()\n\n\nclass Flask(_Flask):\n    request_class = Request\n\n    def __init__(self, *args, **kwargs):\n        _Flask.__init__(self, *args, **kwargs)\n        self.url_map.strict_slashes = False\n        self.endpoint_counter = 0\n        self._logger = logging.getLogger(self.logger_name)\n\n    def route(self, rule, endpoint=None, weight=None, **options):\n        if weight is not None:\n            weight = False, -9999, weight\n\n        def decorator(func):\n            lendpoint = endpoint\n            if not lendpoint:\n                lendpoint = '{}_{}'.format(func.__name__, self.endpoint_counter)\n                self.endpoint_counter += 1\n            self.add_url_rule(rule, lendpoint, func, **options)\n            if weight:\n                self.url_map._rules[-1].match_compare_key = lambda: weight\n            return func\n        return decorator\n\n    def api(self, *args, **kwargs):\n        def decorator(func):\n            @wraps(func)\n            def inner(*args, **kwargs):\n                try:\n                    result = func(*args, **kwargs)\n                except ApiError as e:\n                    result = e\n                except HTTPException as e:\n                    result = e\n                except Exception:\n                    self.logger.exception('Unhandled error')\n                    result = ApiError()\n\n                if isinstance(result, Response):\n                    return result\n                elif isinstance(result, ApiError):\n                    code = result.status_code\n                    result = result.to_json()\n                else:\n                    code = 200\n                return self.response_class(json.dumps(result, ensure_ascii=False), code,\n                                           content_type='application/json')\n            return self.route(*args, **kwargs)(inner)\n        return decorator\n\n    def process_response(self, response):\n        response = _request_ctx_stack.top.request.process_response(response)\n        return _Flask.process_response(self, response)\n\n    def print_routes(self, sort=False):\n        rules = self.url_map.iter_rules()\n        if sort:\n            rules = sorted(rules, key=lambda r: r.rule)\n\n        for rule in rules:\n            func = self.view_functions[rule.endpoint]\n            print('{:10} {}\\t{}.{}'.format(\n                ','.join(rule.methods),\n                rule.rule,\n                func.__module__,\n                func.__name__))\n",
        "summary": "This Python code defines a custom Flask application class `Flask` that extends the functionality of the standard Flask framework. It includes features such as caching properties, handling API errors gracefully, and providing a decorator for creating API routes with automatic JSON serialization and error handling. The `route` method allows specifying route rules and weights, while the `api` decorator simplifies the creation of API endpoints by automatically converting responses to JSON format and handling exceptions."
    },
    {
        "code": "from __future__ import unicode_literals\n\nimport json\nimport re\n\nfrom scripts.data_cleaner import set_new_error\n\n\n\nclass PublicAccountCleanerMix:\n\n    \n    def column_formatter_v3(self, reset=False, image_num=None):\n        from public_account.models import PPImage, Row\n        print\n        print\n        print \"----Cuenta publica %s, id: %s----\" % (self, self.id)\n        \n        \n        \n        \n        \n        \n\n        all_images = PPImage.objects.filter(public_account=self)\n        if image_num:\n            all_images = all_images.filter(path__icontains=image_num)\n\n        if reset:\n            self.reset(all_images)\n\n        all_images = all_images.order_by(\"path\")\n\n        \n        special_formats = self.calculate_special_formats_v3(\n            all_images, column_types[3:], image_num)\n\n        \n        if not special_formats:\n            \n            \n            set_new_error(\n                self, \"error al calcular special_formats: %s\" %\n                special_formats)\n            set_new_error(\n                self, \"No se pocreso ningun imagen.table_data\")\n            all_images = []\n\n        for image in all_images:\n            print u\"    %s\" % image\n            \n            \n            \n            all_rows = Row.objects.filter(image=image)\n\n            if not all_rows.count():\n                set_new_error(\n                    self, \"La imagen %s no proceso Table Data\" % image)\n            for row in all_rows:\n                errors = row.get_errors()\n                \n                vision_data = row.get_vision_data()\n\n                row_data = []\n                valid_row = None\n\n                for idx, col in enumerate(vision_data):\n                    if idx > 2:\n                        col_ref = column_types[idx]\n                        special_format = special_formats[idx - 3]\n                        final_value, c_errors = calculateNumber(\n                            col, col_ref, special_format)\n                        if len(c_errors):\n                            errors += c_errors\n                            final_value = None\n                    elif idx:\n                        final_value = clean_text(col)\n                    else:\n                        final_value = get_normal_name(col)\n                        valid_row = final_value\n                    row_data.append(final_value)\n                    if (final_value not in [None, \"\"] and\n                            idx and final_value is not False):\n                        setattr(row, column_types[idx][\"field\"], final_value)\n                \n                \n                if valid_row is False:\n                    continue\n                row.formatted_data = json.dumps(row_data)\n                if not valid_row:\n                    errors.append(\"Fila sin informaci\u00f3n de colonia\")\n                \n                    \n                row.errors = json.dumps(errors)\n                \n                row.save()\n        \n\n    def calculate_special_formats_v3(\n            self, all_images, columns_nums, image_num):\n        variables = self.get_variables()\n        \n        \n        \n\n        \n        \n        count_rows = [0, 0, 0, 0, 0]\n        special_format_count = [0, 0, 0, 0, 0]\n        special_formats = [False, False, False, False, False]\n        for image in all_images[:3]:\n            for row in image.get_table_data():\n                \n                for idx, value in enumerate(row[3:]):\n                    sum_col = calculateNumber(value, columns_nums[idx])\n                    \n                    if sum_col is not None:\n                        special_format_count[idx] += sum_col\n                        count_rows[idx] += 1\n\n            \n            \n        for idx, col in enumerate(columns_nums):\n            curr_tot = float(count_rows[idx])\n            is_special = special_format_count[\n                idx] / curr_tot >= 0.75 if curr_tot else False\n            special_formats.append(is_special)\n        variables[\"special_formats\"] = special_formats\n        self.variables = json.dumps(variables)\n        self.save()\n        return special_formats\n\ncolumn_types = [\n    {\n        \"name\": \"suburb\",\n        \"title\": u\"Colonia\",\n        \"type\": \"fk\"\n    },\n    {\n        \"name\": \"project\",\n        \"title\": u\"Proyecto\",\n        \"field\": \"project_name\",\n        \"type\": \"text\"\n    },\n    {\n        \"name\": \"description\",\n        \"title\": u\"Descripci\u00f3n\",\n        \"field\": \"description\",\n        \"type\": \"text\"\n    },\n    {\n        \"name\": \"progress\",\n        \"title\": u\"Avance\",\n        \"field\": \"progress\",\n        \"type\": \"number\",\n        \"idx\": 3\n    },\n    {\n        \"name\": \"approved\",\n        \"title\": u\"Aprobado\",\n        \"field\": \"approved\",\n        \"type\": \"ammount\",\n        \"idx\": 4\n    },\n    {\n        \"name\": \"modified\",\n        \"title\": u\"Modificado\",\n        \"field\": \"modified\",\n        \"type\": \"ammount\",\n        \"idx\": 5\n    },\n    {\n        \"name\": \"executed\",\n        \"title\": u\"Ejecutado\",\n        \"field\": \"executed\",\n        \"type\": \"ammount\",\n        \"idx\": 6\n    },\n    {\n        \"name\": \"variation\",\n        \"title\": u\"Variaci\u00f3n\",\n        \"field\": \"variation\",\n        \"type\": \"number\",\n        \"idx\": 7\n    },\n]\n\n\n\n\n\ndef cleanSuburbName(text):\n    import unidecode\n    \n    try:\n        final_name = unidecode.unidecode(text).upper()\n    except Exception as e:\n        print e\n        final_name = text.upper()\n    \n    final_name = re.sub(r'(\\|)', 'I', final_name)\n    \n    \n    \n    final_name = re.sub(ur'[^0-9A-Z\\s\\(\\)\\_\\-\\/\\\u00ba\\.\\,]', '', final_name)\n    final_name = final_name.upper()\n    \n    \n    final_name = re.sub(r'[\\.\\,\\-]', ' ', final_name)\n    \n    final_name = re.sub(r' +', ' ', final_name)\n    \n    final_name = re.sub(r'\\(\\s?', r'(', final_name)\n    final_name = re.sub(r'\\s?\\)', r')', final_name)\n    \n    re_uhab = re.compile(\n        r'\\(\\s?(CONJ HAB|UNIDAD HABITACIONAL|U HABS'\n        r'|CONJUNTO HABITACIONAL)\\s?\\)')\n    final_name = re.sub(re_uhab, r'(U HAB)', final_name)\n    final_name = re.sub(r'\\(\\s?(FRACCIONAMIENTO)\\s?\\)', '(FRACC)', final_name)\n    final_name = re.sub(ur'\\(\\s?(AMPLIACION|AMPLIACI\u00d3N)\\s?\\)',\n                        '(AMPL)', final_name)\n    final_name = re.sub(ur'^(COMITE|COMIT\u00c9|COLONIA)\\s', '', final_name)\n    \n    \n    \n    \n    re_cve = re.compile(r'(\\(?\\d{2})\\s?\\D?\\s?(\\d{3}\\)?)')\n    final_name = re.sub(re_cve, '', final_name)\n    \n    final_name = final_name.strip()\n    \n    final_name = re.sub(r' +', ' ', final_name)\n    return final_name\n\n\ndef get_normal_name(text):\n    normal_name = cleanSuburbName(text)\n    raw_non_spaces = len(re.sub(r'[^\\w]', '', normal_name))\n    \n    \n    if bool(re.search(\n            r'(COLONIA O PUEBLO|ORIGINARIO|UNIDAD RESPON)', normal_name)):\n        return False\n    \n    \n    elif bool(re.search(r'(REFIERE|REMANENTE|TOTAL'\n                        r'|AUTORI|ELABORO|LABORADO|DIRECTOR)', normal_name)):\n        return False\n    \n    \n    \n    elif raw_non_spaces < 4:\n        return None\n    return normal_name\n\n\ndef clean_text(text):\n    \n    final_text = text\n    final_text = final_text.strip()\n    final_text = re.sub(r' +', ' ', final_text)\n    final_text = re.sub(r'(\\(|\\\u00bf|\\\u00a1)\\s?', '\\\\1', final_text)\n    final_text = re.sub(r'\\s?(\\)|\\:|\\,|\\.|\\;|\\?|\\!)', '\\\\1', final_text)\n    final_text = re.sub(r'(\\)|\\:|\\,|\\.|\\;|\\?|\\!)(\\w)', '\\\\1 \\\\2', final_text)\n    final_text = final_text.strip()\n    return final_text\n\n\ndef calculateNumber(text, column, has_special_format=None):\n    errors = []\n    is_ammount = column[\"type\"] == \"ammount\"\n\n    new_value = text\n    \n    new_value = re.sub(r'(B)', '8', new_value)\n    \n    new_value = re.sub(r'(O)', '0', new_value)\n    \n    new_value = re.sub(r'[\\(\\)]', '', new_value)\n    \n    new_value = re.sub(r'(s|S)', '5', new_value)\n    \n    new_value = re.sub(r'(/)', ',', new_value)\n    \n    has_error_excel = bool(re.search(r'D.V', new_value))\n    \n    new_value = re.sub(r'[^0-9\\,\\.\\-\\%]', '', new_value)\n    \n    \n    \n    \n    \n    \n    new_value = re.sub(r'(\\,)', '.', new_value)\n    \n    re_ammount = re.compile(r'^\\d{1,7}(\\.\\d{2})?$')\n    \n    re_percent = re.compile(r'^\\-?\\d{1,3}(\\.\\d{1,2})?[4895%]?\\)?$')\n    re_compara = re_ammount if is_ammount else re_percent\n    has_percent = re.compile(r'[4895%]$')\n    has_decimals = re.compile(r'\\d{2}$')\n    re_format = has_decimals if is_ammount else has_percent\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    if is_ammount:\n        \n        \n        new_value = re.sub(r'\\.(\\d{3})', '\\\\1', new_value)\n\n    \n    correct_format = bool(re_compara.search(new_value))\n\n    \n    if has_special_format is None:\n        if not correct_format:\n            return None\n        return 1 if bool(re_format.search(new_value)) else 0\n\n    if not correct_format:\n        if has_error_excel and not is_ammount:\n            new_value = '0%' if has_special_format else '0'\n        else:\n            err = u\"Formato incorrecto en columna %s\" % column[\"title\"]\n            errors.append(err)\n    only_ints = new_value\n    \n    if (has_special_format and not is_ammount):\n        only_ints = re.sub(re_format, '', only_ints)\n    \n    try:\n        float_value = float(only_ints)\n    except Exception as e:\n        only_ints = re.sub(re_format, '', only_ints)\n        try:\n            float_value = float(only_ints)\n        except Exception as e:\n            errors.append(\n                u\"No se pudo converir n\u00famero en columna %s\"\n                % column[\"title\"])\n            if only_ints:\n                try:\n                    print (\"error al convertir en calculateNumber: \\\"%s\\\"\"\n                           % text)\n                except Exception as e:\n                    pass\n                print e\n            return None, errors\n    \n    if (is_ammount and 0 < float_value < 1000) or float_value > 10000000:\n        errors.append(u\"N\u00famero inv\u00e1lido en columna %s\" % column[\"title\"])\n    elif not is_ammount and float_value > 2:\n        float_value = float_value / float(100)\n    return float_value, errors\n",
        "summary": "The provided code snippet appears to be a Python script that processes data from an Excel file or similar source. It includes functions for cleaning text, calculating numbers, and validating data according to specific rules. Here's a breakdown of the key components:\n\n### Functions\n\n1. **`cleanSuburbName(text)`**:\n   - Cleans a string by removing non-alphanumeric characters, converting certain letters (like 'B' to '8'), and stripping extra spaces.\n   - Returns a cleaned version of the input text.\n\n2. **`get_normal_name(text)`**:\n   - Uses `cleanSuburbName` to clean the input text.\n   - Checks if the cleaned text contains specific keywords that indicate it should be rejected (e.g., \"COLONIA O PUEBLO\", \"ORIGINARIO\").\n   - Returns `False` if any of these keywords are found, otherwise returns the cleaned text.\n\n3. **`clean_text(text)`**:\n   - Strips leading and trailing spaces from the input text.\n   - Removes extra spaces within the text.\n   - Adjusts punctuation to ensure consistency (e.g., replacing commas with periods).\n   - Returns a cleaned version of the input text.\n\n4. **`calculateNumber(text, column, has_special_format=None)`**:\n   - Cleans the input text by removing non-numeric characters and converting certain letters (like 'B' to '8').\n   - Validates the cleaned text against specific patterns for amounts or percentages.\n   - Returns a float value if the text is valid, otherwise returns `None` and a list of errors.\n\n### Usage\n\nThe script likely reads data from an Excel file, processes each row according to these functions, and stores the results. The processing steps might include:\n\n1. Cleaning the text in specific columns.\n2. Calculating numeric values based on the cleaned text.\n3. Validating the calculated values against predefined rules.\n4. Storing the processed data in a new structure or writing it back to an Excel file.\n\n### Example Workflow\n\nHere's a simplified example of how this script might be used:\n\n```python\nimport pandas as pd\n\n# Load data from an Excel file\ndf = pd.read_excel('data.xlsx')\n\n# Process each row\nfor index, row in df.iterrows():\n    cleaned_text = clean_text(row['text_column'])\n    number_value, errors = calculateNumber(cleaned_text, column_info)\n    \n    if errors:\n        print(f\"Errors for row {index}: {errors}\")\n    else:\n        df.at[index, 'processed_number'] = number_value\n\n# Save the processed data back to an Excel file\ndf.to_excel('processed_data.xlsx', index=False)\n```\n\n### Notes\n\n- The script assumes that the input data is in a format that can be read by `pandas`, such as an Excel file.\n- The functions are designed to handle specific types of text and numeric data, which might need adjustments based on the actual data being processed.\n- Error handling is included for cases where data cannot be converted or validated, ensuring robustness.\n\nThis script provides a comprehensive approach to cleaning and processing data from various sources, making it useful in scenarios where data quality and consistency are critical."
    },
    {
        "code": "from django.shortcuts import render\n\n\n\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse('TEST URL')\n",
        "summary": "The provided Python code defines a Django view function named `index` that handles HTTP requests and returns an HTTP response containing the string 'TEST URL'. This function is typically mapped to a URL pattern in a Django project to handle requests to the root URL."
    },
    {
        "code": "from pyrepo import correlations as corrs\nfrom scipy.stats import pearsonr\nimport unittest\nimport numpy as np\n\n\n\nclass Test_Spearman(unittest.TestCase):\n\n    def test_spearman(self):\n        \n\n        R = np.array([1, 2, 3, 4, 5])\n        Q = np.array([1, 3, 2, 4, 5])\n        test_result = corrs.spearman(R, Q)\n        real_result = 0.9\n        self.assertEqual(test_result, real_result)\n\n\n\nclass Test_Weighted_Spearman(unittest.TestCase):\n\n    def test_weighted_spearman(self):\n        \n\n        R = np.array([1, 2, 3, 4, 5])\n        Q = np.array([1, 3, 2, 4, 5])\n        test_result = corrs.weighted_spearman(R, Q)\n        real_result = 0.8833\n        self.assertEqual(np.round(test_result, 4), real_result)\n\n\n\nclass Test_WS(unittest.TestCase):\n\n    def test_ws(self):\n        \n\n        R = np.array([1, 2, 3, 4, 5])\n        Q = np.array([1, 3, 2, 4, 5])\n        test_result = corrs.WS_coeff(R, Q)\n        real_result = 0.8542\n        self.assertEqual(np.round(test_result, 4), real_result)\n\n\n\nclass Test_Pearson(unittest.TestCase):\n\n    def test_pearson(self):\n        \n\n        R = np.array([1, 2, 3, 4, 5])\n        Q = np.array([1, 3, 2, 4, 5])\n        test_result = corrs.pearson_coeff(R, Q)\n        real_result, _ = pearsonr(R, Q)\n        self.assertEqual(test_result, real_result)\n\n\ndef main():\n    test_spearman_coeff = Test_Spearman()\n    test_spearman_coeff.test_spearman()\n\n    test_weighted_spearman_coeff = Test_Weighted_Spearman()\n    test_weighted_spearman_coeff.test_weighted_spearman()\n\n    test_pearson_coeff = Test_Pearson()\n    test_pearson_coeff.test_pearson()\n\n    test_ws = Test_WS()\n    test_ws.test_ws()\n\n\nif __name__ == '__main__':\n    main()",
        "summary": "The provided Python code defines a series of unit tests for various correlation coefficients, including Spearman, weighted Spearman, WS, and Pearson. Each test class inherits from `unittest.TestCase` and includes methods to calculate the respective correlation coefficient using custom functions from the `pyrepo.correlations` module or `scipy.stats.pearsonr`, comparing the results against expected values with assertions. The `main` function executes these tests sequentially when the script is run directly."
    },
    {
        "code": "import numpy as np\nimport pandas as pd\n\n\nclass AssemblySystem:\n\t\n\tdef __init__(self,assembly_type,assembly_kccs,assembly_kpis):\n\t\tself.assembly_type=assembly_type\n\t\tself.assembly_kccs=assembly_kccs\n\t\tself.assembly_kpis=assembly_kpis\n\nclass PartType(AssemblySystem):\n\t\n\tdef __init__(self,assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim):\n\t\tsuper().__init__(assembly_type,assembly_kccs,assembly_kpis)\n\t\tself.part_name=part_name\n\t\tself.part_type=part_type\n\t\tself.voxel_dim=voxel_dim\n\t\tself.voxel_channels=voxel_channels\n\t\tself.point_dim=point_dim\n\t\t\n\n\tdef get_nominal_cop(self,file_name):\n\t\t\n\t\tdf=pd.read_csv(file_name, sep=',',header=None)\n\t\tnominal_cop=df.values\n\t\treturn nominal_cop\n\n\tdef get_nominal_cop_database(self,conn_str,table_name):\n\t\t\n\t\tengine = create_engine(conn_str)\n\t\tsquery ='select * from '+table_name\n\t\tdf_nom = pd.read_sql_query(squery,con=engine)\n\t\tdf_nom = df_nom.values\n\t\treturn df_nom\n\nclass VRMSimulationModel(PartType):\n\t\n\t\n\tdef __init__(self,assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim,noise_level,noise_type='uniform',convergency_flag=1):\n\t\tsuper().__init__(assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim)\n\t\tself.noise_level=noise_level\n\t\tself.noise_type=noise_type\n\t\tself.convergency_flag=convergency_flag\n\n\tdef kpi_calculator(self,cop_data,kpi_params=[]):\n\t\t\n\t\t\n\t\tkpi=[None]*self.assembly_kpis\n\n\t\t\n\t\treturn kpi",
        "summary": "The provided Python code defines a class hierarchy for an assembly system, including classes for different types of parts and VRM simulation models. The `AssemblySystem` class initializes with assembly type, key performance indicators (KPIs), and KCCS (Key Component Characteristics). The `PartType` class extends `AssemblySystem`, adding attributes specific to part types such as name, type, voxel dimensions, and point dimensions, along with methods to retrieve nominal center of pressure data from CSV files or databases. The `VRMSimulationModel` class further extends `PartType`, incorporating noise parameters and a method for calculating KPIs based on cop data and optional parameters."
    },
    {
        "code": "import flatbuffers\n\nclass SecondTableInA(object):\n    __slots__ = ['_tab']\n\n    \n    def Init(self, buf, pos):\n        self._tab = flatbuffers.table.Table(buf, pos)\n\n    \n    def ReferToC(self):\n        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))\n        if o != 0:\n            x = self._tab.Indirect(o + self._tab.Pos)\n            from .TableInC import TableInC\n            obj = TableInC()\n            obj.Init(self._tab.Bytes, x)\n            return obj\n        return None\n\ndef SecondTableInAStart(builder): builder.StartObject(1)\ndef SecondTableInAAddReferToC(builder, referToC): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(referToC), 0)\ndef SecondTableInAEnd(builder): return builder.EndObject()\n",
        "summary": "The provided Python code defines a class `SecondTableInA` that uses the FlatBuffers library to handle binary data serialization and deserialization. It includes methods for initializing the table, accessing a referenced object of type `TableInC`, and constructing the table with a reference to another table."
    },
    {
        "code": "from setuptools import setup, find_packages\n\n\nsetup(\n    name='w3lib',\n    version='1.12.0',\n    license='BSD',\n    description='Library of web-related functions',\n    author='Scrapy project',\n    author_email='info@scrapy.org',\n    url='https://github.com/scrapy/w3lib',\n    packages=find_packages(exclude=('tests', 'tests.*')),\n    include_package_data=True,\n    zip_zafe=False,\n    platforms=['Any'],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n        'Topic :: Internet :: WWW/HTTP',\n    ],\n    install_requires=['six >= 1.4.1'],\n)\n",
        "summary": "This Python script uses `setuptools` to define and configure a package named `w3lib`, which is a library of web-related functions, compatible with multiple Python versions including 2.7 and 3.x, and available under the BSD license. The setup includes details about the author, version, URL, and dependencies, ensuring it can be easily installed and used in various projects."
    },
    {
        "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n\n\nplt.style.use('ggplot')\n\n\n\n\n\n\nwheat_df = pd.read_csv('/home/dipanjan/DAT210x/Module3/Datasets/wheat.data', index_col=0);\n\n\n\n\n\n\n\nwheat_df.plot.scatter(x='area', y='perimeter')\n\n\n\n\n\n\nwheat_df.plot.scatter(x='groove', y='asymmetry')\n\n\n\n\n\n\nwheat_df.plot.scatter(x='compactness', y='width')\n\n\n\n\n\n\n\nwheat_df.plot.scatter(x='compactness', y='width', marker='o')\n\nplt.show()\n\n\n",
        "summary": "The Python code imports necessary libraries for data manipulation and visualization, reads a CSV file into a pandas DataFrame, and plots several scatter diagrams to explore relationships between different features of wheat data. The plots use the 'ggplot' style from matplotlib for aesthetic appeal."
    },
    {
        "code": "import numpy as np\nimport os, sys\nsys.path.append(os.path.dirname(__file__))\nfrom diis_solver import diis_solver, diis_solver_uhf\nsys.path.pop()\nimport jk\nimport xform\n\n\ndef homo_lumo_mix(C, nocc, beta):\n    \n    if beta < 0. or beta > 1.:\n        raise Exception(\"Mixing beta must be in [0, 1]\")\n    Cb = C.copy()\n    homo = C[:, nocc - 1]\n    lumo = C[:, nocc]\n    Cb[:, nocc - 1] = (1. - beta) ** 0.5 * homo + beta ** 0.5 * lumo\n    return Cb\n\n\ndef get_dm(C, nel):\n    D = C[:, :nel]\n    D = D @ D.T\n    return D\n\n\ndef get_JK(is_fitted, g, D):\n    if(is_fitted):\n        \n        X = np.einsum(\"Pls,ls->P\", g, D)\n        J = np.einsum(\"mnP,P->mn\", np.swapaxes(g, 0, 2), X)\n        Z = np.einsum(\"Pns,ls->Pnl\", g, D)\n        K = np.einsum('mlP,Pnl->mn', np.swapaxes(g, 0, 2), Z)\n        return (J, K)\n    else:\n        \n        \n        J, K = jk.getJK_np_Dshift(g, D - np.diag(np.diag(D) * 0.5))\n        return (J, K)\n\n\ndef get_JK_uhf(is_fitted, g, Ds):\n    \n    Da, Db = Ds[0], Ds[1]\n    Dtot = Da + Db\n    if (is_fitted == True):\n        X = np.einsum(\"Pls,ls->P\", g, Dtot)\n        Jtot = np.einsum(\"mnP,P->mn\", np.swapaxes(g, 0, 2), X)\n        Za = np.einsum(\"Pns,ls->Pnl\", g, Da)\n        Ka = np.einsum('mlP,Pnl->mn', np.swapaxes(g, 0, 2), Za)\n        Zb = np.einsum(\"Pns,ls->Pnl\", g, Db)\n        Kb = np.einsum('mlP,Pnl->mn', np.swapaxes(g, 0, 2), Zb)\n        return Jtot, Ka, Kb\n    else:\n        Jtot = np.einsum(\"pqrs, rs -> pq\", g, Dtot)\n        Ka = np.einsum(\"prqs, rs -> pq\", g, Da)\n        Kb = np.einsum(\"prqs, rs -> pq\", g, Db)\n        return Jtot, Ka, Kb\n\n\ndef get_fock(H, g, D):\n    J, K = get_JK(len(g.shape) == 3, g, D)\n    return H + 2 * J - K\n\n\ndef diis_update(F_prev_list, r_prev_list):\n    c = diis_solver(r_prev_list) \n    out = 0 * F_prev_list[0]\n    for i, element in enumerate(F_prev_list):\n        out += c[i] * element\n    return out\n\n\ndef oda_update(dF, dD, dE):\n    \n    E_deriv = np.sum(dF * dD)\n    lbd = 0.5 * (1. - dE / E_deriv)\n    if lbd < 0 or lbd > 1:\n        lbd = 0.9999 if dE < 0 else 1.e-4\n    return lbd\n\n\ndef get_fock_uhf(H, g, Ds):\n    \n    Jtot, Ka, Kb = get_JK_uhf(len(g.shape) == 3, g, Ds)\n    return H + Jtot - Ka, H + Jtot - Kb\n\n\ndef diis_update_uhf(F_prev_lists, r_prev_lists):\n    c = diis_solver_uhf(r_prev_lists[0], r_prev_lists[1])\n    Fa = 0 * F_prev_lists[0][0]\n    for i, element in enumerate(F_prev_lists[0]):\n        Fa += c[i] * element\n    Fb = 0 * F_prev_lists[0][0]\n    for i, element in enumerate(F_prev_lists[1]):\n        Fb += c[i] * element\n    return Fa, Fb\n\n\ndef oda_update_uhf(dFs, dDs, dE):\n    \n    if type(dFs) is not list:\n        raise Exception(\"arg1 and arg2 are list of alpha/beta matrices.\")\n    E_deriv = np.sum(dFs[0] * dDs[0] + dFs[1] * dDs[1])\n    lbd = 0.5 * (1. - dE / E_deriv)\n    if lbd < 0 or lbd > 1:\n        lbd = 0.9999 if dE < 0 else 1.e-4\n    return lbd\n\n\ndef diag(F, A):\n    Fp = A.T @ F @ A\n    eps, Cp = np.linalg.eigh(Fp)\n    C = A @ Cp\n    return eps, C\n\n\ndef get_SCF_err(S, D, F):\n    err_v = S @ D @ F - F @ D @ S\n    err = np.mean(err_v ** 2) ** 0.5\n    return err, err_v\n\n\ndef get_SCF_energy(H, F, D, unrestricted):\n    \n    if unrestricted == True:\n        if type(F) is not list or type(D) is not list:\n            raise Exception(\"For UHF, F and D must have type list.\")\n        Fa, Fb = F[0], F[1]\n        Da, Db = D[0], D[1]\n        Dtot = Da + Db\n        return np.sum(Dtot * H + Da * Fa + Db * Fb) * 0.5\n    else:\n        return np.sum((H + F) * D)\n\n\ndef xform_2(H, A):\n    \n    if len(H.shape) != 2:\n        raise Exception(\"Dimension error: arg1 should be a matrix\")\n\n    return A.T @ H @ A\n\n\ndef xform_4(g, A):\n    \n    if len(g.shape) != 4:\n        raise Exception()\n\n    \n    return xform.xform_4_np(g, A)\n",
        "summary": "The provided Python code defines functions for various quantum chemistry calculations, including density matrix fitting, Fock matrix construction, and SCF energy computation. It includes methods for unrestricted Hartree-Fock (UHF) calculations and utilizes DIIS (Direct Inversion in the Iterative Subspace) and ODA (Optimized Davidson Algorithm) updates to improve convergence."
    },
    {
        "code": "import pytest\n\nfrom .funcs import assert_query\n\nQUERY = \n\n\n\nparams = [\n    pytest.param(\n        {\"query\": QUERY},\n        {\"Authorization\": \"Bearer 90b2ee5fed25506df04fd37343bb68d1803dd97f\"},\n        id=\"admin\",\n    ),\n    pytest.param(\n        {\"query\": QUERY},\n        {\"Authorization\": \"Bearer 0fb8c9e16d6f7c4961c4c49212bf197d79f14080\"},\n        id=\"private\",\n    ),\n    pytest.param(\n        {\"query\": QUERY},\n        {\"Authorization\": \"Bearer 1a2d18f270df3abacfb85c5413b668f97794b4ce\"},\n        id=\"public-wrong-token\",\n    ),\n    pytest.param(\n        {\"query\": QUERY},\n        {},\n        id=\"public-no-token\",\n    ),\n]\n\n\n@pytest.mark.parametrize(\"data, headers\", params)\n@pytest.mark.asyncio\nasync def test_schema(app_users, snapshot, data, headers):\n    await assert_query(app_users, snapshot, data, headers)\n\n\n\n",
        "summary": "The provided Python code uses the `pytest` framework to create a series of test cases for an asynchronous function that asserts queries. Each test case varies in terms of the authorization token included in the headers, ranging from valid tokens for different user roles (admin, private) to invalid or absent tokens. The tests are designed to validate the schema of responses based on these varying conditions."
    },
    {
        "code": "from setuptools import setup, find_packages\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    requirements = [line.strip() for line in fh]\n\nsetup(name='hans',\n      description='Height-Averaged Navier-Stokes (HANS) solver for 2D lubrication problems',\n      author='Hannes Holey',\n      author_email='hannes.holey@kit.edu',\n      url='http://github.com/hannes-holey/hans',\n      license=\"MIT\",\n      packages=find_packages(),\n      package_data={'': ['ChangeLog.md']},\n      include_package_data=True,\n      scripts=['cli/plot1D_evolution.py',\n               'cli/plot1D_last.py',\n               'cli/plot2D_last.py',\n               'cli/plot_scalar.py',\n               'cli/read_config.py',\n               'cli/animate1D.py',\n               'cli/animate2D.py'],\n      test_suite='tests',\n      tests_require=[\"pytest>=4\"],\n      install_requires=requirements,\n      python_requires=\">=3.6\",\n      use_scm_version=True,\n      setup_requires=['setuptools_scm>=3.5.0'],\n      zip_safe=False)\n",
        "summary": "This Python script sets up a package named 'hans' for solving 2D lubrication problems using the Height-Averaged Navier-Stokes (HANS) solver, with dependencies managed through a requirements file and including various scripts for plotting and configuration management."
    },
    {
        "code": "import numpy as np\nfrom .format_converter import FileBasedAnnotationConverter, ConverterReturn\nfrom ..representation import MultiLabelRecognitionAnnotation\nfrom ..utils import read_xml, check_file_existence\nfrom ..config import StringField, PathField, ConfigError\n\n\nclass CVATMultilabelAttributesRecognitionConverter(FileBasedAnnotationConverter):\n    __provider__ = 'cvat_multilabel_binary_attributes_recognition'\n    annotation_types = (MultiLabelRecognitionAnnotation, )\n\n    @classmethod\n    def parameters(cls):\n        configuration_parameters = super().parameters()\n        configuration_parameters.update({\n            'label': StringField(description='specific label for attribute collection'),\n            'images_dir': PathField(\n                is_directory=True, optional=True,\n                description='path to dataset images, used only for content existence check'\n            )\n        })\n        return configuration_parameters\n\n    def configure(self):\n        super().configure()\n        self.label = self.get_value_from_config('label')\n        self.images_dir = self.get_value_from_config('images_dir') or self.annotation_file.parent\n\n    def convert(self, check_content=False, progress_callback=None, progress_interval=100, **kwargs):\n        annotation = read_xml(self.annotation_file)\n        meta = annotation.find('meta')\n        size = int(meta.find('task').find('size').text)\n        label = self.select_label(meta)\n        label_to_id = {attribute.find('name').text: idx for idx, attribute in enumerate(label.iter('attribute'))}\n        num_attributes = len(label_to_id)\n\n        annotations = []\n        content_errors = None if not check_content else []\n        for image_id, image in enumerate(annotation.iter('image')):\n            identifier = image.attrib['name'].split('/')[-1]\n            if check_content:\n                if not check_file_existence(self.images_dir / identifier):\n                    content_errors.append('{}: does not exist'.format(self.images_dir / identifier))\n            for bbox in image:\n                if 'label' not in bbox.attrib.keys() or bbox.attrib['label'] != self.label:\n                    continue\n                bbox_rect = [\n                    float(bbox.attrib['xtl']), float(bbox.attrib['ytl']),\n                    float(bbox.attrib['xbr']), float(bbox.attrib['ybr'])\n                ]\n                attributes = -np.ones(num_attributes)\n                for attribute in bbox.iter('attribute'):\n                    attribute_name = attribute.attrib['name']\n                    attribute_label = label_to_id[attribute_name]\n                    attributes[attribute_label] = 1 if attribute.text == 'T' else 0\n                attributes_annotation = MultiLabelRecognitionAnnotation(identifier, attributes)\n                attributes_annotation.metadata['rect'] = bbox_rect\n                annotations.append(attributes_annotation)\n\n                if progress_callback is not None and image_id % progress_interval == 0:\n                    progress_callback(image_id * 100 / size)\n\n        return ConverterReturn(annotations, self.generate_meta(label_to_id), content_errors)\n\n    @staticmethod\n    def generate_meta(attribute_values_mapping):\n        return {'label_map': {value: key for key, value in attribute_values_mapping.items()}}\n\n    def select_label(self, meta):\n        label = [label for label in meta.iter('label') if label.find('name').text == self.label]\n        if not label:\n            raise ConfigError('{} does not present in annotation'.format(self.label))\n        return label[0]\n",
        "summary": "The `CVATMultilabelAttributesRecognitionConverter` class is a custom converter for handling multi-label recognition annotations from CVAT format, specifically designed to extract binary attributes associated with a specified label. It reads XML annotations, processes image data, and outputs structured MultiLabelRecognitionAnnotation objects, optionally checking the existence of image files and providing progress updates during conversion."
    },
    {
        "code": "import datetime\nimport json\nimport sys\nimport xml.parsers.expat\nimport xml.dom.minidom\n\nimport colorama\n\nfrom awscli.compat import six\nfrom awscli.customizations.history.commands import HistorySubcommand\nfrom awscli.customizations.history.filters import RegexFilter\n\n\nclass Formatter(object):\n    def __init__(self, output=None, include=None, exclude=None):\n        \n        self._output = output\n        if self._output is None:\n            self._output = sys.stdout\n        if include and exclude:\n            raise ValueError(\n                'Either input or exclude can be provided but not both')\n        self._include = include\n        self._exclude = exclude\n\n    def display(self, event_record):\n        \n        if self._should_display(event_record):\n            self._display(event_record)\n\n    def _display(self, event_record):\n        raise NotImplementedError('_display()')\n\n    def _should_display(self, event_record):\n        if self._include:\n            return event_record['event_type'] in self._include\n        elif self._exclude:\n            return event_record['event_type'] not in self._exclude\n        else:\n            return True\n\n\nclass DetailedFormatter(Formatter):\n    _SIG_FILTER = RegexFilter(\n        'Signature=([a-z0-9]{4})[a-z0-9]{60}',\n        r'Signature=\\1...',\n    )\n\n    _SECTIONS = {\n        'CLI_VERSION': {\n            'title': 'AWS CLI command entered',\n            'values': [\n                {'description': 'with AWS CLI version'}\n            ]\n        },\n        'CLI_ARGUMENTS': {\n            'values': [\n                {'description': 'with arguments'}\n            ]\n        },\n        'API_CALL': {\n            'title': 'API call made',\n            'values': [\n                {\n                    'description': 'to service',\n                    'payload_key': 'service'\n                },\n                {\n                    'description': 'using operation',\n                    'payload_key': 'operation'\n                },\n                {\n                    'description': 'with parameters',\n                    'payload_key': 'params',\n                    'value_format': 'dictionary'\n                }\n            ]\n        },\n        'HTTP_REQUEST': {\n            'title': 'HTTP request sent',\n            'values': [\n                {\n                    'description': 'to URL',\n                    'payload_key': 'url'\n                },\n                {\n                    'description': 'with method',\n                    'payload_key': 'method'\n                },\n                {\n                    'description': 'with headers',\n                    'payload_key': 'headers',\n                    'value_format': 'dictionary',\n                    'filters': [_SIG_FILTER]\n                },\n                {\n                    'description': 'with body',\n                    'payload_key': 'body',\n                    'value_format': 'http_body'\n                }\n\n            ]\n        },\n        'HTTP_RESPONSE': {\n            'title': 'HTTP response received',\n            'values': [\n                {\n                    'description': 'with status code',\n                    'payload_key': 'status_code'\n                },\n                {\n                    'description': 'with headers',\n                    'payload_key': 'headers',\n                    'value_format': 'dictionary'\n                },\n                {\n                    'description': 'with body',\n                    'payload_key': 'body',\n                    'value_format': 'http_body'\n                }\n            ]\n        },\n        'PARSED_RESPONSE': {\n            'title': 'HTTP response parsed',\n            'values': [\n                {\n                    'description': 'parsed to',\n                    'value_format': 'dictionary'\n                }\n            ]\n        },\n        'CLI_RC': {\n            'title': 'AWS CLI command exited',\n            'values': [\n                {'description': 'with return code'}\n            ]\n        },\n    }\n\n    _COMPONENT_COLORS = {\n        'title': colorama.Style.BRIGHT,\n        'description': colorama.Fore.CYAN\n    }\n\n    def __init__(self, output=None, include=None, exclude=None, colorize=True):\n        super(DetailedFormatter, self).__init__(output, include, exclude)\n        self._request_id_to_api_num = {}\n        self._num_api_calls = 0\n        self._colorize = colorize\n        self._value_pformatter = SectionValuePrettyFormatter()\n        if self._colorize:\n            colorama.init(autoreset=True, strip=False)\n\n    def _display(self, event_record):\n        section_definition = self._SECTIONS.get(event_record['event_type'])\n        if section_definition is not None:\n            self._display_section(event_record, section_definition)\n\n    def _display_section(self, event_record, section_definition):\n        if 'title' in section_definition:\n            self._display_title(section_definition['title'], event_record)\n        for value_definition in section_definition['values']:\n            self._display_value(value_definition, event_record)\n\n    def _display_title(self, title, event_record):\n        formatted_title = self._format_section_title(title, event_record)\n        self._write_output(formatted_title)\n\n    def _display_value(self, value_definition, event_record):\n        value_description = value_definition['description']\n        event_record_payload = event_record['payload']\n        value = event_record_payload\n        if 'payload_key' in value_definition:\n            value = event_record_payload[value_definition['payload_key']]\n        formatted_value = self._format_description(value_description)\n        formatted_value += self._format_value(\n            value, event_record, value_definition.get('value_format')\n        )\n        if 'filters' in value_definition:\n            for text_filter in value_definition['filters']:\n                formatted_value = text_filter.filter_text(formatted_value)\n        self._write_output(formatted_value)\n\n    def _write_output(self, content):\n        if isinstance(content, six.text_type):\n            content = content.encode('utf-8')\n        self._output.write(content)\n\n    def _format_section_title(self, title, event_record):\n        formatted_title = title\n        api_num = self._get_api_num(event_record)\n        if api_num is not None:\n            formatted_title = ('[%s] ' % api_num) + formatted_title\n        formatted_title = self._color_if_configured(formatted_title, 'title')\n        formatted_title += '\\n'\n\n        formatted_timestamp = self._format_description('at time')\n        formatted_timestamp += self._format_value(\n            event_record['timestamp'], event_record, value_format='timestamp')\n\n        return '\\n' + formatted_title + formatted_timestamp\n\n    def _get_api_num(self, event_record):\n        request_id = event_record['request_id']\n        if request_id:\n            if request_id not in self._request_id_to_api_num:\n                self._request_id_to_api_num[\n                    request_id] = self._num_api_calls\n                self._num_api_calls += 1\n            return self._request_id_to_api_num[request_id]\n\n    def _format_description(self, value_description):\n        return self._color_if_configured(\n            value_description + ': ', 'description')\n\n    def _format_value(self, value, event_record, value_format=None):\n        if value_format:\n            formatted_value = self._value_pformatter.pformat(\n                value, value_format, event_record)\n        else:\n            formatted_value = str(value)\n        return formatted_value + '\\n'\n\n    def _color_if_configured(self, text, component):\n        if self._colorize:\n            color = self._COMPONENT_COLORS[component]\n            return color + text + colorama.Style.RESET_ALL\n        return text\n\n\nclass SectionValuePrettyFormatter(object):\n    def pformat(self, value, value_format, event_record):\n        return getattr(self, '_pformat_' + value_format)(value, event_record)\n\n    def _pformat_timestamp(self, event_timestamp, event_record=None):\n        return datetime.datetime.fromtimestamp(\n            event_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n\n    def _pformat_dictionary(self, obj, event_record=None):\n        return json.dumps(obj=obj, sort_keys=True, indent=4)\n\n    def _pformat_http_body(self, body, event_record):\n        if not body:\n            return 'There is no associated body'\n        elif event_record['payload'].get('streaming', False):\n            return 'The body is a stream and will not be displayed'\n        elif self._is_xml(body):\n            \n            \n            \n            \n            \n            \n            return self._get_pretty_xml(body)\n        elif self._is_json_structure(body):\n            return self._get_pretty_json(body)\n        else:\n            return body\n\n    def _get_pretty_xml(self, body):\n        \n        \n        \n        \n        \n        \n        stripped_body = self._strip_whitespace(body)\n        xml_dom = xml.dom.minidom.parseString(stripped_body)\n        return xml_dom.toprettyxml(indent=' '*4, newl='\\n')\n\n    def _get_pretty_json(self, body):\n        \n        \n        obj = json.loads(body)\n        return self._pformat_dictionary(obj)\n\n    def _is_xml(self, body):\n        try:\n            xml.dom.minidom.parseString(body)\n        except xml.parsers.expat.ExpatError:\n            return False\n        return True\n\n    def _strip_whitespace(self, xml_string):\n        xml_dom = xml.dom.minidom.parseString(xml_string)\n        return ''.join(\n            [line.strip() for line in xml_dom.toxml().splitlines()]\n        )\n\n    def _is_json_structure(self, body):\n        if body.startswith('{'):\n            try:\n                json.loads(body)\n                return True\n            except json.decoder.JSONDecodeError:\n                return False\n        return False\n\n\nclass ShowCommand(HistorySubcommand):\n    NAME = 'show'\n    DESCRIPTION = (\n        'Shows the various events related to running a specific CLI command. '\n        'If this command is ran without any positional arguments, it will '\n        'display the events for the last CLI command ran.'\n    )\n    FORMATTERS = {\n        'detailed': DetailedFormatter\n    }\n    ARG_TABLE = [\n        {'name': 'command_id', 'nargs': '?', 'default': 'latest',\n         'positional_arg': True,\n         'help_text': (\n             'The ID of the CLI command to show. If this positional argument '\n             'is omitted, it will show the last the CLI command ran.')},\n        {'name': 'include', 'nargs': '+',\n         'help_text': (\n             'Specifies which events to **only** include when showing the '\n             'CLI command. This argument is mutually exclusive with '\n             '``--exclude``.')},\n        {'name': 'exclude', 'nargs': '+',\n         'help_text': (\n             'Specifies which events to exclude when showing the '\n             'CLI command. This argument is mutually exclusive with '\n             '``--include``.')},\n        {'name': 'format', 'choices': FORMATTERS.keys(),\n         'default': 'detailed', 'help_text': (\n            'Specifies which format to use in showing the events for '\n            'the specified CLI command. The following formats are '\n            'supported:\\n\\n'\n            '<ul>'\n            '<li> detailed - This the default format. It prints out a '\n            'detailed overview of the CLI command ran. It displays all '\n            'of the key events in the command lifecycle where each '\n            'important event has a title and its important values '\n            'underneath. The events are ordered by timestamp and events of '\n            'the same API call are associated together with the '\n            '[``api_id``] notation where events that share the same '\n            '``api_id`` belong to the lifecycle of the same API call.'\n            '</li>'\n            '</ul>'\n            )\n         }\n    ]\n\n    def _run_main(self, parsed_args, parsed_globals):\n        self._connect_to_history_db()\n        try:\n            self._validate_args(parsed_args)\n            with self._get_output_stream() as output_stream:\n                formatter = self._get_formatter(\n                    parsed_args, parsed_globals, output_stream)\n                for record in self._get_record_iterator(parsed_args):\n                    formatter.display(record)\n        finally:\n            self._close_history_db()\n        return 0\n\n    def _validate_args(self, parsed_args):\n        if parsed_args.exclude and parsed_args.include:\n            raise ValueError(\n                'Either --exclude or --include can be provided but not both')\n\n    def _get_formatter(self, parsed_args, parsed_globals, output_stream):\n        format_type = parsed_args.format\n        formatter_kwargs = {\n            'include': parsed_args.include,\n            'exclude': parsed_args.exclude,\n            'output': output_stream\n        }\n        if format_type == 'detailed':\n            formatter_kwargs['colorize'] = self._should_use_color(\n                parsed_globals)\n        return self.FORMATTERS[format_type](**formatter_kwargs)\n\n    def _get_record_iterator(self, parsed_args):\n        if parsed_args.command_id == 'latest':\n            return self._db_reader.iter_latest_records()\n        else:\n            return self._db_reader.iter_records(parsed_args.command_id)\n",
        "summary": "This code defines a command-line interface (CLI) tool for displaying events related to running AWS CLI commands. The main functionality is provided by the `ShowCommand` class, which extends `HistorySubcommand`. Here's a breakdown of its key components and functionalities:\n\n### Key Components\n\n1. **Formatter Classes**:\n   - `DetailedFormatter`: This formatter provides a detailed overview of the CLI command, displaying important events with titles and values. Events are ordered by timestamp, and events from the same API call are grouped together.\n\n2. **Command Arguments**:\n   - The `ARG_TABLE` defines the arguments that can be passed to the `show` command:\n     - `command_id`: Optional positional argument specifying the ID of the CLI command to show. If omitted, it shows the last CLI command.\n     - `include`: List of event types to include in the output.\n     - `exclude`: List of event types to exclude from the output.\n     - `format`: Specifies the format of the output (currently only 'detailed' is supported).\n\n3. **Main Execution Method**:\n   - `_run_main`: This method connects to the history database, validates arguments, and iterates through records to display them using the selected formatter.\n\n### Detailed Breakdown\n\n#### Formatter Classes\n\n- **DetailedFormatter**:\n  - Inherits from `BaseFormatter`.\n  - Uses a dictionary of event types to determine which events to include or exclude.\n  - Groups events by their API call ID for better organization.\n  - Supports colorized output if specified.\n\n#### Command Arguments\n\n- **ARG_TABLE**:\n  - Defines the command-line arguments:\n    - `command_id`: Optional positional argument specifying the CLI command ID.\n    - `include`: List of event types to include.\n    - `exclude`: List of event types to exclude.\n    - `format`: Specifies the output format (currently only 'detailed' is supported).\n\n#### Main Execution Method\n\n- **_run_main**:\n  - Connects to the history database using `_connect_to_history_db`.\n  - Validates arguments using `_validate_args`.\n  - Opens an output stream and creates a formatter based on the specified format.\n  - Iterates through records using `_get_record_iterator` and displays them using the formatter.\n\n### Usage Example\n\nTo display events for the last CLI command:\n```sh\naws history show\n```\n\nTo display events for a specific CLI command by ID:\n```sh\naws history show <command_id>\n```\n\nTo include only certain event types:\n```sh\naws history show --include event1 event2\n```\n\nTo exclude certain event types:\n```sh\naws history show --exclude event3 event4\n```\n\n### Notes\n\n- The code assumes the existence of a `HistorySubcommand` class and related methods (`_connect_to_history_db`, `_close_history_db`, etc.).\n- The `DetailedFormatter` uses a custom XML parser to format XML data in a more readable way.\n- The `ShowCommand` class extends `HistorySubcommand`, which suggests that it is part of a larger system for managing and displaying AWS CLI command history.\n\nThis tool provides a convenient way to review the events associated with specific AWS CLI commands, making it easier to understand the flow and outcomes of these commands."
    },
    {
        "code": "from __future__ import unicode_literals\n\nimport platform as pf\n\nfrom . import core\n\n\nclass PlatformCollector(object):\n    \n\n    def __init__(self, registry=core.REGISTRY, platform=None):\n        self._platform = pf if platform is None else platform\n        info = self._info()\n        system = self._platform.system()\n        if system == \"Java\":\n            info.update(self._java())\n        self._metrics = [\n            self._add_metric(\"python_info\", \"Python platform information\", info)\n        ]\n        if registry:\n            registry.register(self)\n\n    def collect(self):\n        return self._metrics\n\n    @staticmethod\n    def _add_metric(name, documentation, data):\n        labels = data.keys()\n        values = [data[k] for k in labels]\n        g = core.GaugeMetricFamily(name, documentation, labels=labels)\n        g.add_metric(values, 1)\n        return g\n\n    def _info(self):\n        major, minor, patchlevel = self._platform.python_version_tuple()\n        return {\n            \"version\": self._platform.python_version(),\n            \"implementation\": self._platform.python_implementation(),\n            \"major\": major,\n            \"minor\": minor,\n            \"patchlevel\": patchlevel\n        }\n\n    def _java(self):\n        java_version, _, vminfo, osinfo = self._platform.java_ver()\n        vm_name, vm_release, vm_vendor = vminfo\n        return {\n            \"jvm_version\": java_version,\n            \"jvm_release\": vm_release,\n            \"jvm_vendor\": vm_vendor,\n            \"jvm_name\": vm_name\n        }\n\n\nPLATFORM_COLLECTOR = PlatformCollector()\n\n",
        "summary": "The provided Python code defines a `PlatformCollector` class that collects and organizes platform information, including Python version details and JVM information if running on Java. It uses the `platform` module to gather data and registers itself with a registry for metric collection. The `_add_metric` method is used to format this collected data into metrics suitable for monitoring systems."
    },
    {
        "code": "from client_database_connection import mycursor\r\nimport os\r\n\r\n\r\nsql = \"INSERT INTO free_node (node_id) VALUES (%s)\"\r\nval = (node_id)\r\nmycursor.execute(sql, val)\r\ncommand = 'python get_code_when_free.py'\r\nos.system(command)",
        "summary": "The Python script inserts a new node ID into the `free_node` table using a cursor from a client database connection and then executes an external command to run another Python script named `get_code_when_free.py`."
    },
    {
        "code": "import os\nimport sys\n\n\ndef main():\n    \n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'translate.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "This Python script sets the Django settings module, imports Django's management utilities, and executes commands passed via command line arguments. It ensures that Django is properly installed and accessible in the environment before running any management commands."
    },
    {
        "code": "import binascii\nfrom binascii import unhexlify\nimport unittest\n\nimport aiounittest\nfrom monero_glue.xmr import common, crypto\nfrom monero_glue.xmr.core import ec_py\n\n\nclass CryptoTest(aiounittest.AsyncTestCase):\n    \n\n    def __init__(self, *args, **kwargs):\n        super(CryptoTest, self).__init__(*args, **kwargs)\n\n    def test_ed_crypto(self):\n        sqr = ec_py.fe_expmod(ec_py.py_fe_sqrtm1, 2)\n        self.assertEqual(sqr, ec_py.fe_mod(-1))\n        self.assertEqual(\n            ec_py.py_fe_A, ec_py.fe_mod(2 * (1 - ec_py.d) * ec_py.inv(1 + ec_py.py_d))\n        )\n\n        self.assertEqual(\n            ec_py.fe_expmod(ec_py.py_fe_fffb1, 2),\n            ec_py.fe_mod(-2 * ec_py.py_fe_A * (ec_py.py_fe_A + 2)),\n        )\n        self.assertEqual(\n            ec_py.fe_expmod(ec_py.py_fe_fffb2, 2),\n            ec_py.fe_mod(2 * ec_py.py_fe_A * (ec_py.py_fe_A + 2)),\n        )\n        self.assertEqual(\n            ec_py.fe_expmod(ec_py.py_fe_fffb3, 2),\n            ec_py.fe_mod(-ec_py.py_fe_sqrtm1 * ec_py.py_fe_A * (ec_py.py_fe_A + 2)),\n        )\n        self.assertEqual(\n            ec_py.fe_expmod(ec_py.py_fe_fffb4, 2),\n            ec_py.fe_mod(ec_py.py_fe_sqrtm1 * ec_py.py_fe_A * (ec_py.py_fe_A + 2)),\n        )\n\n    def test_encoding(self):\n        point = unhexlify(\n            b\"2486224797d05cae3cba4be043be2db0df381f3f19cfa113f86ab38e3d8d2bd0\"\n        )\n        self.assertEqual(point, crypto.encodepoint(crypto.decodepoint(point)))\n        self.assertTrue(\n            crypto.point_eq(\n                crypto.decodepoint(point),\n                crypto.decodepoint(crypto.encodepoint(crypto.decodepoint(point))),\n            )\n        )\n\n    def test_scalarmult_base(self):\n        scalar = crypto.decodeint(\n            unhexlify(\n                b\"a0eea49140a3b036da30eacf64bd9d56ce3ef68ba82ef13571ec511edbcf8303\"\n            )\n        )\n        exp = unhexlify(\n            b\"16bb4a3c44e2ced511fc0d4cd86b13b3af21efc99fb0356199fac489f2544c09\"\n        )\n        res = crypto.scalarmult_base(scalar)\n        self.assertEqual(exp, crypto.encodepoint(res))\n        self.assertTrue(crypto.point_eq(crypto.decodepoint(exp), res))\n\n        scalar = crypto.decodeint(\n            unhexlify(\n                b\"fd290dce39f781aebbdbd24584ed6d48bd300de19d9c3decfda0a6e2c6751d0f\"\n            )\n        )\n        exp = unhexlify(\n            b\"123daf90fc26f13c6529e6b49bfed498995ac383ef19c0db6771143f24ba8dd5\"\n        )\n        res = crypto.scalarmult_base(scalar)\n        self.assertEqual(exp, crypto.encodepoint(res))\n        self.assertTrue(crypto.point_eq(crypto.decodepoint(exp), res))\n\n    def test_scalarmult(self):\n        priv = unhexlify(\n            b\"3482fb9735ef879fcae5ec7721b5d3646e155c4fb58d6cc11c732c9c9b76620a\"\n        )\n        pub = unhexlify(\n            b\"2486224797d05cae3cba4be043be2db0df381f3f19cfa113f86ab38e3d8d2bd0\"\n        )\n        exp = unhexlify(\n            b\"adcd1f5881f46f254900a03c654e71950a88a0236fa0a3a946c9b8daed6ef43d\"\n        )\n        res = crypto.scalarmult(crypto.decodepoint(pub), crypto.decodeint(priv))\n        self.assertEqual(exp, crypto.encodepoint(res))\n        self.assertTrue(crypto.point_eq(crypto.decodepoint(exp), res))\n\n    def test_cn_fast_hash(self):\n        inp = unhexlify(\n            b\"259ef2aba8feb473cf39058a0fe30b9ff6d245b42b6826687ebd6b63128aff6405\"\n        )\n        res = crypto.cn_fast_hash(inp)\n        self.assertEqual(\n            res,\n            unhexlify(\n                b\"86db87b83fb1246efca5f3b0db09ce3fa4d605b0d10e6507cac253dd31a3ec16\"\n            ),\n        )\n\n    def test_hash_to_scalar(self):\n        inp = unhexlify(\n            b\"259ef2aba8feb473cf39058a0fe30b9ff6d245b42b6826687ebd6b63128aff6405\"\n        )\n        res = crypto.hash_to_scalar(inp)\n        exp = crypto.decodeint(binascii.unhexlify(\n            b\"9907925b254e12162609fc0dfd0fef2aa4d605b0d10e6507cac253dd31a3ec06\"))\n        self.assertTrue(crypto.sc_eq(res, exp))\n\n    def test_hash_to_point(self):\n        data = unhexlify(\n            b\"42f6835bf83114a1f5f6076fe79bdfa0bd67c74b88f127d54572d3910dd09201\"\n        )\n        res = crypto.hash_to_point(data)\n        res_p = crypto.encodepoint(res)\n        self.assertEqual(\n            res_p,\n            unhexlify(\n                b\"54863a0464c008acc99cffb179bc6cf34eb1bbdf6c29f7a070a7c6376ae30ab5\"\n            ),\n        )\n\n    def test_derivation_to_scalar(self):\n        derivation = unhexlify(\n            b\"e720a09f2e3a0bbf4e4ba7ad93653bb296885510121f806acb2a5f9168fafa01\"\n        )\n        scalar = unhexlify(\n            b\"25d08763414c379aa9cf989cdcb3cadd36bd5193b500107d6bf5f921f18e470e\"\n        )\n        sc_int = crypto.derivation_to_scalar(crypto.decodepoint(derivation), 0)\n        self.assertEqual(scalar, crypto.encodeint(sc_int))\n\n    def test_generate_key_derivation(self):\n        key_pub = crypto.decodepoint(\n            unhexlify(\n                b\"7739c95d3298e2f87362dba9e0e0b3980a692ae8e2f16796b0e382098cd6bd83\"\n            )\n        )\n        key_priv = crypto.decodeint(\n            unhexlify(\n                b\"3482fb9735ef879fcae5ec7721b5d3646e155c4fb58d6cc11c732c9c9b76620a\"\n            )\n        )\n        deriv_exp = unhexlify(\n            b\"fa188a45a0e4daccc0e6d4f6f6858fd46392104be74183ec0047e7e9f4eaf739\"\n        )\n        self.assertEqual(\n            deriv_exp,\n            crypto.encodepoint(crypto.generate_key_derivation(key_pub, key_priv)),\n        )\n\n    def test_h(self):\n        H = unhexlify(\n            b\"8b655970153799af2aeadc9ff1add0ea6c7251d54154cfa92c173a0dd39c1f94\"\n        )\n        self.assertEqual(crypto.encodepoint(crypto.xmr_H()), H)\n\n    def test_h_pow(self):\n        hp = crypto.gen_Hpow(10)\n        self.assertEqual(crypto.encodepoint(hp[0]), crypto.encodepoint(crypto.xmr_H()))\n        for i in range(1, 10):\n            crypto.check_ed25519point(hp[i])\n            self.assertEqual(\n                crypto.encodepoint(hp[i]),\n                crypto.encodepoint(\n                    crypto.scalarmult(crypto.xmr_H(), crypto.sc_init(2 ** i))\n                ),\n            )\n\n    def test_signature(self):\n        for i in range(10):\n            priv = crypto.random_scalar()\n            data = crypto.cn_fast_hash(bytes(bytearray([i])))\n\n            c, r, pub = crypto.generate_signature(data, priv)\n            res = crypto.check_signature(data, c, r, pub)\n            self.assertEqual(res, 1)\n\n            res2 = crypto.check_signature(\n                data, crypto.sc_add(c, crypto.sc_init(1)), r, pub\n            )\n            self.assertEqual(res2, 0)\n\n    def test_edhex(self):\n        inputs = [crypto.q - 2 ** 9, crypto.q - 10, 0, 100, 2 ** 200 + 10] + [\n            common.rand.randrange(0, crypto.q - 2) for _ in range(20)\n        ]\n\n        for x in inputs:\n            l = crypto.encode_ed25519(x)\n            d = crypto.decode_ed25519(l)\n            self.assertEqual(x, d)\n\n    def test_modm(self):\n        inputs = [crypto.l - 2 ** 9, crypto.l - 10, 0, 100, 2 ** 200 + 10] + [\n            common.rand.randrange(0, crypto.l - 2) for _ in range(20)\n        ]\n\n        for x in inputs:\n            l = crypto.encode_modm(x)\n            d = crypto.decode_modm(l)\n            self.assertEqual(x, d)\n\n    def test_ge25519_double_scalarmult_vartime2(self):\n        for i in range(10):\n            ap = crypto.random_scalar()\n            bp = crypto.random_scalar()\n            A = crypto.scalarmult_base(ap)\n            B = crypto.scalarmult_base(bp)\n            a = crypto.random_scalar()\n            b = crypto.random_scalar()\n\n            R = crypto.ge_double_scalarmult_base_vartime2(a, A, b, B)\n            R_exp = crypto.point_add(crypto.scalarmult(A, a), crypto.scalarmult(B, b))\n            self.assertTrue(crypto.point_eq(R, R_exp))\n\n    def test_ge25519_double_scalarmult_vartime(self):\n        for i in range(10):\n            ap = crypto.random_scalar()\n            A = crypto.scalarmult_base(ap)\n            a = crypto.random_scalar()\n            b = crypto.random_scalar()\n\n            R = crypto.ge_double_scalarmult_base_vartime(a, A, b)\n            R_exp = crypto.point_add(crypto.scalarmult(A, a), crypto.scalarmult_base(b))\n            self.assertTrue(crypto.point_eq(R, R_exp))\n\n    def test_pointadd(self):\n        a = crypto.random_scalar()\n        A = crypto.scalarmult_base(a)\n        A2 = crypto.point_add(A, A)\n        A3 = crypto.point_add(A2, A)\n        A4 = crypto.point_add(A3, A)\n        A8 = crypto.scalarmult(A4, crypto.sc_init(2))\n\n        A8p = crypto.point_mul8(A)\n        self.assertTrue(crypto.point_eq(A8p, A8))\n        self.assertTrue(crypto.point_eq(A4, crypto.scalarmult(A, crypto.sc_init(4))))\n        self.assertTrue(crypto.point_eq(A3, crypto.scalarmult(A, crypto.sc_init(3))))\n\n    def test_sc_inversion(self):\n        res = crypto.new_scalar()\n        inp = crypto.decodeint(\n            unhexlify(\n                b\"3482fb9735ef879fcae5ec7721b5d3646e155c4fb58d6cc11c732c9c9b76620a\"\n            )\n        )\n\n        crypto.sc_inv_into(res, inp)\n        self.assertEqual(\n            binascii.hexlify(crypto.encodeint(res)),\n            b\"bcf365a551e6358f3f281a6241d4a25eded60230b60a1d48c67b51a85e33d70e\",\n        )\n        \n\nif __name__ == \"__main__\":\n    unittest.main()  \n",
        "summary": "This code is a Python unit test suite for a cryptographic library. It tests various functions and operations related to elliptic curve cryptography, specifically using the Ed25519 curve. The library appears to be designed for use in cryptocurrency applications.\n\nHere's a breakdown of what each part of the code does:\n\n1. **Imports**: The script imports necessary modules from Python's standard library (`unittest`, `binascii`) and custom modules (`common`, `crypto`).\n\n2. **Test Class**: A class named `TestCrypto` is defined, inheriting from `unittest.TestCase`. This class contains several test methods that will be executed to verify the correctness of cryptographic functions.\n\n3. **Test Methods**:\n   - Each method starts with the word \"test\" and follows a specific pattern related to the function being tested.\n   - For example, `test_signature` tests the signature generation and verification process.\n   - `test_edhex` and `test_modm` test encoding and decoding functions for large integers.\n   - `test_ge25519_double_scalarmult_vartime2` and `test_ge25519_double_scalarmult_vartime` test double scalar multiplication operations.\n   - `test_pointadd` tests point addition on the elliptic curve.\n   - `test_sc_inversion` tests scalar inversion.\n\n4. **Assertions**: Each test method uses assertions to check that the output of cryptographic functions matches expected results. For example, `self.assertEqual(res, 1)` checks if a signature verification returns `True`.\n\n5. **Setup and Teardown**: The class includes `setUp` and `tearDown` methods for setting up test environments before each test and cleaning them up afterward.\n\n6. **Main Execution**: The script ends with `if __name__ == \"__main__\": unittest.main()`, which allows the tests to be run directly from the command line when the script is executed.\n\nThis code serves as a comprehensive test suite for ensuring that the cryptographic library functions correctly, providing confidence in its reliability for use in secure applications like cryptocurrencies."
    },
    {
        "code": "from prometheus_client import CollectorRegistry\n\nfrom asyncworker.conf import settings\nfrom asyncworker.metrics.collectors.gc import GCCollector\nfrom asyncworker.metrics.collectors.platform import PlatformCollector\nfrom asyncworker.metrics.collectors.process import ProcessCollector\n\nNAMESPACE = (\n    f\"{settings.METRICS_NAMESPACE}_{settings.METRICS_APPPREFIX}\"\n    if settings.METRICS_APPPREFIX\n    else f\"{settings.METRICS_NAMESPACE}\"\n)\n\n\nREGISTRY = CollectorRegistry(auto_describe=True)\n\nPLATFORM_COLLECTOR = PlatformCollector(registry=REGISTRY, namespace=NAMESPACE)\nPROCESS_COLLECTOR = ProcessCollector(namespace=NAMESPACE, registry=REGISTRY)\nGC_COLLECTOR = GCCollector(registry=REGISTRY, namespace=NAMESPACE)\n",
        "summary": "The Python code sets up a Prometheus metrics collection system using the `prometheus_client` library. It defines a collector registry and initializes several specific collectors for platform, process, and garbage collection metrics, each configured with a custom namespace based on application settings."
    },
    {
        "code": "from typing import Dict, Optional, Union\n\nfrom ...error import GraphQLError\nfrom ...language import (\n    OperationTypeDefinitionNode,\n    OperationType,\n    SchemaDefinitionNode,\n    SchemaExtensionNode,\n)\nfrom ...type import GraphQLObjectType\nfrom . import SDLValidationContext, SDLValidationRule\n\n__all__ = [\"UniqueOperationTypesRule\"]\n\n\nclass UniqueOperationTypesRule(SDLValidationRule):\n    \n\n    def __init__(self, context: SDLValidationContext):\n        super().__init__(context)\n        schema = context.schema\n        self.defined_operation_types: Dict[\n            OperationType, OperationTypeDefinitionNode\n        ] = {}\n        self.existing_operation_types: Dict[\n            OperationType, Optional[GraphQLObjectType]\n        ] = (\n            {\n                OperationType.QUERY: schema.query_type,\n                OperationType.MUTATION: schema.mutation_type,\n                OperationType.SUBSCRIPTION: schema.subscription_type,\n            }\n            if schema\n            else {}\n        )\n        self.schema = schema\n\n    def check_operation_types(\n        self, node: Union[SchemaDefinitionNode, SchemaExtensionNode], *_args\n    ):\n        for operation_type in node.operation_types or []:\n            operation = operation_type.operation\n            already_defined_operation_type = self.defined_operation_types.get(operation)\n\n            if self.existing_operation_types.get(operation):\n                self.report_error(\n                    GraphQLError(\n                        f\"Type for {operation.value} already defined in the schema.\"\n                        \" It cannot be redefined.\",\n                        operation_type,\n                    )\n                )\n            elif already_defined_operation_type:\n                self.report_error(\n                    GraphQLError(\n                        f\"There can be only one {operation.value} type in schema.\",\n                        [already_defined_operation_type, operation_type],\n                    )\n                )\n            else:\n                self.defined_operation_types[operation] = operation_type\n        return self.SKIP\n\n    enter_schema_definition = enter_schema_extension = check_operation_types\n",
        "summary": "The `UniqueOperationTypesRule` class in Python is a schema validation rule for GraphQL that ensures there is only one definition of each operation type (query, mutation, subscription) in the schema. It checks both the main schema definition and any extensions for duplicate or conflicting definitions, reporting errors if such conflicts are found."
    },
    {
        "code": "from enum import Enum\n\n__all__ = [\n    'CostAllocationPolicyType',\n    'CostAllocationResourceType',\n    'RuleStatus',\n]\n\n\nclass CostAllocationPolicyType(str, Enum):\n    \n    FIXED_PROPORTION = \"FixedProportion\"\n\n\nclass CostAllocationResourceType(str, Enum):\n    \n    DIMENSION = \"Dimension\"\n    TAG = \"Tag\"\n\n\nclass RuleStatus(str, Enum):\n    \n    NOT_ACTIVE = \"NotActive\"\n    ACTIVE = \"Active\"\n    PROCESSING = \"Processing\"\n",
        "summary": "The Python code defines an enumeration module for cost allocation policies and resources, including types of policies (FIXED_PROPORTION), resource types (DIMENSION, TAG), and rule statuses (NOT_ACTIVE, ACTIVE, PROCESSING)."
    },
    {
        "code": "import os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom lib.utils.bbox_transform import decode_bbox_target\nfrom tools.kitti_object_eval_python.evaluate import evaluate as kitti_evaluate\n\nfrom lib.config import cfg\nimport lib.utils.kitti_utils as kitti_utils\nimport lib.utils.iou3d.iou3d_utils as iou3d_utils\nfrom datetime import datetime\nfrom tensorboardX import SummaryWriter\nimport tqdm\n\nnp.random.seed(1024)  \n\n\ndef save_kitti_format(sample_id, calib, bbox3d, kitti_output_dir, scores, img_shape):\n    corners3d = kitti_utils.boxes3d_to_corners3d(bbox3d)\n    img_boxes, _ = calib.corners3d_to_img_boxes(corners3d)\n\n    img_boxes[:, 0] = np.clip(img_boxes[:, 0], 0, img_shape[1] - 1)\n    img_boxes[:, 1] = np.clip(img_boxes[:, 1], 0, img_shape[0] - 1)\n    img_boxes[:, 2] = np.clip(img_boxes[:, 2], 0, img_shape[1] - 1)\n    img_boxes[:, 3] = np.clip(img_boxes[:, 3], 0, img_shape[0] - 1)\n\n    img_boxes_w = img_boxes[:, 2] - img_boxes[:, 0]\n    img_boxes_h = img_boxes[:, 3] - img_boxes[:, 1]\n    box_valid_mask = np.logical_and(\n        img_boxes_w < img_shape[1] * 0.8, img_boxes_h < img_shape[0] * 0.8)\n\n    kitti_output_file = os.path.join(kitti_output_dir, '%06d.txt' % sample_id)\n    with open(kitti_output_file, 'w') as f:\n        for k in range(bbox3d.shape[0]):\n            if box_valid_mask[k] == 0:\n                continue\n            x, z, ry = bbox3d[k, 0], bbox3d[k, 2], bbox3d[k, 6]\n            beta = np.arctan2(z, x)\n            alpha = -np.sign(beta) * np.pi / 2 + beta + ry\n\n            print('%s -1 -1 %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f' %\n                  (cfg.CLASSES, alpha, img_boxes[k, 0], img_boxes[k, 1], img_boxes[k, 2], img_boxes[k, 3],\n                   bbox3d[k, 3], bbox3d[k, 4], bbox3d[k,\n                                                      5], bbox3d[k, 0], bbox3d[k, 1], bbox3d[k, 2],\n                   bbox3d[k, 6], scores[k]), file=f)\n\n\ndef eval_one_epoch_joint(model, dataloader, epoch_id, result_dir):\n    \n    np.random.seed(666)\n    MEAN_SIZE = torch.from_numpy(cfg.CLS_MEAN_SIZE[0]).cuda()\n    mode = 'EVAL'\n\n    final_output_dir = os.path.join(result_dir, 'final_result', 'data')\n    os.makedirs(final_output_dir, exist_ok=True)\n\n    if True:\n        \n        roi_output_dir = os.path.join(result_dir, 'roi_result', 'data')\n        refine_output_dir = os.path.join(result_dir, 'refine_result', 'data')\n        rpn_output_dir = os.path.join(result_dir, 'rpn_result', 'data')\n        os.makedirs(rpn_output_dir, exist_ok=True)\n        os.makedirs(roi_output_dir, exist_ok=True)\n        os.makedirs(refine_output_dir, exist_ok=True)\n\n    model.eval()\n\n    thresh_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n    total_recalled_bbox_list, total_gt_bbox = [0] * 5, 0\n    total_roi_recalled_bbox_list = [0] * 5\n    dataset = dataloader.dataset\n    cnt = final_total = total_cls_acc = total_cls_acc_refined = total_rpn_iou = 0\n\n    progress_bar = tqdm.tqdm(total=len(dataloader), leave=True, desc='eval')\n    for data in dataloader:\n        cnt += 1\n        calib = data['calib']\n        sample_id, pts_rect, pts_features, pts_input = \\\n            data['sample_id'], data['pts_rect'], data['pts_features'], data['pts_input']\n        batch_size = len(sample_id)\n        inputs = torch.from_numpy(pts_input).cuda(non_blocking=True).float()\n        input_data = {'pts_input': inputs, 'calib': calib}\n\n        \n        ret_dict = model(input_data)\n        print(ret_dict.key())\n\n        roi_scores_raw = ret_dict['roi_scores_raw']  \n        roi_boxes3d = ret_dict['rois']  \n        seg_result = ret_dict['seg_result'].long()  \n\n        rcnn_cls = ret_dict['rcnn_cls'].view(\n            batch_size, -1, ret_dict['rcnn_cls'].shape[1])\n        rcnn_reg = ret_dict['rcnn_reg'].view(\n            batch_size, -1, ret_dict['rcnn_reg'].shape[1])  \n\n        \n        anchor_size = MEAN_SIZE\n        if cfg.RCNN.SIZE_RES_ON_ROI:\n            assert False\n\n        pred_boxes3d = decode_bbox_target(roi_boxes3d.view(-1, 7), rcnn_reg.view(-1, rcnn_reg.shape[-1]),\n                                          anchor_size=anchor_size,\n                                          loc_scope=cfg.RCNN.LOC_SCOPE,\n                                          loc_bin_size=cfg.RCNN.LOC_BIN_SIZE,\n                                          num_head_bin=cfg.RCNN.NUM_HEAD_BIN,\n                                          get_xz_fine=True, get_y_by_bin=cfg.RCNN.LOC_Y_BY_BIN,\n                                          loc_y_scope=cfg.RCNN.LOC_Y_SCOPE, loc_y_bin_size=cfg.RCNN.LOC_Y_BIN_SIZE,\n                                          get_ry_fine=True).view(batch_size, -1, 7)\n\n        \n        if rcnn_cls.shape[2] == 1:\n            raw_scores = rcnn_cls  \n\n            norm_scores = torch.sigmoid(raw_scores)\n            pred_classes = (norm_scores > cfg.RCNN.SCORE_THRESH).long()\n        else:\n            pred_classes = torch.argmax(rcnn_cls, dim=1).view(-1)\n            cls_norm_scores = F.softmax(rcnn_cls, dim=1)\n            raw_scores = rcnn_cls[:, pred_classes]\n            norm_scores = cls_norm_scores[:, pred_classes]\n\n        \n        recalled_num = gt_num = rpn_iou = 0\n        if not False:\n            if not cfg.RPN.FIXED:\n                rpn_cls_label, rpn_reg_label = data['rpn_cls_label'], data['rpn_reg_label']\n                rpn_cls_label = torch.from_numpy(\n                    rpn_cls_label).cuda(non_blocking=True).long()\n\n            gt_boxes3d = data['gt_boxes3d']\n\n            for k in range(batch_size):\n                \n                cur_gt_boxes3d = gt_boxes3d[k]\n                tmp_idx = cur_gt_boxes3d.__len__() - 1\n\n                while tmp_idx >= 0 and cur_gt_boxes3d[tmp_idx].sum() == 0:\n                    tmp_idx -= 1\n\n                if tmp_idx >= 0:\n                    cur_gt_boxes3d = cur_gt_boxes3d[:tmp_idx + 1]\n\n                    cur_gt_boxes3d = torch.from_numpy(\n                        cur_gt_boxes3d).cuda(non_blocking=True).float()\n                    iou3d = iou3d_utils.boxes_iou3d_gpu(\n                        pred_boxes3d[k], cur_gt_boxes3d)\n                    gt_max_iou, _ = iou3d.max(dim=0)\n                    refined_iou, _ = iou3d.max(dim=1)\n\n                    for idx, thresh in enumerate(thresh_list):\n                        total_recalled_bbox_list[idx] += (\n                            gt_max_iou > thresh).sum().item()\n                    recalled_num += (gt_max_iou > 0.7).sum().item()\n                    gt_num += cur_gt_boxes3d.shape[0]\n                    total_gt_bbox += cur_gt_boxes3d.shape[0]\n\n                    \n                    iou3d_in = iou3d_utils.boxes_iou3d_gpu(\n                        roi_boxes3d[k], cur_gt_boxes3d)\n                    gt_max_iou_in, _ = iou3d_in.max(dim=0)\n\n                    for idx, thresh in enumerate(thresh_list):\n                        total_roi_recalled_bbox_list[idx] += (\n                            gt_max_iou_in > thresh).sum().item()\n\n                if not cfg.RPN.FIXED:\n                    fg_mask = rpn_cls_label > 0\n                    correct = ((seg_result == rpn_cls_label)\n                               & fg_mask).sum().float()\n                    union = fg_mask.sum().float() + (seg_result > 0).sum().float() - correct\n                    rpn_iou = correct / torch.clamp(union, min=1.0)\n                    total_rpn_iou += rpn_iou.item()\n\n        disp_dict = {\n            'mode': mode, 'recall': '%d/%d' % (total_recalled_bbox_list[3], total_gt_bbox)}\n        progress_bar.set_postfix(disp_dict)\n        progress_bar.update()\n\n        if True:\n            \n            roi_boxes3d_np = roi_boxes3d.cpu().numpy()\n            pred_boxes3d_np = pred_boxes3d.cpu().numpy()\n            roi_scores_raw_np = roi_scores_raw.cpu().numpy()\n            raw_scores_np = raw_scores.cpu().numpy()\n\n            rpn_cls_np = ret_dict['rpn_cls'].cpu().numpy()\n            rpn_xyz_np = ret_dict['backbone_xyz'].cpu().numpy()\n            seg_result_np = seg_result.cpu().numpy()\n            output_data = np.concatenate((rpn_xyz_np, rpn_cls_np.reshape(batch_size, -1, 1),\n                                          seg_result_np.reshape(batch_size, -1, 1)), axis=2)\n\n            for k in range(batch_size):\n                cur_sample_id = sample_id[k]\n                calib = dataset.get_calib(cur_sample_id)\n                image_shape = dataset.get_image_shape(cur_sample_id)\n                save_kitti_format(cur_sample_id, calib, roi_boxes3d_np[k], roi_output_dir,\n                                  roi_scores_raw_np[k], image_shape)\n                save_kitti_format(cur_sample_id, calib, pred_boxes3d_np[k], refine_output_dir,\n                                  raw_scores_np[k], image_shape)\n\n                output_file = os.path.join(\n                    rpn_output_dir, '%06d.npy' % cur_sample_id)\n                np.save(output_file, output_data.astype(np.float32))\n\n        \n        inds = norm_scores > cfg.RCNN.SCORE_THRESH\n\n        for k in range(batch_size):\n            cur_inds = inds[k].view(-1)\n            if cur_inds.sum() == 0:\n                continue\n\n            pred_boxes3d_selected = pred_boxes3d[k, cur_inds]\n            raw_scores_selected = raw_scores[k, cur_inds]\n            norm_scores_selected = norm_scores[k, cur_inds]\n\n            \n            \n            boxes_bev_selected = kitti_utils.boxes3d_to_bev_torch(\n                pred_boxes3d_selected)\n            keep_idx = iou3d_utils.nms_gpu(\n                boxes_bev_selected, raw_scores_selected, cfg.RCNN.NMS_THRESH).view(-1)\n            pred_boxes3d_selected = pred_boxes3d_selected[keep_idx]\n            scores_selected = raw_scores_selected[keep_idx]\n            pred_boxes3d_selected, scores_selected = pred_boxes3d_selected.cpu(\n            ).numpy(), scores_selected.cpu().numpy()\n\n            cur_sample_id = sample_id[k]\n            calib = dataset.get_calib(cur_sample_id)\n            final_total += pred_boxes3d_selected.shape[0]\n            image_shape = dataset.get_image_shape(cur_sample_id)\n            save_kitti_format(cur_sample_id, calib, pred_boxes3d_selected,\n                              final_output_dir, scores_selected, image_shape)\n\n    progress_bar.close()\n    \n    split_file = os.path.join(dataset.imageset_dir,\n                              '..', '..', 'ImageSets', dataset.split + '.txt')\n    split_file = os.path.abspath(split_file)\n    image_idx_list = [x.strip() for x in open(split_file).readlines()]\n    empty_cnt = 0\n    for k in range(image_idx_list.__len__()):\n        cur_file = os.path.join(final_output_dir, '%s.txt' % image_idx_list[k])\n        if not os.path.exists(cur_file):\n            with open(cur_file, 'w') as temp_f:\n                pass\n            empty_cnt += 1\n\n    ret_dict = {'empty_cnt': empty_cnt}\n\n    avg_rpn_iou = (total_rpn_iou / max(cnt, 1.0))\n    avg_cls_acc = (total_cls_acc / max(cnt, 1.0))\n    avg_cls_acc_refined = (total_cls_acc_refined / max(cnt, 1.0))\n    avg_det_num = (final_total / max(len(dataset), 1.0))\n\n    ret_dict['rpn_iou'] = avg_rpn_iou\n    ret_dict['rcnn_cls_acc'] = avg_cls_acc\n    ret_dict['rcnn_cls_acc_refined'] = avg_cls_acc_refined\n    ret_dict['rcnn_avg_num'] = avg_det_num\n\n    for idx, thresh in enumerate(thresh_list):\n        cur_roi_recall = total_roi_recalled_bbox_list[idx] / max(\n            total_gt_bbox, 1.0)\n\n        ret_dict['rpn_recall(thresh=%.2f)' % thresh] = cur_roi_recall\n\n    for idx, thresh in enumerate(thresh_list):\n        cur_recall = total_recalled_bbox_list[idx] / max(total_gt_bbox, 1.0)\n\n        ret_dict['rcnn_recall(thresh=%.2f)' % thresh] = cur_recall\n\n    if cfg.TEST.SPLIT != 'test':\n        name_to_class = {'Car': 0, 'Pedestrian': 1, 'Cyclist': 2}\n        ap_result_str, ap_dict = kitti_evaluate(dataset.label_dir, final_output_dir, label_split_file=split_file,\n                                                current_class=name_to_class[cfg.CLASSES])\n\n        ret_dict.update(ap_dict)\n\n    return ap_result_str\n",
        "summary": "This code snippet appears to be part of a larger system for evaluating the performance of an object detection model on the KITTI dataset. The function `evaluate` takes several arguments, including a dataset object, configuration settings, and paths for saving output files.\n\nThe main steps in this function are:\n\n1. Initialize counters and variables to keep track of various metrics during evaluation.\n2. Iterate over each sample in the dataset:\n   - Load the sample data (e.g., point cloud, image).\n   - Run inference using a pre-trained model to get predictions.\n   - Post-process the predictions to filter out low-confidence detections and apply non-maximum suppression.\n   - Save the raw predictions and refined detections in KITTI format for visualization or further analysis.\n3. After processing all samples, calculate various metrics such as average precision (AP), recall, RPN IoU, and classification accuracy.\n4. If evaluating on a split other than 'test', also compute AP values for each class.\n5. Return the results as a dictionary containing various performance metrics.\n\nThe function uses several helper functions and classes, including:\n- `kitti_utils`: For converting between different representations of 3D bounding boxes.\n- `iou3d_utils`: For computing IoU (Intersection over Union) between bounding boxes.\n- `dataset.get_calib`, `dataset.get_image_shape`: To retrieve calibration information and image dimensions for each sample.\n- `save_kitti_format`: A function to save predictions in KITTI format.\n\nThis code is designed to be flexible, allowing for different configurations and splits by using the provided arguments. It also includes detailed comments explaining each step of the evaluation process."
    },
    {
        "code": "from math import log10\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\nimport numpy as np\nfrom .utils import create_rng\n\n\nclass BO:\n    \n\n    def __init__(self, k, hidden_dim=(100, 10000),\n                 spectral_radius=(.9, 1.3), p=(0, 1),\n                 alpha=(0, 1), beta=(1e-5, 1e3), random_state=None):\n        \n        \n        \n        hyper_params = [k, hidden_dim, spectral_radius, p, alpha, beta]\n        for param in hyper_params:\n            assert isinstance(param, tuple), \"{} must be a tuple\".format(param)\n            assert len(param) == 2, \"{} must have two arguments; the upper\" \\\n                                    \"and lower bound\".format(param)\n\n        self.lwr_k = k[0]\n        self.upr_k = k[1]\n        self.lwr_hidden_dim = hidden_dim[0]\n        self.upr_hidden_dim = hidden_dim[1]\n        self.lwr_spectral_radius = spectral_radius[0]\n        self.upr_spectral_radius = spectral_radius[1]\n        self.lwr_p = p[0]\n        self.upr_p = p[1]\n        self.lwr_alpha = alpha[0]\n        self.upr_alpha = alpha[1]\n        self.lwr_beta = beta[0]\n        self.upr_beta = beta[1]\n\n        self.rng = create_rng(random_state)\n        self.gpr = GaussianProcessRegressor(kernel=Matern(),\n                                            random_state=self.rng)\n\n        \n        \n        self.H = []\n        self.y = []\n\n    def update_gpr(self, X, y):\n        \n        self.H.append(X)\n        self.y.append(y)\n\n        self.gpr.fit(self.H, self.y)\n\n    def _sample_uniformly(self, num_samples, lwr_bound, upr_bound):\n        \n        \n        \n        \n        new_lwr_bound = log10(lwr_bound)\n        new_upr_bound = log10(upr_bound)\n        samples = self.rng.uniform(low=new_lwr_bound, high=new_upr_bound,\n                                   size=(num_samples, 1))\n        param_vals = np.power(10, samples)\n        return param_vals\n\n    def _build_options(self, num_samples=1000):\n        \n        k_vals = self.rng.randint(low=self.lwr_k, high=self.upr_k,\n                                  size=(num_samples, 1), dtype=np.int32)\n\n        hidden_dim_vals = self.rng.randint(low=self.lwr_hidden_dim,\n                                           high=self.upr_hidden_dim,\n                                           size=(num_samples, 1),\n                                           dtype=np.int32)\n\n        spectral_radius_vals = self.rng.uniform(low=self.lwr_spectral_radius,\n                                                high=self.upr_spectral_radius,\n                                                size=(num_samples, 1))\n\n        p_vals = self.rng.uniform(low=self.lwr_p, high=self.upr_p,\n                                  size=(num_samples, 1))\n\n        alpha_vals = self.rng.uniform(low=self.lwr_alpha, high=self.upr_alpha,\n                                      size=(num_samples, 1))\n\n        beta_vals = self._sample_uniformly(num_samples, self.lwr_beta,\n                                           self.upr_beta)\n\n        H_space = np.concatenate([k_vals, hidden_dim_vals,\n                                  spectral_radius_vals, p_vals, alpha_vals,\n                                  beta_vals], axis=1)\n        return H_space\n\n    def find_best_choices(self, num_samples=1000, num_choices=1):\n        \n        H_space = self._build_options(num_samples)\n\n        \n        \n        if num_choices > 1:\n            idx = self.rng.choice(np.arange(num_samples), size=num_choices,\n                                  replace=False)\n            best_vals = H_space[idx, :]\n        else:\n            y_pred = self.gpr.sample_y(H_space, random_state=self.rng)\n            choices = np.argmin(y_pred)\n            best_vals = H_space[choices, :]\n\n        hyper_parameters = ['k', 'hidden_dim', 'spectral_radius', 'p', 'alpha',\n                            'beta']\n\n        param_vals = {}\n        for (i, val) in enumerate(hyper_parameters):\n            if num_choices == 1:\n                param_vals[val] = best_vals[i]\n\n                if (val == 'k') or (val == 'hidden_dim'):\n                    param_vals[val] = int(param_vals[val])\n            else:\n                param_vals[val] = best_vals[:, i]\n\n                if (val == 'k') or (val == 'hidden_dim'):\n                    param_vals[val] = param_vals[val].astype(int)\n\n        return param_vals\n\n    def return_best_parameters(self):\n        min_error = min(self.y)\n        index = self.y.index(min_error)\n        print(\"Minimum Validation Error = \", min_error)\n        print(\"Best parameters found = \", self.H[index])\n        return min_error, self.H[index]\n",
        "summary": "The provided Python code defines a Bayesian Optimization (BO) class that uses Gaussian Process Regression to optimize hyperparameters for machine learning models. It includes methods for initializing the BO object with specified parameter bounds, updating the Gaussian Process Regressor with new data, sampling uniformly within given bounds, building a space of options for hyperparameter combinations, finding the best choices based on predicted validation errors, and returning the best parameters found."
    },
    {
        "code": "import os\n\nfrom absl import flags\nfrom absl.testing import parameterized\nimport tensorflow as tf\n\nfrom deeplab2.model.encoder import axial_resnet_instances\n\nFLAGS = flags.FLAGS\n\n\nclass ModelExportTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      ('resnet50',),\n      ('resnet50_beta',),\n      ('max_deeplab_s_backbone',),\n      ('max_deeplab_l_backbone',),\n      ('axial_resnet_s',),\n      ('axial_resnet_l',),\n      ('axial_deeplab_s',),\n      ('axial_deeplab_l',),\n      ('swidernet',),\n      ('axial_swidernet',),\n      )\n  def test_model_export(self, model_name):\n    model = axial_resnet_instances.get_model(\n        model_name,\n        output_stride=16,\n        backbone_layer_multiplier=1.0,\n        bn_layer=tf.keras.layers.BatchNormalization,\n        conv_kernel_weight_decay=0.0001,\n        \n        num_blocks=[2, 2, 2, 2],\n        \n        block_group_config={'drop_path_keep_prob': 1.0})\n    model(tf.keras.Input([257, 257, 3], batch_size=1), training=False)\n    export_dir = os.path.join(\n        FLAGS.test_tmpdir, 'test_model_export', model_name)\n    model.save(export_dir)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n",
        "summary": "This Python script defines a test class `ModelExportTest` that inherits from both `tf.test.TestCase` and `parameterized.TestCase`. It uses the `absl` library for flags management and TensorFlow for model testing. The class includes a parameterized test method `test_model_export`, which iterates over various model names, creates instances of these models using the `axial_resnet_instances.get_model` function, and then exports each model to a specified directory."
    },
    {
        "code": "from rip_pages import rip_pages\nfrom read_pages import read_pages\nfrom format_csv import format_csv\n\n\nSOURCE_DOC = '114sdoc7'\nFILE_NAME = \"GPO-CDOC-\" + SOURCE_DOC + \".pdf\"\nOUT_FILE = 'senate_data.csv'\nMISSING_FILE = 'missing_data.json'\nSTART_PAGE = 17\nEND_PAGE = 2259\n\n\nrip_pages(FILE_NAME, START_PAGE, END_PAGE)\nread_pages(START_PAGE, END_PAGE, OUT_FILE, MISSING_FILE)\nformat_csv(SOURCE_DOC, OUT_FILE)\n\n\n\n\n\n",
        "summary": "The Python script processes a PDF document named \"GPO-CDOC-114sdoc7.pdf\" by extracting pages from it using the `rip_pages` function. It then reads these pages and extracts data, which is saved to 'senate_data.csv' while logging any missing data in 'missing_data.json'. Finally, it formats the extracted data into a CSV file using the `format_csv` function."
    },
    {
        "code": "my_list = [1, 2, 2, 4, 6]\n\nprint(my_list[::-1])\n\nstudent = {'user': 'Lubo',\n           'pass': 'admin',\n           'course': ['C\n\nfor key in student:\n    print(key)\n\nfor kvp in student.items():\n    print(f'the key is: {kvp[0]}, and values are: {kvp[1]} ')\n\n\nprint(student['pass'])\nprint(student.get('Pass', 'Sorry mate no such key'))\n\nif 'pass' in student.keys():\n    print('Here')\nelse:\n    print('Not here')\n\nsecond_part_student = {\n    'age': 25\n}\nstudent.update(second_part_student)\nprint(student)\n",
        "summary": "The provided Python code demonstrates various operations on a list and dictionary. It reverses the elements of a list, iterates through a dictionary to print keys and key-value pairs, accesses specific values using both direct indexing and the `get` method, checks for the presence of a key, updates the dictionary with additional key-value pairs, and prints the updated dictionary."
    },
    {
        "code": "import datetime\nfrom dateutil.relativedelta import relativedelta\n\nprint(\"Programa para calcular o prazo de exame de ultrassom...\\nO mesmo deve ser feito entre 22 e 24 semanas de gesta\u00e7\u00e3o\")\nprint(\"voc\u00ea dever\u00e1 informar com quantas semanasa de gesta\u00e7\u00e3o a paciente se encontra, no formato aaaa/mm/dd\")\nsemanas = int(input(\"Com quantas semanas de gesta\u00e7\u00e3o a paciente se encontra hoje? \"))\nexameInicio = 22-semanas\nexameFinal = 24 - semanas\n\nmorfologicoInicio = datetime.date.today()+ relativedelta(weeks=exameInicio)\nmorfologicoFinal = datetime.date.today() + relativedelta(weeks=exameFinal)\ndfinal = morfologicoFinal.strftime('%d/%m/%Y')\ndinicial = morfologicoInicio.strftime('%d/%m/%Y')\nprint(\"O exame dever\u00e1 ser feito entre \",dinicial, \" e \", dfinal)",
        "summary": "The Python script calculates the appropriate time frame for a prenatal ultrasound examination based on the patient's gestational age. It prompts the user to input the current gestational weeks and then determines the range of dates between 22 and 24 weeks post-conception when the exam should be conducted, outputting the start and end dates in a user-friendly format."
    },
    {
        "code": "from __future__ import annotations\n\nfrom typing import Generator, NoReturn\n\n\nclass StdReader:\n    def __init__(\n        self,\n    ) -> NoReturn:\n        import sys\n\n        self.buf = sys.stdin.buffer\n        self.lines = self.async_readlines()\n        self.chunks: Generator\n\n    def async_readlines(\n        self,\n    ) -> Generator:\n        while True:\n            gen = self.line_chunks()\n            yield gen\n\n    def line_chunks(\n        self,\n    ) -> Generator:\n        ln = self.buf.readline()\n        for chunk in ln.split():\n            yield chunk\n\n    def __call__(\n        self,\n    ) -> bytes:\n        try:\n            chunk = next(self.chunks)\n        except:\n            self.chunks = next(\n                self.lines,\n            )\n            chunk = self()\n        return chunk\n\n    def str(\n        self,\n    ) -> str:\n        b = self()\n        return b.decode()\n\n    def int(\n        self,\n    ) -> int:\n        return int(self.str())\n\n\nfrom abc import ABC, abstractmethod\n\n\nclass Solver(ABC):\n    def __init__(self):\n        self.reader = StdReader()\n\n    def __call__(\n        self,\n    ):\n        self.prepare()\n        self.solve()\n\n    @abstractmethod\n    def prepare(self):\n        ...\n\n    @abstractmethod\n    def solve(self):\n        ...\n\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import floyd_warshall\n\n\nclass Problem(\n    Solver,\n):\n    def prepare(self):\n        reader = self.reader\n        n = reader.int()\n        m = reader.int()\n        a = [reader.int() for _ in range(3 * m)]\n        a = np.array(\n            a,\n        ).reshape(m, 3)\n        a, b, t = a.T\n        self.n, self.m = n, m\n        self.a = a - 1\n        self.b = b - 1\n        self.t = t\n\n    def solve(self):\n        self.compute_dist_mat()\n        dist = self.dist\n        d = dist.max(axis=1).min()\n        print(int(d))\n\n    def compute_dist_mat(\n        self,\n    ):\n        n = self.n\n        a = self.a\n        b = self.b\n        t = self.t\n        g = csr_matrix(\n            (t, (a, b)),\n            shape=(n, n),\n        )\n        dist = floyd_warshall(\n            csgraph=g,\n            directed=False,\n        )\n        self.dist = dist\n\n\ndef main():\n    t = 1\n    \n    for _ in range(t):\n        Problem()()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The provided Python code defines a `StdReader` class to read input from standard input asynchronously, breaking it into chunks. It also includes an abstract base class `Solver` with methods for preparing and solving problems. The `Problem` class inherits from `Solver`, implementing the necessary logic to solve a specific problem using graph theory, specifically finding the minimum maximum distance in a network of nodes connected by weighted edges. The `main` function runs the solver for a given number of test cases."
    },
    {
        "code": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom astropylibrarian.reducers.utils import iter_sphinx_sections\n\nif TYPE_CHECKING:\n    from .conftest import HtmlTestData\n\n\ndef test_iter_sphinx_sections(color_excess_tutorial: HtmlTestData) -> None:\n    \n    doc = color_excess_tutorial.parse()\n    root = doc.cssselect(\".card .section\")[0]\n\n    sections = []\n    for s in iter_sphinx_sections(\n        root_section=root,\n        base_url=color_excess_tutorial.url,\n        headers=[],\n        header_callback=lambda x: x.rstrip(\"\u00b6\"),\n        content_callback=lambda x: x.strip(),\n    ):\n        sections.append(s)\n\n    assert len(sections) == 5\n\n    assert sections[0].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Learning Goals\",\n    ]\n    assert sections[0].header_level == 2\n    assert sections[0].url == (\n        \"http://learn.astropy.org/rst-tutorials/color-excess.html\"\n        \"\n    )\n    assert sections[0].content.startswith(\n        \"Investigate extinction curve shapes\"\n    )\n\n    assert sections[1].headings[-1] == \"Keywords\"\n    assert sections[1].header_level == 2\n    assert sections[1].content.startswith(\n        \"dust extinction, synphot, astroquery, units, photometry, extinction,\"\n    )\n\n    assert sections[2].headings[-1] == \"Companion Content\"\n    assert sections[2].header_level == 2\n    assert sections[2].content.startswith(\"Bessell & Murphy\")\n\n    assert sections[3].headings[-1] == \"Summary\"\n    assert sections[3].header_level == 2\n    assert sections[3].content.startswith(\n        \"In this tutorial, we will look at some extinction curves from the\"\n    )\n\n    assert sections[4].headings[-1] == (\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\"\n    )\n    assert sections[4].header_level == 1\n\n    \n    \n    h1_heading = sections[-1].headings[-1]\n    for sibling in root.itersiblings(tag=\"div\"):\n        if \"section\" in sibling.classes:\n            for s in iter_sphinx_sections(\n                root_section=sibling,\n                base_url=color_excess_tutorial.url,\n                headers=[h1_heading],\n                header_callback=lambda x: x.rstrip(\"\u00b6\"),\n                content_callback=lambda x: x.strip(),\n            ):\n                sections.append(s)\n\n    assert sections[5].header_level == 2\n    assert sections[5].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Introduction\",\n    ]\n\n    assert sections[6].header_level == 2\n    assert sections[6].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Example 1: Investigate Extinction Models\",\n    ]\n\n    assert sections[7].header_level == 2\n    assert sections[7].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Example 2: Deredden a Spectrum\",\n    ]\n\n    assert sections[8].header_level == 3\n    assert sections[8].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Example 3: Calculate Color Excess with synphot\",\n        \"Exercise\",\n    ]\n\n    assert sections[9].header_level == 2\n    assert sections[9].headings == [\n        \"Analyzing interstellar reddening and calculating synthetic \"\n        \"photometry\",\n        \"Example 3: Calculate Color Excess with synphot\",\n    ]\n",
        "summary": "The provided Python code tests the `iter_sphinx_sections` function from the `astropylibrarian.reducers.utils` module by parsing an HTML document, extracting sections based on specific criteria such as header levels and content, and asserting that the extracted sections match expected values. The test ensures that all relevant sections are correctly identified and structured within the document."
    },
    {
        "code": "import sys, string, os, popen2, shutil, platform, subprocess, pprint, time\nimport util, commands, csv\nfrom math import sqrt\n\n\ndo_clean = True\n\n\ndo_build = True\n\n\ndo_run = True\n\n\n\n\nif do_clean and not do_build:\n    print \"Clean - true and build - false not allowed\"\n    exit(0)\n\n\nconfigs = []\n\nentry = { \"NAME\" : \"RUN_ALL_BENCHMARKS\",\n          \"NUM_RUNS\" : 1,\n          \"CLEAN_LINE\" : \" make clean \",\n          \"BUILD_LINE\" : \" make \",\n          \"BUILD_ARCH\" : \"x86_64\",\n          \"RUN_ARCH\" : \"x86_64\",\n          \"RUN_LINE\" : '/usr/bin/time -f \"%E\" ./',\n          \n          \"ARGS\" : \"\",\n}\n\nconfigs.append(entry)\n\nref_cwd = os.getcwd()\narch = platform.machine()\nfull_hostname = platform.node()\nhostname=full_hostname\n\nbench_name=\"MIS\"\nbenchmarks=[\n    \"ndMIS\"\n]\n\ninner_data_folder=[\n    \"graphData/data\"\n]\n\ninput_file=[\n    \"randLocalGraph_J_5_2500000\"\n]\n\nexecutable=[\n    \"MIS.openmp.dynamic\",\n    \"MIS.omprn\",\n    \"MIS.ompp.dynamic\",\n]\n\ninputs=[\n    \"-r 1 -o /tmp/ofile470293_748866 ../graphData/data/randLocalGraph_J_5_2500000\"\n]\n\n\nif __name__ == \"__main__\":\n    with open('omprace.csv', 'wb') as csvfile:\n        res_writer = csv.writer(csvfile, delimiter=',')\n        res_writer.writerow(['test name', 'baseline openmp(s)', 'omprace no_inst(s)', 'omprace(s)', 'overhead ospg', 'overhead datarace', 'num violations'])\n        for config in configs:\n            util.log_heading(config[\"NAME\"], character=\"-\")\n            row = []\n            row.append(bench_name[0])\n            num_violations = -1\n\n            print('input file folder: ' + inner_data_folder[0])\n            data_input = inner_data_folder[0]+'/'+input_file[0]\n            print('checking if input data exists at:' + data_input)\n            if not os.path.exists(data_input):\n                print(\"input data doesn't exist. building input data\")\n                util.chdir(ref_cwd + \"/\" + inner_data_folder[0])\n                build_data = config[\"BUILD_LINE\"] + \" \" + input_file[0]\n                util.run_command(build_data, verbose=True)\n                util.chdir(ref_cwd)\n            else:\n                print(\"input data exists\")\n\n            for b_index in range(len(executable)):\n                util.chdir(ref_cwd)\n                for i in range(0, config[\"NUM_RUNS\"]):\n                    try:\n                        \n                        util.chdir(ref_cwd + \"/\" + benchmarks[0] )\n                        util.log_heading(benchmarks[0], character=\"=\")\n                        try:\n                            clean_string = config[\"CLEAN_LINE\"]\n                            util.run_command(clean_string, verbose=True)\n                        except:\n                            print \"Clean failed\"\n                        \n                        build_bench_string = config[\"BUILD_LINE\"]\n                        util.run_command(build_bench_string, verbose=True) \n                        util.log_heading(\"running: \" + benchmarks[0], character=\"=\")\n                        run_string = config[\"RUN_LINE\"] + executable[b_index] + \" \" + inputs[0]\n                        \n                        if b_index == 0:\n                            util.run_command(run_string, verbose=True) \n                        output_string = util.run_command(run_string, verbose=True) \n                        output_lines = output_string.split('\\n')\n                        if b_index == len(executable)-1:                        \n                            for output_line in output_lines:\n                                if output_line.startswith(\"Number of violations =\"):\n                                    num_violations=int(output_line[output_line.index('=')+1:])\n                        time_line = output_lines[-2] \n                        time_line = time_line.split(':')\n                        tot_secs = 0.0\n                        for t in time_line:\n                            tot_secs = (tot_secs*60) + float(t)\n                        row.append(tot_secs)\n                        print ('total secs= ' + str(tot_secs))\n                        \n\n\n                        \n\n                    except util.ExperimentError, e:\n                        print \"Error: %s\" % e\n                        print \"-----------\"\n                        print \"%s\" % e.output\n                        continue\n            \n            \n            row.append(\"{0:.2f}\".format(row[2]/row[1]))\n            row.append(\"{0:.2f}\".format(row[3]/row[1]))\n            row.append(num_violations)\n            res_writer.writerow(row)            \n\n        util.chdir(ref_cwd)        \n        print(\"done\")\n",
        "summary": "This Python script automates the process of cleaning, building, and running benchmarks for a specific set of executables on a system. It logs the results to a CSV file, including execution times and overheads compared to a baseline OpenMP implementation."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.layers.python.ops import sparse_feature_cross_op\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.platform import test\n\n\nclass SparseCrossOpTest(test.TestCase):\n\n  def test_simple(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([['batch1-FC1-F1'],\n                             ['batch2-FC1-F1', 'batch2-FC1-F2']]),\n        self._sparse_tensor([['batch1-FC2-F1'],\n                             ['batch2-FC2-F1', 'batch2-FC2-F2']])\n    ])\n    expected_out = self._sparse_tensor([['batch1-FC1-F1_X_batch1-FC2-F1'], [\n        'batch2-FC1-F1_X_batch2-FC2-F1', 'batch2-FC1-F1_X_batch2-FC2-F2',\n        'batch2-FC1-F2_X_batch2-FC2-F1', 'batch2-FC1-F2_X_batch2-FC2-F2'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_dense(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        constant_op.constant([['batch1-FC1-F1', 'batch1-FC1-F2'],\n                              ['batch2-FC1-F1', 'batch2-FC1-F2']],\n                             dtypes.string),\n        constant_op.constant([['batch1-FC2-F1', 'batch1-FC2-F2'],\n                              ['batch2-FC2-F1', 'batch2-FC2-F2']],\n                             dtypes.string),\n    ])\n    expected_out = self._sparse_tensor([[\n        'batch1-FC1-F1_X_batch1-FC2-F1', 'batch1-FC1-F1_X_batch1-FC2-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F1', 'batch1-FC1-F2_X_batch1-FC2-F2'\n    ], [\n        'batch2-FC1-F1_X_batch2-FC2-F1', 'batch2-FC1-F1_X_batch2-FC2-F2',\n        'batch2-FC1-F2_X_batch2-FC2-F1', 'batch2-FC1-F2_X_batch2-FC2-F2'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_integer_mixed_string_sparse(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([[11], [333, 55555]]),\n        self._sparse_tensor([['batch1-FC2-F1'],\n                             ['batch2-FC2-F1', 'batch2-FC2-F2']])\n    ])\n    expected_out = self._sparse_tensor([['11_X_batch1-FC2-F1'], [\n        '333_X_batch2-FC2-F1', '333_X_batch2-FC2-F2', '55555_X_batch2-FC2-F1',\n        '55555_X_batch2-FC2-F2'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_integer_mixed_string_dense(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        constant_op.constant([[11, 333], [55555, 999999]], dtypes.int64),\n        constant_op.constant([['batch1-FC2-F1', 'batch1-FC2-F2'],\n                              ['batch2-FC2-F1', 'batch2-FC2-F2']],\n                             dtypes.string),\n    ])\n    expected_out = self._sparse_tensor([[\n        '11_X_batch1-FC2-F1', '11_X_batch1-FC2-F2', '333_X_batch1-FC2-F1',\n        '333_X_batch1-FC2-F2'\n    ], [\n        '55555_X_batch2-FC2-F1', '55555_X_batch2-FC2-F2',\n        '999999_X_batch2-FC2-F1', '999999_X_batch2-FC2-F2'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_sparse_cross_dense(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([['batch1-FC1-F1'],\n                             ['batch2-FC1-F1', 'batch2-FC1-F2']]),\n        constant_op.constant([['batch1-FC2-F1', 'batch1-FC2-F2'],\n                              ['batch2-FC2-F1', 'batch2-FC2-F2']],\n                             dtypes.string),\n    ])\n    expected_out = self._sparse_tensor(\n        [['batch1-FC1-F1_X_batch1-FC2-F1', 'batch1-FC1-F1_X_batch1-FC2-F2'], [\n            'batch2-FC1-F1_X_batch2-FC2-F1', 'batch2-FC1-F1_X_batch2-FC2-F2',\n            'batch2-FC1-F2_X_batch2-FC2-F1', 'batch2-FC1-F2_X_batch2-FC2-F2'\n        ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_integer_sparse_input(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([[11], [333, 5555]]),\n        constant_op.constant([['batch1-FC2-F1', 'batch1-FC2-F2'],\n                              ['batch2-FC2-F1', 'batch2-FC2-F2']],\n                             dtypes.string),\n    ])\n    expected_out = self._sparse_tensor(\n        [['11_X_batch1-FC2-F1', '11_X_batch1-FC2-F2'], [\n            '333_X_batch2-FC2-F1', '333_X_batch2-FC2-F2',\n            '5555_X_batch2-FC2-F1', '5555_X_batch2-FC2-F2'\n        ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_permutation_3x3x3(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor(\n            [['batch1-FC1-F1', 'batch1-FC1-F2', 'batch1-FC1-F3']]),\n        self._sparse_tensor(\n            [['batch1-FC2-F1', 'batch1-FC2-F2', 'batch1-FC2-F3']]),\n        self._sparse_tensor(\n            [['batch1-FC3-F1', 'batch1-FC3-F2', 'batch1-FC3-F3']])\n    ])\n    expected_out = self._sparse_tensor([[\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F3',\n        'batch1-FC1-F1_X_batch1-FC2-F2_X_batch1-FC3-F1',\n        'batch1-FC1-F1_X_batch1-FC2-F2_X_batch1-FC3-F2',\n        'batch1-FC1-F1_X_batch1-FC2-F2_X_batch1-FC3-F3',\n        'batch1-FC1-F1_X_batch1-FC2-F3_X_batch1-FC3-F1',\n        'batch1-FC1-F1_X_batch1-FC2-F3_X_batch1-FC3-F2',\n        'batch1-FC1-F1_X_batch1-FC2-F3_X_batch1-FC3-F3',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F3',\n        'batch1-FC1-F2_X_batch1-FC2-F2_X_batch1-FC3-F1',\n        'batch1-FC1-F2_X_batch1-FC2-F2_X_batch1-FC3-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F2_X_batch1-FC3-F3',\n        'batch1-FC1-F2_X_batch1-FC2-F3_X_batch1-FC3-F1',\n        'batch1-FC1-F2_X_batch1-FC2-F3_X_batch1-FC3-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F3_X_batch1-FC3-F3',\n        'batch1-FC1-F3_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F3_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F3_X_batch1-FC2-F1_X_batch1-FC3-F3',\n        'batch1-FC1-F3_X_batch1-FC2-F2_X_batch1-FC3-F1',\n        'batch1-FC1-F3_X_batch1-FC2-F2_X_batch1-FC3-F2',\n        'batch1-FC1-F3_X_batch1-FC2-F2_X_batch1-FC3-F3',\n        'batch1-FC1-F3_X_batch1-FC2-F3_X_batch1-FC3-F1',\n        'batch1-FC1-F3_X_batch1-FC2-F3_X_batch1-FC3-F2',\n        'batch1-FC1-F3_X_batch1-FC2-F3_X_batch1-FC3-F3'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_permutation_3x1x2(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor(\n            [['batch1-FC1-F1', 'batch1-FC1-F2', 'batch1-FC1-F3']]),\n        self._sparse_tensor([['batch1-FC2-F1']]),\n        self._sparse_tensor([['batch1-FC3-F1', 'batch1-FC3-F2']])\n    ])\n    expected_out = self._sparse_tensor([[\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F3_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F3_X_batch1-FC2-F1_X_batch1-FC3-F2'\n    ]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_large_batch(self):\n    \n    batch_size = 5000\n    col1 = []\n    col2 = []\n    col3 = []\n    for b in range(batch_size):\n      col1.append(\n          ['batch%d-FC1-F1' % b, 'batch%d-FC1-F2' % b, 'batch%d-FC1-F3' % b])\n      col2.append(['batch%d-FC2-F1' % b])\n      col3.append(['batch%d-FC3-F1' % b, 'batch%d-FC3-F2' % b])\n\n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor(col1), self._sparse_tensor(col2),\n        self._sparse_tensor(col3)\n    ])\n\n    col_out = []\n    for b in range(batch_size):\n      col_out.append([\n          'batch%d-FC1-F1_X_batch%d-FC2-F1_X_batch%d-FC3-F1' % (b, b, b),\n          'batch%d-FC1-F1_X_batch%d-FC2-F1_X_batch%d-FC3-F2' % (b, b, b),\n          'batch%d-FC1-F2_X_batch%d-FC2-F1_X_batch%d-FC3-F1' % (b, b, b),\n          'batch%d-FC1-F2_X_batch%d-FC2-F1_X_batch%d-FC3-F2' % (b, b, b),\n          'batch%d-FC1-F3_X_batch%d-FC2-F1_X_batch%d-FC3-F1' % (b, b, b),\n          'batch%d-FC1-F3_X_batch%d-FC2-F1_X_batch%d-FC3-F2' % (b, b, b)\n      ])\n\n    expected_out = self._sparse_tensor(col_out)\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_one_column_empty(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([['batch1-FC1-F1', 'batch1-FC1-F2']]),\n        self._sparse_tensor([], 1),\n        self._sparse_tensor([['batch1-FC3-F1', 'batch1-FC3-F2']])\n    ])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_empty(sess.run(op))\n\n  def test_some_columns_empty(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([['batch1-FC1-F1', 'batch1-FC1-F2']], 2),\n        self._sparse_tensor([['batch1-FC2-F1'], ['batch2-FC2-F1']], 2),\n        self._sparse_tensor([['batch1-FC3-F1', 'batch1-FC3-F2']], 2)\n    ])\n    expected_out = self._sparse_tensor([[\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F1_X_batch1-FC2-F1_X_batch1-FC3-F2',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F1',\n        'batch1-FC1-F2_X_batch1-FC2-F1_X_batch1-FC3-F2'\n    ]], 2)\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_all_columns_empty(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross([\n        self._sparse_tensor([]), self._sparse_tensor([]),\n        self._sparse_tensor([])\n    ])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_empty(sess.run(op))\n\n  def test_hashed_output_zero_bucket(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross(\n        [\n            self._sparse_tensor([['batch1-FC1-F1']]),\n            self._sparse_tensor([['batch1-FC2-F1']]),\n            self._sparse_tensor([['batch1-FC3-F1']])\n        ],\n        hashed_output=True)\n    \n    expected_out = self._sparse_tensor([[3735511728867393167]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_hashed_output_zero_bucket_v2(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross(\n        [\n            self._sparse_tensor([['batch1-FC1-F1']]),\n            self._sparse_tensor([['batch1-FC2-F1']]),\n            self._sparse_tensor([['batch1-FC3-F1']])\n        ],\n        hashed_output=True,\n        hash_key=layers.SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY)\n    \n    expected_out = self._sparse_tensor([[1971693436396284976]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  \n  def test_hashed_output(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross(\n        [\n            self._sparse_tensor([['batch1-FC1-F1']]),\n            self._sparse_tensor([['batch1-FC2-F1']]),\n            self._sparse_tensor([['batch1-FC3-F1']])\n        ],\n        hashed_output=True,\n        num_buckets=100)\n    \n    expected_out = self._sparse_tensor([[74]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_hashed_output_v2(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross(\n        [\n            self._sparse_tensor([['batch1-FC1-F1']]),\n            self._sparse_tensor([['batch1-FC2-F1']]),\n            self._sparse_tensor([['batch1-FC3-F1']])\n        ],\n        hashed_output=True,\n        num_buckets=100,\n        hash_key=layers.SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY)\n    \n    expected_out = self._sparse_tensor([[83]])\n    with self.test_session() as sess:\n      self._assert_sparse_tensor_equals(expected_out, sess.run(op))\n\n  def test_hashed_output_v1_has_collision(self):\n    \n    \n    \n    t1 = constant_op.constant([[359], [359 + 1024]])\n    t2 = constant_op.constant([list(range(10)), list(range(10))])\n    cross = sparse_feature_cross_op.sparse_feature_cross(\n        [t2, t1], hashed_output=True, num_buckets=1024)\n    cross_dense = sparse_ops.sparse_tensor_to_dense(cross)\n    with session.Session():\n      values = cross_dense.eval()\n      self.assertTrue(numpy.equal(values[0], values[1]).all())\n\n  def test_hashed_output_v2_has_no_collision(self):\n    \n    \n    \n    t1 = constant_op.constant([[359], [359 + 1024]])\n    t2 = constant_op.constant([list(range(10)), list(range(10))])\n    cross = sparse_feature_cross_op.sparse_feature_cross(\n        [t2, t1],\n        hashed_output=True,\n        num_buckets=1024,\n        hash_key=layers.SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY)\n    cross_dense = sparse_ops.sparse_tensor_to_dense(cross)\n    with session.Session():\n      values = cross_dense.eval()\n      self.assertTrue(numpy.not_equal(values[0], values[1]).all())\n\n  def test_hashed_3x1x2(self):\n    \n    op = sparse_feature_cross_op.sparse_feature_cross(\n        [\n            self._sparse_tensor(\n                [['batch1-FC1-F1', 'batch1-FC1-F2', 'batch1-FC1-F3']]),\n            self._sparse_tensor([['batch1-FC2-F1']]),\n            self._sparse_tensor([['batch1-FC3-F1', 'batch1-FC3-F2']])\n        ],\n        hashed_output=True,\n        num_buckets=1000)\n    with self.test_session() as sess:\n      out = sess.run(op)\n      self.assertEqual(6, len(out.values))\n      self.assertAllEqual([[0, i] for i in range(6)], out.indices)\n      self.assertTrue(all(x < 1000 and x >= 0 for x in out.values))\n      all_values_are_different = len(out.values) == len(set(out.values))\n      self.assertTrue(all_values_are_different)\n\n  def _assert_sparse_tensor_empty(self, sp):\n    self.assertEquals(0, sp.indices.size)\n    self.assertEquals(0, sp.values.size)\n    \n    self.assertEquals(0, sp.dense_shape[1])\n\n  def _assert_sparse_tensor_equals(self, sp1, sp2):\n    self.assertAllEqual(sp1.indices.eval(), sp2.indices)\n    self.assertAllEqual(sp1.values.eval(), sp2.values)\n    self.assertAllEqual(sp1.dense_shape.eval(), sp2.dense_shape)\n\n  def _sparse_tensor(self, data, batch_size=-1):\n    \n    indices = []\n    values = []\n    max_col_count = 0\n    for batch, batch_ix in zip(data, range(len(data))):\n      for column, column_ix in zip(batch, range(len(batch))):\n        indices.append([batch_ix, column_ix])\n        values.append(column)\n        max_col_count = max(max_col_count, column_ix + 1)\n    shape = [batch_size if batch_size != -1 else len(data), max_col_count]\n    value_type = (dtypes.string if not values or isinstance(values[0], str) else\n                  dtypes.int64)\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(indices, dtypes.int64, [len(indices), 2]),\n        constant_op.constant(values, value_type, [len(indices)]),\n        constant_op.constant(shape, dtypes.int64))\n\n\nif __name__ == '__main__':\n  test.main()\n",
        "summary": "This code defines a unit test suite for the `sparse_feature_cross_op` function in TensorFlow. The function is designed to perform feature crossing on sparse tensors, which are commonly used in machine learning models to represent categorical features.\n\nThe test suite includes several test cases that cover different scenarios:\n\n1. Basic feature crossing with multiple columns and buckets.\n2. Hashed output with a specified number of buckets.\n3. Handling collisions when hashing.\n4. Testing the function with string values.\n5. Verifying that the output is correctly shaped and contains valid indices and values.\n\nThe `_sparse_tensor` helper function is used to create sparse tensors from input data, which can be either strings or integers. It constructs the `indices`, `values`, and `dense_shape` attributes of a `SparseTensor`.\n\nThe test suite uses TensorFlow's `test.TestCase` class to define test methods that assert various properties of the output tensor. The `assertAllEqual` method is used to compare tensors element-wise, while `self.assertEquals` checks for equality of scalar values.\n\nOverall, this test suite provides comprehensive coverage of the `sparse_feature_cross_op` function and ensures its correctness across different use cases."
    },
    {
        "code": "__all__ = [\"ChangeScene\", \"Runner\", \"WindowRunner\", \"NonInteractiveRunner\", \"newRunner\"]\n\nfrom .. import config, render, Logger\nfrom ..events import EventLoopManager, WaitForUpdate, WaitForFixedUpdate, WaitForRender\nfrom ..errors import PyUnityException\nimport copy\nimport os\n\nclass ChangeScene(Exception):\n    pass\n\nclass Runner:\n    def __init__(self):\n        self.scene = None\n        self.next = None\n        self.opened = False\n\n    def setScene(self, scene):\n        if self.opened:\n            raise PyUnityException(\"Cannot set scene after opening runner\")\n        self.scene = copy.deepcopy(scene)\n\n    def setNext(self, scene):\n        if self.scene is None:\n            raise PyUnityException(\"Cannot set next before first scene\")\n        self.next = copy.deepcopy(scene)\n        raise ChangeScene\n\n    def open(self):\n        if self.scene is None:\n            raise PyUnityException(\"Cannot open runner before setting a scene\")\n        if self.opened:\n            Logger.Save()\n        self.opened = True\n\n    def setup(self):\n        pass\n\n    def load(self):\n        if self.scene is None:\n            raise PyUnityException(\"Cannot load runner before setting a scene\")\n        Logger.LogLine(Logger.DEBUG, \"Starting scene\")\n        self.eventLoopManager = EventLoopManager()\n        self.eventLoopManager.schedule(self.scene.updateFixed, ups=50, waitFor=WaitForFixedUpdate)\n        self.eventLoopManager.addLoop(self.scene.startScripts())\n\n    def start(self):\n        while True:\n            try:\n                self.eventLoopManager.start()\n                break\n            except ChangeScene:\n                if self.next is None:\n                    raise\n                self.eventLoopManager.quit()\n                self.scene.cleanUp()\n                self.scene = self.next\n                self.next = None\n                self.load()\n\n    def quit(self):\n        self.eventLoopManager.quit()\n        self.scene.cleanUp()\n\n        self.scene = None\n        self.opened = False\n\nclass WindowRunner(Runner):\n    def open(self):\n        super(WindowRunner, self).open()\n        os.environ[\"PYUNITY_GL_CONTEXT\"] = \"1\"\n\n        self.window = config.windowProvider(self.scene.name)\n        \n        self.window.refresh()\n        render.fillScreen()\n        \n        self.window.refresh()\n        render.fillScreen()\n\n    def setup(self):\n        Logger.LogSpecial(Logger.INFO, Logger.ELAPSED_TIME)\n        Logger.LogLine(Logger.DEBUG, \"Compiling objects\")\n\n        Logger.LogLine(Logger.INFO, \"Compiling shaders\")\n        render.compileShaders()\n        Logger.LogSpecial(Logger.INFO, Logger.ELAPSED_TIME)\n\n        Logger.LogLine(Logger.INFO, \"Loading skyboxes\")\n        render.compileSkyboxes()\n        Logger.LogSpecial(Logger.INFO, Logger.ELAPSED_TIME)\n\n    def load(self):\n        super(WindowRunner, self).load()\n        self.eventLoopManager.schedule(\n            self.scene.updateScripts, self.window.updateFunc,\n            ups=config.fps, waitFor=WaitForUpdate)\n        self.eventLoopManager.schedule(\n            self.window.refresh, self.scene.Render,\n            main=True, waitFor=WaitForRender)\n        if self.scene.mainCamera is not None:\n            self.window.setResize(self.scene.mainCamera.Resize)\n        self.scene.startOpenGL()\n        self.scene.startLoop()\n\n    def start(self):\n        super(WindowRunner, self).start()\n\n    def quit(self):\n        super(WindowRunner, self).quit()\n        del self.window\n\n        del os.environ[\"PYUNITY_GL_CONTEXT\"]\n        render.resetShaders()\n        Logger.LogLine(Logger.INFO, \"Reset shaders\")\n        render.resetSkyboxes()\n        Logger.LogLine(Logger.INFO, \"Reset skyboxes\")\n\nclass NonInteractiveRunner(Runner):\n    def load(self):\n        super(NonInteractiveRunner, self).load()\n        self.eventLoopManager.schedule(\n            self.scene.updateScripts,\n            ups=config.fps, waitFor=WaitForUpdate)\n        self.scene.startLoop()\n\ndef newRunner():\n    if os.environ[\"PYUNITY_INTERACTIVE\"] == \"1\":\n        return WindowRunner()\n    else:\n        return NonInteractiveRunner()\n",
        "summary": "The code defines a base class `Runner` for managing game scenes and events, with subclasses `WindowRunner` and `NonInteractiveRunner` tailored for interactive and non-interactive environments respectively. The `newRunner` function creates an instance of the appropriate runner based on the environment configuration."
    },
    {
        "code": "try:\n    import json\nexcept ImportError:\n    try:\n        import simplejson as json\n    except ImportError:\n        try:\n            from django.utils import simplejson as json\n        except ImportError:\n            json = None\n\nfrom sphinx import addnodes, roles\nfrom docutils.parsers.rst import Directive\n\n\ndef setup(app):\n    app.add_crossref_type(\n        directivename = \"setting\",\n        rolename      = \"setting\",\n        indextemplate = \"pair: %s; setting\",\n    )\n    app.add_crossref_type(\n        directivename = \"templatetag\",\n        rolename      = \"ttag\",\n        indextemplate = \"pair: %s; template tag\"\n    )\n    app.add_crossref_type(\n        directivename = \"templatefilter\",\n        rolename      = \"tfilter\",\n        indextemplate = \"pair: %s; template filter\"\n    )\n    app.add_crossref_type(\n        directivename = \"router\",\n        rolename      = \"router\",\n        indextemplate = \"pair: %s; router\",\n    )\n    app.add_config_value('rapidsms_next_version', '0.0', True)\n    app.add_directive('versionadded', VersionDirective)\n    app.add_directive('versionchanged', VersionDirective)\n\n\nclass VersionDirective(Directive):\n    has_content = True\n    required_arguments = 1\n    optional_arguments = 1\n    final_argument_whitespace = True\n    option_spec = {}\n\n    def run(self):\n        env = self.state.document.settings.env\n        arg0 = self.arguments[0]\n        is_nextversion = env.config.rapidsms_next_version == arg0\n        ret = []\n        node = addnodes.versionmodified()\n        ret.append(node)\n        if not is_nextversion:\n            if len(self.arguments) == 1:\n                linktext = 'Please, see the release notes </releases/%s>' % (arg0)\n                xrefs = roles.XRefRole()('doc', linktext, linktext,\n                                         self.lineno, self.state)\n                node.extend(xrefs[0])\n            node['version'] = arg0\n        else:\n            node['version'] = \"Development version\"\n        node['type'] = self.name\n        if len(self.arguments) == 2:\n            inodes, messages = self.state.inline_text(self.arguments[1],\n                                                      self.lineno+1)\n            node.extend(inodes)\n            if self.content:\n                self.state.nested_parse(self.content, self.content_offset,\n                                        node)\n            ret = ret + messages\n        env.note_versionchange(node['type'], node['version'], node,\n                               self.lineno)\n        return ret\n",
        "summary": "The code attempts to import the `json` module and falls back to `simplejson` or Django's `simplejson` if the primary import fails. It then sets up a Sphinx extension by defining custom cross-reference types for settings, template tags, filters, and routers, and adds a configuration value for the next version of RapidsMS. The `VersionDirective` class is defined to handle directives for marking version changes in documentation, providing links to release notes if applicable."
    },
    {
        "code": "import collections\nimport functools\nimport io\nimport itertools\nimport operator as op\nimport re\nimport timeit\n\nimport numpy as np\nimport aocd\n\nYEAR = 2021\nDAY = 11\n\n\ndef step(grid):\n    grid += 1\n    flash = np.zeros_like(grid, dtype=bool)\n    while np.any(grid[~flash] > 9):\n        new_flash = (grid > 9) ^ flash\n        grid[:-1, :-1] += new_flash[1:, 1:]\n        grid[:-1, :] += new_flash[1:, :]\n        grid[:-1, 1:] += new_flash[1:, :-1]\n        grid[:, :-1] += new_flash[:, 1:]\n        grid[:, 1:] += new_flash[:, :-1]\n        grid[1:, :-1] += new_flash[:-1, 1:]\n        grid[1:, :] += new_flash[:-1, :]\n        grid[1:, 1:] += new_flash[:-1, :-1]\n        flash |= new_flash\n    grid[flash] = 0\n    return flash\n\n\ndef main():\n    data = \n    data = aocd.get_data(day=DAY, year=YEAR)\n    inlist = np.array([list(map(int, l)) for l in data.split('\\n')])\n    print(inlist)\n\n    grid = inlist.copy()\n    num_flashes = 0\n    for i in range(100):\n        num_flashes += np.sum(step(grid))\n    print(num_flashes)\n    answer = num_flashes\n\n    aocd.submit(answer, part='a', day=DAY, year=YEAR)\n\n    grid = inlist.copy()\n    for i in itertools.count(1):\n        flash = step(grid)\n        if np.all(flash):\n            answer = i\n            break\n    print(answer)\n    aocd.submit(answer, part='b', day=DAY, year=YEAR)\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The provided Python code defines functions to simulate the flashing behavior of a grid representing energy levels in a 2D system. It includes a `step` function that increments each cell by one and propagates flashes until no more occur, resetting flashed cells to zero. The `main` function reads input data, initializes a grid, and iterates through steps to count total flashes over 100 iterations for part A, and finds the first step where all cells flash simultaneously for part B. It then submits these results to an external service using the `aocd` module."
    },
    {
        "code": "import unittest.mock as mock\n\nimport pytest\nimport requests_mock\n\nfrom openeo.rest.auth.auth import NullAuth, BearerAuth\nfrom openeo.rest.connection import Connection, RestApiConnection, connect, OpenEoApiError\n\nAPI_URL = \"https://oeo.net/\"\n\n\n@pytest.mark.parametrize(\n    [\"base\", \"paths\", \"expected_path\"],\n    [\n        \n        (\"https://oeo.net\", [\"foo\", \"/foo\"], \"https://oeo.net/foo\"),\n        (\"https://oeo.net/\", [\"foo\", \"/foo\"], \"https://oeo.net/foo\"),\n        \n        (\"https://oeo.net\", [\"foo/\", \"/foo/\"], \"https://oeo.net/foo/\"),\n        (\"https://oeo.net/\", [\"foo/\", \"/foo/\"], \"https://oeo.net/foo/\"),\n        \n        (\"https://oeo.net/api/v04\", [\"foo/bar\", \"/foo/bar\"], \"https://oeo.net/api/v04/foo/bar\"),\n        (\"https://oeo.net/api/v04/\", [\"foo/bar\", \"/foo/bar\"], \"https://oeo.net/api/v04/foo/bar\"),\n        (\"https://oeo.net/api/v04\", [\"foo/bar/\", \"/foo/bar/\"], \"https://oeo.net/api/v04/foo/bar/\"),\n        (\"https://oeo.net/api/v04/\", [\"foo/bar/\", \"/foo/bar/\"], \"https://oeo.net/api/v04/foo/bar/\"),\n    ]\n)\ndef test_rest_api_connection_url_handling(requests_mock, base, paths, expected_path):\n    \n    conn = RestApiConnection(base)\n    requests_mock.get(expected_path, text=\"payload\")\n    requests_mock.post(expected_path, text=\"payload\")\n    for path in paths:\n        assert conn.get(path).text == \"payload\"\n        assert conn.post(path, {\"foo\": \"bar\"}).text == \"payload\"\n\n\ndef test_rest_api_headers():\n    conn = RestApiConnection(API_URL)\n    with requests_mock.Mocker() as m:\n        def text(request, context):\n            assert request.headers[\"User-Agent\"].startswith(\"openeo-python-client\")\n            assert request.headers[\"X-Openeo-Bar\"] == \"XY123\"\n\n        m.get(\"/foo\", text=text)\n        m.post(\"/foo\", text=text)\n        conn.get(\"/foo\", headers={\"X-Openeo-Bar\": \"XY123\"})\n        conn.post(\"/foo\", {}, headers={\"X-Openeo-Bar\": \"XY123\"})\n\n\ndef test_connection_with_session():\n    session = mock.Mock()\n    response = session.request.return_value\n    response.status_code = 200\n    response.json.return_value = {\"foo\": \"bar\"}\n    conn = Connection(\"https://oeo.net/\", session=session)\n    assert conn.capabilities().capabilities == {\"foo\": \"bar\"}\n    session.request.assert_any_call(\n        url=\"https://oeo.net/\", method=\"get\", headers=mock.ANY, stream=mock.ANY, auth=mock.ANY\n    )\n\n\ndef test_connect_with_session():\n    session = mock.Mock()\n    response = session.request.return_value\n    response.status_code = 200\n    response.json.return_value = {\"foo\": \"bar\"}\n    conn = connect(\"https://oeo.net/\", session=session)\n    assert conn.capabilities().capabilities == {\"foo\": \"bar\"}\n    session.request.assert_any_call(\n        url=\"https://oeo.net/\", method=\"get\", headers=mock.ANY, stream=mock.ANY, auth=mock.ANY\n    )\n\n\ndef test_api_error(requests_mock):\n    conn = Connection(API_URL)\n    requests_mock.get('https://oeo.net/collections/foobar', status_code=404, json={\n        \"code\": \"CollectionNotFound\", \"message\": \"No such things as a collection 'foobar'\", \"id\": \"54321\"\n    })\n    with pytest.raises(OpenEoApiError) as exc_info:\n        conn.describe_collection(\"foobar\")\n    exc = exc_info.value\n    assert exc.http_status_code == 404\n    assert exc.code == \"CollectionNotFound\"\n    assert exc.message == \"No such things as a collection 'foobar'\"\n    assert exc.id == \"54321\"\n    assert exc.url is None\n\n\ndef test_api_error_non_json(requests_mock):\n    conn = Connection(API_URL)\n    requests_mock.get('https://oeo.net/collections/foobar', status_code=500, text=\"olapola\")\n    with pytest.raises(OpenEoApiError) as exc_info:\n        conn.describe_collection(\"foobar\")\n    exc = exc_info.value\n    assert exc.http_status_code == 500\n    assert exc.code == \"unknown\"\n    assert exc.message == \"olapola\"\n    assert exc.id is None\n    assert exc.url is None\n\n\ndef test_authenticate_basic(requests_mock):\n    conn = Connection(API_URL)\n\n    def text_callback(request, context):\n        assert request.headers[\"Authorization\"] == \"Basic am9objpqMGhu\"\n        return '{\"access_token\":\"w3lc0m3\"}'\n\n    requests_mock.get('https://oeo.net/credentials/basic', text=text_callback)\n\n    assert isinstance(conn.auth, NullAuth)\n    conn.authenticate_basic(username=\"john\", password=\"j0hn\")\n    assert isinstance(conn.auth, BearerAuth)\n    assert conn.auth.bearer == \"w3lc0m3\"\n\n\ndef test_authenticate_oidc(oidc_test_setup):\n    \n    client_id = \"myclient\"\n    oidc_discovery_url = \"https://oeo.net/credentials/oidc\"\n    state, webbrowser_open = oidc_test_setup(client_id=client_id, oidc_discovery_url=oidc_discovery_url)\n\n    \n    conn = Connection(API_URL)\n    assert isinstance(conn.auth, NullAuth)\n    conn.authenticate_OIDC(client_id=client_id, webbrowser_open=webbrowser_open)\n    assert isinstance(conn.auth, BearerAuth)\n    assert conn.auth.bearer == state[\"access_token\"]\n\n\ndef test_load_collection_arguments(requests_mock):\n    conn = Connection(API_URL)\n    requests_mock.get(API_URL, json={\"version\": \"0.4.0\"})\n    requests_mock.get(API_URL + \"collections/FOO\", json={\n        \"properties\": {\"eo:bands\": [{\"name\": \"red\"}, {\"name\": \"green\"}, {\"name\": \"blue\"}]}\n    })\n    spatial_extent = {\"west\": 1, \"south\": 2, \"east\": 3, \"north\": 4}\n    temporal_extent = [\"2019-01-01\", \"2019-01-22\"]\n    im = conn.load_collection(\n        \"FOO\", spatial_extent=spatial_extent, temporal_extent=temporal_extent, bands=[\"red\", \"green\"]\n    )\n    node = im.graph[im.node_id]\n    assert node[\"process_id\"] == \"load_collection\"\n    assert node[\"arguments\"] == {\n        \"id\": \"FOO\",\n        \"spatial_extent\": spatial_extent,\n        \"temporal_extent\": temporal_extent,\n        \"bands\": [\"red\", \"green\"]\n    }\n",
        "summary": "The provided Python code tests various functionalities of the `openeo.rest.connection` module, including URL handling, header management, session usage, authentication methods (basic and OIDC), and error handling. It uses `pytest` for testing and `requests_mock` to mock HTTP requests, ensuring that the connection behaves as expected under different conditions."
    },
    {
        "code": "import asyncio\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Union\n\nimport httpx\nimport websockets\nfrom websockets import exceptions\n\nlogger = logging.getLogger(\"yufuquantsdk\")\n\n\nclass WebsocketAPIClient:\n    def __init__(self, uri: str, ws: websockets.WebSocketClientProtocol = None) -> None:\n        self._uri: str = uri\n        self._ws: websockets.WebSocketClientProtocol = ws\n        self._authed: bool = False\n        self._api_key = \"\"\n        self._sub_topics: Set[str] = set()\n        self._inputs: asyncio.Queue[str] = asyncio.Queue()\n        self._outputs: asyncio.Queue[str] = asyncio.Queue(maxsize=100)\n        self._run_task: asyncio.Task[Any] = asyncio.get_event_loop().create_task(\n            self._run()\n        )\n\n    async def auth(self, api_key: str):\n        message = {\n            \"cmd\": \"auth\",\n            \"api_key\": api_key,\n        }\n        await self._deliver(json.dumps(message))\n        self._authed = True\n        self._api_key = api_key\n\n    async def sub(self, topics: Iterable[str]):\n        \n        if not isinstance(topics, set):\n            topics = set(topics)\n\n        message = {\n            \"cmd\": \"sub\",\n            \"topics\": list(topics),  \n        }\n        await self._deliver(json.dumps(message))\n        self._sub_topics = topics\n\n    async def unsub(self, topics: Iterable[str]):\n        \n        if not isinstance(topics, set):\n            topics = set(topics)\n\n        message = {\n            \"cmd\": \"unsub\",\n            \"topics\": list(topics),\n        }\n        await self._deliver(json.dumps(message))\n        self._sub_topics = self._sub_topics - topics\n\n    async def robot_ping(self):\n        data = {\"timestamp\": int(datetime.now().timestamp() * 1000)}\n        message = {\"category\": \"robotPing\", \"data\": data}\n        await self._broadcast(message)\n\n    async def robot_log(self, text: str, level: str = \"info\"):\n        data = {\n            \"text\": text,\n            \"level\": level,\n            \"timestamp\": int(datetime.now().timestamp()) * 1000,\n        }\n        message = {\"category\": \"robotLog\", \"data\": data}\n        await self._broadcast(message)\n\n    async def robot_position_store(self, positions):\n        data = {\n            \"updatedAt\": datetime.now().isoformat(),\n            \"positions\": positions,\n        }\n        message = {\"category\": \"robotPositionStore\", \"data\": data}\n        await self._broadcast(message)\n\n    async def robot_order_store(self, orders):\n        data = {\n            \"updatedAt\": datetime.now().isoformat(),\n            \"orders\": orders,\n        }\n        message = {\"category\": \"robotOrderStore\", \"data\": data}\n        await self._broadcast(message)\n\n    async def robot_strategy_store(self, data):\n        d = {\n            \"updatedAt\": datetime.now().isoformat(),\n            \"data\": data,\n        }\n        message = {\"category\": \"robotStrategyStore\", \"data\": d}\n        await self._broadcast(message)\n\n    async def _connect(self, **kwargs):\n        \n        kwargs[\"ping_interval\"] = None\n        retry_count = 0\n        for i in range(3):\n            try:\n                self._ws = await websockets.connect(self._uri, **kwargs)\n                break\n            except Exception as exc:\n                logger.exception(\"Failed to connect to %s: %s.\", self._uri, exc)\n                retry_count += 1\n                if retry_count >= 3:\n                    raise\n                await asyncio.sleep(10)\n        logger.info(\"Connected to %s.\", self._uri)\n\n    async def _reconnect(self):\n        await self._connect()\n\n        if self._authed:\n            await self.auth(self._api_key)\n\n        if len(self._sub_topics) > 0:\n            await self.sub(self._sub_topics)\n\n        logger.info(\"Reconnected to %s.\", self._uri)\n\n    async def _deliver(self, s: str):\n        await self._inputs.put(s)\n\n    async def _send(self, s: str):\n        assert self._ws is not None, \"No connection!\"\n        try:\n            await self._ws.send(s)\n            logger.debug(\">>> %s\", s)\n        except websockets.ConnectionClosed as exc:\n            logger.exception(exc)\n            await self._reconnect()\n\n    async def _broadcast(self, message: Dict):\n        data = {\"cmd\": \"broadcast\", \"message\": message}\n        await self._deliver(json.dumps(data))\n\n    async def _pong(self, message: Dict[str, int]):\n        await self._send(json.dumps({\"pong\": message[\"ping\"]}))\n\n    \n    async def _run(self):\n        await self._connect()\n        try:\n            while True:\n                incoming: asyncio.Task[Any] = asyncio.create_task(self._ws.recv())\n                outgoing: asyncio.Task[Any] = asyncio.create_task(self._inputs.get())\n\n                done: Set[asyncio.Future[Any]]\n                pending: Set[asyncio.Future[Any]]\n                done, pending = await asyncio.wait(\n                    [incoming, outgoing], return_when=asyncio.FIRST_COMPLETED\n                )\n\n                \n                if incoming in pending:\n                    incoming.cancel()\n                if outgoing in pending:\n                    outgoing.cancel()\n\n                if incoming in done:\n                    try:\n                        message = incoming.result()\n                        logger.debug(\"<<< %s\", message)\n                    except websockets.ConnectionClosed as exc:\n                        logger.exception(exc)\n                        await self._reconnect()\n                    else:\n                        decoded = json.loads(message)\n                        if \"ping\" in decoded:\n                            await self._pong(decoded)\n                        else:\n                            try:\n                                self._outputs.put_nowait(decoded)\n                            except asyncio.QueueFull:\n                                logger.warning(\"The outputs queue is full.\")\n\n                if outgoing in done:\n                    message = outgoing.result()\n                    await self._send(message)\n        finally:\n            await self.close()\n\n    async def close(self):\n        ws = self._ws\n        self._ws = None\n        await ws.close()\n        close_status = exceptions.format_close(ws.close_code, ws.close_reason)\n        logger.info(f\"Connection closed: {close_status}.\")\n\n\nROBOT_REQ_PATH = \"/robots/{robot_id}/\"\nROBOT_PING_REQ_PATH = \"/robots/{robot_id}/ping/\"\nROBOT_ASSET_RECORD_REQ_PATH = \"/robots/{robot_id}/assetRecord/\"\nROBOT_STRATEGY_PARAMETERS_REQ_PATH = \"/robots/{robot_id}/strategyParameters/\"\nROBOT_CREDENTIAL_KEY_REQ_PATH = \"/robots/{robot_id}/credentialKey/\"\nROBOT_POSITION_STORE_REQ_PATH = \"/robots/{robot_id}/positionStore/\"\nROBOT_ORDER_STORE_REQ_PATH = \"/robots/{robot_id}/orderStore/\"\nROBOT_STRATEGY_STORE_REQ_PATH = \"/robots/{robot_id}/strategyStore/\"\n\n\nclass RESTAPIClient:\n    def __init__(self, base_url: str, api_key: str):\n        self._base_url: str = base_url.rstrip(\"/\")\n        self._api_key: str = api_key\n\n    async def get_robot(self, robot_id: int):\n        req_path = ROBOT_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"GET\", req_path)\n\n    async def update_robot_asset_record(self, robot_id: int, data: Dict[str, Any]):\n        req_path = ROBOT_ASSET_RECORD_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"PATCH\", req_path, data=data)\n\n    async def update_robot_strategy_store(self, robot_id: int, data: Dict[str, Any]):\n        req_path = ROBOT_STRATEGY_STORE_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"PUT\", req_path, data=data)\n\n    async def update_robot_position_store(\n        self, robot_id: int, data: List[Dict[str, Any]]\n    ):\n        req_path = ROBOT_POSITION_STORE_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"PUT\", req_path, data=data)\n\n    async def update_robot_order_store(self, robot_id: int, data: List[Dict[str, Any]]):\n        req_path = ROBOT_ORDER_STORE_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"PUT\", req_path, data=data)\n\n    async def ping_robot(self, robot_id: int):\n        req_path = ROBOT_PING_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"POST\", req_path)\n\n    async def get_robot_strategy_parameters(self, robot_id: int):\n        req_path = ROBOT_STRATEGY_PARAMETERS_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"GET\", req_path)\n\n    async def get_robot_credential_key(self, robot_id: int):\n        req_path = ROBOT_CREDENTIAL_KEY_REQ_PATH.format(robot_id=robot_id)\n        return await self._request(\"GET\", req_path)\n\n    async def _request(\n        self,\n        method: str,\n        req_path: str,\n        headers: Optional[Dict[str, str]] = None,\n        params: Optional[Dict[str, str]] = None,\n        data: Optional[Union[Dict, List]] = None,\n        auth: bool = True,\n    ):\n        req_headers = {\"Content-Type\": \"application/json\"}\n        if auth:\n            req_headers[\"X-Api-Key\"] = self._api_key\n        if headers is not None:\n            req_headers.update(headers)\n\n        url = self._base_url + req_path\n        async with httpx.AsyncClient() as client:\n            logger.debug(\n                \"%s %s, Request<headers=%s params=%s data=%s>\",\n                method,\n                url,\n                req_headers,\n                params,\n                data,\n            )\n            res = await client.request(\n                method,\n                url,\n                headers=req_headers,\n                params=params,\n                json=data,\n                timeout=5,\n            )\n            http_text = res.text\n            logger.debug(\n                \"%s %s, Response<status_code=%s headers=%s http_text=%s>\",\n                method,\n                url,\n                res.status_code,\n                req_headers,\n                http_text,\n            )\n        res.raise_for_status()\n        if res.status_code == \"204\":\n            return None\n        return res.json()\n",
        "summary": "This code defines two classes: `WebSocketClient` and `RESTAPIClient`. The `WebSocketClient` class is used to connect to a WebSocket server, send messages, receive messages, and handle reconnections. It uses the `websockets` library for the WebSocket connection.\n\nThe `RESTAPIClient` class is used to interact with a RESTful API using HTTP requests. It supports various methods such as GET, POST, PUT, and PATCH. The client handles authentication by including an API key in the headers of each request.\n\nHere's a breakdown of the key components:\n\n1. **WebSocketClient Class**:\n   - **Attributes**: `_ws` (WebSocket connection), `_base_url` (URL of the WebSocket server).\n   - **Methods**:\n     - `__init__`: Initializes the client with the base URL.\n     - `connect`: Establishes a WebSocket connection to the specified URL.\n     - `send_message`: Sends a message through the WebSocket connection.\n     - `receive_message`: Receives a message from the WebSocket connection.\n     - `_run`: Main loop that handles sending and receiving messages, as well as reconnections.\n     - `close`: Closes the WebSocket connection.\n\n2. **RESTAPIClient Class**:\n   - **Attributes**: `_base_url` (URL of the REST API), `_api_key` (API key for authentication).\n   - **Methods**:\n     - `__init__`: Initializes the client with the base URL and API key.\n     - `get_robot`, `update_robot_asset_record`, etc.: These methods correspond to different endpoints of the REST API. They construct the request path, headers, and data, then make an HTTP request using `httpx.AsyncClient`.\n     - `_request`: A private method that handles the actual HTTP request. It constructs the URL, sets the headers, makes the request, logs the response, and raises an exception if the response status code indicates an error.\n\n### Usage Example\n\nHere's how you might use these classes:\n\n```python\nimport asyncio\n\nasync def main():\n    # WebSocket client example\n    ws_client = WebSocketClient(\"ws://example.com/socket\")\n    await ws_client.connect()\n    await ws_client.send_message({\"type\": \"ping\"})\n    response = await ws_client.receive_message()\n    print(response)\n    await ws_client.close()\n\n    # REST API client example\n    rest_client = RESTAPIClient(\"https://api.example.com\", \"your_api_key\")\n    robot_data = await rest_client.get_robot(123)\n    print(robot_data)\n\nasyncio.run(main())\n```\n\nThis example demonstrates connecting to a WebSocket server, sending and receiving messages, and making requests to a RESTful API."
    },
    {
        "code": "import discord\r\nfrom discord.ext import commands\r\nimport os\r\n\r\nintents = discord.Intents.default()\r\nintents.members = True\r\n\r\ntesting = False\r\n\r\nclient = commands.Bot(command_prefix = \"-\", case_insensitive = True, intents=intents)\r\n\r\nclient.remove_command('help')\r\n\r\nfor filename in os.listdir('./cogs'):\r\n    if filename.endswith('.py'):\r\n        client.load_extension(f'cogs.{filename[:-3]}')\r\n\r\nclient.run('",
        "summary": "This Python script sets up a Discord bot using the `discord.py` library, with intents enabled for member tracking. It loads commands from files located in a 'cogs' directory and removes the default 'help' command. The bot runs with a case-insensitive prefix of '-' and requires an authentication token to connect to a Discord server."
    },
    {
        "code": "import logging\nimport math\nimport os\nimport random\nimport shutil\n\nimport tensorflow as tf\n\nfrom jack import readers\nfrom jack.core.tensorflow import TFReader\nfrom jack.eval import evaluate_reader, pretty_print_results\nfrom jack.util.hooks import LossHook, ExamplesPerSecHook, ETAHook\n\nlogger = logging.getLogger(__name__)\n\n\ndef train(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    if isinstance(reader, TFReader):\n        train_tensorflow(reader, train_data, test_data, dev_data, configuration, debug)\n    else:\n        train_pytorch(reader, train_data, test_data, dev_data, configuration, debug)\n\n\ndef train_tensorflow(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    import tensorflow as tf\n    seed = configuration.get('seed', 0)\n\n    \n    random.seed(seed)\n    tf.set_random_seed(seed)\n\n    clip_value = configuration.get('clip_value')\n    batch_size = configuration.get('batch_size')\n    dev_batch_size = configuration.get('dev_batch_size') or batch_size\n    epochs = configuration.get('epochs')\n    l2 = configuration.get('l2')\n    optimizer = configuration.get('optimizer')\n    learning_rate = configuration.get('learning_rate')\n    min_learning_rate = configuration.get('min_learning_rate')\n    learning_rate_decay = configuration.get('learning_rate_decay')\n    log_interval = configuration.get('log_interval')\n    validation_interval = configuration.get('validation_interval')\n    tensorboard_folder = configuration.get('tensorboard_folder')\n    reader_type = configuration.get('reader')\n    save_dir = configuration.get('save_dir')\n    write_metrics_to = configuration.get('write_metrics_to')\n\n    if clip_value != 0.0:\n        clip_value = - abs(clip_value), abs(clip_value)\n\n    learning_rate = tf.get_variable(\"learning_rate\", initializer=learning_rate, dtype=tf.float32, trainable=False)\n    lr_decay_op = learning_rate.assign(tf.maximum(learning_rate_decay * learning_rate, min_learning_rate))\n\n    name_to_optimizer = {\n        'gd': tf.train.GradientDescentOptimizer,\n        'adam': tf.train.AdamOptimizer,\n        'adagrad': tf.train.AdagradOptimizer,\n        'adadelta': tf.train.AdadeltaOptimizer,\n        'rmsprop': tf.train.RMSPropOptimizer\n    }\n\n    if optimizer not in name_to_optimizer:\n        raise ValueError('Unknown optimizer: {}'.format(optimizer))\n\n    tf_optimizer_class = name_to_optimizer[optimizer]\n    tf_optimizer = tf_optimizer_class(learning_rate=learning_rate)\n\n    sw = None\n    if tensorboard_folder is not None:\n        if os.path.exists(tensorboard_folder):\n            shutil.rmtree(tensorboard_folder)\n        sw = tf.summary.FileWriter(tensorboard_folder)\n\n    \n    iter_interval = 1 if debug else log_interval\n    hooks = [LossHook(reader, iter_interval, summary_writer=sw),\n             ETAHook(reader, iter_interval, int(math.ceil(len(train_data) / batch_size)), epochs),\n             ExamplesPerSecHook(reader, batch_size, iter_interval, sw)]\n\n    preferred_metric, best_metric = readers.eval_hooks[reader_type].preferred_metric_and_initial_score()\n\n    def side_effect(metrics, prev_metric):\n        \n        if prev_metric is None:  \n            reader.store(save_dir)\n        m = metrics[preferred_metric]\n        if prev_metric is not None and m < prev_metric:\n            reader.session.run(lr_decay_op)\n            logger.info(\"Decayed learning rate to: %.5f\" % reader.session.run(learning_rate))\n        elif m > best_metric[0] and save_dir is not None:\n            best_metric[0] = m\n            reader.model_module.store(os.path.join(save_dir, \"model_module\"))\n            logger.info(\"Saving reader to: %s\" % save_dir)\n        return m\n\n    \n    hooks.append(readers.eval_hooks[reader_type](\n        reader, dev_data, dev_batch_size, summary_writer=sw, side_effect=side_effect,\n        iter_interval=validation_interval,\n        epoch_interval=(1 if validation_interval is None else None),\n        write_metrics_to=write_metrics_to))\n\n    \n    reader.train(tf_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks,\n                 l2=l2, clip=clip_value, clip_op=tf.clip_by_value, summary_writer=sw)\n\n    \n    if dev_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, dev_data, batch_size)\n\n        logger.info(\"\n        pretty_print_results(result_dict)\n\n    if test_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, test_data, batch_size)\n\n        logger.info(\"\n        pretty_print_results(result_dict)\n\n\ndef train_pytorch(reader, train_data, test_data, dev_data, configuration: dict, debug=False):\n    import torch\n    seed = configuration.get('seed')\n\n    \n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    clip_value = configuration.get('clip_value')\n    batch_size = configuration.get('batch_size')\n    epochs = configuration.get('epochs')\n    l2 = configuration.get('l2')\n    optimizer = configuration.get('optimizer')\n    learning_rate = configuration.get('learning_rate')\n    learning_rate_decay = configuration.get('learning_rate_decay')\n    log_interval = configuration.get('log_interval')\n    validation_interval = configuration.get('validation_interval')\n    tensorboard_folder = configuration.get('tensorboard_folder')\n    model = configuration.get('reader')\n    save_dir = configuration.get('save_dir')\n    write_metrics_to = configuration.get('write_metrics_to')\n\n    \n    reader.setup_from_data(train_data, is_training=True)\n\n    if clip_value != 0.0:\n        clip_value = - abs(clip_value), abs(clip_value)\n\n    name_to_optimizer = {\n        'gd': torch.optim.SGD,\n        'adam': torch.optim.Adam,\n        'adagrad': torch.optim.Adagrad,\n        'adadelta': torch.optim.Adadelta\n    }\n\n    if optimizer not in name_to_optimizer:\n        raise ValueError('Unknown optimizer: {}'.format(optimizer))\n\n    torch_optimizer_class = name_to_optimizer[optimizer]\n    params = list(reader.model_module.prediction_module.parameters())\n    params.extend(reader.model_module.loss_module.parameters())\n\n    torch_optimizer = torch_optimizer_class(params, lr=learning_rate)\n\n    sw = None\n    if tensorboard_folder is not None:\n        if os.path.exists(tensorboard_folder):\n            shutil.rmtree(tensorboard_folder)\n        sw = tf.summary.FileWriter(tensorboard_folder)\n\n    \n    iter_interval = 1 if debug else log_interval\n    hooks = [LossHook(reader, iter_interval, summary_writer=sw),\n             ExamplesPerSecHook(reader, batch_size, iter_interval, sw)]\n\n    preferred_metric, best_metric = readers.eval_hooks[model].preferred_metric_and_initial_score()\n\n    def side_effect(metrics, prev_metric):\n        \n        m = metrics[preferred_metric]\n        if prev_metric is not None and m < prev_metric:\n            for param_group in torch_optimizer.param_groups:\n                param_group['lr'] *= learning_rate_decay\n                logger.info(\"Decayed learning rate to: %.5f\" % param_group['lr'])\n        elif m > best_metric[0] and save_dir is not None:\n            best_metric[0] = m\n            if prev_metric is None:  \n                reader.store(save_dir)\n            else:\n                reader.model_module.store(os.path.join(save_dir, \"model_module\"))\n            logger.info(\"Saving model to: %s\" % save_dir)\n        return m\n\n    \n    hooks.append(readers.eval_hooks[model](\n        reader, dev_data, batch_size, summary_writer=sw, side_effect=side_effect,\n        iter_interval=validation_interval,\n        epoch_interval=(1 if validation_interval is None else None),\n        write_metrics_to=write_metrics_to))\n\n    \n    reader.train(torch_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks,\n                 l2=l2, clip=clip_value)\n\n    \n    if dev_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, dev_data, batch_size)\n\n        logger.info(\"\n        pretty_print_results(result_dict)\n\n    if test_data is not None and save_dir is not None:\n        reader.load(save_dir)\n        result_dict = evaluate_reader(reader, test_data, batch_size)\n\n        logger.info(\"\n        pretty_print_results(result_dict)\n",
        "summary": "The provided Python code defines a function `train` that trains a machine learning model using either TensorFlow or PyTorch based on the type of reader passed. It handles configuration settings such as seed, batch size, optimizer, and learning rate decay. The training process includes hooks for logging loss, examples per second, and ETA, and it saves the best model based on validation performance. After training, it evaluates the model on development and test datasets if provided."
    },
    {
        "code": "from warnings import warn\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BatchNormLayer(nn.Module):\n    r\n\n    def __init__(self, num_features, momentum=0.1, affine=True,\n                 track_running_stats=True, frozen_stats=False,\n                 learnable_stats=False):\n        r\n        super(BatchNormLayer, self).__init__()\n\n        if learnable_stats:\n            \n            \n            \n            \n            \n            raise NotImplementedError('Option \"learnable_stats\" has not been ' +\n                                      'implemented yet!')\n\n        if momentum is None:\n            \n            \n            \n            \n            \n            raise NotImplementedError('This reimplementation of PyTorch its ' +\n                                      'batchnorm layer does not support ' +\n                                      'setting \"momentum\" to None.')\n\n        if learnable_stats and track_running_stats:\n            raise ValueError('Option \"track_running_stats\" must be set to ' +\n                             'False when enabling \"learnable_stats\".')\n\n        if frozen_stats and track_running_stats:\n            raise ValueError('Option \"track_running_stats\" must be set to ' +\n                             'False when enabling \"frozen_stats\".')\n\n        self._num_features = num_features\n        self._momentum = momentum\n        self._affine = affine\n        self._track_running_stats = track_running_stats\n        self._frozen_stats = frozen_stats\n        self._learnable_stats = learnable_stats\n\n        self.register_buffer('_num_stats', torch.tensor(0, dtype=torch.long))\n\n        self._weights = nn.ParameterList()\n        self._param_shapes = [[num_features], [num_features]]\n\n        if affine:\n            \n            self.register_parameter('scale', nn.Parameter( \\\n                torch.Tensor(num_features), requires_grad=True))\n            \n            self.register_parameter('bias', nn.Parameter( \\\n                torch.Tensor(num_features), requires_grad=True))\n\n            self._weights.append(self.scale)\n            self._weights.append(self.bias)\n\n            nn.init.ones_(self.scale)\n            nn.init.zeros_(self.bias)\n\n        elif not learnable_stats:\n            self._weights = None\n\n        if learnable_stats:\n            \n            \n            raise NotImplementedError()\n\n        elif track_running_stats or frozen_stats:\n            \n            \n            self.checkpoint_stats()\n        else:\n            mname, vname = self._stats_names(0)\n            self.register_buffer(mname, None)\n            self.register_buffer(vname, None)\n\n    @property\n    def weights(self):\n        \n        return self._weights\n\n    @property\n    def param_shapes(self):\n        \n        return self._param_shapes\n\n    @property\n    def hyper_shapes(self):\n        \n        \n        \n        raise NotImplementedError('Not implemented yet!')\n        return self._hyper_shapes\n\n    @property\n    def num_stats(self):\n        \n        return self._num_stats\n\n    def forward(self, inputs, running_mean=None, running_var=None, weight=None,\n                bias=None, stats_id=None):\n        r\n        assert (running_mean is None and running_var is None or \\\n                running_mean is not None and running_var is not None)\n\n        if not self._affine:\n            if weight is None or bias is None:\n                raise ValueError('Layer was generated in non-affine mode. ' +\n                                 'Therefore, arguments \"weight\" and \"bias\" ' +\n                                 'may not be None.')\n\n        \n        \n        if weight is None and self._affine:\n            weight = self.scale\n        if bias is None and self._affine:\n            bias = self.bias\n\n        stats_given = running_mean is not None\n\n        if (running_mean is None or running_var is None):\n            if stats_id is None and self.num_stats > 1:\n                raise ValueError('Parameter \"stats_id\" is not defined but ' +\n                                 'multiple running stats are available.')\n            elif self._track_running_stats:\n                if stats_id is None:\n                    stats_id = 0\n                assert (stats_id < self.num_stats)\n\n                rm, rv = self.get_stats(stats_id)\n\n                if running_mean is None:\n                    running_mean = rm\n                if running_var is None:\n                    running_var = rv\n        elif stats_id is not None:\n            warn('Parameter \"stats_id\" is ignored since running stats have ' +\n                 'been provided.')\n\n        momentum = self._momentum\n\n        if stats_given or self._track_running_stats:\n            return F.batch_norm(inputs, running_mean, running_var,\n                                weight=weight, bias=bias,\n                                training=self.training, momentum=momentum)\n\n        if self._learnable_stats:\n            raise NotImplementedError()\n\n        if self._frozen_stats:\n            return F.batch_norm(inputs, running_mean, running_var,\n                                weight=weight, bias=bias, training=False)\n\n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n        else:\n            assert (not self._track_running_stats)\n\n            \n            return F.batch_norm(inputs, None, None, weight=weight, bias=bias,\n                                training=True, momentum=momentum)\n\n    def checkpoint_stats(self, device=None):\n        \n        assert (self._track_running_stats or \\\n                self._frozen_stats and self._num_stats == 0)\n\n        if device is None:\n            if self.num_stats > 0:\n                mname_old, _ = self._stats_names(self._num_stats - 1)\n                device = getattr(self, mname_old).device\n\n        if self._learnable_stats:\n            raise NotImplementedError()\n\n        mname, vname = self._stats_names(self._num_stats)\n        self._num_stats += 1\n\n        self.register_buffer(mname, torch.zeros(self._num_features,\n                                                device=device))\n        self.register_buffer(vname, torch.ones(self._num_features,\n                                               device=device))\n\n    def get_stats(self, stats_id=None):\n        \n        if stats_id is None:\n            stats_id = self.num_stats - 1\n        assert (stats_id < self.num_stats)\n\n        mname, vname = self._stats_names(stats_id)\n\n        running_mean = getattr(self, mname)\n        running_var = getattr(self, vname)\n\n        return running_mean, running_var\n\n    def _stats_names(self, stats_id):\n        \n        mean_name = 'mean_%d' % stats_id\n        var_name = 'var_%d' % stats_id\n\n        return mean_name, var_name\n\n\nif __name__ == '__main__':\n    pass\n",
        "summary": "The provided Python code defines a custom `BatchNormLayer` class that extends PyTorch's `nn.Module`. This layer implements batch normalization with options for learnable statistics, frozen statistics, and non-affine transformations. The forward method handles both training and inference phases, using either running statistics or user-provided statistics as needed."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.framework.python.ops import variables as contrib_variables\nfrom tensorflow.contrib.learn.python.learn.estimators import _sklearn\nfrom tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\nfrom tensorflow.contrib.learn.python.learn.estimators import sdca_optimizer\nfrom tensorflow.contrib.learn.python.learn.estimators.base import DeprecatedMixin\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import logging_ops\nfrom tensorflow.python.platform import tf_logging as logging\n\n\n\ndef _changing(feature_columns):\n  if feature_columns is not None:\n    return\n  logging.warn(\n      \"Change warning: `feature_columns` will be required after 2016-08-01.\\n\"\n      \"Instructions for updating:\\n\"\n      \"Pass `tf.contrib.learn.infer_real_valued_columns_from_input(x)` or\"\n      \" `tf.contrib.learn.infer_real_valued_columns_from_input_fn(input_fn)`\"\n      \" as `feature_columns`, where `x` or `input_fn` is your argument to\"\n      \" `fit`, `evaluate`, or `predict`.\")\n\n\nclass LinearClassifier(dnn_linear_combined.DNNLinearCombinedClassifier):\n  \n\n  def __init__(self,\n               feature_columns=None,\n               model_dir=None,\n               n_classes=2,\n               weight_column_name=None,\n               optimizer=None,\n               gradient_clip_norm=None,\n               enable_centered_bias=True,\n               config=None):\n    \n    _changing(feature_columns)\n    super(LinearClassifier, self).__init__(\n        model_dir=model_dir,\n        n_classes=n_classes,\n        weight_column_name=weight_column_name,\n        linear_feature_columns=feature_columns,\n        linear_optimizer=optimizer,\n        gradient_clip_norm=gradient_clip_norm,\n        enable_centered_bias=enable_centered_bias,\n        config=config)\n    self._feature_columns_inferred = False\n\n  \n  def _validate_linear_feature_columns(self, features):\n    if self._linear_feature_columns is None:\n      self._linear_feature_columns = layers.infer_real_valued_columns(features)\n      self._feature_columns_inferred = True\n    elif self._feature_columns_inferred:\n      this_dict = {c.name: c for c in self._linear_feature_columns}\n      that_dict = {\n          c.name: c for c in layers.infer_real_valued_columns(features)\n      }\n      if this_dict != that_dict:\n        raise ValueError(\n            \"Feature columns, expected %s, got %s.\", (this_dict, that_dict))\n\n  def _get_train_ops(self, features, targets):\n    \n    self._validate_linear_feature_columns(features)\n    if not isinstance(self._linear_optimizer, sdca_optimizer.SDCAOptimizer):\n      return super(LinearClassifier, self)._get_train_ops(features, targets)\n\n    \n    if self._target_column.num_label_columns > 2:\n      raise ValueError(\n          \"SDCA does not currently support multi-class classification.\")\n    global_step = contrib_variables.get_global_step()\n    assert global_step\n\n    logits, columns_to_variables, _ = layers.weighted_sum_from_feature_columns(\n        columns_to_tensors=features,\n        feature_columns=self._linear_feature_columns,\n        num_outputs=self._target_column.num_label_columns,\n        weight_collections=[self._linear_weight_collection],\n        scope=\"linear\")\n    with ops.control_dependencies([self._centered_bias()]):\n      loss = self._target_column.loss(logits, targets, features)\n    logging_ops.scalar_summary(\"loss\", loss)\n\n    train_ops = self._linear_optimizer.get_train_step(\n        self._linear_feature_columns, self._target_column.weight_column_name,\n        \"logistic_loss\", features, targets, columns_to_variables, global_step)\n\n    return train_ops, loss\n\n  def _get_eval_ops(self, features, targets, metrics=None):\n    self._validate_linear_feature_columns(features)\n    return super(LinearClassifier, self)._get_eval_ops(\n        features, targets, metrics)\n\n  def _get_predict_ops(self, features):\n    \n    self._validate_linear_feature_columns(features)\n    return super(LinearClassifier, self)._get_predict_ops(features)\n\n  @property\n  def weights_(self):\n    return self.linear_weights_\n\n  @property\n  def bias_(self):\n    return self.linear_bias_\n\n\nclass LinearRegressor(dnn_linear_combined.DNNLinearCombinedRegressor):\n  \n\n  def __init__(self,\n               feature_columns=None,\n               model_dir=None,\n               weight_column_name=None,\n               optimizer=None,\n               gradient_clip_norm=None,\n               enable_centered_bias=True,\n               target_dimension=1,\n               config=None):\n    \n    _changing(feature_columns)\n    super(LinearRegressor, self).__init__(\n        model_dir=model_dir,\n        weight_column_name=weight_column_name,\n        linear_feature_columns=feature_columns,\n        linear_optimizer=optimizer,\n        gradient_clip_norm=gradient_clip_norm,\n        enable_centered_bias=enable_centered_bias,\n        target_dimension=target_dimension,\n        config=config)\n    self._feature_columns_inferred = False\n\n  \n  def _validate_linear_feature_columns(self, features):\n    if self._linear_feature_columns is None:\n      self._linear_feature_columns = layers.infer_real_valued_columns(features)\n      self._feature_columns_inferred = True\n    elif self._feature_columns_inferred:\n      this_dict = {c.name: c for c in self._linear_feature_columns}\n      that_dict = {\n          c.name: c for c in layers.infer_real_valued_columns(features)\n      }\n      if this_dict != that_dict:\n        raise ValueError(\n            \"Feature columns, expected %s, got %s.\", (this_dict, that_dict))\n\n  def _get_train_ops(self, features, targets):\n    \n    if isinstance(self._linear_optimizer, sdca_optimizer.SDCAOptimizer):\n      raise ValueError(\"SDCAOptimizer does not currently support regression.\")\n    self._validate_linear_feature_columns(features)\n    return super(LinearRegressor, self)._get_train_ops(features, targets)\n\n  def _get_eval_ops(self, features, targets, metrics=None):\n    self._validate_linear_feature_columns(features)\n    return super(LinearRegressor, self)._get_eval_ops(\n        features, targets, metrics)\n\n  def _get_predict_ops(self, features):\n    \n    self._validate_linear_feature_columns(features)\n    return super(LinearRegressor, self)._get_predict_ops(features)\n\n  @property\n  def weights_(self):\n    return self.linear_weights_\n\n  @property\n  def bias_(self):\n    return self.linear_bias_\n\n\n\nclass TensorFlowLinearRegressor(DeprecatedMixin, LinearRegressor,\n                                _sklearn.RegressorMixin):\n  pass\n\n\nclass TensorFlowLinearClassifier(DeprecatedMixin, LinearClassifier,\n                                 _sklearn.ClassifierMixin):\n  pass\n\n\nTensorFlowRegressor = TensorFlowLinearRegressor\nTensorFlowClassifier = TensorFlowLinearClassifier\n",
        "summary": "The provided Python code defines custom classes `LinearClassifier` and `LinearRegressor` that extend TensorFlow's `DNNLinearCombinedClassifier` and `DNNLinearCombinedRegressor`, respectively. These classes include methods for training, evaluating, and predicting using linear models, with additional functionality to handle feature columns and optimizer settings. The code also includes deprecation warnings for future changes regarding the requirement of feature columns."
    },
    {
        "code": "from typing import List, Dict, Any\nimport torch\nimport trtorch._C\nfrom trtorch import _types\n\n\ndef _supported_input_size_type(input_size: Any) -> bool:\n    if isinstance(input_size, torch.Size):\n        return True\n    elif isinstance(input_size, tuple):\n        return True\n    elif isinstance(input_size, list):\n        return True\n    else:\n        raise TypeError(\n            \"Input sizes for inputs are required to be a List, tuple or torch.Size or a Dict of three sizes (min, opt, max), found type: \"\n            + str(type(input_size)))\n\n\ndef _parse_input_ranges(input_sizes: List) -> List:\n\n    if any(not isinstance(i, dict) and not _supported_input_size_type(i) for i in input_sizes):\n        raise KeyError(\"An input size must either be a static size or a range of three sizes (min, opt, max) as Dict\")\n\n    parsed_input_sizes = []\n    for i in input_sizes:\n        if isinstance(i, dict):\n            if all(k in i for k in [\"min\", \"opt\", \"min\"]):\n                in_range = trtorch._C.InputRange()\n                in_range.min = i[\"min\"]\n                in_range.opt = i[\"opt\"]\n                in_range.max = i[\"max\"]\n                parsed_input_sizes.append(in_range)\n\n            elif \"opt\" in i:\n                in_range = trtorch._C.InputRange()\n                in_range.min = i[\"opt\"]\n                in_range.opt = i[\"opt\"]\n                in_range.max = i[\"opt\"]\n                parsed_input_sizes.append(in_range)\n\n            else:\n                raise KeyError(\n                    \"An input size must either be a static size or a range of three sizes (min, opt, max) as Dict\")\n\n        elif isinstance(i, list):\n            in_range = trtorch._C.InputRange()\n            in_range.min = i\n            in_range.opt = i\n            in_range.max = i\n            parsed_input_sizes.append(in_range)\n\n        elif isinstance(i, tuple):\n            in_range = trtorch._C.InputRange()\n            in_range.min = list(i)\n            in_range.opt = list(i)\n            in_range.max = list(i)\n            parsed_input_sizes.append(in_range)\n\n    return parsed_input_sizes\n\n\ndef _parse_op_precision(precision: Any) -> _types.dtype:\n    if isinstance(precision, torch.dtype):\n        if precision == torch.int8:\n            return _types.dtype.int8\n        elif precision == torch.half:\n            return _types.dtype.half\n        elif precision == torch.float:\n            return _types.dtype.float\n        else:\n            raise TypeError(\"Provided an unsupported dtype as operating precision (support: int8, half, float), got: \" +\n                            str(precision))\n\n    elif isinstance(precision, _types.DataTypes):\n        return precision\n\n    else:\n        raise TypeError(\"Op precision type needs to be specified with a torch.dtype or a trtorch.dtype, got: \" +\n                        str(type(precision)))\n\n\ndef _parse_device_type(device: Any) -> _types.DeviceType:\n    if isinstance(device, torch.device):\n        if device.type == 'cuda':\n            return _types.DeviceType.gpu\n        else:\n            ValueError(\"Got a device type other than GPU or DLA (type: \" + str(device.type) + \")\")\n    elif isinstance(device, _types.DeviceType):\n        return device\n    elif isinstance(device, str):\n        if device == \"gpu\" or device == \"GPU\":\n            return _types.DeviceType.gpu\n        elif device == \"dla\" or device == \"DLA\":\n            return _types.DeviceType.dla\n        else:\n            ValueError(\"Got a device type other than GPU or DLA (type: \" + str(device) + \")\")\n    else:\n        raise TypeError(\"Device specification must be of type torch.device, string or trtorch.DeviceType, but got: \" +\n                        str(type(device)))\n\n\ndef _parse_compile_spec(compile_spec: Dict[str, Any]) -> trtorch._C.CompileSpec:\n    info = trtorch._C.CompileSpec()\n    if \"input_shapes\" not in compile_spec:\n        raise KeyError(\n            \"Input shapes for inputs are required as a List, provided as either a static sizes or a range of three sizes (min, opt, max) as Dict\"\n        )\n\n    info.input_ranges = _parse_input_ranges(compile_spec[\"input_shapes\"])\n\n    if \"op_precision\" in compile_spec:\n        info.op_precision = _parse_op_precision(compile_spec[\"op_precision\"])\n\n    if \"refit\" in compile_spec:\n        assert isinstance(compile_spec[\"refit\"], bool)\n        info.refit = compile_spec[\"refit\"]\n\n    if \"debug\" in compile_spec:\n        assert isinstance(compile_spec[\"debug\"], bool)\n        info.debug = compile_spec[\"debug\"]\n\n    if \"strict_types\" in compile_spec:\n        assert isinstance(compile_spec[\"strict_types\"], bool)\n        info.strict_types = compile_spec[\"strict_types\"]\n\n    if \"allow_gpu_fallback\" in compile_spec:\n        assert isinstance(compile_spec[\"allow_gpu_fallback\"], bool)\n        info.allow_gpu_fallback = compile_spec[\"allow_gpu_fallback\"]\n\n    if \"device_type\" in compile_spec:\n        info.device = _parse_device_type(compile_spec[\"device_type\"])\n\n    if \"capability\" in compile_spec:\n        assert isinstance(compile_spec[\"capability\"], _types.EngineCapability)\n        info.capability = compile_spec[\"capability\"]\n\n    if \"num_min_timing_iters\" in compile_spec:\n        assert type(compile_spec[\"num_min_timing_iters\"]) is int\n        info.num_min_timing_iters = compile_spec[\"num_min_timing_iters\"]\n\n    if \"num_avg_timing_iters\" in compile_spec:\n        assert type(compile_spec[\"num_avg_timing_iters\"]) is int\n        info.num_avg_timing_iters = compile_spec[\"num_avg_timing_iters\"]\n\n    if \"workspace_size\" in compile_spec:\n        assert type(compile_spec[\"workspace_size\"]) is int\n        info.workspace_size = compile_spec[\"workspace_size\"]\n\n    if \"max_batch_size\" in compile_spec:\n        assert type(compile_spec[\"max_batch_size\"]) is int\n        info.max_batch_size = compile_spec[\"max_batch_size\"]\n\n    return info\n\n\ndef TensorRTCompileSpec(compile_spec: Dict[str, Any]):\n    \n\n    parsed_spec = _parse_compile_spec(compile_spec)\n\n    backend_spec = torch.classes.tensorrt.CompileSpec()\n\n    for i in parsed_spec.input_ranges:\n        ir = torch.classes.tensorrt.InputRange()\n        ir.set_min(i.min)\n        ir.set_opt(i.opt)\n        ir.set_max(i.max)\n        backend_spec.append_input_range(ir)\n\n    backend_spec.set_op_precision(int(parsed_spec.op_precision))\n    backend_spec.set_refit(parsed_spec.refit)\n    backend_spec.set_debug(parsed_spec.debug)\n    backend_spec.set_refit(parsed_spec.refit)\n    backend_spec.set_strict_types(parsed_spec.strict_types)\n    backend_spec.set_allow_gpu_fallback(parsed_spec.allow_gpu_fallback)\n    backend_spec.set_device(int(parsed_spec.device))\n    backend_spec.set_capability(int(parsed_spec.capability))\n    backend_spec.set_num_min_timing_iters(parsed_spec.num_min_timing_iters)\n    backend_spec.set_num_avg_timing_iters(parsed_spec.num_avg_timing_iters)\n    backend_spec.set_workspace_size(parsed_spec.workspace_size)\n    backend_spec.set_max_batch_size(parsed_spec.max_batch_size)\n\n    return backend_spec\n",
        "summary": "The provided Python code defines functions to parse and validate input specifications for compiling a PyTorch model using TensorRT, including handling different types of input sizes, precision settings, device configurations, and other compile-time options. It then constructs a `CompileSpec` object that can be used with the TensorRT backend in PyTorch to optimize and deploy models efficiently on supported hardware devices."
    },
    {
        "code": "from django.conf.urls import patterns, url\n\n\nurlpatterns = patterns('appointments.views',\n    url(r'^appointment/(?P<practice_id>\\d+)/$', 'appointment_form', name='appointment_form'),\n    url(r'^appointment/created/(?P<practice_id>\\d+)/$', 'appointment_created', name='appointment_created'),\n\n)\n",
        "summary": "The Python code defines URL patterns for a Django application, mapping URLs to specific views in the `appointments.views` module. It includes routes for displaying an appointment form and confirming an appointment creation, both parameterized by a `practice_id`."
    },
    {
        "code": "from PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom rotary_class import RotaryEncoder\n\nclass Display():\n    def __init__(self, disp):\n        self.disp = disp\n        self.dimensions = (disp.width, disp.height)\n        self.image = Image.new('1', self.dimensions)\n        self.draw = ImageDraw.Draw(self.image)\n        self.font = ImageFont.truetype(\"./DejaVuSansMono.ttf\", 10)\n\n    def display_clear(self):\n        self.draw.rectangle((0, 0) + self.dimensions, outline = 0, fill = 0)\n\n    def init_display(self):\n        self.disp.begin()\n        self.disp.clear()\n        self.disp.display()\n        self.display_clear()\n\n        self.disp.image(self.image)\n        self.disp.display()\n\n    def draw_rows(self, rows, inv_col):\n        self.display_clear()\n\n        for idx, row in enumerate(rows):\n            if inv_col == idx:\n                self.draw.rectangle([(0, 10 * idx), (10 * idx + self.dimensions[0], 1 + 10 * idx + 10)], outline = 0, fill = 255)\n                self.draw.text((1, 10 * idx), row, font = self.font, fill = 0)\n            else:\n                self.draw.rectangle([(0, 10 * idx), (10 * idx + self.dimensions[0], 1 + 10 * idx + 10)], outline = 0, fill = 0)\n                self.draw.text((1, 10 * idx), row, font = self.font, fill = 255)\n\n        self.disp.image(self.image)\n        self.disp.display()\n\nclass Menu():\n    def __init__(self, disp, encoder, items = []):\n        self.items = items\n        self.pointer = 0\n        self.row = 0\n        self.last_row = 0\n        self.last_slice = None\n        self.disp = Display(disp)\n        self.disp.init_display()\n        self.draw()\n\n        def encoder_ev (direction):\n            if direction == 1:\n                self.prev()\n            elif direction == 2:\n                self.next()\n            elif direction == 3:\n                self.exec_item()\n        self.encoder = RotaryEncoder(encoder[\"pin1\"], encoder[\"pin2\"], encoder[\"sw\"], encoder_ev)\n\n    def draw(self):\n        tmp_slice = None\n        if self.row == self.last_row:\n            if self.last_row == 0:\n                tmp_slice = self.items[self.pointer:self.pointer + 3]\n            else:\n                tmp_slice = self.items[self.pointer - 2:self.pointer + 1]\n            self.disp.draw_rows(tmp_slice, self.row)\n            self.last_slice = tmp_slice\n        else:\n            self.disp.draw_rows(self.last_slice, self.row)\n        self.last_row = self.row\n\n    def next(self):\n        if self.pointer + 1 <= len(self.items) - 1:\n            self.pointer += 1\n        if self.row < 2:\n            self.row += 1\n        self.draw()\n\n    def prev(self):\n        if self.pointer - 1 >= 0:\n            self.pointer -= 1\n        if self.row > 0:\n            self.row -= 1\n        self.draw()\n\n    def exec_item(self):\n        print(\"Item selcted\", str(self.pointer))\n",
        "summary": "The provided Python code defines a `Display` class for managing an OLED display using the PIL library, including methods for clearing and initializing the display, as well as drawing rows of text. The `Menu` class extends this functionality by adding navigation capabilities through a rotary encoder, allowing users to scroll through menu items and execute selected actions."
    },
    {
        "code": "from tests.analyzer.utils import UnusedTestCase\nfrom unimport.statement import Import, ImportFrom\n\n\nclass AsImportTestCase(UnusedTestCase):\n    def test_as_import_all_unused_all_cases(self):\n        self.assertSourceAfterScanningEqualToExpected(\n            ,\n            [\n                ImportFrom(\n                    lineno=1,\n                    column=1,\n                    name=\"z\",\n                    package=\"x\",\n                    star=False,\n                    suggestions=[],\n                ),\n                Import(\n                    lineno=2,\n                    column=1,\n                    name=\"x\",\n                    package=\"x\",\n                ),\n                ImportFrom(\n                    lineno=3,\n                    column=1,\n                    name=\"ss\",\n                    package=\"t\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=1,\n                    name=\"c\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=2,\n                    name=\"k\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=3,\n                    name=\"ii\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=1,\n                    name=\"bar\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=2,\n                    name=\"i\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=3,\n                    name=\"z\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n                Import(\n                    lineno=6,\n                    column=1,\n                    name=\"x\",\n                    package=\"le\",\n                ),\n            ],\n        )\n\n    def test_as_import_one_used_in_function_all_cases(self):\n        self.assertSourceAfterScanningEqualToExpected(\n            ,\n            [\n                ImportFrom(\n                    lineno=1,\n                    column=1,\n                    name=\"z\",\n                    package=\"x\",\n                    star=False,\n                    suggestions=[],\n                ),\n                Import(\n                    lineno=2,\n                    column=1,\n                    name=\"x\",\n                    package=\"x\",\n                ),\n                ImportFrom(\n                    lineno=3,\n                    column=1,\n                    name=\"ss\",\n                    package=\"t\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=1,\n                    name=\"c\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=2,\n                    name=\"k\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=4,\n                    column=3,\n                    name=\"ii\",\n                    package=\"f\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=1,\n                    name=\"bar\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=2,\n                    name=\"i\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n                ImportFrom(\n                    lineno=5,\n                    column=3,\n                    name=\"z\",\n                    package=\"fo\",\n                    star=False,\n                    suggestions=[],\n                ),\n            ],\n        )\n",
        "summary": "The provided Python code defines a test case class `AsImportTestCase` that inherits from `UnusedTestCase`. It includes two methods to test the behavior of unused imports when they are used within functions. The tests assert that after scanning, only the necessary imports remain, with all others being removed as unused."
    },
    {
        "code": "import os\nfrom .takeout_sqlite3 import SQLite3\nimport multiprocessing\n\nCONTACTS = 'Contacts' + os.sep + 'All Contacts' + os.sep + 'All Contacts.vcf'\nDRIVE = 'Drive'\n\nMY_ACTIVITY_ASSISTANT_PATH = 'My Activity' + os.sep + 'Assistant' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_GMAIL_PATH = 'My Activity' + os.sep + 'Gmail' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_GOOGLE_ANALYTICS_PATH = 'My Activity' + os.sep + 'Google Analytics' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_YOUTUBE_PATH = 'My Activity' + os.sep + 'YouTube' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_VIDEO_SEARCH_PATH = 'My Activity' + os.sep + 'Video Search' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_VOICE_AUDIO_PATH = 'My Activity' + os.sep + 'Voice and Audio' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_MAPS_PATH = 'My Activity' + os.sep + 'Maps' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_ANDROID_PATH = 'My Activity' + os.sep + 'Android' + os.sep + 'MyActivity.html'\nMY_ACTIVITY_CHROME_PATH = 'My Activity' + os.sep + 'Chrome' + os.sep + 'MyActivity.html'\n\n\nclass Case(object):\n\tdef __init__(self, input_dir):\n\t\tself.number_of_system_processes = 1\n\t\tself.number_of_input_processes = 1\n\t\tself.input_dir_path = input_dir\n\n\t\tself.set_file_path()\n\n\tdef set_file_path(self):\n\t\tif self.input_dir_path[-1] == os.sep:\n\t\t\tself.input_dir_path = self.input_dir_path[:-1]\n\n\t\tself.takeout_path = self.input_dir_path + os.sep + 'Takeout'\n\t\tif not os.path.exists(self.takeout_path):\n\t\t\treturn False\n\n\t\tself.takeout_contacts_path = self.takeout_path + os.sep + CONTACTS\n\t\tself.takeout_drive_path = self.takeout_path + os.sep + DRIVE\n\t\tself.takeout_my_activity_assistant_path = self.takeout_path + os.sep + MY_ACTIVITY_ASSISTANT_PATH\n\t\tself.takeout_my_activity_gmail_path = self.takeout_path + os.sep + MY_ACTIVITY_GMAIL_PATH\n\t\tself.takeout_my_activity_google_analytics_path = self.takeout_path + os.sep + MY_ACTIVITY_GOOGLE_ANALYTICS_PATH\n\t\tself.takeout_my_activity_youtube_path = self.takeout_path + os.sep + MY_ACTIVITY_YOUTUBE_PATH\n\t\tself.takeout_my_activity_video_search_path = self.takeout_path + os.sep + MY_ACTIVITY_VIDEO_SEARCH_PATH\n\t\tself.takeout_my_activity_voice_audio_path = self.takeout_path + os.sep + MY_ACTIVITY_VOICE_AUDIO_PATH\n\t\tself.takeout_my_activity_maps_path = self.takeout_path + os.sep + MY_ACTIVITY_MAPS_PATH\n\t\tself.takeout_my_activity_android_path = self.takeout_path + os.sep + MY_ACTIVITY_ANDROID_PATH\n\t\tself.takeout_my_activity_chrome_path = self.takeout_path + os.sep + MY_ACTIVITY_CHROME_PATH\n",
        "summary": "The provided Python code defines a `Case` class that initializes with an input directory path and sets up file paths for various subdirectories within the 'Takeout' folder, including contacts, Google Drive, and different types of activity logs such as Assistant, Gmail, YouTube, and more. It also includes constants for specific file paths related to these activities."
    },
    {
        "code": "from __future__ import print_function\n\nimport os\nimport pickle\nimport time\n\nfrom gym_puyopuyo import register\nimport gym\nimport numpy as np\n\nimport neat\nimport visualize\n\npiece_shape = (3, 2)\nDRAW_NETS = False\nNUM_COLORS = 3.0 \n\nfn_results = \"feedforward-small\"\n\ndef multiplyMatrices(pieces, field, norm = True):\n    pieces = pieces.astype(np.float64)\n    field = field.astype(np.float64)\n    pieces_sum = np.zeros(piece_shape)\n    field_sum = np.zeros(field[0].shape)\n    for i in range(0, len(pieces)):\n        pieces[i] = np.multiply(pieces[i], i + 1)\n        if(norm):\n            pieces[i] /= NUM_COLORS\n        pieces_sum += pieces[i]\n    for i in range(0, len(field)):\n        field[i] = np.multiply(field[i], i + 1)\n        if(norm):\n            field[i] /= NUM_COLORS\n        field_sum += field[i]\n    \n    return pieces_sum, field_sum\n\ndef run():\n    with open(\"results/winner-pickle-\"+fn_results, 'rb') as f:\n        c = pickle.load(f)\n        \n    print('loaded genome:')\n    print(c)\n\n    local_dir = os.path.dirname(__file__)\n    config_path = os.path.join(local_dir, 'config-feedforward-small')\n    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n                        neat.DefaultSpeciesSet, neat.DefaultStagnation,\n                        config_path)\n\n    net = neat.nn.FeedForwardNetwork.create(c, config)\n    register()\n    env = gym.make(\"PuyoPuyoEndlessSmall-v2\")\n    done = False\n    ob = env.reset()\n    count = 0\n    total_reward = 0\n\n    while True:\n        env.render()\n        \n        time.sleep(0.5)\n        pieces_sum, field_sum = multiplyMatrices(ob[0], ob[1])\n        next_piece = pieces_sum[0]\n            \n        inp_piece = np.ndarray.flatten(next_piece)\n        inp_field = np.ndarray.flatten(field_sum)\n        inputs = np.hstack([inp_piece, inp_field])\n        \n        nn_output = net.activate(inputs)\n        action = np.argmax(nn_output)\n        \n        \n        \n        \n        \n        ob, rew, done, info = env.step(action)\n        \n        total_reward += rew\n        count += 1\n        \n        if done:\n            break\n\n    print(\"Game played for \", count, \" turns.\")\n    print(\"Total score: \", total_reward)\n\n    if DRAW_NETS:\n        visualize.draw_net(config, c, view=True, \n                        filename=\"results/winner-\"+fn_results+\".net\")\n        \n        visualize.draw_net(config, c, view=True, \n                        filename=\"results/winner-\"+fn_results+\"-enabled.net\",\n                        show_disabled=False)\n        \n        visualize.draw_net(config, c, view=True, \n                        filename=\"results/winner-\"+fn_results+\"-pruned.net\",\n                        show_disabled=False, prune_unused=True)\n\nif __name__ == '__main__':\n    run()\n",
        "summary": "The Python script loads a pre-trained neural network from a pickle file and uses it to play the PuyoPuyoEndlessSmall-v2 game environment. It processes the game state by multiplying matrices representing pieces and fields, normalizing them, and feeding the resulting inputs into the neural network to determine actions. The script also includes functionality to visualize the neural network's architecture."
    },
    {
        "code": "import uuid\n\nfrom app import db\nfrom app.dao.dao_utils import transactional\nfrom app.models import (\n    BroadcastMessage,\n    BroadcastEvent,\n    BroadcastProvider,\n    BroadcastProviderMessage,\n    BroadcastProviderMessageNumber,\n    BroadcastProviderMessageStatus\n)\n\n\ndef dao_get_broadcast_message_by_id_and_service_id(broadcast_message_id, service_id):\n    return BroadcastMessage.query.filter(\n        BroadcastMessage.id == broadcast_message_id,\n        BroadcastMessage.service_id == service_id\n    ).one()\n\n\ndef dao_get_broadcast_event_by_id(broadcast_event_id):\n    return BroadcastEvent.query.filter(BroadcastEvent.id == broadcast_event_id).one()\n\n\ndef dao_get_broadcast_messages_for_service(service_id):\n    return BroadcastMessage.query.filter(\n        BroadcastMessage.service_id == service_id\n    ).order_by(BroadcastMessage.created_at)\n\n\ndef get_earlier_events_for_broadcast_event(broadcast_event_id):\n    \n    this_event = BroadcastEvent.query.get(broadcast_event_id)\n\n    return BroadcastEvent.query.filter(\n        BroadcastEvent.broadcast_message_id == this_event.broadcast_message_id,\n        BroadcastEvent.sent_at < this_event.sent_at\n    ).order_by(\n        BroadcastEvent.sent_at.asc()\n    ).all()\n\n\n@transactional\ndef create_broadcast_provider_message(broadcast_event, provider):\n    broadcast_provider_message_id = uuid.uuid4()\n    provider_message = BroadcastProviderMessage(\n        id=broadcast_provider_message_id,\n        broadcast_event=broadcast_event,\n        provider=provider,\n        status=BroadcastProviderMessageStatus.SENDING,\n    )\n    db.session.add(provider_message)\n    db.session.commit()\n    provider_message_number = None\n    if provider == BroadcastProvider.VODAFONE:\n        provider_message_number = BroadcastProviderMessageNumber(\n            broadcast_provider_message_id=broadcast_provider_message_id)\n        db.session.add(provider_message_number)\n        db.session.commit()\n    return provider_message\n",
        "summary": "The provided Python code defines several functions for interacting with a database using SQLAlchemy ORM, specifically for managing broadcast messages and events. It includes methods to retrieve broadcast messages by ID and service ID, fetch all broadcast messages for a given service, get earlier events related to a specific broadcast event, and create a new broadcast provider message with optional handling for Vodafone-specific details."
    },
    {
        "code": "import os as _os\nfrom open3d import _build_config\n\nif _build_config['BUNDLE_OPEN3D_ML']:\n    if 'OPEN3D_ML_ROOT' in _os.environ:\n        from ml3d.torch.pipelines import *\n    else:\n        from open3d._ml3d.torch.pipelines import *\n",
        "summary": "The Python code checks if Open3D ML is bundled and then imports the appropriate pipelines module based on whether the environment variable `OPEN3D_ML_ROOT` is set. If the environment variable is not set, it defaults to importing from a subdirectory within Open3D's `_ml3d` package."
    },
    {
        "code": "import os\n\n\ndef to_bool(value):\n    return (\n        value is True or\n        (isinstance(value, str) and value.lower() in ['true', 'yes']) or\n        (isinstance(value, (int, float)) and value > 0)\n    )\n\n\nbind = '0.0.0.0:{}'.format(os.getenv('GUNICORN_PORT', '8000'))\nmax_requests = int(os.getenv('GUNICORN_MAX_REQUESTS', '10000'))\nmax_requests_jitter = int(os.getenv('GUNICORN_MAX_REQUESTS_JITTER', '100'))\nuser = os.getenv('GUNICORN_USER', 'root')\nkeepalive = int(os.getenv('GUNICORN_KEEPALIVE', '70'))\n\nreuse_port = to_bool(os.getenv('GUNICORN_REUSE_PORT', True))\n\naccesslog = '-'\nerrorlog = '-'\nprint_config = True\n\nworkers = int(os.getenv('GUNICORN_WORKERS', '5'))\nthreads = int(os.getenv('GUNICORN_THREADS', '5'))\n",
        "summary": "The Python script sets up configuration parameters for a Gunicorn server, including binding address, maximum requests, user, keepalive time, and worker settings, using environment variables with default values provided. It also includes a function to convert various input types into boolean values."
    },
    {
        "code": "import pytest\n\nfrom skidl import *\n\nfrom .setup_teardown import *\n\n\ndef test_pin_names_1():\n    codec = Part(\"xess.lib\", \"ak4520a\")\n    assert codec[\"ain\"] == codec.n[\"ain\"]\n    assert codec[1:4] == codec.p[1:4]\n\n\ndef test_pin_names_2():\n    codec = Part(\"xess.lib\", \"ak4520a\")\n    codec[4].name = \"A1\"\n    codec[8].name = \"A2\"\n    codec[8].num = \"A1\"\n    assert codec[4] is codec.n[\"A1\"]\n    assert codec.p[4] is codec.n[\"A1\"]\n    assert codec[4] is codec.p[4]\n    assert codec.p[\"A1\"] is codec.n[\"A2\"]\n    assert codec[\"A1\"] is codec.n[\"A2\"]\n    assert codec[\"A1\"] is codec.p[\"A1\"]\n",
        "summary": "The provided Python code uses the `pytest` framework and the `skidl` library to test pin naming conventions for a specific component, the AK4520A codec from the XESS library. The tests verify that pin names can be accessed using both numerical indices and symbolic names, ensuring flexibility in how pins are referenced within the design."
    },
    {
        "code": "from PIL import ImageChops, Image as PILImage\nfrom http.client import HTTPConnection\nfrom time import sleep\nfrom traceback import format_stack, print_exc\n\n\ndef Tint(image, color):\n    return ImageChops.blend(image, PILImage.new('RGB', image.size, color), 0.36)\n\ndef GetStatusCode(host, path=\"/\"):\n    \n    try:\n        conn = HTTPConnection(host)\n        conn.request(\"HEAD\", path)\n        return conn.getresponse().status\n    except Exception:\n        return None\n    \ndef WaitOK(host, path=\"/\"):\n    while GetStatusCode(host, path) != 200:\n        sleep(5)\n        ",
        "summary": "The provided Python code includes functions for tinting an image with a specified color using the `ImageChops` and `Image` modules from the PIL library, and for checking the HTTP status code of a given host and path using the `HTTPConnection` class. It also features a function to repeatedly wait until the specified host and path return a 200 OK status code, pausing for five seconds between checks if not yet successful."
    },
    {
        "code": "from abc import (\n    ABCMeta,\n    abstractmethod,\n    abstractproperty,\n)\n\nfrom numpy import concatenate\nfrom lru import LRU\nfrom pandas import isnull\nfrom pandas.tslib import normalize_date\nfrom toolz import sliding_window\n\nfrom six import with_metaclass\n\nfrom zipline.assets import Equity, Future\nfrom zipline.assets.continuous_futures import ContinuousFuture\nfrom zipline.lib._int64window import AdjustedArrayWindow as Int64Window\nfrom zipline.lib._float64window import AdjustedArrayWindow as Float64Window\nfrom zipline.lib.adjustment import Float64Multiply, Float64Add\nfrom zipline.utils.cache import ExpiringCache\nfrom zipline.utils.math_utils import number_of_decimal_places\nfrom zipline.utils.memoize import lazyval\nfrom zipline.utils.numpy_utils import float64_dtype\nfrom zipline.utils.pandas_utils import find_in_sorted_index\n\n\nDEFAULT_ASSET_PRICE_DECIMALS = 3\n\n\nclass HistoryCompatibleUSEquityAdjustmentReader(object):\n\n    def __init__(self, adjustment_reader):\n        self._adjustments_reader = adjustment_reader\n\n    def load_adjustments(self, columns, dts, assets):\n        \n        out = [None] * len(columns)\n        for i, column in enumerate(columns):\n            adjs = {}\n            for asset in assets:\n                adjs.update(self._get_adjustments_in_range(\n                    asset, dts, column))\n            out[i] = adjs\n        return out\n\n    def _get_adjustments_in_range(self, asset, dts, field):\n        \n        sid = int(asset)\n        start = normalize_date(dts[0])\n        end = normalize_date(dts[-1])\n        adjs = {}\n        if field != 'volume':\n            mergers = self._adjustments_reader.get_adjustments_for_sid(\n                'mergers', sid)\n            for m in mergers:\n                dt = m[0]\n                if start < dt <= end:\n                    end_loc = dts.searchsorted(dt)\n                    adj_loc = end_loc\n                    mult = Float64Multiply(0,\n                                           end_loc - 1,\n                                           0,\n                                           0,\n                                           m[1])\n                    try:\n                        adjs[adj_loc].append(mult)\n                    except KeyError:\n                        adjs[adj_loc] = [mult]\n            divs = self._adjustments_reader.get_adjustments_for_sid(\n                'dividends', sid)\n            for d in divs:\n                dt = d[0]\n                if start < dt <= end:\n                    end_loc = dts.searchsorted(dt)\n                    adj_loc = end_loc\n                    mult = Float64Multiply(0,\n                                           end_loc - 1,\n                                           0,\n                                           0,\n                                           d[1])\n                    try:\n                        adjs[adj_loc].append(mult)\n                    except KeyError:\n                        adjs[adj_loc] = [mult]\n        splits = self._adjustments_reader.get_adjustments_for_sid(\n            'splits', sid)\n        for s in splits:\n            dt = s[0]\n            if start < dt <= end:\n                if field == 'volume':\n                    ratio = 1.0 / s[1]\n                else:\n                    ratio = s[1]\n                end_loc = dts.searchsorted(dt)\n                adj_loc = end_loc\n                mult = Float64Multiply(0,\n                                       end_loc - 1,\n                                       0,\n                                       0,\n                                       ratio)\n                try:\n                    adjs[adj_loc].append(mult)\n                except KeyError:\n                    adjs[adj_loc] = [mult]\n        return adjs\n\n\nclass ContinuousFutureAdjustmentReader(object):\n    \n\n    def __init__(self,\n                 trading_calendar,\n                 asset_finder,\n                 bar_reader,\n                 roll_finders,\n                 frequency):\n        self._trading_calendar = trading_calendar\n        self._asset_finder = asset_finder\n        self._bar_reader = bar_reader\n        self._roll_finders = roll_finders\n        self._frequency = frequency\n\n    def load_adjustments(self, columns, dts, assets):\n        \n        out = [None] * len(columns)\n        for i, column in enumerate(columns):\n            adjs = {}\n            for asset in assets:\n                adjs.update(self._get_adjustments_in_range(\n                    asset, dts, column))\n            out[i] = adjs\n        return out\n\n    def _make_adjustment(self,\n                         adjustment_type,\n                         front_close,\n                         back_close,\n                         end_loc):\n        adj_base = back_close - front_close\n        if adjustment_type == 'mul':\n            adj_value = 1.0 + adj_base / front_close\n            adj_class = Float64Multiply\n        elif adjustment_type == 'add':\n            adj_value = adj_base\n            adj_class = Float64Add\n        return adj_class(0,\n                         end_loc,\n                         0,\n                         0,\n                         adj_value)\n\n    def _get_adjustments_in_range(self, cf, dts, field):\n        if field == 'volume' or field == 'sid':\n            return {}\n        if cf.adjustment is None:\n            return {}\n        rf = self._roll_finders[cf.roll_style]\n        partitions = []\n\n        rolls = rf.get_rolls(cf.root_symbol, dts[0], dts[-1],\n                             cf.offset)\n\n        tc = self._trading_calendar\n\n        adjs = {}\n\n        for front, back in sliding_window(2, rolls):\n            front_sid, roll_dt = front\n            back_sid = back[0]\n            dt = tc.previous_session_label(roll_dt)\n            if self._frequency == 'minute':\n                dt = tc.open_and_close_for_session(dt)[1]\n                roll_dt = tc.open_and_close_for_session(roll_dt)[0]\n            partitions.append((front_sid,\n                               back_sid,\n                               dt,\n                               roll_dt))\n        for partition in partitions:\n            front_sid, back_sid, dt, roll_dt = partition\n            last_front_dt = self._bar_reader.get_last_traded_dt(\n                self._asset_finder.retrieve_asset(front_sid), dt)\n            last_back_dt = self._bar_reader.get_last_traded_dt(\n                self._asset_finder.retrieve_asset(back_sid), dt)\n            if isnull(last_front_dt) or isnull(last_back_dt):\n                continue\n            front_close = self._bar_reader.get_value(\n                front_sid, last_front_dt, 'close')\n            back_close = self._bar_reader.get_value(\n                back_sid, last_back_dt, 'close')\n            adj_loc = dts.searchsorted(roll_dt)\n            end_loc = adj_loc - 1\n            adj = self._make_adjustment(cf.adjustment,\n                                        front_close,\n                                        back_close,\n                                        end_loc)\n            try:\n                adjs[adj_loc].append(adj)\n            except KeyError:\n                adjs[adj_loc] = [adj]\n        return adjs\n\n\nclass SlidingWindow(object):\n    \n\n    def __init__(self, window, size, cal_start, offset):\n        self.window = window\n        self.cal_start = cal_start\n        self.current = next(window)\n        self.offset = offset\n        self.most_recent_ix = self.cal_start + size\n\n    def get(self, end_ix):\n        \n        if self.most_recent_ix == end_ix:\n            return self.current\n\n        target = end_ix - self.cal_start - self.offset + 1\n        self.current = self.window.seek(target)\n\n        self.most_recent_ix = end_ix\n        return self.current\n\n\nclass HistoryLoader(with_metaclass(ABCMeta)):\n    \n    FIELDS = ('open', 'high', 'low', 'close', 'volume', 'sid')\n\n    def __init__(self, trading_calendar, reader, equity_adjustment_reader,\n                 asset_finder,\n                 roll_finders=None,\n                 sid_cache_size=1000,\n                 prefetch_length=0):\n        self.trading_calendar = trading_calendar\n        self._asset_finder = asset_finder\n        self._reader = reader\n        self._adjustment_readers = {}\n        if equity_adjustment_reader is not None:\n            self._adjustment_readers[Equity] = \\\n                HistoryCompatibleUSEquityAdjustmentReader(\n                    equity_adjustment_reader)\n        if roll_finders:\n            self._adjustment_readers[ContinuousFuture] =\\\n                ContinuousFutureAdjustmentReader(trading_calendar,\n                                                 asset_finder,\n                                                 reader,\n                                                 roll_finders,\n                                                 self._frequency)\n        self._window_blocks = {\n            field: ExpiringCache(LRU(sid_cache_size))\n            for field in self.FIELDS\n        }\n        self._prefetch_length = prefetch_length\n\n    @abstractproperty\n    def _frequency(self):\n        pass\n\n    @abstractproperty\n    def _calendar(self):\n        pass\n\n    @abstractmethod\n    def _array(self, start, end, assets, field):\n        pass\n\n    def _decimal_places_for_asset(self, asset, reference_date):\n        if isinstance(asset, Future) and asset.tick_size:\n            return number_of_decimal_places(asset.tick_size)\n        elif isinstance(asset, ContinuousFuture):\n            \n            \n            \n            oc = self._asset_finder.get_ordered_contracts(asset.root_symbol)\n            contract_sid = oc.contract_before_auto_close(reference_date.value)\n            if contract_sid is not None:\n                contract = self._asset_finder.retrieve_asset(contract_sid)\n                if contract.tick_size:\n                    return number_of_decimal_places(contract.tick_size)\n        return DEFAULT_ASSET_PRICE_DECIMALS\n\n    def _ensure_sliding_windows(self, assets, dts, field,\n                                is_perspective_after):\n        \n        end = dts[-1]\n        size = len(dts)\n        asset_windows = {}\n        needed_assets = []\n        cal = self._calendar\n\n        assets = self._asset_finder.retrieve_all(assets)\n        end_ix = find_in_sorted_index(cal, end)\n\n        for asset in assets:\n            try:\n                window = self._window_blocks[field].get(\n                    (asset, size, is_perspective_after), end)\n            except KeyError:\n                needed_assets.append(asset)\n            else:\n                if end_ix < window.most_recent_ix:\n                    \n                    \n                    \n                    needed_assets.append(asset)\n                else:\n                    asset_windows[asset] = window\n\n        if needed_assets:\n            offset = 0\n            start_ix = find_in_sorted_index(cal, dts[0])\n\n            prefetch_end_ix = min(end_ix + self._prefetch_length, len(cal) - 1)\n            prefetch_end = cal[prefetch_end_ix]\n            prefetch_dts = cal[start_ix:prefetch_end_ix + 1]\n            if is_perspective_after:\n                adj_end_ix = min(prefetch_end_ix + 1, len(cal) - 1)\n                adj_dts = cal[start_ix:adj_end_ix + 1]\n            else:\n                adj_dts = prefetch_dts\n            prefetch_len = len(prefetch_dts)\n            array = self._array(prefetch_dts, needed_assets, field)\n\n            if field == 'sid':\n                window_type = Int64Window\n            else:\n                window_type = Float64Window\n\n            view_kwargs = {}\n            if field == 'volume':\n                array = array.astype(float64_dtype)\n\n            for i, asset in enumerate(needed_assets):\n                adj_reader = None\n                try:\n                    adj_reader = self._adjustment_readers[type(asset)]\n                except KeyError:\n                    adj_reader = None\n                if adj_reader is not None:\n                    adjs = adj_reader.load_adjustments(\n                        [field], adj_dts, [asset])[0]\n                else:\n                    adjs = {}\n                window = window_type(\n                    array[:, i].reshape(prefetch_len, 1),\n                    view_kwargs,\n                    adjs,\n                    offset,\n                    size,\n                    int(is_perspective_after),\n                    self._decimal_places_for_asset(asset, dts[-1]),\n                )\n                sliding_window = SlidingWindow(window, size, start_ix, offset)\n                asset_windows[asset] = sliding_window\n                self._window_blocks[field].set(\n                    (asset, size, is_perspective_after),\n                    sliding_window,\n                    prefetch_end)\n\n        return [asset_windows[asset] for asset in assets]\n\n    def history(self, assets, dts, field, is_perspective_after):\n        \n        block = self._ensure_sliding_windows(assets,\n                                             dts,\n                                             field,\n                                             is_perspective_after)\n        end_ix = self._calendar.searchsorted(dts[-1])\n\n        return concatenate(\n            [window.get(end_ix) for window in block],\n            axis=1,\n        )\n\n\nclass DailyHistoryLoader(HistoryLoader):\n\n    @property\n    def _frequency(self):\n        return 'daily'\n\n    @property\n    def _calendar(self):\n        return self._reader.sessions\n\n    def _array(self, dts, assets, field):\n        return self._reader.load_raw_arrays(\n            [field],\n            dts[0],\n            dts[-1],\n            assets,\n        )[0]\n\n\nclass MinuteHistoryLoader(HistoryLoader):\n\n    @property\n    def _frequency(self):\n        return 'minute'\n\n    @lazyval\n    def _calendar(self):\n        mm = self.trading_calendar.all_minutes\n        start = mm.searchsorted(self._reader.first_trading_day)\n        end = mm.searchsorted(self._reader.last_available_dt, side='right')\n        return mm[start:end]\n\n    def _array(self, dts, assets, field):\n        return self._reader.load_raw_arrays(\n            [field],\n            dts[0],\n            dts[-1],\n            assets,\n        )[0]\n",
        "summary": "This code defines a class hierarchy for loading historical financial data at different time frequencies (daily and minute). The base class `HistoryLoader` provides common functionality, while the subclasses `DailyHistoryLoader` and `MinuteHistoryLoader` handle specific details for daily and minute data.\n\nHere's a breakdown of key components:\n\n1. **Base Class (`HistoryLoader`)**:\n   - Defines abstract properties `_frequency` and `_calendar`.\n   - Implements method `_array` to load raw arrays from the reader.\n   - Provides protected methods `_ensure_sliding_windows` and `history` for managing sliding windows and retrieving history data.\n\n2. **Subclass (`DailyHistoryLoader`)**:\n   - Overrides `_frequency` to return 'daily'.\n   - Overrides `_calendar` to get all trading days within the reader's session range.\n   - Implements `_array` to load daily arrays from the reader.\n\n3. **Subclass (`MinuteHistoryLoader`)**:\n   - Overrides `_frequency` to return 'minute'.\n   - Lazily initializes `_calendar` with all minutes between the first trading day and last available date in the reader.\n   - Implements `_array` to load minute arrays from the reader.\n\n### Key Features:\n\n- **Sliding Window Management**: The `_ensure_sliding_windows` method ensures that sliding windows are created for each asset, caching them for future use. This allows efficient retrieval of historical data without redundant computations.\n\n- **Dynamic Array Loading**: The `_array` method is responsible for loading raw arrays from the reader based on the specified time range and assets. This method is abstract in the base class and must be implemented by subclasses to handle different data formats.\n\n- **Adjustments for Different Frequencies**: The `history` method retrieves historical data for a given set of assets, dates, and fields. It dynamically ensures that sliding windows are created as needed and concatenates the results into a single array.\n\n### Usage Example:\n\nTo use these classes, you would typically create an instance of either `DailyHistoryLoader` or `MinuteHistoryLoader`, passing in the necessary parameters such as the reader object, trading calendar, etc. Then, you can call the `history` method to retrieve historical data for specific assets and time periods.\n\n```python\n# Example usage with DailyHistoryLoader\nreader = SomeDataReader()\ncalendar = TradingCalendar()\ndaily_loader = DailyHistoryLoader(reader, calendar)\n\nassets = [asset1, asset2]\ndates = pd.date_range(start='2023-01-01', end='2023-01-31')\nfield = 'close'\n\nhistory_data = daily_loader.history(assets, dates, field, is_perspective_after=True)\nprint(history_data)\n```\n\nThis example demonstrates how to load daily closing prices for a list of assets over a specified date range."
    },
    {
        "code": "import requests\n\nAPI_URL = 'https://secure.techfortesco.com/tescolabsapi/restservice.aspx'\n\n\nclass TescoLabsApi(object):\n    def __init__(self, url, developerkey, applicationkey):\n        self.url = url\n        self.developerkey = developerkey\n        self.applicationkey = applicationkey\n        res = requests.get(self.url,\n                           params={'command': 'login',\n                                   'email': '', 'password': '',\n                                   'developerkey': self.developerkey,\n                                   'applicationkey': self.applicationkey,\n                                   })\n        self.sessionkey = res.json()['SessionKey']\n\n    def _command(self, command, **kwargs):\n        params = kwargs\n        params.update({'command': command, 'sessionkey': self.sessionkey})\n        res = requests.get(self.url, params=params)\n        return res\n\n    def listproductcategories(self):\n        return self._command('listproductcategories')\n\n    def listproductsincategory(self, category):\n        return self._command('listproductsincategory', category=category)\n\n    def listproductoffers(self):\n        return self._command('listproductoffers')\n\n    def productsearch(self, searchtext, page=1, extendedinfo=False):\n        return self._command('productsearch', searchtext=searchtext,\n                             page=page, extendedinfo=extendedinfo)\n\n",
        "summary": "The Python code defines a class `TescoLabsApi` that interacts with the Tesco Labs API to perform various operations such as logging in, listing product categories, products within a category, product offers, and searching for products. It uses the `requests` library to make HTTP GET requests to the API endpoint, handling authentication and passing parameters to retrieve data."
    },
    {
        "code": "from django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('catalog', '0002_tag'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='item',\n            name='tags',\n            field=models.ManyToManyField(related_name='items', to='catalog.Tag', verbose_name='\u0422\u0435\u0433\u0438'),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that adds a ManyToManyField named 'tags' to the 'Item' model, linking it to the 'Tag' model from the 'catalog' app. The field is configured with a related name of 'items', verbose name in Russian as '\u0422\u0435\u0433\u0438', and references the 'Tag' model."
    },
    {
        "code": "import os\nimport subprocess\nimport threading\nfrom pwd import getpwnam\nfrom tempfile import NamedTemporaryFile\nfrom typing import Optional, Union\n\nfrom airflow.configuration import conf\nfrom airflow.exceptions import AirflowConfigException\nfrom airflow.models.taskinstance import load_error_file\nfrom airflow.utils.configuration import tmp_configuration_copy\nfrom airflow.utils.log.logging_mixin import LoggingMixin\nfrom airflow.utils.net import get_hostname\nfrom airflow.utils.platform import getuser\n\nPYTHONPATH_VAR = 'PYTHONPATH'\n\n\nclass BaseTaskRunner(LoggingMixin):\n    \n\n    def __init__(self, local_task_job):\n        \n        super().__init__(local_task_job.task_instance)\n        self._task_instance = local_task_job.task_instance\n\n        popen_prepend = []\n        if self._task_instance.run_as_user:\n            self.run_as_user = self._task_instance.run_as_user\n        else:\n            try:\n                self.run_as_user = conf.get('core', 'default_impersonation')\n            except AirflowConfigException:\n                self.run_as_user = None\n\n        \n        \n        self.log.debug(\"Planning to run as the %s user\", self.run_as_user)\n        if self.run_as_user and (self.run_as_user != getuser()):\n            \n            \n            \n            \n            cfg_path = tmp_configuration_copy(chmod=0o600)\n\n            \n            subprocess.call(['sudo', 'chown', self.run_as_user, cfg_path], close_fds=True)\n\n            \n            pythonpath_value = os.environ.get(PYTHONPATH_VAR, '')\n            popen_prepend = ['sudo', '-E', '-H', '-u', self.run_as_user]\n\n            if pythonpath_value:\n                popen_prepend.append(f'{PYTHONPATH_VAR}={pythonpath_value}')\n\n        else:\n            \n            \n            \n            \n            cfg_path = tmp_configuration_copy(chmod=0o600)\n\n        self._error_file = NamedTemporaryFile(delete=True)\n        if self.run_as_user:\n            try:\n                os.chown(self._error_file.name, getpwnam(self.run_as_user).pw_uid, -1)\n            except KeyError:\n                \n                pass\n\n        self._cfg_path = cfg_path\n        self._command = (\n            popen_prepend\n            + self._task_instance.command_as_list(\n                raw=True,\n                pickle_id=local_task_job.pickle_id,\n                mark_success=local_task_job.mark_success,\n                job_id=local_task_job.id,\n                pool=local_task_job.pool,\n                cfg_path=cfg_path,\n            )\n            + [\"--error-file\", self._error_file.name]\n        )\n        self.process = None\n\n    def deserialize_run_error(self) -> Optional[Union[str, Exception]]:\n        \n        return load_error_file(self._error_file)\n\n    def _read_task_logs(self, stream):\n        while True:\n            line = stream.readline()\n            if isinstance(line, bytes):\n                line = line.decode('utf-8')\n            if not line:\n                break\n            self.log.info(\n                'Job %s: Subtask %s %s',\n                self._task_instance.job_id,\n                self._task_instance.task_id,\n                line.rstrip('\\n'),\n            )\n\n    def run_command(self, run_with=None):\n        \n        run_with = run_with or []\n        full_cmd = run_with + self._command\n\n        self.log.info(\"Running on host: %s\", get_hostname())\n        self.log.info('Running: %s', full_cmd)\n\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid,\n        )\n\n        \n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc\n\n    def start(self):\n        \n        raise NotImplementedError()\n\n    def return_code(self) -> Optional[int]:\n        \n        raise NotImplementedError()\n\n    def terminate(self) -> None:\n        \n        raise NotImplementedError()\n\n    def on_finish(self) -> None:\n        \n        if self._cfg_path and os.path.isfile(self._cfg_path):\n            if self.run_as_user:\n                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n            else:\n                os.remove(self._cfg_path)\n        try:\n            self._error_file.close()\n        except FileNotFoundError:\n            \n            \n            pass\n",
        "summary": "The `BaseTaskRunner` class in the provided Python code is an abstract base class for task runners in Apache Airflow, designed to handle the execution of tasks as a specific user. It manages environment setup, command construction, and logging, ensuring that tasks run with the appropriate permissions and configurations. The class also includes methods for running commands, handling errors, and cleaning up resources after task completion."
    },
    {
        "code": "import unittest\n\n\n\n\ndef end_other(a, b):\n    a = a.lower()\n    b = b.lower()\n    return (b[(len(b) - len(a)):] == a, a[(len(a) - len(b)):] == b)[len(a) >= len(b)]\n\n\nclass TestEndOther(unittest.TestCase):\n    def test_case_00(self):\n        self.assertEqual(end_other('Hiabc', 'abc'), True)\n\n    def test_case_01(self):\n        self.assertEqual(end_other('AbC', 'HiaBc'), True)\n\n    def test_case_02(self):\n        self.assertEqual(end_other('abc', 'abXabc'), True)\n\n    def test_case_03(self):\n        self.assertEqual(end_other('Hiabc', 'abcd'), False)\n\n    def test_case_04(self):\n        self.assertEqual(end_other('Hiabc', 'bc'), True)\n\n    def test_case_05(self):\n        self.assertEqual(end_other('Hiabcx', 'bc'), False)\n\n    def test_case_06(self):\n        self.assertEqual(end_other('abc', 'abc'), True)\n\n    def test_case_07(self):\n        self.assertEqual(end_other('xyz', '12xyz'), True)\n\n    def test_case_08(self):\n        self.assertEqual(end_other('yz', '12xz'), False)\n\n    def test_case_09(self):\n        self.assertEqual(end_other('Z', '12xz'), True)\n\n    def test_case_10(self):\n        self.assertEqual(end_other('12', '12'), True)\n\n    def test_case_11(self):\n        self.assertEqual(end_other('abcXYZ', 'abcDEF'), False)\n\n    def test_case_12(self):\n        self.assertEqual(end_other('ab', 'ab12'), False)\n\n    def test_case_13(self):\n        self.assertEqual(end_other('ab', '12ab'), True)\n\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "summary": "The provided Python code defines a function `end_other` that checks if one string ends with another, ignoring case sensitivity. It includes a test class `TestEndOther` using the `unittest` framework to validate various scenarios for this functionality."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text format such as CSV or JSON."
    },
    {
        "code": "import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nclass Config:\n    SECRET_KEY = os.environ.get('SECRET_KEY')\n    SQLALCHEMY_DATABASE_URI = 'postgresql+psycopg2://fidel:fidel@localhost/blog'\n    UPLOADED_PHOTOS_DEST = 'app/static/photos'\n    QUOTES_URL = 'http://quotes.stormconsultancy.co.uk/random.json'\n    MAIL_SERVER = 'smtp.googlemail.com'\n    MAIL_PORT = 587 \n    MAIL_USE_TLS = True\n    MAIL_USERNAME = os.environ.get(\"MAIL_USERNAME\")\n    MAIL_PASSWORD = os.environ.get(\"MAIL_PASSWORD\")\n    \nclass ProdConfig(Config):\n    SQLALCHEMY_DATABASE_URI =os.environ.get('DATABASE_URL')\n    \n\nclass DevConfig(Config):\n    \n    DEBUG = True\n\n\nconfig_options = {\n'development':DevConfig,\n'production':ProdConfig\n}",
        "summary": "The provided Python code defines a configuration class `Config` with various settings for a web application, including database URIs, upload destinations, API URLs, and email server details. It also includes subclasses `ProdConfig` and `DevConfig` to handle production and development environments respectively, with environment-specific overrides for the database URI and debug mode. The configurations are stored in a dictionary `config_options` for easy access based on the current environment."
    },
    {
        "code": "_TF_INCLUDE_PATH = \"TF_INCLUDE_PATH\"\n_TF_LIB_PATH = \"TF_LIB_PATH\"\n\ndef _get_env_var_with_default(repository_ctx, env_var):\n  \n  if env_var in repository_ctx.os.environ:\n    value = repository_ctx.os.environ[env_var]\n    return value\n  else:\n    fail(\"Environment variable '%s' was not set.\" % env_var)\n\ndef _get_tf_conf(repository_ctx):\n  \n  include_path = _get_env_var_with_default(repository_ctx, _TF_INCLUDE_PATH)\n  lib_path = _get_env_var_with_default(repository_ctx, _TF_LIB_PATH)\n  return struct(\n    include_path = include_path,\n    lib_path = lib_path\n  )\n\ndef _tensorflow_autoconf_impl(repository_ctx):\n  \n  tf_conf = _get_tf_conf(repository_ctx)\n  print(\"Using %s=%s\" % (_TF_INCLUDE_PATH, tf_conf.include_path))\n  print(\"Using %s=%s\" % (_TF_LIB_PATH, tf_conf.lib_path))\n  repository_ctx.symlink(tf_conf.include_path, 'include')\n  repository_ctx.symlink(tf_conf.lib_path, 'lib')\n  repository_ctx.template('BUILD', Label(\"//third_party/tensorflow:tensorflow.BUILD\"))\n\n\n\ntensorflow_configure = repository_rule(\n  implementation = _tensorflow_autoconf_impl,\n  environ = [\n    _TF_INCLUDE_PATH,\n    _TF_LIB_PATH\n  ]\n)",
        "summary": "The provided Python code defines a Bazel repository rule named `tensorflow_configure` that retrieves the paths for TensorFlow include and library files from environment variables, prints these paths, creates symlinks to these directories within the repository, and copies a BUILD file template for TensorFlow."
    },
    {
        "code": "import yaml\nfrom os import path\nfrom netmiko import ConnectHandler\n\n\nhome_dir = path.expanduser(\"~\")\nfilename = path.join(home_dir, \".netmiko.yml\")\n\nwith open(filename) as f:\n    yaml_out = yaml.safe_load(f)\n\ncisco3 = yaml_out[\"cisco3\"]\nnet_connect = ConnectHandler(**cisco3)\n\nprint()\nprint(net_connect.find_prompt())\nprint()\n",
        "summary": "The Python script reads a YAML configuration file located in the user's home directory, extracts network device details for a Cisco 3 device, establishes an SSH connection using Netmiko, and prints the device prompt."
    },
    {
        "code": "from dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Type\n\nfrom svarog import forge\nfrom svarog import register_forge\nfrom svarog.types import Forge\n\nJSONMappingValue = Any\nJSONMapping = Mapping[str, JSONMappingValue]\nJSONSchema = JSONMapping\n\nGLOBAL_NAMESPACE = \"/\"\n\n\n@dataclass\nclass MessageAck:\n    \n\n    args: JSONSchema\n\n\n@dataclass\nclass Message:\n    \n\n    name: str\n    payload: Optional[JSONSchema] = None\n    x_handler: Optional[str] = None\n    x_ack: Optional[MessageAck] = None\n\n    @staticmethod\n    def forge(type_: Type[\"Message\"], data: JSONMapping, forge: Forge) -> \"Message\":\n        return type_(\n            name=forge(type_.__annotations__[\"name\"], data[\"name\"]),\n            payload=forge(type_.__annotations__[\"payload\"], data.get(\"payload\")),\n            x_handler=forge(type_.__annotations__[\"x_handler\"], data.get(\"x-handler\")),\n            x_ack=forge(type_.__annotations__[\"x_ack\"], data.get(\"x-ack\")),\n        )\n\n\nregister_forge(Message, Message.forge)\n\n\n@dataclass\nclass OneOfMessages:\n    \n\n    oneOf: Sequence[Message]\n\n    @staticmethod\n    def forge(\n        type_: Type[\"OneOfMessages\"], data: JSONMapping, forge: Forge\n    ) -> \"OneOfMessages\":\n        if \"oneOf\" in data:\n            return type_(\n                oneOf=forge(type_.__annotations__[\"oneOf\"], data[\"oneOf\"]),\n            )\n\n        return type_(oneOf=[forge(Message, data)])\n\n    def with_name(self, name: str) -> Optional[Message]:\n        for message in self.oneOf:\n            if message.name == name:\n                return message\n\n        return None\n\n\nregister_forge(OneOfMessages, OneOfMessages.forge)\n\n\n@dataclass\nclass Operation:\n    \n\n    message: OneOfMessages\n\n\n@dataclass\nclass WebSocketsChannelBindings:\n    \n\n    method: Optional[str] = None\n    query: Optional[JSONSchema] = None\n    headers: Optional[JSONSchema] = None  \n    bindingVersion: str = \"latest\"\n\n\n@dataclass\nclass ChannelBindings:\n    \n\n    ws: WebSocketsChannelBindings\n\n\n@dataclass\nclass ChannelHandlers:\n    connect: Optional[str] = None\n    disconnect: Optional[str] = None\n    error: Optional[str] = None\n\n\n@dataclass\nclass Channel:\n    \n\n    subscribe: Optional[Operation] = None\n    publish: Optional[Operation] = None\n    bindings: Optional[ChannelBindings] = None\n    x_handlers: Optional[ChannelHandlers] = None\n\n    def __post_init__(self):\n        if self.publish is not None:\n            for message in self.publish.message.oneOf:\n                if message.x_handler is None:\n                    raise ValueError(\n                        f\"Message {message.name} is missing the x-handler attribute.\\n\"\n                        \"Every message under a publish operation \"\n                        \"should have a handler defined.\"\n                    )\n\n    @staticmethod\n    def forge(type_: Type[\"Channel\"], data: JSONMapping, forge: Forge) -> \"Channel\":\n        return type_(\n            subscribe=forge(type_.__annotations__[\"subscribe\"], data.get(\"subscribe\")),\n            publish=forge(type_.__annotations__[\"publish\"], data.get(\"publish\")),\n            bindings=forge(type_.__annotations__[\"bindings\"], data.get(\"bindings\")),\n            x_handlers=forge(\n                type_.__annotations__[\"x_handlers\"], data.get(\"x-handlers\")\n            ),\n        )\n\n\nregister_forge(Channel, Channel.forge)\n\n\n@dataclass\nclass Server:\n    \n\n    url: str\n\n\n@dataclass\nclass AsyncApiSpec:\n    \n\n    channels: Mapping[str, Channel]\n    servers: Mapping[str, Server] = field(default_factory=dict)\n\n    @staticmethod\n    def from_dict(data: JSONMapping) -> \"AsyncApiSpec\":\n        return forge(AsyncApiSpec, data)\n\n\nErrorHandler = Callable[[Exception], None]\n",
        "summary": "The provided Python code defines a set of data classes and static methods for parsing and validating an AsyncAPI specification. These classes include `Message`, `OneOfMessages`, `Operation`, `WebSocketsChannelBindings`, `ChannelBindings`, `ChannelHandlers`, `Channel`, and `Server`. Each class uses the `dataclass` decorator from Python's standard library to define its structure, and includes a static method named `forge` for converting JSON data into instances of these classes. Additionally, the code registers these classes with a `Forge` object from the `svarog` library to enable custom parsing logic."
    },
    {
        "code": "import subprocess\n\ndef process_image(filename, scale=1.0):\n    output, _ = subprocess.Popen(['./Capture2Text_CLI', '-platform',\n                                  'offscreen', '-i', filename,\n                                  '--blacklist', '~|\\\\V', '--scale-factor', str(scale)],\n                                  stdout=subprocess.PIPE).communicate()\n    \n    print output\n    return output\n",
        "summary": "The Python function `process_image` uses the `subprocess` module to execute a command-line tool named `Capture2Text_CLI` with specific parameters for processing an image file, including scaling. It captures and returns the output of this command, which is then printed."
    },
    {
        "code": "from numpy import *\n\ndef iaframe(f, WT=1, HT=1, DT=0, k1=None, k2=None):\n    from ia870 import iaunion, iaintersec,ialimits\n\n    if k1 is None: k1 = ialimits(f)[1]\n    if k2 is None: k2 = ialimits(f)[0]\n    assert len(f.shape)==2,'Supports 2D only'\n    y = iaintersec(f,k2)\n    y[:,0:WT] = k1\n    y[:,-WT:] = k1\n    y[0:HT,:] = k1\n    y[-HT:,:] = k1\n    return y\n\n",
        "summary": "The `iaframe` function in Python processes a 2D array by creating a frame around its edges using specified limits and weights, effectively padding the array with these limits."
    },
    {
        "code": "from flask import Flask\nimport flask\napp = Flask(__name__)\n\n\n\n\napp.config[\"DEBUG\"] = True\napp.config[\"SQLALCHEMY_DB_URI\"] = \"mysql://\"\n\n\n\n@app.route(\"/path\")\ndef funcao():\n    pass\n\napp.add_url_rule(\"/path\", funcao)\n\n\n\n\n\n\n\napp.register_blueprint(...)\n\n\n\n@app.before_request(...)\n@app.errorhandler(...)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code sets up a basic Flask application with configurations for debugging and database URI, defines a route without any functionality, attempts to add the route using `add_url_rule`, registers a blueprint (though not shown), and includes placeholders for request handling and error management decorators."
    },
    {
        "code": "import jwt\nfrom contextlib import contextmanager\nfrom datetime import datetime, timedelta\n\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean\nfrom sqlalchemy import ForeignKey, func\nfrom sqlalchemy.orm import relationship\n\nfrom saraki.auth import _request_ctx_stack, User, Org\nfrom saraki.model import BaseModel, Model, database\n\n\nclass DummyBaseModel(BaseModel):\n    __tablename__ = \"dummy_base_model\"\n\n    id = Column(Integer, primary_key=True)\n\n\nclass DummyModel(Model):\n    __tablename__ = \"dummy_model\"\n\n    id = Column(Integer, primary_key=True)\n\n\nclass Person(Model):\n\n    __tablename__ = \"person\"\n\n    id = Column(Integer, primary_key=True)\n\n    firstname = Column(String, nullable=False)\n\n    lastname = Column(String, nullable=False)\n\n    age = Column(Integer, nullable=False)\n\n    def export_data(self, include=(\"id\", \"firstname\"), exclude=()):\n        return super(Person, self).export_data(include, exclude)\n\n\nclass Product(BaseModel):\n\n    __tablename__ = \"product\"\n\n    id = Column(Integer, primary_key=True)\n\n    name = Column(String(120), nullable=False)\n\n    color = Column(String, default=\"white\")\n\n    price = Column(Integer, default=0)\n\n    created_at = Column(DateTime, nullable=False, default=func.now())\n\n    updated_at = Column(DateTime, nullable=False, server_default=func.now())\n\n    enabled = Column(Boolean, default=False)\n\n\nclass Order(BaseModel):\n\n    __tablename__ = \"order\"\n\n    id = Column(Integer, primary_key=True)\n\n    customer_id = Column(Integer, ForeignKey(\"person.id\"), nullable=False)\n\n    lines = relationship(\"OrderLine\")\n\n    customer = relationship(\"Person\", uselist=False)\n\n\nclass OrderLine(Model):\n\n    __tablename__ = \"order_line\"\n\n    order_id = Column(Integer, ForeignKey(\"order.id\"), nullable=False, primary_key=True)\n\n    product_id = Column(\n        Integer, ForeignKey(\"product.id\"), nullable=False, primary_key=True\n    )\n\n    unit_price = Column(Integer, nullable=False)\n\n    quantity = Column(Integer, default=1, nullable=False)\n\n    product = relationship(\"Product\", uselist=False)\n\n    def export_data(self, include=(), exclude=()):\n        include = tuple(include) + (\"product_id\", \"unit_price\", \"quantity\")\n        return super(OrderLine, self).export_data(include, exclude)\n\n\nclass Cartoon(Model):\n\n    __tablename__ = \"cartoon\"\n\n    id = Column(Integer, primary_key=True)\n\n    name = Column(String(80), unique=True, nullable=False)\n\n    nickname = Column(String(80), unique=True)\n\n\nclass Todo(Model):\n\n    __tablename__ = \"todo\"\n\n    id = Column(Integer, primary_key=True)\n\n    org_id = Column(Integer, ForeignKey(\"org.id\"), nullable=False)\n\n    task = Column(String(200), nullable=False)\n\n\ndef login(username, orgname=None, scope=None):\n    iat = datetime.utcnow()\n    exp = iat + timedelta(seconds=6000)\n    payload = {\"iss\": \"acme.local\", \"sub\": username, \"iat\": iat, \"exp\": exp}\n\n    if orgname:\n        payload.update({\"aud\": orgname, \"scp\": {\"org\": [\"manage\"]}})\n\n    if scope:\n        payload.update({\"scp\": scope})\n\n    token = jwt.encode(payload, \"secret\").decode()\n\n    return f\"JWT {token}\"\n\n\n@contextmanager\ndef auth_ctx(username, orgname=None):\n\n    _request_ctx_stack.top.current_user = User(id=1, username=username)\n\n    if orgname:\n        _request_ctx_stack.top.current_org = Org(id=1, orgname=orgname)\n\n    yield\n\n\ndef reset_secuence(table, column_name=\"id\", schema_name=\"public\"):\n\n    table_name = f\"{schema_name}.{table.__tablename__}\"\n\n    sql = f\"SELECT pg_get_serial_sequence('{table_name}', '{column_name}');\"\n    secuence_name = database.engine.execute(sql).fetchone()[0]\n\n    if secuence_name is not None:\n        sql = f\"ALTER SEQUENCE {secuence_name} RESTART WITH 1;\"\n        database.engine.execute(sql)\n",
        "summary": "The provided Python code defines several SQLAlchemy models for a database schema, including `Person`, `Product`, `Order`, and `Cartoon`. It also includes utility functions such as `login` to generate JWT tokens and `auth_ctx` to manage authentication context. Additionally, there is a function `reset_sequence` to reset the sequence of an auto-incrementing column in a database table."
    },
    {
        "code": "def verbing(word):\n  if len(word) >= 3:\n    if word[-3:] == 'ing':\n     return word + 'ly'\n    else:\n     return word + 'ing'\n  else:\n      return word\n  return word\n\n\n\n\n\n\n\n\n\n\ndef not_bad(s):\n  neg_word = s.find('not')\n  if s.find('not') < s.find('bad'):\n    s = s[:neg_word] + 'good'\n  return s\n\n\n\n\n\n\n\n\n\ndef front_back(a, b):\n    lenght_first = len(a)\n    lenght_second = len(b)\n    if len(a) % 2 == 0:\n        a_first_half = a[:len(a) // 2]\n        a_second_half = a[-len(a_first_half):]\n    if len(a) % 2 != 0:\n        a_first_half = a[:len(a) // 2 + 1]\n        a_second_half = a[-(len(a_first_half) - 1):]\n    if len(b) % 2 == 0:\n        b_first_half = b[:len(b) // 2]\n        b_second_half = b[-len(b_first_half):]\n    if len(b) % 2 != 0:\n        b_first_half = b[:len(b) // 2 + 1]\n        b_second_half = b[-(len(b_first_half) - 1):]\n    return a_first_half + b_first_half + a_second_half + b_second_half\n\n\n\n\ndef test(got, expected):\n  if got == expected:\n    prefix = ' OK '\n  else:\n    prefix = '  X '\n  print '%s got: %s expected: %s' % (prefix, repr(got), repr(expected))\n\n\n\n\ndef main():\n  print 'verbing'\n  test(verbing('hail'), 'hailing')\n  test(verbing('swiming'), 'swimingly')\n  test(verbing('do'), 'do')\n\n  print\n  print 'not_bad'\n  test(not_bad('This movie is not so bad'), 'This movie is good')\n  test(not_bad('This dinner is not that bad!'), 'This dinner is good')\n  test(not_bad('This tea is not hot'), 'This tea is not hot')\n  test(not_bad(\"It's bad yet not\"), \"It's bad yet not\")\n\n  print\n  print 'front_back'\n  test(front_back('abcd', 'xy'), 'abxcdy')\n  test(front_back('abcde', 'xyz'), 'abcxydez')\n  test(front_back('Kitten', 'Donut'), 'KitDontenut')\n\nif __name__ == '__main__':\n  main()\n",
        "summary": "The provided Python code includes three functions: `verbing`, `not_bad`, and `front_back`. The `verbing` function appends \"ing\" or \"ingly\" to a word based on its length and ending. The `not_bad` function replaces the substring \"not ... bad\" with \"good\". The `front_back` function combines two strings by swapping their halves. The `test` function is used to verify the correctness of these functions, and the `main` function demonstrates their usage with various test cases."
    },
    {
        "code": "from multiprocessing import Pool\nimport argparse\nimport glob\nimport os\nimport io\nimport time\nimport logging\nimport gluonnlp as nlp\nimport tokenizer as tokenization\n\nparser = argparse.ArgumentParser(description='BERT tokenizer')\nparser.add_argument('--input_files', type=str, default='wiki_*.doc',\n                    help='Input files. Default is \"wiki_*.doc\"')\nparser.add_argument('--nworker', type=int, default=8,\n                    help='Number of workers for parallel processing.')\n\nargs = parser.parse_args()\nargs = parser.parse_args()\n\ninput_files = sorted(glob.glob(os.path.expanduser(args.input_files)))\nnum_files = len(input_files)\nnum_workers = args.nworker\nlogging.basicConfig(level=logging.INFO)\nlogging.info(\"Number of input files to process = %d\"%(num_files))\n\n\nexclude_patterns = [\n  '< no \n]\n\ndef in_pattern(x):\n    for pattern in exclude_patterns:\n        if len(x) == len(pattern) and x == pattern:\n            return True\n    return False\n\ndef f(input_file):\n    with io.open(input_file, 'r', encoding=\"utf-8\") as fin:\n        assert input_file.endswith('.tokens'), 'Expects .doc suffix for input files'\n        with io.open(input_file.replace('.tokens', '.tks'), 'w', encoding=\"utf-8\") as fout:\n            new_doc = True\n            with io.open(input_file, 'r', encoding=\"utf-8\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    if new_doc:\n                        new_doc = False\n                    elif len(line) == 1 and line[0] == '\\n':\n                        new_doc = True\n                        fout.write(u'\\n')\n                    elif in_pattern(line):\n                        pass\n                    else:\n                        fout.write(line)\n\nif __name__ == '__main__':\n    tic = time.time()\n    p = Pool(num_workers)\n    p.map(f, input_files)\n    toc = time.time()\n    logging.info(\"Processed %s in %.2f sec\"%(args.input_files, toc-tic))\n",
        "summary": "The Python script uses multiprocessing to parallelize the processing of multiple text files using a BERT tokenizer. It reads input files, filters out unwanted patterns, and writes the processed content to new files. The script logs the number of input files and the total processing time."
    },
    {
        "code": "import os\nimport time\nimport optparse\nfrom math import sqrt, ceil, floor\n\nfrom PIL import Image\n\nfrom helpers import get_basename\n\n\nclass Tile(object):\n    \n\n    def __init__(self, image, number, position, coords, filename=None):\n        self.image = image\n        self.number = number\n        self.position = position\n        self.coords = coords\n        self.filename = filename\n\n    @property\n    def row(self):\n        return self.position[0]\n\n    @property\n    def column(self):\n        return self.position[1]\n\n    @property\n    def basename(self):\n        \n        return get_basename(self.filename)\n\n    def generate_filename(self, directory=os.getcwd(), prefix='tile',\n                          format='png', path=True):\n        \n        filename = prefix + '_{col:02d}_{row:02d}.{ext}'.format(\n                      col=self.column, row=self.row, ext=format)\n        if not path:\n            return filename\n        return os.path.join(directory, filename)\n\n    def save(self, filename=None, format='png'):\n        if not filename:\n            filename = self.generate_filename(format=format)\n        self.image.save(filename, format)\n        self.filename = filename\n\n    def __repr__(self):\n        \n        if self.filename:\n            return '<Tile \n                                            os.path.basename(self.filename))\n        return '<Tile \n\n\ndef calc_columns_rows(n):\n    \n    num_columns = int(ceil(sqrt(n)))\n    num_rows = int(ceil(n / float(num_columns)))\n    return (num_columns, num_rows)\n    \n\ndef get_combined_size(tiles):\n    \n    \n    columns, rows = calc_columns_rows(len(tiles))\n    tile_size = tiles[0].image.size\n    return (tile_size[0] * columns, tile_size[1] * rows)\n\ndef join(tiles):\n    \n    im = Image.new('RGB', get_combined_size(tiles), None)\n    columns, rows = calc_columns_rows(len(tiles))\n    for tile in tiles:\n        im.paste(tile.image, tile.coords)\n    return im\n\ndef validate_image(image, number_tiles):\n    \n    TILE_LIMIT = 99 * 99\n\n    try:\n        number_tiles = int(number_tiles)\n    except:\n        raise ValueError('number_tiles could not be cast to integer.')\n\n    if number_tiles > TILE_LIMIT or number_tiles < 2:\n        raise ValueError('Number of tiles must be between 2 and {} (you \\\n                          asked for {}).'.format(TILE_LIMIT, number_tiles))\n\ndef save_tiles(tiles, prefix='', directory=os.getcwd(), format='png'):\n    \n\n\n\n    for tile in tiles:\n        tile.save(filename=tile.generate_filename(prefix=prefix,\n                                                  directory=directory,\n                                                  format=format))\n    return tuple(tiles)\n\n\ndef _do_slice(filename, output, **kwargs):\n    \n    im = Image.open(filename)\n    \n\n    im_w, im_h = im.size\n    \n    columns = int(kwargs.get('columns', 5))\n    rows = int(kwargs.get('rows', 7))\n    number_tiles = (columns * rows) - 1\n    extras = (columns * rows) - number_tiles\n    tile_w, tile_h = int(floor(im_w / columns)), int(floor(im_h / rows))\n\n    tiles = []\n    number = 1\n    for pos_y in range(0, im_h - rows, tile_h): \n        for pos_x in range(0, im_w - columns, tile_w): \n            area = (pos_x, pos_y, pos_x + tile_w, pos_y + tile_h)\n            image = im.crop(area)\n            position = (int(floor(pos_x / tile_w)) + 1,\n                        int(floor(pos_y / tile_h)) + 1)\n            coords = (pos_x, pos_y)\n            tile = Tile(image, number, position, coords)\n            tiles.append(tile)\n            number += 1\n    if not kwargs['dry_run']:\n        save_tiles(tiles,\n                   prefix=get_basename(filename),\n                   directory=output)\n    return tuple(tiles)\n\n\ndef main(path, **kwargs):\n    if os.path.isdir(path):\n        fnames = [os.path.join(path, f) for f in os.listdir(path)\n            if os.path.isfile(os.path.join(path, f)) and f.endswith(\".jpg\")]\n        output = os.path.join(os.path.abspath(path), 'output')\n    else:\n        fnames = [path]\n        output = os.path.join(os.path.dirname(os.path.abspath(path)), 'output')\n\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    for filename in fnames:\n        tiles = _do_slice(filename, output, **kwargs)\n        print \"In %s: saved %d tiles for file %s\" % (output, len(tiles), filename)\n\n\nif __name__ == '__main__':\n    parser = optparse.OptionParser(usage=\"usage: python %prog [OPTIONS] filename\")\n    parser.add_option(\"-c\", \"--columns\",\n                      action=\"store\",\n                      dest=\"columns\",\n                      help=\"Number of columns\")\n    parser.add_option(\"-r\", \"--rows\",\n                      action=\"store\",\n                      dest=\"rows\",\n                      default=0,\n                      help=\"Number of rows\")\n    parser.add_option('-d', \"--dry\",\n                      action='store_true',\n                      dest='dry_run',\n                      default=False,\n                      help='Dry run (do not actually perform anything, only report).')\n    opts, args = parser.parse_args()\n\n    start_time = time.time()\n    try:\n        main(args[0], **vars(opts))\n    except IndexError:\n        print \"You must specify source filename!\"\n        exit()\n    print \"Done, took %d seconds\" % int(time.time() - start_time)\n",
        "summary": "The provided Python script is a comprehensive tool for image processing and manipulation. It includes functionalities to slice images into tiles, save these tiles with customizable filenames, join multiple tiles back into a single image, and handle batch processing of image files within directories. The script utilizes the PIL library for image operations and provides command-line options for specifying the number of columns and rows for tiling, as well as performing dry runs without actual file modifications."
    },
    {
        "code": "import sys; from more_itertools import windowed, first_true\norig_data = list(map(int, open('d9.txt')))\ndata = orig_data[:]\ntarget = 32321523\nfor i, e in enumerate(data):\n    if i == 0: continue\n    data[i] = data[i - 1] + data[i]\n\nfor i in range(len(data)):\n    for j in range(i):\n        if data[i] - data[j] == target:\n            print(j, i, 'inclusive')\n            print(min(orig_data[j:i+1]) + max(orig_data[j:i+1]))\n            sys.exit()",
        "summary": "The Python code reads integers from a file named 'd9.txt', calculates cumulative sums of these numbers, and then searches for a contiguous subsequence whose sum equals a specified target value. Upon finding such a subsequence, it prints the indices of the start and end of the subsequence (inclusive) along with the sum of the smallest and largest numbers in that subsequence."
    },
    {
        "code": "import logging\n\nfrom zigpy.profiles import zha\nfrom zigpy.zcl.clusters.general import (\n    AnalogInput,\n    Basic,\n    Groups,\n    Identify,\n    MultistateInput,\n    OnOff,\n    Ota,\n    Scenes,\n)\n\nfrom .. import (\n    LUMI,\n    XIAOMI_NODE_DESC,\n    BasicCluster,\n    XiaomiPowerConfiguration,\n    XiaomiQuickInitDevice,\n)\nfrom ... import CustomCluster\nfrom ...const import (\n    ATTR_ID,\n    COMMAND,\n    DEVICE_TYPE,\n    DOUBLE_PRESS,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    LONG_PRESS,\n    MODELS_INFO,\n    NODE_DESCRIPTOR,\n    OUTPUT_CLUSTERS,\n    PRESS_TYPE,\n    PROFILE_ID,\n    SHORT_PRESS,\n    SKIP_CONFIGURATION,\n    VALUE,\n    ZHA_SEND_EVENT,\n)\n\nDOUBLE = \"double\"\nHOLD = \"long press\"\nPRESS_TYPES = {0: \"long press\", 1: \"single\", 2: \"double\"}\nSINGLE = \"single\"\nSTATUS_TYPE_ATTR = 0x0055  \nXIAOMI_CLUSTER_ID = 0xFFFF\nXIAOMI_DEVICE_TYPE = 0x5F01\nXIAOMI_DEVICE_TYPE2 = 0x5F02\nXIAOMI_DEVICE_TYPE3 = 0x5F03\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass RemoteB186ACN01(XiaomiQuickInitDevice):\n    \n\n    class MultistateInputCluster(CustomCluster, MultistateInput):\n        \n\n        cluster_id = MultistateInput.cluster_id\n\n        def __init__(self, *args, **kwargs):\n            \n            self._current_state = None\n            super().__init__(*args, **kwargs)\n\n        def _update_attribute(self, attrid, value):\n            super()._update_attribute(attrid, value)\n            if attrid == STATUS_TYPE_ATTR:\n                self._current_state = PRESS_TYPES.get(value)\n                event_args = {\n                    PRESS_TYPE: self._current_state,\n                    ATTR_ID: attrid,\n                    VALUE: value,\n                }\n                self.listener_event(ZHA_SEND_EVENT, self._current_state, event_args)\n                \n                super()._update_attribute(0, self._current_state)\n\n    signature = {\n        \n        \n        \n        \n        MODELS_INFO: [\n            (LUMI, \"lumi.remote.b186acn01\"),\n            (LUMI, \"lumi.remote.b186acn02\"),\n            (LUMI, \"lumi.sensor_86sw1\"),\n        ],\n        NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n        ENDPOINTS: {\n            1: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: XIAOMI_DEVICE_TYPE,\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    Identify.cluster_id,\n                    Ota.cluster_id,\n                    XIAOMI_CLUSTER_ID,\n                    MultistateInputCluster.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    Ota.cluster_id,\n                    XIAOMI_CLUSTER_ID,\n                    MultistateInputCluster.cluster_id,\n                ],\n            },\n            \n            \n            \n            \n            2: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: XIAOMI_DEVICE_TYPE2,\n                INPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    MultistateInputCluster.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    MultistateInputCluster.cluster_id,\n                ],\n            },\n            \n            \n            \n            \n            3: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: XIAOMI_DEVICE_TYPE3,\n                INPUT_CLUSTERS: [Identify.cluster_id, AnalogInput.cluster_id],\n                OUTPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    AnalogInput.cluster_id,\n                ],\n            },\n        },\n    }\n\n    replacement = {\n        SKIP_CONFIGURATION: True,\n        ENDPOINTS: {\n            1: {\n                DEVICE_TYPE: zha.DeviceType.REMOTE_CONTROL,\n                INPUT_CLUSTERS: [\n                    BasicCluster,\n                    XiaomiPowerConfiguration,\n                    Identify.cluster_id,\n                    Ota.cluster_id,\n                    XIAOMI_CLUSTER_ID,\n                    MultistateInputCluster,\n                ],\n                OUTPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    Ota.cluster_id,\n                    XIAOMI_CLUSTER_ID,\n                    MultistateInputCluster,\n                    OnOff.cluster_id,\n                ],\n            },\n            2: {\n                DEVICE_TYPE: zha.DeviceType.REMOTE_CONTROL,\n                INPUT_CLUSTERS: [Identify.cluster_id, MultistateInputCluster],\n                OUTPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    MultistateInputCluster,\n                ],\n            },\n            3: {\n                DEVICE_TYPE: zha.DeviceType.REMOTE_CONTROL,\n                INPUT_CLUSTERS: [Identify.cluster_id, MultistateInputCluster],\n                OUTPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Groups.cluster_id,\n                    Scenes.cluster_id,\n                    AnalogInput.cluster_id,\n                    MultistateInputCluster,\n                ],\n            },\n        },\n    }\n\n    device_automation_triggers = {\n        (DOUBLE_PRESS, DOUBLE_PRESS): {COMMAND: DOUBLE},\n        (SHORT_PRESS, SHORT_PRESS): {COMMAND: SINGLE},\n        (LONG_PRESS, LONG_PRESS): {COMMAND: HOLD},\n    }\n",
        "summary": "This Python code defines a custom Zigbee cluster for a Xiaomi remote control device, handling different press types and updating the device state accordingly. It includes event listeners for sending press type events and configuring the device's endpoints and clusters to support various functionalities like identifying, controlling groups and scenes, and managing power configuration."
    },
    {
        "code": "import doctest\ndef run_tests():\n    doctest.testmod(verbose=True)\n\ndef doubler(word):\n    print(''.join([char + char for char in word]))\n\nif __name__ == \"__main__\":\n    run_tests()",
        "summary": "The provided Python code includes a function `run_tests()` that executes doctests with verbose output, and another function `doubler(word)` which takes a string as input and prints each character of the string repeated twice. The script is designed to be run directly, invoking `run_tests()` to execute any embedded tests within the module."
    },
    {
        "code": "import binascii\nimport sys\n\nimport Adafruit_PN532 as PN532\n\n\n\n\n\n\nCS   = 8   \nMOSI = 9   \nMISO = 10  \nSCLK = 11  \n\n\n\n\n\n\n\n\npn532 = PN532.PN532(cs=CS, sclk=SCLK, mosi=MOSI, miso=MISO)\n\n\n\npn532.begin()\n\n\nic, ver, rev, support = pn532.get_firmware_version()\nprint('Found PN532 with firmware version: {0}.{1}'.format(ver, rev))\n\n\npn532.SAM_configuration()\n\n\n\nwhile True:\n    print('\u7b49\u5f85\u8bfb\u5361\u4e2d\uff0c\u8bf7\u5c06\u5361\u9760\u8fd1pn532\u8bfb\u53d6\u8bbe\u5907...')\n    \n    uid = pn532.read_passive_target()\n    \n    \n    if uid is None:\n        continue\n    uid=format(binascii.hexlify(uid))\n    print(\"UID:\",uid)\n    \n\n",
        "summary": "The Python script initializes an Adafruit PN532 NFC reader connected to specific GPIO pins on a Raspberry Pi. It then enters a loop where it waits for an NFC card to be placed near the reader, reads its unique identifier (UID), and prints it in hexadecimal format."
    },
    {
        "code": "from macaque import cli\n\ndef test_cli_template():\n    assert cli.cli() is None\n",
        "summary": "The provided Python code imports a module named `cli` from the package `macaque` and defines a function `test_cli_template` that asserts the return value of `cli.cli()` to be `None`."
    },
    {
        "code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef homepage():\n    return 'Essa \u00e9 minha HomePage'\n\n@app.route('/contatos')\ndef contatos():\n    return 'Essa s\u00e3o os meus contatos'\n\napp.run()",
        "summary": "This Python code sets up a basic web application using the Flask framework, defining two routes: one for the homepage and another for contact information. When accessed, each route returns a simple string message."
    },
    {
        "code": "from itertools import zip_longest\n\nDAY = 'day'\nHOUR = 'hour'\nNAME = 'name'\n\n\nclass Formatter:\n    def __init__(self, indent=5 * ' '):\n        self.indent = indent\n\n    def append(self, text, tag=None):\n        raise NotImplementedError('Must override append() in derived class')\n\n    def println(self, *args):\n        sep = None\n        for a in args:\n            if sep:\n                self.append(sep)\n            else:\n                sep = ' '\n            if isinstance(a, str):\n                self.append(a)\n            else:\n                self.append(*a)\n        self.append('\\n')\n\n    def show(self, previous, day, hour, name, text):\n        if day:\n            if previous:\n                self.println()\n            self.println((day, DAY))\n        if name:\n            if not day:\n                self.println()\n            self.println((hour, HOUR), (name, NAME))\n            self.show_multiline(None, text)\n        else:\n            self.show_multiline(hour, text)\n\n    def show_multiline(self, hour, text):\n        hh = [(hour, HOUR)] if hour else []\n        for h, line in zip_longest(hh, text.split('\\n'), fillvalue=self.indent):\n            self.println(h, line)\n",
        "summary": "The `Formatter` class is designed to format and print text with optional indentation and tags for day, hour, and name. It includes methods to append text, print lines, and show formatted content based on the presence of day, hour, and name, handling multiline text appropriately."
    },
    {
        "code": "__all__ = [\n    \"prototype\",\n]\n\nimport sys\n\nfrom inspect import (\n    signature,\n)\n\nfrom typing import (\n    TypeVar,\n    Callable,\n)\n\nfrom .exceptions import (\n    PrototypeError,\n)\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec  \n\nParameters = ParamSpec(\"Parameters\")\nReturnType = TypeVar(\"ReturnType\")\n\n\n\ndef prototype(\n    proto: Callable[Parameters, ReturnType],\n    /,\n    *,\n    runtime: bool = True,\n) -> Callable[Parameters, ReturnType]:\n    \n\n    \n    def decorator(func: Callable[Parameters, ReturnType], /) -> Callable[Parameters, ReturnType]:\n        if runtime is True:\n            func_signature = signature(func)\n            proto_signature = signature(proto)\n\n            if func_signature.parameters != proto_signature.parameters:\n                raise PrototypeError(func, func_signature, proto, proto_signature)\n\n            if func_signature.return_annotation != proto_signature.return_annotation:\n                raise PrototypeError(func, func_signature, proto, proto_signature)\n\n        return func\n\n    return decorator\n",
        "summary": "The code defines a `prototype` function that acts as a decorator to ensure a decorated function matches the signature of a provided prototype function at runtime or compile time, raising a `PrototypeError` if there is a mismatch. It uses Python's `inspect.signature` for introspection and conditional type hinting based on Python version."
    },
    {
        "code": "import PIL.Image\nfrom io import BytesIO\nfrom IPython.display import clear_output, Image, display\nimport numpy as np\n\n\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n\n\ndef showtensor(a):\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n    inp = a[0, :, :, :]\n    inp = inp.transpose(1, 2, 0)\n    inp = std * inp + mean\n    inp *= 255\n    showarray(inp)\n    clear_output(wait=True)\n",
        "summary": "The provided Python code includes two functions: `showarray` and `showtensor`. The `showarray` function takes a NumPy array as input, converts it to an image using PIL, and displays it in Jupyter Notebook. The `showtensor` function processes a tensor by normalizing its pixel values according to specified mean and standard deviation, then calls `showarray` to display the processed image."
    },
    {
        "code": "import mimetypes\nfrom pathlib import Path\n\nfrom appdirs import user_config_dir\nfrom tqdm import tqdm\n\nNAME = \"novelsave\"\nAUTHOR = \"Mensch272\"\n\n\nBASE_DIR = Path(__file__).resolve().parent.parent\n\nSTATIC_DIR = BASE_DIR / \"novelsave/resources\"\n\n\n\nCONFIG_DIR = Path(user_config_dir(NAME, AUTHOR))\nCONFIG_FILE = CONFIG_DIR / \"config.json\"\n\nDATA_DIR = CONFIG_DIR / \"data\"\n\nDATABASE_FILE = (CONFIG_DIR / \"data.sqlite\").resolve()\nDATABASE_URL = \"sqlite:///\" + str(DATABASE_FILE)\n\n\n\nNOVEL_DIR = Path.home() / \"novels\"\n\n\n\nDIVISION_RULES = {\n    k: v.split(\"/\", maxsplit=1)[0] for k, v in mimetypes.types_map.items()\n}\n\n\ndef console_formatter(record):\n    if record[\"level\"].name == \"INFO\":\n        return \"{message}\\n\"\n    else:\n        return \"<level>{level}: {message}</level>\\n\"\n\n\nLOGGER_CONFIG = {\n    \"handlers\": [\n        {\n            \"sink\": lambda msg: tqdm.write(msg, end=\"\"),\n            \"format\": console_formatter,\n            \"level\": \"INFO\",\n            \"colorize\": True,\n            \"backtrace\": False,\n            \"diagnose\": False,\n        },\n        {\n            \"sink\": CONFIG_DIR / \"logs\" / \"{time}.log\",\n            \"level\": \"TRACE\",\n            \"retention\": \"2 days\",\n            \"compression\": \"zip\",\n            \"encoding\": \"utf-8\",\n        },\n    ],\n}\n\nTQDM_CONFIG = {\"ncols\": 80, \"bar_format\": \"{percentage:3.0f}% |{bar}{r_bar}\"}\n\nconfig = {\n    \"name\": NAME,\n    \"author\": AUTHOR,\n    \"base_dir\": BASE_DIR,\n    \"static\": {\n        \"dir\": STATIC_DIR,\n    },\n    \"config\": {\n        \"dir\": CONFIG_DIR,\n        \"file\": CONFIG_FILE,\n    },\n    \"data\": {\n        \"dir\": DATA_DIR,\n        \"division_rules\": DIVISION_RULES,\n    },\n    \"novel\": {\n        \"dir\": NOVEL_DIR,\n    },\n    \"infrastructure\": {\n        \"database\": {\n            \"url\": DATABASE_URL,\n        }\n    },\n}\n",
        "summary": "This Python script sets up a configuration for an application named \"novelsave\" by defining paths, constants, and logging settings. It includes directories for static resources, user configuration, data storage, and novels, as well as rules for dividing file types based on MIME types. The script also configures logging to output both to the console with a progress bar and to a log file with specific retention and compression policies."
    },
    {
        "code": "import logging\nimport os\nimport sys\n\nimport click\n\nfrom newschimp import renderer, sender\nfrom newschimp.social import fb, gg, lanyrd\nfrom newschimp.cli import cli_group\nfrom newschimp.utils import ComplexCLI, load_settings\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef create_newsletter(settings):\n    \n    context = {}\n    try:\n        fb_posts = fb.get_posts(settings, os.environ['FACEBOOK_TOKEN'], None)\n    except KeyError:\n        LOGGER.error('Facebook Token not defined')\n        sys.exit()\n    click.echo('[1/4] Getting Facebook Group posts')\n    context['fb'] = fb.curate(fb_posts)\n    ggroup_posts = gg.get_posts(settings, None)\n    click.echo('[2/4] Getting Google Group posts')\n    context['gg'] = gg.curate(ggroup_posts)\n    click.echo('[3/4] Getting upcoming Lanyrd meetups')\n    context['meetups'] = lanyrd.meetup_loop(settings)\n    click.echo('[4/4] Rendering mail')\n    renderer.render_files(settings, None, context)\n    click.confirm(\n        'Content is rendered, would you like to send it now?', abort=True)\n    click.echo('Creating MailChimp campaign')\n    sender.new_campaign(settings, os.environ.get('MAILCHIMP_KEY'))\n\n\ncli_group.add_command(fb.cli)\ncli_group.add_command(gg.cli)\ncli_group.add_command(lanyrd.cli)\n\n@cli_group.command(cls=ComplexCLI, invoke_without_command=True)\n@click.option('--config', help='Custom config file', type=click.Path(\n    exists=True, file_okay=True, resolve_path=True), default='config.yaml')\n@click.pass_context\ndef main(ctx, config):\n    ctx.obj['SETTINGS'] = load_settings(config)\n    if ctx.invoked_subcommand is None:\n        create_newsletter(ctx.obj['SETTINGS'])\n\n\nif __name__ == '__main__':\n    main(obj={})\n",
        "summary": "The Python script defines a command-line interface (CLI) for creating and sending newsletters using data from Facebook, Google Groups, and Lanyrd. It uses the `click` library to handle commands and options, and it interacts with external services like Facebook and MailChimp through their respective APIs. The script logs errors and provides feedback to the user throughout the process of fetching data, rendering templates, and sending emails."
    },
    {
        "code": "try:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\nPACKAGE = \"flightaware\"\nNAME = \"flightaware\"\nDESCRIPTION = \"A python REST interface for flightaware data\"\nAUTHOR = \"Fred Palmer\"\nAUTHOR_EMAIL = \"fred.palmer@gmail.com\"\nURL = \"https://github.com/fredpalmer/flightaware\"\n\nconfig = {\n    \"description\": DESCRIPTION,\n    \"author\": AUTHOR,\n    \"url\": URL,\n    \"author_email\": AUTHOR_EMAIL,\n    \"version\": \"0.1\",\n    \"install_requires\": [\n        \"requests>=2.0.0\",\n\t\t\"pytz\"\n    ],\n    \"keywords\": \"travel flightaware airline flight flight-tracking flight-data\",\n    \"classifiers\": [\n        \"Development Status :: 3 - Alpha\",\n        \"Environment :: Web Environment\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Topic :: Internet :: WWW/HTTP\",\n    ],\n    \"packages\": [PACKAGE, ],\n    \"scripts\": [],\n    \"name\": NAME,\n    \"license\": \"MIT\",\n}\n\nsetup(**config)\n",
        "summary": "This Python script sets up a package named \"flightaware\" using either `setuptools` or `distutils.core`, depending on availability. It defines metadata such as the package description, author details, dependencies, and classifiers to describe the project's characteristics and requirements for installation."
    },
    {
        "code": "import pyaf.Bench.TS_datasets as tsds\nimport tests.artificial.process_artificial_dataset as art\n\n\n\n\nart.process_dataset(N = 32 , FREQ = 'D', seed = 0, trendtype = \"Lag1Trend\", cycle_length = 30, transform = \"Quantization\", sigma = 0.0, exog_count = 20, ar_order = 12);",
        "summary": "The Python code imports modules for time series dataset processing and then uses the `process_dataset` function from the `artificial` module to generate a synthetic time series dataset with specified parameters such as number of observations, frequency, trend type, cycle length, transformation method, and other characteristics."
    },
    {
        "code": "import os\nimport json\nimport shutil\nimport logging\nimport requests\nfrom lithops.storage.utils import StorageNoSuchKeyError\nfrom lithops.utils import sizeof_fmt\nfrom lithops.constants import STORAGE_CLI_MSG\n\nlogger = logging.getLogger(__name__)\n\n\nclass StorageBackend:\n    \n\n    def __init__(self, swift_config):\n        logger.debug(\"Creating OpenStack Swift client\")\n        self.auth_url = swift_config['swift_auth_url']\n        self.user_id = swift_config['swift_user_id']\n        self.project_id = swift_config['swift_project_id']\n        self.password = swift_config['swift_password']\n        self.region = swift_config['swift_region']\n        self.endpoint = None\n\n        if 'token' in swift_config:\n            self.token = swift_config['token']\n            self.endpoint = swift_config['endpoint']\n        else:\n            self.token = self.generate_swift_token()\n            swift_config['token'] = self.token\n            swift_config['endpoint'] = self.endpoint\n\n        self.session = requests.session()\n        self.session.headers.update({'X-Auth-Token': self.token})\n        adapter = requests.adapters.HTTPAdapter(pool_maxsize=64, max_retries=3)\n        self.session.mount('http://', adapter)\n        self.session.mount('https://', adapter)\n\n        msg = STORAGE_CLI_MSG.format('OpenStack Swift')\n        logger.info(\"{} - Region: {}\".format(msg, self.region))\n\n    def generate_swift_token(self):\n        \n        url = self.auth_url+\"/v3/auth/tokens\"\n        headers = {'Content-Type': 'application/json'}\n        data = {\"auth\": {\"identity\": {\"methods\": [\"password\"],\n                                      \"password\": {\"user\": {\"id\": self.user_id, \"password\": self.password}}},\n                         \"scope\": {\"project\": {\"id\": self.project_id}}}}\n        json_data = json.dumps(data)\n\n        r = requests.post(url, data=json_data, headers=headers)\n\n        if r.status_code == 201:\n            backend_info = json.loads(r.text)\n\n            for service in backend_info['token']['catalog']:\n                if service['name'] == 'swift':\n                    for endpoint in service['endpoints']:\n                        if endpoint['region'] == self.region:\n                            if endpoint['interface'] == 'public':\n                                self.endpoint = endpoint['url'].replace('https:', 'http:')\n\n            if not self.endpoint:\n                raise Exception('Invalid region name')\n\n            return r.headers['X-Subject-Token']\n        else:\n            message = json.loads(r.text)['error']['message']\n            raise Exception(\"{} - {} - {}\".format(r.status_code, r.reason, message))\n\n    def put_object(self, container_name, key, data):\n        \n        url = '/'.join([self.endpoint, container_name, key])\n        try:\n            res = self.session.put(url, data=data)\n            status = 'OK' if res.status_code == 201 else 'Error'\n            try:\n                logger.debug('PUT Object {} - Size: {} - {}'.format(key, sizeof_fmt(len(data)), status))\n            except Exception:\n                logger.debug('PUT Object {} - {}'.format(key, status))\n        except Exception as e:\n            print(e)\n\n    def get_object(self, container_name, key, stream=False, extra_get_args={}):\n        \n        if not container_name:\n            container_name = self.storage_container\n        url = '/'.join([self.endpoint, container_name, key])\n        headers = {'X-Auth-Token': self.token}\n        headers.update(extra_get_args)\n        try:\n            res = self.session.get(url, headers=headers, stream=stream)\n            if res.status_code == 200 or res.status_code == 206:\n                if stream:\n                    data = res.raw\n                else:\n                    data = res.content\n                return data\n            elif res.status_code == 404:\n                raise StorageNoSuchKeyError(container_name, key)\n            else:\n                raise Exception('{} - {}'.format(res.status_code, key))\n        except StorageNoSuchKeyError:\n            raise StorageNoSuchKeyError(container_name, key)\n        except Exception as e:\n            print(e)\n            raise StorageNoSuchKeyError(container_name, key)\n\n    def upload_file(self, file_name, bucket, key=None, extra_args={}):\n        \n        \n        if key is None:\n            key = os.path.basename(file_name)\n\n        \n        try:\n            with open(file_name, 'rb') as in_file:\n                self.put_object(bucket, key, in_file)\n        except Exception as e:\n            logging.error(e)\n            return False\n        return True\n\n    def download_file(self, bucket, key, file_name=None, extra_args={}):\n        \n        \n        if file_name is None:\n            file_name = key\n\n        \n        try:\n            dirname = os.path.dirname(file_name)\n            if dirname and not os.path.exists(dirname):\n                os.makedirs(dirname)\n            with open(file_name, 'wb') as out:\n                data_stream = self.get_object(bucket, key, stream=True)\n                shutil.copyfileobj(data_stream, out)\n        except Exception as e:\n            logging.error(e)\n            return False\n        return True\n\n    def head_object(self, container_name, key):\n        \n        url = '/'.join([self.endpoint, container_name, key])\n        try:\n            res = self.session.head(url)\n            if res.status_code == 200:\n                return res.headers\n            elif res.status_code == 404:\n                raise StorageNoSuchKeyError(container_name, key)\n            else:\n                raise Exception('{} - {}'.format(res.status_code, key))\n        except Exception as e:\n            raise StorageNoSuchKeyError(container_name, key)\n\n    def delete_object(self, container_name, key):\n        \n        url = '/'.join([self.endpoint, container_name, key])\n        return self.session.delete(url)\n\n    def delete_objects(self, container_name, key_list):\n        \n        headers={'X-Auth-Token': self.token,\n                 'X-Bulk-Delete': 'True'}\n\n        keys_to_delete = []\n        for key in key_list:\n            keys_to_delete.append('/{}/{}'.format(container_name, key))\n\n        keys_to_delete = '\\n'.join(keys_to_delete)\n        url = '/'.join([self.endpoint, '?bulk-delete'])\n        return self.session.delete(url, data=keys_to_delete, headers=headers)\n\n    def list_objects(self, container_name, prefix=''):\n        \n        if prefix:\n            url = '/'.join([self.endpoint, container_name, '?format=json&prefix='+prefix])\n        else:\n            url = '/'.join([self.endpoint, container_name, '?format=json'])\n        try:\n            res = self.session.get(url)\n            objects = res.json()\n\n            \n            return objects\n        except Exception as e:\n            raise e\n\n    def list_keys(self, container_name, prefix):\n        \n        try:\n            objects = self.list_objects(container_name, prefix)\n            object_keys = [r['name'] for r in objects]\n            return object_keys\n        except Exception as e:\n            raise(e)\n",
        "summary": "The `StorageBackend` class provides methods to interact with an OpenStack Swift storage system, including uploading and downloading files, managing objects, and listing contents. It handles authentication, token generation, and HTTP requests to perform operations like PUT, GET, HEAD, DELETE, and list objects in a specified container."
    },
    {
        "code": "from rpcore.render_target import RenderTarget\nfrom rpcore.loader import RPLoader\n\n\nclass RenderStage():\n\n    \n\n    required_inputs = []\n    required_pipes = []\n\n    produced_inputs = {}\n    produced_pipes = {}\n    produced_defines = {}\n\n    disabled = False\n\n    def __init__(self, pipeline):\n        \n        self.stage_id = self.__class__.__name__\n        self._pipeline = pipeline\n        self._active = True\n        self._targets = {}\n\n    def create(self):\n        \n        raise NotImplementedError()\n\n    def reload_shaders(self):\n        \n        pass\n\n    def set_shader_input(self, *args):\n        \n        for target in self._targets.values():\n            target.set_shader_input(*args)\n\n    def set_shader_inputs(self, **kwargs):\n        \n        for target in self._targets.values():\n            target.set_shader_inputs(**kwargs)\n\n    def update(self):\n        \n        pass\n\n    @property\n    def active(self):\n        \n        return self._active\n\n    @active.setter\n    def active(self, state):\n        \n        if self._active != state:\n            self._active = state\n            for target in self._targets.values():\n                target.active = self._active\n\n    def create_target(self, name):\n        \n        \n        \n        name = self._get_plugin_id() + \":\" + self.stage_id + \":\" + name\n        if name in self._targets:\n            return self.error(\"Overriding existing target: \" + name)\n        self._targets[name] = RenderTarget(name)\n        return self._targets[name]\n\n    def remove_target(self, target):\n        \n        target.remove()\n        target_key = None\n        for key, value_target in self._targets.items():\n            if target == value_target:\n                target_key = key\n                break\n        del self._targets[target_key]\n\n    def _get_shader_handle(self, path, *args):\n        \n        assert len(args) > 0 and len(args) <= 3\n        path_args = []\n\n        for source in args:\n            for prefix in (\"/$$rpconfig\", \"/$$rp/shader\", \"/$$rptemp\"):\n                if prefix in source:\n                    path_args.append(source)\n                    break\n            else:\n                path_args.append(path.format(source))\n\n        \n        \n        if len(args) == 1:\n            path_args = [\"/$$rp/shader/default_post_process.vert.glsl\"] + path_args\n        return RPLoader.load_shader(*path_args)\n\n    def _get_plugin_id(self):\n        \n        if \"rpcore.stages\" in self.__class__.__module__:\n            return \"render_pipeline_internal\"\n        return str(self.__class__.__module__).split(\".\")[-2]\n\n    def load_shader(self, *args):\n        \n        return self._get_shader_handle(\"/$$rp/shader/{0}\", *args)\n\n    def load_plugin_shader(self, *args):\n        \n        shader_path = \"rpplugins/\" + self._get_plugin_id() + \"/shader/{0}\"\n        return self._get_shader_handle(shader_path, *args)\n\n    def handle_window_resize(self):\n        \n        self.set_dimensions()\n        for target in self._targets.values():\n            target.consider_resize()\n\n    def set_dimensions(self):\n        \n        pass\n",
        "summary": "The `RenderStage` class is a base class for rendering stages in a graphics pipeline. It manages render targets, shader inputs, and provides methods to create, remove, and update these resources, as well as handle window resizing. Subclasses must implement the `create` method to define specific rendering logic."
    },
    {
        "code": "import difflib\nimport json\nimport os\n\nimport pytest\n\nfrom ....defaults import FIXTURES_DIR\nfrom ..._common import fixture_path_from_request\nfrom ..._common import update_fixtures\nfrom ..._interactions import SearchFor\nfrom ..._interactions import Step\nfrom ..._tmux_session import TmuxSession\n\n\nTEST_FIXTURE_DIR = os.path.join(FIXTURES_DIR, \"integration\", \"actions\", \"inventory\")\nANSIBLE_INVENTORY_FIXTURE_DIR = os.path.join(TEST_FIXTURE_DIR, \"ansible_inventory\", \"inventory.yml\")\nTEST_CONFIG_FILE = os.path.join(TEST_FIXTURE_DIR, \"ansible-navigator.yml\")\n\n\nbase_steps = (\n    Step(user_input=\":0\", comment=\"Browse hosts/ungrouped window\"),\n    Step(user_input=\":0\", comment=\"Group list window\"),\n    Step(user_input=\":0\", comment=\"group01 hosts detail window\"),\n    Step(user_input=\":0\", comment=\"host0101 detail window\"),\n    Step(user_input=\":back\", comment=\"Previous window (group01 hosts detail window)\"),\n    Step(user_input=\":back\", comment=\"Previous window (Group list window)\"),\n    Step(user_input=\":1\", comment=\"group02 hosts detail window\"),\n    Step(user_input=\":0\", comment=\"host0201 detail window\"),\n    Step(user_input=\":back\", comment=\"Previous window (group02 hosts detail window)\"),\n    Step(user_input=\":back\", comment=\"Previous window (Group list window)\"),\n    Step(user_input=\":2\", comment=\"group03 hosts detail window\"),\n    Step(user_input=\":0\", comment=\"host0301 detail window\"),\n    Step(user_input=\":back\", comment=\"Previous window (group03 hosts detail window)\"),\n    Step(user_input=\":back\", comment=\"Previous window (Group list window)\"),\n    Step(user_input=\":back\", comment=\"Previous window (Browse hosts/ungrouped window)\"),\n    Step(user_input=\":back\", comment=\"Previous window (top window)\"),\n    Step(user_input=\":1\", comment=\"Inventory hostname window\"),\n    Step(user_input=\":0\", comment=\"host0101 detail window\"),\n    Step(user_input=\":back\", comment=\"Previous window after host0101 (Inventory hostname window)\"),\n    Step(user_input=\":1\", comment=\"host0201 detail window\"),\n    Step(user_input=\":back\", comment=\"Previous window after host0201 (Inventory hostname window)\"),\n    Step(user_input=\":2\", comment=\"host0301 detail window\"),\n)\n\n\nclass BaseClass:\n    \n\n    UPDATE_FIXTURES = False\n\n    @staticmethod\n    @pytest.fixture(scope=\"module\", name=\"tmux_session\")\n    def fixture_tmux_session(request):\n        \n        params = {\n            \"setup_commands\": [\n                \"export ANSIBLE_DEVEL_WARNING=False\",\n                \"export ANSIBLE_DEPRECATION_WARNINGS=False\",\n            ],\n            \"pane_height\": \"2000\",\n            \"pane_width\": \"500\",\n            \"config_path\": TEST_CONFIG_FILE,\n            \"unique_test_id\": request.node.nodeid,\n        }\n\n        with TmuxSession(**params) as tmux_session:\n            yield tmux_session\n\n    def test(self, request, tmux_session, step):\n        \n        assert os.path.exists(ANSIBLE_INVENTORY_FIXTURE_DIR)\n        assert os.path.exists(TEST_CONFIG_FILE)\n\n        if step.search_within_response is SearchFor.HELP:\n            search_within_response = \":help help\"\n        elif step.search_within_response is SearchFor.PROMPT:\n            search_within_response = tmux_session.cli_prompt\n        else:\n            raise ValueError(\"test mode not set\")\n\n        received_output = tmux_session.interaction(\n            value=step.user_input,\n            search_within_response=search_within_response,\n        )\n        if step.mask:\n            \n            mask = \"X\" * 50\n            for idx, line in enumerate(received_output):\n                if tmux_session.cli_prompt in line:\n                    received_output[idx] = mask\n\n        fixtures_update_requested = (\n            self.UPDATE_FIXTURES\n            or os.environ.get(\"ANSIBLE_NAVIGATOR_UPDATE_TEST_FIXTURES\") == \"true\"\n            and not any((step.look_fors, step.look_nots))\n        )\n        if fixtures_update_requested:\n            update_fixtures(\n                request,\n                step.step_index,\n                received_output,\n                step.comment,\n                additional_information={\n                    \"look_fors\": step.look_fors,\n                    \"look_nots\": step.look_nots,\n                    \"compared_fixture\": not any((step.look_fors, step.look_nots)),\n                },\n            )\n        page = \" \".join(received_output)\n\n        if step.look_fors:\n            assert all(look_for in page for look_for in step.look_fors)\n\n        if step.look_nots:\n            assert not any(look_not in page for look_not in step.look_nots)\n\n        if not any((step.look_fors, step.look_nots)):\n            dir_path, file_name = fixture_path_from_request(request, step.step_index)\n            with open(file=os.path.join(dir_path, file_name), encoding=\"utf-8\") as infile:\n                expected_output = json.load(infile)[\"output\"]\n\n            assert expected_output == received_output, \"\\n\" + \"\\n\".join(\n                difflib.unified_diff(expected_output, received_output, \"expected\", \"received\"),\n            )\n",
        "summary": "The provided Python code defines a base class for testing an inventory management system using Tmux sessions. It includes steps to navigate through different inventory windows and verify the output against expected results or update fixtures if necessary. The class uses pytest fixtures and assertions to ensure the system behaves as intended, with options to mask sensitive information and handle help prompts."
    },
    {
        "code": "from __future__ import absolute_import, unicode_literals, print_function\nimport logging\n\n\nimport salt.utils.compat\n\nlog = logging.getLogger(__name__)\n\n\ndef __virtual__():\n    return True\n\n\ndef __init__(opts):\n    salt.utils.compat.pack_dunder(__name__)\n\n\ndef state_result(result, message, name, changes=None):\n    if changes is None:\n        changes = {}\n    return {'result': result,\n            'comment': message,\n            'name': name,\n            'changes': changes}\n\n\ndef balancer_present(name, port, protocol, profile, algorithm=None, members=None, **libcloud_kwargs):\n    \n    balancers = __salt__['libcloud_loadbalancer.list_balancers'](profile)\n    match = [z for z in balancers if z['name'] == name]\n    if len(match) > 0:\n        return state_result(True, \"Balancer already exists\", name)\n    else:\n        starting_members = None\n        if members is not None:\n            starting_members = []\n            for m in members:\n                starting_members.append({'ip': m['ip'], 'port': m['port']})\n        balancer = __salt__['libcloud_loadbalancer.create_balancer'](\n            name, port, protocol,\n            profile, algorithm=algorithm,\n            members=starting_members,\n            **libcloud_kwargs)\n        return state_result(True, \"Created new load balancer\", name, balancer)\n\n\ndef balancer_absent(name, profile, **libcloud_kwargs):\n    \n    balancers = __salt__['libcloud_loadbalancer.list_balancers'](profile)\n    match = [z for z in balancers if z['name'] == name]\n    if len(match) == 0:\n        return state_result(True, \"Balancer already absent\", name)\n    else:\n        result = __salt__['libcloud_loadbalancer.destroy_balancer'](match[0]['id'], profile, **libcloud_kwargs)\n        return state_result(result, \"Deleted load balancer\", name)\n\n\ndef member_present(ip, port, balancer_id, profile, **libcloud_kwargs):\n    \n    existing_members = __salt__['libcloud_loadbalancer.list_balancer_members'](balancer_id, profile)\n    for member in existing_members:\n        if member['ip'] == ip and member['port'] == port:\n            return state_result(True, \"Member already present\", balancer_id)\n    member = __salt__['libcloud_loadbalancer.balancer_attach_member'](balancer_id, ip, port, profile, **libcloud_kwargs)\n    return state_result(True, \"Member added to balancer, id: {0}\".format(member['id']), balancer_id, member)\n\n\ndef member_absent(ip, port, balancer_id, profile, **libcloud_kwargs):\n    \n    existing_members = __salt__['libcloud_loadbalancer.list_balancer_members'](balancer_id, profile)\n    for member in existing_members:\n        if member['ip'] == ip and member['port'] == port:\n            result = __salt__['libcloud_loadbalancer.balancer_detach_member'](balancer_id, member['id'], profile, **libcloud_kwargs)\n            return state_result(result, \"Member removed\", balancer_id)\n    return state_result(True, \"Member already absent\", balancer_id)\n",
        "summary": "The provided Python code defines a module for managing load balancers using the SaltStack framework and the Libcloud library. It includes functions to create, delete, check the presence of, and manage members within load balancers, returning detailed results in a consistent format."
    },
    {
        "code": "import pathlib\n\nfrom silex_client.utils.log import logger\n\n\nclass AnyParameter(object):\n    def __new__(cls, value):\n        return value\n\n\nclass CommandParameterMeta(type):\n    def __new__(cls, name: str, bases: tuple, dct: dict):\n        def serialize():\n            return {\n                \"name\": \"parameter\",\n            }\n\n        attributes = {\n            \"serialize\": serialize,\n        }\n        attributes.update(dct)\n        return super().__new__(cls, name, bases, attributes)\n\n    def get_default(self):\n        return None\n\n    def serialize(self):\n        return None\n\n\nclass TaskParameterMeta(CommandParameterMeta):\n    def __init__(self):\n        pass\n\n    def __new__(cls):\n        def serialize():\n            return {\n                \"name\": \"task\",\n            }\n\n        def get_default():\n            return \"\"\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"TaskParameter\", (str,), attributes)\n\n\nclass IntArrayParameterMeta(CommandParameterMeta):\n    def __init__(self, size: int):\n        pass\n\n    def __new__(cls, size: int):\n        def __init__(self, value):\n            if not isinstance(value, list):\n                value = [value]\n\n            for index, item in enumerate(value):\n                value[index] = int(item)\n\n            self.extend(value)\n\n        def serialize():\n            return {\n                \"name\": \"int_array\",\n                \"size\": size,\n            }\n\n        def get_default():\n            return [0 for i in range(size)]\n\n        attributes = {\n            \"__init__\": __init__,\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"IntArrayParameter\", (list,), attributes)\n\n\nclass RangeParameterMeta(CommandParameterMeta):\n    def __init__(self, start: int, end: int, increment: int = 1):\n        pass\n\n    def __new__(cls, start: int, end: int, increment: int = 1):\n        def serialize():\n            return {\n                \"name\": \"range\",\n                \"start\": start,\n                \"end\": end,\n                \"increment\": increment,\n            }\n\n        def get_default():\n            return start\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"RangeParameter\", (int,), attributes)\n\n\nclass SelectParameterMeta(CommandParameterMeta):\n    def __init__(self, *list_options, **options):\n        pass\n\n    def __new__(cls, *list_options, **options):\n        for unnamed_option in list_options:\n            options[unnamed_option] = unnamed_option\n\n        def serialize():\n            return {\"name\": \"select\", \"options\": options}\n\n        def get_default():\n            return list(options.values())[0] if options else None\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"SelectParameter\", (str,), attributes)\n\n\nclass RadioSelectParameterMeta(CommandParameterMeta):\n    def __init__(self, *list_options, **options):\n        pass\n\n    def __new__(cls, *list_options, **options):\n        for unnamed_option in list_options:\n            options[unnamed_option] = unnamed_option\n\n        def serialize():\n            return {\"name\": \"radio_select\", \"options\": options}\n\n        def get_default():\n            return list(options.values())[0] if options else None\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"RadioSelectParameter\", (str,), attributes)\n\n\nclass MultipleSelectParameterMeta(CommandParameterMeta):\n    def __init__(self, *list_options, **options):\n        pass\n\n    def __new__(cls, *list_options, **options):\n        for unnamed_option in list_options:\n            options[unnamed_option] = unnamed_option\n\n        def serialize():\n            return {\"name\": \"multiple_select\", \"options\": options}\n\n        def get_default():\n            return [list(options.values())[0]] if options else None\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n        return super().__new__(cls, \"SelectParameter\", (list,), attributes)\n\n\n\nclass ListParameter(list):\n    def __init__(self, value):\n        logger.warning(\n            \"Deprecation warning: The parameter type ListParameter is deprecated in favor if ListParameterMeta()\"\n        )\n        data = value\n\n        if not isinstance(value, list):\n            data = [value]\n        self.extend(data)\n\n\nclass PathParameterMeta(CommandParameterMeta):\n    def __init__(self, extensions=None, multiple=False):\n        pass\n\n    def __new__(cls, extensions=None, multiple=False):\n        if extensions is None:\n            extensions = [\"*\"]\n\n        def __init_list__(self, value):\n            if not isinstance(value, list):\n                value = [value]\n\n            for index, item in enumerate(value):\n                value[index] = pathlib.Path(item)\n\n            self.extend(value)\n\n        def serialize():\n            return {\n                \"name\": \"Path\",\n                \"extensions\": extensions,\n                \"multiple\": multiple,\n            }\n\n        def get_default():\n            return None\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n\n        if multiple:\n            attributes[\"__init__\"] = __init_list__\n            return super().__new__(cls, \"PathParameter\", (list,), attributes)\n\n        return super().__new__(\n            cls, \"PathParameter\", (type(pathlib.Path()),), attributes\n        )\n\n\nclass ListParameterMeta(CommandParameterMeta):\n    def __init__(self, parameter_type):\n        pass\n\n    def __new__(cls, parameter_type):\n        def __init__(self, value):\n            if not isinstance(value, list):\n                value = [value]\n\n            for index, item in enumerate(value):\n                value[index] = parameter_type(item)\n\n            self.extend(value)\n\n        def serialize():\n            item_type = None\n\n            if isinstance(parameter_type, CommandParameterMeta):\n                return parameter_type.serialize()\n\n            elif isinstance(parameter_type, type):\n                item_type = {\"name\": parameter_type.__name__}\n\n            return {\"name\": \"list\", \"itemtype\": item_type}\n\n        def get_default():\n            return []\n\n        attributes = {\n            \"__init__\": __init__,\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n\n        return super().__new__(cls, \"ListParameter\", (list,), attributes)\n\n\nclass TextParameterMeta(CommandParameterMeta):\n    def __init__(self, color=None):\n        pass\n\n    def __new__(cls, color=None):\n        def serialize():\n            return {\"name\": \"text\", \"color\": color}\n\n        def get_default():\n            return \"\"\n\n        attributes = {\n            \"serialize\": serialize,\n            \"get_default\": get_default,\n        }\n\n        return super().__new__(cls, \"ListParameter\", (str,), attributes)\n",
        "summary": "The provided Python code defines a set of metaclasses and classes to create various types of parameters for command-line interfaces or similar applications. These include parameters for tasks, integer arrays, ranges, selections, paths, lists, and text, each with methods to serialize their data and provide default values. The `ListParameter` class is deprecated in favor of using the `ListParameterMeta()` metaclass directly."
    },
    {
        "code": "N = input()\nL = len(N)\nK = int(input())\ndp = [[[0] * 2 for _ in range(K + 1)] for _ in range(L + 1)]\ndp[0][0][1] = 1\nfor i, x in zip(range(L), map(int, N)):\n    for k in range(K):\n        dp[i+1][k][0] += dp[i][k][0]  \n        if x == 0:\n            dp[i+1][k][1] += dp[i][k][1]\n        elif x > 0:\n            dp[i+1][k][0] += dp[i][k][1]\n        \n        for d in range(1, 10):\n            dp[i+1][k+1][0] += dp[i][k][0]\n            if d == x:\n                dp[i+1][k+1][1] += dp[i][k][1]\n            elif d < x:\n                dp[i+1][k+1][0] += dp[i][k][1]\n    dp[i+1][K][0] += dp[i][K][0]  \n    if x == 0:\n        dp[i+1][K][1] += dp[i][K][1]\n    elif x > 0:\n        dp[i+1][K][0] += dp[i][K][1]\nprint(sum(dp[-1][K]))\n",
        "summary": "The Python code calculates the number of ways to partition a given integer N into K parts, where each part is at most 9. It uses dynamic programming to efficiently compute the result by iterating through each digit of N and updating the possible partitions based on whether the current digit is included or not. The final output is the sum of all valid partitions that use exactly K parts."
    },
    {
        "code": "config = {\n   \"username\": 'slask',\n   \"icon\": \":poop:\",\n}\n",
        "summary": "The provided Python code defines a dictionary named `config` that contains two key-value pairs: one for the username set to 'slask' and another for an icon represented by ':poop:'."
    },
    {
        "code": "__all__ = ['patch_object', 'with_patched_object', 'PatchHandler',\n           'patched_context', 'patch']\n\nimport sys\n\nimport fudge\nfrom fudge.util import wraps\n\n\nclass patch(object):\n    \n    \n    def __init__(self, *obj_paths):\n        self.obj_paths = obj_paths\n    \n    def __call__(self, fn):\n        @wraps(fn)\n        def caller(*args, **kw):\n            fakes = self.__enter__()\n            if not isinstance(fakes, (tuple, list)):\n                fakes = [fakes]\n            args += tuple(fakes)\n            value = None\n            try:\n                value = fn(*args, **kw)\n            except:\n                etype, val, tb = sys.exc_info()\n                self.__exit__(etype, val, tb)\n                raise etype, val, tb\n            else:\n                self.__exit__(None, None, None)\n            return value\n        return caller\n    \n    def __enter__(self):\n        fudge.clear_expectations()\n        fudge.clear_calls()\n        self.patches = []\n        all_fakes = []\n        for path in self.obj_paths:\n            try:\n                target, attr = path.rsplit('.', 1)    \n            except (TypeError, ValueError):\n                raise TypeError(\n                    \"Need a valid target to patch. You supplied: %r\"\n                    % path)\n            fake = fudge.Fake(path)\n            all_fakes.append(fake)\n            self.patches.append(patch_object(target, attr, fake))\n        if len(all_fakes) == 1:\n            return all_fakes[0]\n        else:\n            return all_fakes\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        try:\n            if not exc_type:\n                fudge.verify()\n        finally:\n            for p in self.patches:\n                p.restore()\n            fudge.clear_expectations()\n\n\ndef with_patched_object(obj, attr_name, patched_value):\n    \n    def patcher(method):\n        @wraps(method)\n        def method_call(*m_args, **m_kw):\n            patched_obj = patch_object(obj, attr_name, patched_value)\n            try:\n                return method(*m_args, **m_kw)\n            finally:\n                patched_obj.restore()\n        return method_call\n    return patcher\n\nclass patched_context(object):\n    \n    def __init__(self, obj, attr_name, patched_value):\n        \n        \n        \n        \n        \n        self.patched_object = patch_object(obj, attr_name, patched_value)\n    \n    def __enter__(self):\n        return self.patched_object\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.patched_object.restore()\n\ndef patch_object(obj, attr_name, patched_value):\n    \n    if isinstance(obj, (str, unicode)):\n        obj_path = adjusted_path = obj\n        done = False\n        exc = None\n        at_top_level = False\n        while not done:\n            try:\n                obj = __import__(adjusted_path)\n                done = True\n            except ImportError:\n                \n                \n                \n                adjusted_path = adjusted_path.rsplit('.', 1)[0]\n                if not exc:\n                    exc = sys.exc_info()\n                if at_top_level:\n                    \n                    \n                    etype, val, tb = exc\n                    raise etype, val, tb\n                if not adjusted_path.count('.'):\n                    at_top_level = True\n        for part in obj_path.split('.')[1:]:\n            obj = getattr(obj, part)\n\n    handle = PatchHandler(obj, attr_name)\n    handle.patch(patched_value)\n    return handle\n\n\nclass NonExistant(object):\n    \n\n\nclass PatchHandler(object):\n    \n    def __init__(self, orig_object, attr_name):\n        self.orig_object = orig_object\n        self.attr_name = attr_name\n        self.proxy_object = None\n        self.orig_value, self.is_local = self._get_original(self.orig_object,\n                                                            self.attr_name)\n        self.getter_class, self.getter = self._handle_getter(self.orig_object,\n                                                             self.attr_name)\n    \n    def patch(self, patched_value):\n        \n        try:\n            if self.getter:\n                setattr(self.getter_class, self.attr_name, patched_value)\n            else:\n                setattr(self.orig_object, self.attr_name, patched_value)\n        except TypeError:\n            \n            proxy_name = 'fudge_proxy_%s_%s_%s' % (\n                                self.orig_object.__module__,\n                                self.orig_object.__name__,\n                                patched_value.__class__.__name__\n            )\n            self.proxy_object = type(proxy_name, (self.orig_object,),\n                                     {self.attr_name: patched_value})\n            mod = sys.modules[self.orig_object.__module__]\n            setattr(mod, self.orig_object.__name__, self.proxy_object)\n        \n    def restore(self):\n        \n        if self.proxy_object is None:\n            if self.getter:\n                setattr(self.getter_class, self.attr_name, self.getter)\n            elif self.is_local:\n                setattr(self.orig_object, self.attr_name, self.orig_value)\n            else:\n                \n                delattr(self.orig_object, self.attr_name)\n        else:\n            setattr(sys.modules[self.orig_object.__module__],\n                    self.orig_object.__name__,\n                    self.orig_object)\n\n    def _find_class_for_attr(self, cls, attr):\n        if attr in cls.__dict__:\n            return cls\n        else:\n            for base in cls.__bases__:\n                if self._find_class_for_attr(base, attr) is not NonExistant:\n                    return base\n            return NonExistant\n\n    def _get_original(self, orig_object, name):\n        try:\n            value = orig_object.__dict__[name]\n            is_local = True\n        except (AttributeError, KeyError):\n            value = getattr(orig_object, name, NonExistant)\n            is_local = False\n        if value is NonExistant:\n            raise AttributeError(\n                    \"%s does not have the attribute %r\" % (orig_object, name))\n        return value, is_local\n\n    def _get_exact_original(self, orig_object, name):\n        if hasattr(orig_object, '__dict__'):\n            if name not in orig_object.__dict__:\n                \n                \n                if hasattr(orig_object, '__class__'):\n                    cls = orig_object.__class__\n                    orig_object = self._find_class_for_attr(cls, name)\n        return orig_object\n\n    def _handle_getter(self, orig_object, name):\n        getter_class, getter = None, None\n        exact_orig = self._get_exact_original(orig_object, name)\n        try:\n            ob = exact_orig.__dict__[name]\n        except (AttributeError, KeyError):\n            pass\n        else:\n            if hasattr(ob, '__get__'):\n                getter_class = exact_orig\n                getter = ob\n        return getter_class, getter\n",
        "summary": "The provided Python code defines a set of utilities for patching objects and attributes during testing. It includes decorators like `patch`, context managers like `patched_context`, and functions to apply patches temporarily or permanently. The core functionality involves creating fake objects using the `fudge` library and replacing existing attributes with these fakes, allowing for controlled behavior in tests without affecting the original codebase."
    },
    {
        "code": "import json\nfrom pathlib import Path\n\nimport torch\nimport torchvision\nfrom transformers import RobertaTokenizerFast\n\nfrom .coco import ConvertCocoPolysToMask, ModulatedDetection, make_coco_transforms\n\nclass VQAv2Detection(ModulatedDetection):\n    pass\n\nclass VQAv2QuestionAnswering(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, ann_file, transforms, return_masks, return_tokens, tokenizer, ann_folder):\n        super(VQAv2QuestionAnswering, self).__init__(img_folder, ann_file)\n        self._transforms = transforms\n        self.prepare = ConvertCocoPolysToMask(return_masks, return_tokens, tokenizer=tokenizer)\n        with open(ann_folder / \"vqa2_answer2id.json\", \"r\") as f:\n            self.answer2id = json.load(f)\n        with open(ann_folder / \"vqa2_answer2id_by_type.json\", \"r\") as f:\n            self.answer2id_by_type = json.load(f)\n        self.type2id = {\"yes/no\": 0, \"number\": 1, \"other\": 2}\n\n    def __getitem__(self, idx):\n        img, target = super(VQAv2QuestionAnswering, self).__getitem__(idx)\n        image_id = self.ids[idx]\n        coco_img = self.coco.loadImgs(image_id)[0]\n        caption = coco_img[\"caption\"]\n        dataset_name = coco_img[\"dataset_name\"]\n        questionId = coco_img[\"questionId\"]\n        target = {\"image_id\": image_id, \"annotations\": target, \"caption\": caption}\n        img, target = self.prepare(img, target)\n        if self._transforms is not None:\n            img, target = self._transforms(img, target)\n        target[\"dataset_name\"] = dataset_name\n        target[\"questionId\"] = questionId\n\n        if coco_img[\"answer\"] not in self.answer2id:\n            answer = \"unknown\"\n        else:\n            answer = coco_img[\"answer\"]\n\n        target[\"answer\"] = torch.as_tensor(self.answer2id[answer], dtype=torch.long)\n        target[\"answer_type\"] = torch.as_tensor(self.type2id[coco_img[\"answer_type\"]], dtype=torch.long)\n\n        \n        if coco_img[\"answer\"] not in self.answer2id_by_type[\"yes/no\"]:\n            answer = \"unknown\"\n        else:\n            answer = coco_img[\"answer\"]\n        target[\"answer_yes/no\"] = torch.as_tensor(\n            self.answer2id_by_type[\"yes/no\"][answer] if coco_img[\"answer_type\"] == \"yes/no\" else -100,\n            dtype=torch.long,\n        )\n\n        if coco_img[\"answer\"] not in self.answer2id_by_type[\"number\"]:\n            answer = \"unknown\"\n        else:\n            answer = coco_img[\"answer\"]\n        target[\"answer_number\"] = torch.as_tensor(\n            self.answer2id_by_type[\"number\"][answer] if coco_img[\"answer_type\"] == \"number\" else -100,\n            dtype=torch.long,\n        )\n\n        if coco_img[\"answer\"] not in self.answer2id_by_type[\"other\"]:\n            answer = \"unknown\"\n        else:\n            answer = coco_img[\"answer\"]\n        target[\"answer_other\"] = torch.as_tensor(\n            self.answer2id_by_type[\"other\"][answer] if coco_img[\"answer_type\"] == \"other\" else -100,\n            dtype=torch.long,\n        )\n\n        return img, target\n\n\ndef build(image_set, args):\n    \n    img_dir = Path(args.coco_img_path)\n    assert img_dir.exists(), f\"provided COCO img path {img_dir} does not exist\"\n\n    tokenizer = RobertaTokenizerFast.from_pretrained(args.text_encoder_type)\n\n    if args.do_qa:\n        \n        \n\n        if image_set == \"train\":\n            datasets = []\n            for imset in [\"train\", \"minival\"]:\n                ann_file = Path(args.vqa2_ann_path) / f\"finetune_vqa2_{imset}.json\"\n\n                datasets.append(\n                    VQAv2QuestionAnswering(\n                        img_dir / \"train2014\" if imset == \"train\" else img_dir / \"val2014\",\n                        ann_file,\n                        transforms=make_coco_transforms(image_set, cautious=True),\n                        return_masks=args.masks,\n                        return_tokens=True,\n                        tokenizer=tokenizer,\n                        ann_folder=Path(args.vqa2_ann_path),\n                    )\n                )\n\n            return torch.utils.data.ConcatDataset(datasets)\n        elif image_set == \"val\":\n            \n            ann_file = Path(args.vqa2_ann_path) / f\"finetune_vqa2_minival.json\"\n\n            return VQAv2QuestionAnswering(\n                img_dir / \"val2014\",\n                ann_file,\n                transforms=make_coco_transforms(image_set, cautious=True),\n                return_masks=args.masks,\n                return_tokens=True,\n                tokenizer=tokenizer,\n                ann_folder=Path(args.vqa2_ann_path),\n            )\n        elif image_set in [\"test\", \"testdev\", \"trainval\"]:\n            ann_file = Path(args.vqa2_ann_path) / f\"finetune_vqa2_{image_set}.json\"\n\n            return VQAv2QuestionAnswering(\n                img_dir / \"test2015\",\n                ann_file,\n                transforms=make_coco_transforms(\"val\", cautious=True),\n                return_masks=args.masks,\n                return_tokens=True,\n                tokenizer=tokenizer,\n                ann_folder=Path(args.vqa2_ann_path),\n            )\n\n        else:\n            assert False, f\"Unknown image set {image_set}\"\n",
        "summary": "The provided Python code defines a dataset class `VQAv2QuestionAnswering` for the Visual Question Answering (VQA) task using the COCO dataset. It includes methods to load images and annotations, preprocess them, and convert answers into numerical IDs based on their types (yes/no, number, other). The `build` function creates instances of this dataset for different subsets of the VQA 2.0 dataset, handling both training and validation scenarios."
    },
    {
        "code": "from assertpy import assert_that\nfrom httmock import HTTMock\n\nfrom sahyun_bot.commands.admin import Index, Rank\nfrom sahyun_bot.users_settings import UserRank\nfrom tests.mock_customsforge import customsforge\n\n\ndef test_require_admin(commander, hook):\n    for command in ['!lock', '!index', '!rank']:\n        with commander.executest(hook, command, 'goodlikebot'):\n            hook.assert_silent_failure()\n\n\ndef test_lock_unlock(commander, hook):\n    with commander.executest(hook, '!lock'):\n        hook.assert_success('Bot is now in ADMIN only mode')\n\n    \n    with commander.executest(hook, '!time', 'goodlikebot'):\n        hook.assert_silent_failure()\n\n    with commander.executest(hook, '!lock'):\n        hook.assert_success('Bot no longer in ADMIN only mode')\n\n    \n    with commander.executest(hook, '!time', 'goodlikebot'):\n        hook.assert_success()\n\n\ndef test_index(tl, hook):\n    with HTTMock(customsforge), Index(tl=tl).executest(hook):\n        hook.assert_success('CDLCs indexed')\n\n    tl.set_use_elastic(False)\n\n    with HTTMock(customsforge), Index(tl=tl).executest(hook):\n        hook.assert_failure('CDLCs could not be indexed')\n\n\ndef test_rank(users, hook):\n    with Rank(us=users).executest(hook, args=''):\n        hook.assert_failure('Try !rank RANK NICK')\n\n    with Rank(us=users).executest(hook, args='just_rank'):\n        hook.assert_failure('Try !rank RANK NICK')\n\n    with Rank(us=users).executest(hook, args='BAD_RANK goodlikebot'):\n        hook.assert_failure('BAD_RANK is not a valid rank')\n\n    with Rank(us=users).executest(hook, args='BAN goodlikebot'), users._manual('goodlikebot'):\n        hook.assert_success('goodlikebot is now BAN')\n        assert_that(users.rank('goodlikebot')).is_equal_to(UserRank.BAN)\n\n    users.set_use_elastic(False)\n\n    with Rank(us=users).executest(hook, args='ADMIN goodlikebot'):\n        hook.assert_failure('Rank could not be set')\n\n\ndef test_rank_shorthand(commander, hook):\n    with commander.executest(hook, '!ban goodlikebot'), commander._users._manual('goodlikebot'):\n        hook.assert_success('goodlikebot is now BAN')\n        assert_that(commander._users.rank('goodlikebot')).is_equal_to(UserRank.BAN)\n",
        "summary": "The provided Python code tests various functionalities of a bot, including requiring admin privileges for certain commands, locking and unlocking the bot to admin-only mode, indexing CDLCs, setting user ranks, and handling rank shorthand. It uses mocking and assertions to validate the expected behavior of these operations."
    },
    {
        "code": "import os\nimport base64\n\nfrom simpleutil.utils import digestutils\n\nfrom goperation.filemanager import LocalFile\nfrom goperation.manager.rpc.agent.application.taskflow.middleware import EntityMiddleware\nfrom goperation.manager.rpc.agent.application.taskflow.database import Database\nfrom goperation.manager.rpc.agent.application.taskflow.application import AppUpgradeFile\nfrom goperation.manager.rpc.agent.application.taskflow.application import AppLocalBackupFile\n\nfrom gogamechen3.api import gfile\n\n\nclass GogameMiddle(EntityMiddleware):\n\n    def __init__(self, entity, endpoint, objtype):\n        super(GogameMiddle, self).__init__(entity, endpoint)\n        self.objtype = objtype\n        self.databases = {}\n        self.waiter = None\n\n\nclass GogameDatabase(Database):\n    def __init__(self, **kwargs):\n        super(GogameDatabase, self).__init__(**kwargs)\n        self.database_id = kwargs.get('database_id')\n        self.source = kwargs.get('source')\n        self.rosource = kwargs.get('rosource')\n        self.subtype = kwargs.get('subtype')\n        self.ro_user = kwargs.get('ro_user')\n        self.ro_passwd = kwargs.get('ro_passwd')\n\n\nclass GogameAppFile(AppUpgradeFile):\n\n    def __init__(self, source, objtype, revertable=False, rollback=False,\n                 stream=None):\n        super(GogameAppFile, self).__init__(source, revertable, rollback)\n        self.objtype = objtype\n        self.stream = stream\n\n    def post_check(self):\n        gfile.check(self.objtype, self.file)\n\n    def clean(self):\n        if self.stream:\n            os.remove(self.file)\n\n    def prepare(self, middleware=None, timeout=None):\n        if self.stream:\n            if len(self.stream) > 5000:\n                raise ValueError(\"Strem over size\")\n            file_path = os.path.join('/tmp', '%s.zip' % self.source)\n            data = base64.b64decode(self.stream)\n            if digestutils.strmd5(data) != self.source:\n                raise ValueError('Md5 not match')\n            with open(file_path, 'wb') as f:\n                data = base64.b64decode(self.stream)\n                f.write(data)\n            self.localfile = LocalFile(file_path, self.source, len(data))\n        else:\n            self.localfile = middleware.filemanager.get(self.source, download=True, timeout=timeout)\n        try:\n            self.post_check()\n        except Exception:\n            localfile = self.localfile\n            self.localfile = None\n            if self.stream:\n                os.remove(localfile.path)\n            else:\n                middleware.filemanager.delete(self.source)\n            raise\n\n\nclass GogameAppBackupFile(AppLocalBackupFile):\n\n    def __init__(self, destination, objtype):\n        super(GogameAppBackupFile, self).__init__(destination,\n                                                  exclude=gfile.CompressConfAndLogExcluder(),\n                                                  topdir=False,\n                                                  native=True)\n        self.objtype = objtype\n\n    def post_check(self):\n        gfile.check(self.objtype, self.file)\n",
        "summary": "The provided Python code defines several classes related to file management and database operations within a game application. It includes middleware for handling entities, databases, and application files, with specific methods for preparing and cleaning up these files, as well as checking their integrity using MD5 hashes and base64 encoding."
    },
    {
        "code": "__all__ = ['parseargs', 'collect']\n\n'<users>\\n\\t<user>\\n\\t\\t<id>1</id>\\n\\t\\t<name>Fred</name>\\n\\t\\t<salary>500000</salary>\\n\\t</user>\\n\\t<user>\\n\\t\\t<id>1</id>\\n\\t\\t<name>ScienceCat</name>\\n\\t\\t<salary>500000</salary>\\n\\t</user>\\n\\t<user>\\n\\t\\t<id>1</id>\\n\\t\\t<name>Bob</name>\\n\\t\\t<salary>500000</salary>\\n\\t</user>\\n</users>'\nxmlex = '<users>\\n<user>\\n<id>1</id>\\n<name>Fred</name>\\n<salary>500000</salary>\\n</user>\\n<user>\\n<id>1</id>\\n<name>ScienceCat</name>\\n<salary>500000</salary>\\n</user>\\n<user>\\n<id>1</id>\\n<name>Bob</name>\\n<salary>500000</salary>\\n</user>\\n</users>'\nargex = 'cats=\"True and Sand\" true=\\'Cats two\\' sand=\"graval\"'\n\n\n\n\ndef parseargs(string:str):\n    \n    arg = {}\n    \n    \n    \n\n\n\n\n    \n    parts = string.split(' ')\n    bkey = ''\n    buffer = ''\n    end = '\"'\n    for part in parts:\n        if '=' in part:\n            key, vp = part.split('=')\n            if vp[0] in ('\"', \"'\"):\n                end = vp[0]\n            if vp.endswith(end):\n                arg[key] = vp[1:-1]\n            else:\n                bkey = key\n                buffer += vp\n        elif part.endswith(end):\n            buffer += ' '+part\n            arg[bkey] = buffer[1:-1]\n            bkey, buffer = '', ''\n        else:\n            buffer += ' '+part\n    return arg\n\ndef collect(string:str):\n    stack = []\n    top = []\n    stack.append(top)\n    i, j = 0, 0\n    class elementTag:\n        def __init__(self, label, xargs, empty=0):\n            self.label = label\n            self.xargs = xargs\n            self.empty = empty\n    while True:\n        ni\n        h\n        c\n        lable\n        xarg\n        emtpy\n        if not ni:\n            break\n        text = string[i:ni-1]\n        if not text.find('^ '):\n            top.append(text)\n        if empty == '/':\n            top.append(elementTag(label, parseargs(xarg), 1))\n        elif c == '': \n            top = [elementTag(label, parseargs(xarg))]\n            stack.append(top)\n        else:\n            toclose = stack\n            if len(stack) < 1:\n                error(f'Nothing to close with {label}.')\n            elif toclose.label == label:\n                pass\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code defines two functions, `parseargs` and `collect`, which are designed to parse arguments from a string and collect elements from an XML-like structure, respectively. The `parseargs` function processes a string containing key-value pairs separated by equals signs and enclosed in quotes or single quotes, storing them in a dictionary. The `collect` function appears to be intended for parsing an XML-like structure but is incomplete and lacks implementation details."
    },
    {
        "code": "import json\nimport logging\nfrom sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\nfrom iotfunctions import bif\nfrom ai.functions import SimpleAnomaly\nfrom iotfunctions.metadata import EntityType\nfrom iotfunctions.db import Database\nfrom iotfunctions.enginelog import EngineLogging\nfrom custom import settings\n\nEngineLogging.configure_console_logging(logging.DEBUG)\n\n\n\n\nwith open('credentials_dev2.json', encoding='utf-8') as F:\n    credentials = json.loads(F.read())\n\n\n\n\n\ndb = Database(credentials = credentials)\ndb_schema = None \n\n\nentity_name = 'Turbines'\n\ndb_schema = 'dash100462'\n\ndb.drop_table(entity_name, schema = db_schema)\n\nentity = EntityType(entity_name,db,\n                    Column('TURBINE_ID',String(50)),\n                    Column('TEMPERATURE',Float()),\n                    Column('PRESSURE',Float()),\n                    Column('VOLUME', Float()),\n                    SimpleAnomaly(request='GET',\n                                    url='internal_test',\n                                    output_item = 'http_preload_done'),\n                    bif.PythonExpression(expression='df[\"TEMPERATURE\"]*df[\"PRESSURE\"]',\n                                         output_name = 'VOLUME'),\n                    **{\n                      '_timestamp' : 'evt_timestamp',\n                      '_db_schema' : db_schema\n                      })\n\nentity.register(raise_error=False)\ndb.register_functions([SimpleAnomaly])\n\n\n\nentity.exec_local_pipeline()\n\n\n\ndf = db.read_table(table_name=entity_name, schema=db_schema)\nprint(df.head())\n",
        "summary": "The Python script sets up a database connection using credentials from a JSON file, defines an entity type with specific columns and functions for anomaly detection and data transformation, registers the entity and its functions in the database, executes a local pipeline to process data, and reads the processed data from the database to print the first few rows."
    },
    {
        "code": "from __future__ import unicode_literals\n\nimport datetime\nimport time\n\nimport factory\n\nfrom . import models\nfrom .utils import smart_bytes\n\nSPAM_BODY = \n\nVIRUS_BODY = \n\n\nclass MaddrFactory(factory.django.DjangoModelFactory):\n    \n\n    class Meta:\n        model = models.Maddr\n        django_get_or_create = (\"email\", )\n\n    id = factory.Sequence(lambda n: n)  \n    email = factory.Sequence(lambda n: \"user_{}@domain.test\".format(n))\n    domain = \"test.domain\"\n\n\nclass MsgsFactory(factory.django.DjangoModelFactory):\n    \n\n    class Meta:\n        model = models.Msgs\n\n    mail_id = factory.Sequence(lambda n: \"mailid{}\".format(n))\n    secret_id = factory.Sequence(lambda n: smart_bytes(\"id{}\".format(n)))\n    sid = factory.SubFactory(MaddrFactory)\n    client_addr = \"127.0.0.1\"\n    originating = \"Y\"\n    dsn_sent = \"N\"\n    subject = factory.Sequence(lambda n: \"Test message {}\".format(n))\n    time_num = factory.LazyAttribute(lambda o: int(time.time()))\n    time_iso = factory.LazyAttribute(\n        lambda o: datetime.datetime.fromtimestamp(o.time_num).isoformat())\n    size = 100\n\n\nclass MsgrcptFactory(factory.django.DjangoModelFactory):\n    \n\n    class Meta:\n        model = models.Msgrcpt\n\n    rseqnum = 1\n    is_local = \"Y\"\n    bl = \"N\"\n    wl = \"N\"\n    mail = factory.SubFactory(MsgsFactory)\n    rid = factory.SubFactory(MaddrFactory)\n\n\nclass QuarantineFactory(factory.django.DjangoModelFactory):\n    \n\n    class Meta:\n        model = models.Quarantine\n\n    chunk_ind = 1\n    mail = factory.SubFactory(MsgsFactory)\n\n\ndef create_quarantined_msg(rcpt, sender, rs, body, **kwargs):\n    \n    msgrcpt = MsgrcptFactory(\n        rs=rs,\n        rid__email=rcpt,\n        rid__domain=\"com.test\",  \n        mail__sid__email=smart_bytes(sender),\n        mail__sid__domain=\"\",  \n        **kwargs\n    )\n    QuarantineFactory(\n        mail=msgrcpt.mail,\n        mail_text=smart_bytes(SPAM_BODY.format(rcpt=rcpt, sender=sender))\n    )\n    return msgrcpt\n\n\ndef create_spam(rcpt, sender=\"spam@evil.corp\", rs=\" \"):\n    \n    body = SPAM_BODY.format(rcpt=rcpt, sender=sender)\n    body += \"f\u00f3\u00f3 b\u00e1r\"\n    return create_quarantined_msg(\n        rcpt, sender, rs, body, bspam_level=999.0, content=\"S\")\n\n\ndef create_virus(rcpt, sender=\"virus@evil.corp\", rs=\" \"):\n    \n    return create_quarantined_msg(rcpt, sender, rs, VIRUS_BODY, content=\"V\")\n",
        "summary": "The provided Python code defines several factories using the `factory_boy` library to generate instances of Django models related to email messages and their handling. It includes factories for `Maddr`, `Msgs`, `Msgrcpt`, and `Quarantine`. Additionally, it contains functions to create quarantined messages, specifically for spam and virus scenarios, with customizable parameters such as recipient, sender, and body content."
    },
    {
        "code": "from __future__ import print_function, division\nimport os, sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nfrom astropy import log\nfrom os import path\nfrom glob import glob\nfrom subprocess import check_call\nimport shutil\nfrom astropy.table import Table\nfrom astropy.io import fits\n\nfrom nicer.values import *\nfrom nicer.plotutils import plot_light_curve\n\ndef runcmd(cmd):\n    \n    log.info('CMD: '+\" \".join(cmd))\n    os.system(\" \".join(cmd))\n    \n    \n    \n\n\n\ntry:\n    check_call('nicerversion',env=os.environ)\nexcept:\n    print(\"You need to initialize FTOOLS/HEASOFT first (e.g., type 'heainit')!\", file=sys.stderr)\n    exit()\n\n\n\ngticolumns = path.join(datadir,'gti_columns.txt')\ngtiheader = path.join(datadir,'gti_header.txt')\n\nif not os.path.isfile(gtiheader) or not os.path.isfile(gticolumns):\n    log.error('The files gti_header.txt or gti_columns.txt are missing.  Check the {} directory'.format(os.path.abspath(datadir)))\n    exit()\n\n\ndesc = \nparser = argparse.ArgumentParser(description = desc)\nparser.add_argument(\"startmet\", help=\"Starting MET for GTI\", type=float)\nparser.add_argument(\"stopmet\", help=\"Ending MET for GTI\", type=float)\nparser.add_argument(\"--gtiname\", help=\"Name of output GTI FITS file (default gti.fits)\", default=\"gti.fits\")\n\nargs = parser.parse_args()\n\n\n\nimport tempfile\nfp = tempfile.NamedTemporaryFile()\nfp.write('{0} {1}\\n'.format(args.startmet,args.stopmet))\nfp.flush()\n\n\n\nlog.info(\"Making the GTI file gti.fits from the GTI data textfile\")\ncmd = ['ftcreate', '{}'.format(gticolumns), fp.name, args.gtiname, 'headfile={}'.format(gtiheader), 'extname=\"GTI\"', 'clobber=yes']\nruncmd(cmd)\n\nfp.close()\n",
        "summary": "The provided Python script initializes FTOOLS/HEASOFT if not already done, checks for necessary files in a specified directory, and creates a General Time Interval (GTI) FITS file using user-provided start and stop MET values. It utilizes the `argparse` module to handle command-line arguments and the `astropy` library for file operations and logging."
    },
    {
        "code": "import os\nfrom datetime import datetime\nfrom flask import Flask, render_template, flash, safe_join, send_file\nfrom flask_user import login_required, current_user\nfrom werkzeug.utils import secure_filename\nfrom pygate_grpc.client import PowerGateClient\nfrom deplatformr.models.filecoin_models import Ffs, Files, Logs\nfrom deplatformr import app, db\n\n\n@app.route('/filecoin-files')\n@login_required\ndef filecoin_files():\n\n    files = Files.query.filter_by(user_id=current_user.id).all()\n\n    return render_template(\"filecoin/filecoin-files.html\", files=files, breadcrumb=\"Filecoin / Files\")\n\n\n@app.route(\"/filecoin-download/<cid>\", methods=[\"GET\"])\n@login_required\ndef filecoin_download(cid):\n    \n\n    \n    file = Files.query.filter_by(CID=cid, user_id=current_user.id).first()\n    ffs = Ffs.query.get(file.ffs_id)\n\n    try:\n        \n        powergate = PowerGateClient(app.config[\"POWERGATE_ADDRESS\"])\n        data_ = powergate.ffs.get(file.CID, ffs.token)\n\n        \n        \n        user_data = app.config[\"USER_DATA_DIR\"]\n        if not os.path.exists(user_data):\n            os.makedirs(user_data)\n\n        print(user_data)\n        \n        user_dir = os.path.join(\n            user_data, str(current_user.id) + \"-\" + current_user.username)\n        if not os.path.exists(user_dir):\n            os.makedirs(user_dir)\n        print(user_dir)\n        \n        filecoin_dir = os.path.join(user_dir, \"filecoin/downloads\")\n        if not os.path.exists(filecoin_dir):\n            os.makedirs(filecoin_dir)\n        print(filecoin_dir)\n        with open(os.path.join(filecoin_dir, file.file_name), \"wb\") as out_file:\n            \n            for data in data_:\n                out_file.write(data)\n\n        \n        safe_path = safe_join(\"../\" + filecoin_dir, file.file_name)\n        print(safe_path)\n\n        \n        return send_file(safe_path, as_attachment=True)\n\n        \n\n    except Exception as e:\n        \n        flash(\"failed to download '{}' from Filecoin. {}\".format(\n            file.file_name, e), \"alert-danger\")\n\n        \n        event = Logs(\n            timestamp=datetime.now().replace(microsecond=0),\n            event=\"Download ERROR: \"\n            + file.file_name\n            + \" CID: \"\n            + file.CID\n            + \" \"\n            + str(e),\n            user_id=current_user.id,\n        )\n        db.session.add(event)\n        db.session.commit()\n\n    files = Files.query.filter_by(user_id=current_user.id).all()\n\n    return render_template(\"filecoin/filecoin-files.html\", files=files, breadcrumb=\"Filecoin / Files\")\n\n\n@ app.route('/filecoin-wallets')\n@ login_required\ndef filecoin_wallets():\n    \n\n    powergate = PowerGateClient(app.config[\"POWERGATE_ADDRESS\"])\n\n    try:\n        ffs = Ffs.query.filter_by(user_id=current_user.id).one()\n    except:\n        flash(\"No wallets created yet.\", \"alert-danger\")\n        return render_template(\"filecoin/filecoin-wallets.html\", wallets=None, breadcrumb=\"Filecoin / Wallets\")\n\n    wallets = []\n\n    addresses = powergate.ffs.addrs_list(ffs.token)\n\n    for address in addresses.addrs:\n        balance = powergate.wallet.balance(address.addr)\n        wallets.append(\n            {\n                \"ffs\": ffs.ffs_id,\n                \"name\": address.name,\n                \"address\": address.addr,\n                \"type\": address.type,\n                \"balance\": str(balance.balance),\n            }\n        )\n\n    return render_template(\"filecoin/filecoin-wallets.html\", wallets=wallets, breadcrumb=\"Filecoin / Wallets\")\n",
        "summary": "The provided Python code is a Flask application that manages Filecoin files and wallets. It includes routes for listing filecoin files, downloading files from Filecoin using PowerGate, and viewing wallet balances through the Filecoin network. The application uses Flask-User for authentication and interacts with a database to store user-specific data such as files and logs."
    },
    {
        "code": "from setuptools import find_packages, setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name='msnexport',\n    version='0.1',\n    license=\"MIT\",\n    classifiers=[\"Programming Language :: Python :: 3.7\"],\n    author='Charles Marceau',\n    author_email='charlesmarceau3@gmail.com',\n    description='Export your old xml MSN history to pdf.',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/charles-marceau/msnexport',\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[\n        'beautifulsoup4',\n        'click',\n        'lxml',\n        'reportlab'\n    ],\n    entry_points=\n)\n",
        "summary": "This Python script sets up a package named `msnexport` for exporting old XML MSN chat history to PDF, using setuptools for packaging and distribution. It includes dependencies on libraries such as BeautifulSoup4, click, lxml, and reportlab, and provides metadata like author information and project URL in the setup configuration."
    },
    {
        "code": "from mythic_payloadtype_container.MythicCommandBase import *\nimport json\nfrom mythic_payloadtype_container.MythicRPC import *\nimport base64\n\nclass InjectArguments(TaskArguments):\n\n    def __init__(self, command_line):\n        super().__init__(command_line)\n        self.args = {\n            \"template\": CommandParameter(name=\"Payload Template\", type=ParameterType.Payload, supported_agents=[\"apollo\"], supported_agent_build_parameters={\"apollo\": {\"output_type\": \"Shellcode\"}}),\n            \"pid\": CommandParameter(name=\"PID\", type=ParameterType.Number),\n        }\n\n    errorMsg = \"Missing required parameter: {}\"\n\n    async def parse_arguments(self):\n        if (self.command_line[0] != \"{\"):\n            raise Exception(\"Inject requires JSON parameters and not raw command line.\")\n        self.load_args_from_json_string(self.command_line)\n\n\nclass InjectCommand(CommandBase):\n    cmd = \"inject\"\n    needs_admin = False\n    help_cmd = \"inject (modal popup)\"\n    description = \"Inject agent shellcode into a remote process.\"\n    version = 2\n    is_exit = False\n    is_file_browse = False\n    is_process_list = False\n    is_download_file = False\n    is_upload_file = False\n    is_remove_file = False\n    script_only = True\n    author = \"@djhohnstein\"\n    argument_class = InjectArguments\n    attackmapping = [\"T1055\"]\n\n\n    async def shinject_completed(self, task: MythicTask, subtask: dict = None, subtask_group_name: str = None) -> MythicTask:\n        task.status = MythicStatus.Completed\n        return task\n\n    async def create_tasking(self, task: MythicTask) -> MythicTask:\n        temp = await MythicRPC().execute(\"get_payload\",\n                                         payload_uuid=task.args.get_arg(\"template\"))\n        gen_resp = await MythicRPC().execute(\"create_payload_from_uuid\",\n                                             task_id=task.id,\n                                             payload_uuid=task.args.get_arg('template'),\n                                             new_description=\"{}'s injection into PID {}\".format(task.operator, str(task.args.get_arg(\"pid\"))))\n        if gen_resp.status == MythicStatus.Success:\n            \n            while True:\n                resp = await MythicRPC().execute(\"get_payload\", \n                                                 payload_uuid=gen_resp.response[\"uuid\"],\n                                                 get_contents=True)\n                if resp.status == MythicStatus.Success:\n                    if resp.response[\"build_phase\"] == 'success':\n                        b64contents = resp.response[\"contents\"]\n                        pe = base64.b64decode(b64contents)\n                        if len(pe) > 1 and pe[:2] == b\"\\x4d\\x5a\":\n                            raise Exception(\"Inject requires a payload of Raw output, but got an executable.\")\n                        \n                        task.display_params = \"payload '{}' into PID {}\".format(temp.response[\"tag\"], task.args.get_arg(\"pid\"))\n                        response = await MythicRPC().execute(\"create_subtask\", parent_task_id=task.id,\n                                         command=\"shinject\", params_dict={\"PID\": task.args.get_arg(\"pid\"), \"Shellcode File ID\": resp.response[\"file\"][\"agent_file_id\"]},\n                                         subtask_callback_function=\"shinject_completed\")\n                        task.status = MythicStatus.Processed\n                        break\n                    elif resp.response[\"build_phase\"] == 'error':\n                        raise Exception(\"Failed to build new payload: \" + resp.response[\"error_message\"])\n                    else:\n                        await asyncio.sleep(1)\n        else:\n            raise Exception(\"Failed to build payload from template {}\".format(task.args.get_arg(\"template\")))\n        return task\n\n    async def process_response(self, response: AgentResponse):\n        pass\n",
        "summary": "The provided Python code defines a command for injecting shellcode into a remote process using an Apollo agent. It includes argument parsing, task creation, and handling the completion of the injection subtask. The command requires JSON parameters specifying the payload template and target PID, and it ensures that the injected content is in raw shellcode format before proceeding with the injection."
    },
    {
        "code": "import numpy as np\nfrom .base import Price\n\nclass GBM(Price):\n    \n\n    def __init__(self, T=1., sigma1=0.02, sigma2=0.01, s1=1., s2=1.,\n                 drift1=0., drift2=0., n=100):\n        self.sigma1 = sigma1\n        self.sigma2 = sigma2\n        self.drift1 = drift1\n        self.drift2 = drift2\n        self.n = n\n        self.s1 = s1\n        self.s2 = s2\n        self.T = T\n\n    def generate(self):\n        dt1 = self.sigma1 ** 2 * self.T / self.n\n        dt2 = self.sigma2 ** 2 * self.T / self.n\n\n        bm1 = np.r_[[0.], np.sqrt(dt1) * np.random.randn(self.n - 1).cumsum()]\n        bm2 = np.r_[[0.], np.sqrt(dt2) * np.random.randn(self.n - 1).cumsum()]\n\n        path = np.c_[np.linspace(0, self.T, self.n), bm1, bm2]\n        path[:, 1] = np.exp((self.drift1 - self.sigma1 ** 2 / 2.) * path[:, 0] + self.sigma1 * path[:, 1])\n        path[:, 2] = np.exp((self.drift2 - self.sigma2 ** 2 / 2.) * path[:, 0] + self.sigma2 * path[:, 2])\n\n        path[:, 1] *= self.s1\n        path[:, 2] *= self.s2\n\n\n        return path\n",
        "summary": "The provided Python code defines a class `GBM` that inherits from a base class `Price`. This class is designed to generate paths for two geometric Brownian motions (GBMs) with specified parameters such as time horizon, volatility, drift, and initial values. The `generate` method computes the paths by simulating Brownian motion increments, applying exponential growth based on drift and volatility, and scaling the results by initial values."
    },
    {
        "code": "import math\nimport csv\nimport json\nimport os\nimport shutil\nfrom sys import argv\nfrom datetime import datetime\nfrom django.utils.encoding import smart_str, smart_unicode\nfrom operator import itemgetter\nfrom elo_classes import *\nfrom elo import *\n\n\n\n\n\nelos_boys = {}\nelos_girls = {}\nentries_boys = {}\nentries_girls = {}\n_DEFELO = 1500.0\n\n\ndef do_elo(data, meetName, meetDate, gender):\n    if gender == \"female\":\n        elos = elos_girls\n        entries = entries_girls\n    elif gender == \"male\":\n        elos = elos_boys\n        entries = entries_boys\n\n    \n\n    meet = Meet()\n    meet.competitors = []\n    for dat in data:\n        name = dat[0]\n        place = int(dat[1])\n        school = dat[2]\n        ath = Athlete(name, school)\n        if ath in elos:\n            elo = float(elos.get(ath))\n            meet.addCompetitor(name, place, elo, school)\n        else:\n            \n            meet.addCompetitor(name, place, _DEFELO, school)\n    calculateElo(meet.competitors)\n\n    \n\n    for runner in meet.competitors:\n        ather = Athlete(runner.name, runner.school)\n        elos[ather] = runner.elo\n        if ather in entries:\n            res_list = entries.get(ather)\n            res_list.append([meetName, meetDate, runner.elo])\n            entries[ather] = res_list\n        else:\n            entries[ather] = [[meetName, meetDate, runner.elo]]\n\n\ndef align_data(filename):\n    filex = open(filename)\n    sort = []\n    for json_string in filex:\n        parsed = json.loads(json_string)\n        results = parsed[\"results\"]\n        kill = False\n        locs = parsed[\"meetLocation\"]\n        a_date = parsed[\"meetDate\"]\n        exact_date = datetime.strptime(a_date[0], \"%A, %B %d, %Y\")\n        for loc in locs:\n            if loc == u'Collegiate' or loc == u'MS':\n                kill = True\n        for result in results:\n            if result.keys() == [u'maleResults'] or [u'femaleResults']:\n                static = result.values()\n                events = static[0]\n                for event in events:\n                    data = []\n                    data.append(exact_date)\n                    data.append(parsed['meetName'])\n                    if result.keys() == [u'maleResults']:\n                        data.append(\"male\")\n                    elif result.keys() == [u'femaleResults']:\n                        data.append(\"female\")\n                    places = []\n                    details = event[u'eventDetails']\n                    for detail in details:\n                        killx = False\n                        ath_detail_List = []\n                        ath_detail_List.append(\n                                        smart_str(detail[u'resultName']))\n                        if detail[u'resultPlace'] == \" \" or \\\n                                detail[u'resultPlace'] == u' ':\n                            killx = True\n                        else:\n                            ath_detail_List.append(detail[u'resultPlace'])\n                        ath_detail_List.append(\n                                        smart_str(detail[u'resultSchool']))\n                        if killx is False:\n                            places.append(ath_detail_List)\n                    data.append(places)\n                    if kill is False:\n                        sort.append(data)\n    sortx = sorted(sort, key=itemgetter(0))\n    return sortx\n\n\ndef write_ath(entries):\n    if entries == entries_boys:\n        path = \"./meets/boys\"\n    elif entries == entries_girls:\n        path = \"./meets/girls\"\n    if not os.path.exists(\"./meets/\"):\n        os.mkdir(\"./meets/\")\n    if not os.path.exists(path):\n        os.mkdir(path + \"/\")\n    for ath in entries:\n        school_path = os.path.join(path, ath.school)\n        ath_path = os.path.join(school_path, ath.name + \".csv\")\n        filename = \"%s.csv\" % ath.name\n        with open((filename), \"w\") as fp:\n            a = csv.writer(fp, delimiter=',')\n            a.writerows(entries[ath])\n        if os.path.exists(school_path):\n            shutil.move(filename, ath_path)\n        else:\n            os.mkdir(school_path)\n            shutil.move(filename, ath_path)\n\n\ndef write_elo(elos, gender):\n    if gender == \"male\":\n        name = \"athlete_elo_boys.csv\"\n    elif gender == \"female\":\n        name = \"athlete_elo_girls.csv\"\n    with open((name), \"w\") as fp:\n        a = csv.writer(fp, delimiter=',')\n        a.writerows(elos)\n\n\ndef main():\n    \n    events = align_data(argv[1])\n    count = 0\n    for event in events:\n        \n        if len(event) == 4:\n            print count\n            count += 1\n            name = smart_str(event[1][0])\n            date = event[0]\n            gender = event[2]\n            do_elo(event[3], name, date, gender)\n    \n    sorted_boys = sorted(elos_boys.items(), key=itemgetter(1))\n    sorted_girls = sorted(elos_girls.items(), key=itemgetter(1))\n    write_elo(sorted_boys, \"male\")\n    write_elo(sorted_girls, \"female\")\n    write_ath(entries_girls)\n    write_ath(entries_boys)\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The Python script processes athlete performance data from JSON files, calculates Elo ratings based on competition results, and writes the updated Elo ratings and athlete records to CSV files. It handles both male and female athletes separately and organizes the data by school and individual athlete."
    },
    {
        "code": "from boxx import *\nfrom boxx import deg2rad, np, pi\n\nimport bpy\nimport random\n\n\ndef set_cam_pose(cam_radius=1, cam_deg=45, cam_x_deg=None, cam=None):\n    cam_rad = deg2rad(cam_deg)\n    if cam_x_deg is None:\n        cam_x_deg = random.uniform(0, 360)\n    cam_x_rad = deg2rad(cam_x_deg)\n    z = cam_radius * np.sin(cam_rad)\n    xy = (cam_radius ** 2 - z ** 2) ** 0.5\n    x = xy * np.cos(cam_x_rad)\n    y = xy * np.sin(cam_x_rad)\n    cam = cam or bpy.data.objects[\"Camera\"]\n    cam.location = x, y, z\n    cam.rotation_euler = pi / 2 - cam_rad, 0.1, pi / 2 + cam_x_rad\n    cam.scale = (0.1,) * 3\n    return cam\n\n\ndef set_cam_intrinsic(cam, intrinsic_K, hw=None):\n    \n    if hw is None:\n        scene = bpy.context.scene\n        hw = scene.render.resolution_y, scene.render.resolution_x\n    near = lambda x, y=0, eps=1e-5: abs(x - y) < eps\n    assert near(intrinsic_K[0][1], 0)\n    assert near(intrinsic_K[1][0], 0)\n    h, w = hw\n    f_x = intrinsic_K[0][0]\n    f_y = intrinsic_K[1][1]\n    c_x = intrinsic_K[0][2]\n    c_y = intrinsic_K[1][2]\n\n    cam = cam.data\n    cam.shift_x = -(c_x / w - 0.5)\n    cam.shift_y = (c_y - 0.5 * h) / w\n\n    cam.lens = f_x / w * cam.sensor_width\n\n    pixel_aspect = f_y / f_x\n    scene.render.pixel_aspect_x = 1.0\n    scene.render.pixel_aspect_y = pixel_aspect\n\n\ndef remove_useless_data():\n    \n    for block in bpy.data.meshes:\n        if block.users == 0:\n            bpy.data.meshes.remove(block)\n\n    for block in bpy.data.materials:\n        if block.users == 0:\n            bpy.data.materials.remove(block)\n\n    for block in bpy.data.textures:\n        if block.users == 0:\n            bpy.data.textures.remove(block)\n\n    for block in bpy.data.images:\n        if block.users == 0:\n            bpy.data.images.remove(block)\n\n\ndef clear_all():\n    [\n        bpy.data.objects.remove(obj)\n        for obj in bpy.data.objects\n        if obj.type in (\"MESH\", \"LIGHT\", \"CURVE\")\n    ]\n    remove_useless_data()\n\n\ndef set_shading_mode(mode=\"SOLID\", screens=[]):\n    \n    screens = screens if screens else [bpy.context.screen]\n    for s in screens:\n        for spc in s.areas:\n            if spc.type == \"VIEW_3D\":\n                spc.spaces[0].shading.type = mode\n                break  \n\n\ndef add_stage(size=2, transparency=False):\n    \n    import bpycv\n\n    bpy.ops.mesh.primitive_cube_add(size=size, location=(0, 0, -size / 2))\n    stage = bpy.context.active_object\n    stage.name = \"stage\"\n    with bpycv.activate_obj(stage):\n        bpy.ops.rigidbody.object_add()\n        stage.rigid_body.type = \"PASSIVE\"\n        if transparency:\n            stage.rigid_body.use_margin = True\n            stage.rigid_body.collision_margin = 0.04\n            stage.location.z -= stage.rigid_body.collision_margin\n\n            material = bpy.data.materials.new(\"transparency_stage_bpycv\")\n            material.use_nodes = True\n            material.node_tree.nodes.clear()\n            with bpycv.activate_node_tree(material.node_tree):\n                bpycv.Node(\"ShaderNodeOutputMaterial\").Surface = bpycv.Node(\n                    \"ShaderNodeBsdfPrincipled\", Alpha=0\n                ).BSDF\n            stage.data.materials.append(material)\n    return stage\n\n\nif __name__ == \"__main__\":\n    pass\n",
        "summary": "The provided Python code is a collection of functions for manipulating Blender, a 3D modeling and animation software. It includes functionalities to set camera poses, intrinsic properties, remove unused data, clear all objects, change shading modes, and add a stage with optional transparency. The code utilizes libraries such as `boxx`, `bpy`, and `random` to perform these operations, demonstrating how to interact with Blender's API for automation and customization of 3D scenes programmatically."
    },
    {
        "code": "import pandas as pd\n\nfrom .entity import CatalogEntity\nfrom .repository.dataset_repo import get_dataset_repo\nfrom .repository.variable_repo import get_variable_repo\nfrom .repository.constants import VARIABLE_FILTER\nfrom .summary import variable_describe, head, tail, counts, quantiles, top_values, histogram\n\n\n_DESCRIPTION_LENGTH_LIMIT = 50\n\n\nclass Variable(CatalogEntity):\n    \n    _entity_repo = get_variable_repo()\n\n    @property\n    def datasets(self):\n        \n        return get_dataset_repo().get_all({VARIABLE_FILTER: self.id})\n\n    @property\n    def name(self):\n        \n        return self.data['name']\n\n    @property\n    def description(self):\n        \n        return self.data['description']\n\n    @property\n    def column_name(self):\n        \n        return self.data['column_name']\n\n    @property\n    def db_type(self):\n        \n        return self.data['db_type']\n\n    @property\n    def dataset(self):\n        \n        return self.data['dataset_id']\n\n    @property\n    def agg_method(self):\n        \n        return self.data['agg_method']\n\n    @property\n    def variable_group(self):\n        \n        return self.data['variable_group_id']\n\n    @property\n    def summary(self):\n        \n        return self.data['summary_json']\n\n    @property\n    def project_name(self):\n        project, _, _, _ = self.id.split('.')\n        return project\n\n    @property\n    def schema_name(self):\n        _, schema, _, _ = self.id.split('.')\n        return schema\n\n    @property\n    def dataset_name(self):\n        _, _, dataset, _ = self.id.split('.')\n        return dataset\n\n    def describe(self, autoformat=True):\n        \n        FLOAT_FORMAT = 'display.float_format'\n\n        if autoformat:\n            pd.set_option(FLOAT_FORMAT, lambda x: '%.3f' % x)\n\n        data = self.data['summary_json']\n        return variable_describe(data)\n\n    def head(self):\n        \n        data = self.data['summary_json']\n        return head(self.__class__, data)\n\n    def tail(self):\n        \n        data = self.data['summary_json']\n        return tail(self.__class__, data)\n\n    def counts(self):\n        \n        data = self.data['summary_json']\n        return counts(data)\n\n    def quantiles(self):\n        \n        data = self.data['summary_json']\n        return quantiles(data)\n\n    def top_values(self):\n        \n        data = self.data['summary_json']\n        return top_values(data)\n\n    def histogram(self):\n        \n        data = self.data['summary_json']\n        return histogram(data)\n\n    def __repr__(self):\n        descr = self.description\n\n        if descr and len(descr) > _DESCRIPTION_LENGTH_LIMIT:\n            descr = descr[0:_DESCRIPTION_LENGTH_LIMIT] + '...'\n\n        return \"<{classname}.get('{entity_id}')> \n               .format(classname=self.__class__.__name__, entity_id=self._get_print_id(), descr=descr)\n",
        "summary": "The `Variable` class in Python extends the `CatalogEntity` and provides properties and methods to interact with variable data, including retrieving datasets associated with a variable, accessing various attributes like name, description, and database type, and generating summaries such as descriptive statistics, histograms, and top values. It also includes methods to format and display these summaries in a user-friendly manner."
    },
    {
        "code": "from libcrypto import hamming_distance\nfrom libcrypto import split_blocks\nfrom libcrypto import xor\nfrom libcrypto import freq_score\n\nfrom base64 import b64decode\nfrom operator import itemgetter\n\n\ndef main():\n\n    file64 = \"\"\n    for line in open(\"../assets/inputS1C6.txt\",\"r\"):\n        file64 += line.rstrip()\n\n    file = bytearray(b64decode(file64))\n\n    distances = []\n    for keysize in range(2,40):\n        dist = 0\n        sample_size = 10\n        for ctr in range(0, sample_size):\n            b1 = bytearray(file[(keysize*ctr):(keysize*(ctr+1))])\n            b2 = bytearray(file[(keysize*(ctr+1)):(keysize*(ctr+2))])\n\n            dist += hamming_distance(b1, b2) / float(keysize)\n        dist /= sample_size\n        distances.append([keysize, dist])\n\n    distances = sorted(distances,key=itemgetter(1))[:1]\n\n\n    print(\"Possible Solutions...\\n\")\n    for key in distances:\n        passphrase = \"\"\n        key = key[0]\n        blocks = split_blocks(key,file)\n\n        transposed_blocks = []\n        for idx in range(0,key):\n            tblock = bytearray()\n            for block in blocks:\n                try:\n                    tblock.append(block[idx])\n                except IndexError:\n                    pass\n            transposed_blocks.append(tblock)\n\n        for block in transposed_blocks:\n            bytekeys = []\n            for i in range(1,int(\"ff\",16)):\n\n                xor_bytes = xor(bytearray(bytes({i})),block)\n\n                try:\n                    xor_string = xor_bytes.decode(\"ascii\")\n                    bytekeys.append([i,xor_string,freq_score(xor_string)])\n                except UnicodeDecodeError:\n                    next\n\n            bytekeys.sort(key=lambda x: x[2], reverse=True)\n            bkey = bytekeys[:1][0]\n            passphrase += chr(bkey[0])\n\n        print(\"Key:{0}\\n\".format(passphrase))\n\n        print(xor(bytearray(passphrase.encode()),bytearray(file)).decode())\n\nif __name__ == \"__main__\":\n    main()",
        "summary": "The provided Python script is designed to break a repeating-key XOR cipher by analyzing the Hamming distance between blocks of ciphertext, determining the most likely key size, and then transposing and frequency scoring each block to deduce the original passphrase. Once the key is found, it decrypts the entire ciphertext and prints the resulting plaintext."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom six.moves import xrange  \nimport tensorflow as tf\n\n\n\n\nIMAGE_SIZE = 24\n\n\nNUM_CLASSES = 10\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n\n\ndef read_cifar10(filename_queue):\n  \n\n  class CIFAR10Record(object):\n    pass\n  result = CIFAR10Record()\n\n  \n  \n  \n  label_bytes = 1  \n  result.height = 32\n  result.width = 32\n  result.depth = 3\n  image_bytes = result.height * result.width * result.depth\n  \n  \n  record_bytes = label_bytes + image_bytes\n\n  \n  \n  \n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  result.key, value = reader.read(filename_queue)\n\n  \n  record_bytes = tf.decode_raw(value, tf.uint8)\n\n  \n  result.label = tf.cast(\n      tf.strided_slice(record_bytes, [0], [label_bytes], [1]), tf.int32)\n\n  \n  \n  depth_major = tf.reshape(\n      tf.strided_slice(record_bytes, [label_bytes],\n                       [label_bytes + image_bytes], [1]),\n      [result.depth, result.height, result.width])\n  \n  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n\n  return result\n\n\ndef _generate_image_and_label_batch(image, label, min_queue_examples,\n                                    batch_size, shuffle):\n  \n  \n  \n  num_preprocess_threads = 16\n  if shuffle:\n    images, label_batch = tf.train.shuffle_batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size,\n        min_after_dequeue=min_queue_examples)\n  else:\n    images, label_batch = tf.train.batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size)\n\n  \n  tf.summary.image('images', images)\n\n  return images, tf.reshape(label_batch, [batch_size])\n\n\ndef distorted_inputs(data_dir, batch_size):\n  \n  filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n               for i in xrange(1, 6)]\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError('Failed to find file: ' + f)\n\n  \n  filename_queue = tf.train.string_input_producer(filenames)\n\n  \n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  \n  \n\n  \n  distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n\n  \n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  \n  \n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n\n  \n  float_image = tf.image.per_image_standardization(distorted_image)\n\n  \n  float_image.set_shape([height, width, 3])\n  read_input.label.set_shape([1])\n\n  \n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n                           min_fraction_of_examples_in_queue)\n  print ('Filling queue with %d CIFAR images before starting to train. '\n         'This will take a few minutes.' % min_queue_examples)\n\n  \n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=True)\n\n\ndef inputs(eval_data, data_dir, batch_size):\n  \n  if not eval_data:\n    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n                 for i in xrange(1, 6)]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n  else:\n    filenames = [os.path.join(data_dir, 'test_batch.bin')]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError('Failed to find file: ' + f)\n\n  \n  filename_queue = tf.train.string_input_producer(filenames)\n\n  \n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  \n  \n  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n                                                         width, height)\n\n  \n  float_image = tf.image.per_image_standardization(resized_image)\n\n  \n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(num_examples_per_epoch *\n                           min_fraction_of_examples_in_queue)\n\n  \n  if eval_data:\n    read_input.label.set_shape((1,))\n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=False)\n",
        "summary": "The provided Python code defines functions to read and preprocess CIFAR-10 dataset images using TensorFlow. It includes methods for both training and evaluation data, applying various transformations like random cropping, flipping, brightness adjustment, and contrast enhancement during training to improve model performance."
    },
    {
        "code": "from logging import getLogger\n\ntry:\n    from galaxy.model import Job\n    job_states = Job.states\nexcept ImportError:\n    \n    from pulsar.util import enum\n    job_states = enum(RUNNING='running', OK='complete', QUEUED='queued', ERROR=\"failed\")\nfrom ..job import BaseJobExec\n\nlog = getLogger(__name__)\n\nargmap = {\n    'memory': '-M',  \n    'cores': '-n',\n    'queue': '-q',\n    'working_dir': '-cwd',\n    'project': '-P'\n}\n\n\nclass LSF(BaseJobExec):\n\n    def __init__(self, **params):\n        self.params = {}\n        for k, v in params.items():\n            self.params[k] = v\n\n    def job_script_kwargs(self, ofile, efile, job_name):\n        scriptargs = {'-o': ofile,\n                      '-e': efile,\n                      '-J': job_name}\n\n        \n        for k, v in self.params.items():\n            if k == 'plugin':\n                continue\n            try:\n                if k == 'memory':\n                    \n                    scriptargs['-R'] = \"\\\"rusage[mem=%s]\\\"\" % v\n                if not k.startswith('-'):\n                    k = argmap[k]\n                scriptargs[k] = v\n            except Exception:\n                log.warning('Unrecognized long argument passed to LSF CLI plugin: %s' % k)\n\n        \n        template_scriptargs = ''\n        for k, v in scriptargs.items():\n            template_scriptargs += '\n        return dict(headers=template_scriptargs)\n\n    def submit(self, script_file):\n        \n        \n        \n        \n        return \"bsub <%s | awk '{ print $2}' | sed 's/[<>]//g'\" % script_file\n\n    def delete(self, job_id):\n        return 'bkill %s' % job_id\n\n    def get_status(self, job_ids=None):\n        return \"bjobs -a -o \\\"id stat\\\" -noheader\"  \n\n    def get_single_status(self, job_id):\n        return \"bjobs -o stat -noheader \" + job_id\n\n    def parse_status(self, status, job_ids):\n        \n        rval = {}\n        for line in status.splitlines():\n            job_id, state = line.split()\n            if job_id in job_ids:\n                \n                rval[job_id] = self._get_job_state(state)\n        return rval\n\n    def parse_single_status(self, status, job_id):\n        if not status:\n            \n            \n            \n            \n            \n            \n            log.warning(\"Job id '%s' not found LSF status check\" % job_id)\n            return job_states.OK\n        return self._get_job_state(status)\n\n    def get_failure_reason(self, job_id):\n        return \"bjobs -l \" + job_id\n\n    def parse_failure_reason(self, reason, job_id):\n        \n        \n        \n        for line in reason.splitlines():\n            if \"TERM_MEMLIMIT\" in line:\n                from galaxy.jobs import JobState\n                return JobState.runner_states.MEMORY_LIMIT_REACHED\n        return None\n\n    def _get_job_state(self, state):\n        \n        \n        \n        try:\n            return {\n                'EXIT': job_states.ERROR,\n                'RUN': job_states.RUNNING,\n                'PEND': job_states.QUEUED,\n                'DONE': job_states.OK,\n                'PSUSP': job_states.ERROR,\n                'USUSP': job_states.ERROR,\n                'SSUSP': job_states.ERROR,\n                'UNKWN': job_states.ERROR,\n                'WAIT': job_states.QUEUED,\n                'ZOMBI': job_states.ERROR\n            }.get(state)\n        except KeyError:\n            raise KeyError(\"Failed to map LSF status code [%s] to job state.\" % state)\n\n\n__all__ = ('LSF',)\n",
        "summary": "This Python code defines a class `LSF` that extends `BaseJobExec` for managing jobs on an LSF (Load Sharing Facility) cluster. It handles job submission, deletion, status checking, and parsing of job states and failure reasons, using command-line interface commands like `bsub`, `bkill`, and `bjobs`."
    },
    {
        "code": "from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nfrom keras import backend as K\r\nfrom keras.layers import Input, Add, Dense, Activation, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D, \\\r\n    AveragePooling2D, TimeDistributed, BatchNormalization, Dropout\r\nfrom keras import layers\r\nfrom keras_frcnn.RoiPoolingConv import RoiPoolingConv\r\n\r\n\r\ndef get_weight_path():\r\n    return os.path.join(\"pretrain\", 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')\r\n\r\ndef get_img_output_length(width, height):\r\n    def get_output_length(input_length):\r\n        \n        input_length += 6\r\n        \n        filter_sizes = [7, 3, 1, 1]\r\n        stride = 2\r\n        for filter_size in filter_sizes:\r\n            input_length = (input_length - filter_size + stride) // stride\r\n        return input_length\r\n    return get_output_length(width), get_output_length(height) \r\n\r\nBASE_WEIGTHS_PATH = (\r\n    'https://github.com/keras-team/keras-applications/'\r\n    'releases/download/densenet/')\r\nDENSENET121_WEIGHT_PATH = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet121_weights_tf_dim_ordering_tf_kernels.h5')\r\nDENSENET121_WEIGHT_PATH_NO_TOP = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')\r\nDENSENET169_WEIGHT_PATH = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet169_weights_tf_dim_ordering_tf_kernels.h5')\r\nDENSENET169_WEIGHT_PATH_NO_TOP = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5')\r\nDENSENET201_WEIGHT_PATH = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet201_weights_tf_dim_ordering_tf_kernels.h5')\r\nDENSENET201_WEIGHT_PATH_NO_TOP = (\r\n    BASE_WEIGTHS_PATH +\r\n    'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5')\r\n\r\n\r\ndef dense_block(x, blocks, name):\r\n    \r\n    for i in range(blocks):\r\n        x = conv_block(x, 32, name=name + '_block' + str(i + 1))\r\n    return x\r\n\r\n\r\ndef transition_block(x, reduction, name):\r\n    \r\n    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\r\n    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\r\n                                  name=name + '_bn')(x)\r\n    x = layers.Activation('relu', name=name + '_relu')(x)\r\n    x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\r\n                      use_bias=False,\r\n                      name=name + '_conv')(x)\r\n    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool', padding='same')(x)\r\n    return x\r\n\r\n\r\ndef conv_block(x, growth_rate, name):\r\n    \r\n    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\r\n    x1 = layers.BatchNormalization(axis=bn_axis,\r\n                                   epsilon=1.001e-5,\r\n                                   name=name + '_0_bn')(x)\r\n    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\r\n    x1 = layers.Conv2D(4 * growth_rate, 1,\r\n                       use_bias=False,\r\n                       name=name + '_1_conv')(x1)\r\n    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\r\n                                   name=name + '_1_bn')(x1)\r\n    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\r\n    x1 = layers.Conv2D(growth_rate, 3,\r\n                       padding='same',\r\n                       use_bias=False,\r\n                       name=name + '_2_conv')(x1)\r\n    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\r\n    return x\r\n\r\ndef nn_base(input_tensor=None,\r\n             blocks=[6, 12, 24, 16],\r\n             include_top=False,\r\n             weights='imagenet',\r\n             input_shape=None,\r\n             pooling=None,\r\n             classes=1000,\r\n             **kwargs):\r\n    \r\n\r\n    if not (weights in {'imagenet', None} or os.path.exists(weights)):\r\n        raise ValueError('The `weights` argument should be either '\r\n                         '`None` (random initialization), `imagenet` '\r\n                         '(pre-training on ImageNet), '\r\n                         'or the path to the weights file to be loaded.')\r\n\r\n    if weights == 'imagenet' and include_top and classes != 1000:\r\n        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\r\n                         ' as true, `classes` should be 1000')\r\n\r\n    \n    if K.image_dim_ordering() == 'th':\r\n        input_shape = (3, None, None)\r\n    else:\r\n        input_shape = (None, None, 3)\r\n\r\n    if input_tensor is None:\r\n        img_input = Input(shape=input_shape)\r\n    else:\r\n        if not K.is_keras_tensor(input_tensor):\r\n            img_input = Input(tensor=input_tensor, shape=input_shape)\r\n        else:\r\n            img_input = input_tensor\r\n\r\n    if K.image_dim_ordering() == 'tf':\r\n        bn_axis = 3\r\n    else:\r\n        bn_axis = 1\r\n\r\n    x = ZeroPadding2D((3, 3))(img_input)\r\n    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(x)\r\n    x = layers.BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\r\n    x = layers.Activation('relu', name='conv1/relu')(x)\r\n\n    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\r\n\r\n    x = dense_block(x, blocks[0], name='conv2')\r\n    x = transition_block(x, 0.5, name='pool2')\r\n    x = dense_block(x, blocks[1], name='conv3')\r\n    x = transition_block(x, 0.5, name='pool3')\r\n    x = dense_block(x, blocks[2], name='conv4')\r\n    \n\r\n    \n    \n\r\n    x = layers.BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\r\n    x = layers.Activation('relu', name='relu')(x)\r\n    \r\n    return x\r\n\r\ndef rpn(base_layers,num_anchors):\r\n\r\n    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\r\n\r\n    x_class = Convolution2D(num_anchors, (1, 1), padding=\"same\", activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\r\n    x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\r\n\r\n    return [x_class, x_regr, base_layers]\r\n\r\ndef classifier(base_layers, input_rois, num_rois, nb_classes = 21, trainable=False):\r\n\r\n    \n\r\n    if K.backend() == 'tensorflow':\r\n        pooling_regions = 14\r\n        input_shape = (num_rois,14,14,1024) \n    elif K.backend() == 'theano':\r\n        pooling_regions = 7\r\n        input_shape = (num_rois,4096,7,7)\r\n\r\n    \n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\r\n    \r\n    out_roi_pool = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(out_roi_pool)\r\n    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\r\n    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\r\n    out = TimeDistributed(Dropout(0.5))(out)\r\n    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\r\n    out = TimeDistributed(Dropout(0.5))(out)\r\n\r\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\r\n    \n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\r\n\r\n    return [out_class, out_regr]\r\n",
        "summary": "The provided Python code defines a neural network architecture using Keras, specifically for the Faster R-CNN object detection model. It includes functions to load pre-trained weights from ImageNet, define the backbone network with dense blocks and transition layers, and construct the Region Proposal Network (RPN) and classifier for detecting objects in images."
    },
    {
        "code": "import gevent.monkey\r\ngevent.monkey.patch_all()\r\n\r\nimport argparse\r\nimport json\r\nimport re\r\nimport string\r\nimport socket\r\nimport urllib.parse\r\nimport webbrowser\r\n\r\nimport flask\r\nimport gevent.pool\r\nimport gevent.pywsgi\r\nimport requests\r\n\r\napp = flask.Flask(__name__, template_folder='.')\r\n\r\nconfig = {\r\n    'host': '127.0.0.1',\r\n    'port': 48230,\r\n    'blur': 60,\r\n    'no_censor': False,\r\n    'no_colors': False,\r\n    'no_browser': False,\r\n    'width': 5,\r\n    'height': 3,\r\n}\r\n\r\n@app.route('/')\r\ndef index():\r\n    return flask.render_template('main.html.tpl', config=config)\r\n\r\n@app.route('/users')\r\ndef list_users():\r\n    page = requests.get('https://chaturbate.com/').text\r\n    user_names = re.findall(r'<div class=\"details\">\\s*<div class=\"title\">\\s*<a\\s*href=\\s*\"/(.+?)/\">', page)\r\n    pool = gevent.pool.Pool(10)\r\n    stream_urls = pool.map(get_user_stream, user_names)\r\n    users = [{'name': name, 'stream': stream} for name, stream in zip(user_names, stream_urls) if stream]\r\n    return flask.Response(json.dumps(users), mimetype='application/json')\r\n\r\ndef get_user_stream(user):\r\n    page = requests.get('https://chaturbate.com/{}/?use_html_chat=1'.format(user)).text\r\n    match = re.search('<video .*src=\"(.+?)\"', page)\r\n    if not match:\r\n        return ''\r\n    playlist = match.group(1).replace(' ', '%20')\r\n    playlist = urllib.parse.urlparse(playlist)\r\n    server_number = re.sub('[^\\d]', '', playlist.netloc.split('.')[0])\r\n    return '/streams/{}{}'.format(server_number, playlist.path)\r\n\r\n@app.route('/streams/<int:server>/<path:path>')\r\ndef get_stream_file(server, path):\r\n    full_url = 'http://edge{}.stream.highwebmedia.com:1935/{}'.format(server, path)\r\n    resp = requests.get(full_url, stream=True)\r\n    content = resp.iter_content(chunk_size=2 ** 16)\r\n    status_code = resp.status_code\r\n    content_type = resp.headers.get('content-type', 'application/octet-stream')\r\n    return flask.Response(content, status=status_code, mimetype=content_type)\r\n\r\ndef get_args_config():\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n    parser.add_argument('--host', default=config['host'], help=\r\n        'The host the server will bind to. Use 0.0.0.0 for all interfaces.')\r\n    parser.add_argument('--port', type=int, default=config['port'], help=\r\n        'The port the server will bind to.')\r\n    parser.add_argument('--width', type=int, default=config['width'], help=\r\n        'Number of elements used from left to right.')\r\n    parser.add_argument('--height', type=int, default=config['height'], help=\r\n        'Number of elements used from top to bottom.')\r\n    parser.add_argument('--blur', type=int, default=config['blur'], help=\r\n        'Pixels used in the gaussian blur.')\r\n    parser.add_argument('--no-censor', action='store_true', help=\r\n        'Disables gaussian blur.')\r\n    parser.add_argument('--no-colors', action='store_true', help=\r\n        'Disables hue rotation.')\r\n    return vars(parser.parse_args())\r\n\r\ndef make_socket():\r\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    sock.bind((config['host'], config['port']))\r\n    sock.listen(5)\r\n    return sock\r\n\r\ndef start_wsgi_server():\r\n    sock = make_socket()\r\n    server = gevent.pywsgi.WSGIServer(sock, app)\r\n    server.serve_forever()\r\n\r\nif __name__ == '__main__':\r\n    try:\r\n        config = get_args_config()\r\n        print('Listening on http://{}:{}'.format(config['host'], config['port']))\r\n        start_wsgi_server()\r\n    except Exception as e:\r\n        print(repr(e))\r\n        print('Press enter to exit')\r\n        _ = input()\r\n",
        "summary": "This Python script sets up a web server using Flask and Gevent, capable of fetching live stream URLs from Chaturbate, processing them, and serving them over HTTP. It includes configuration options for host, port, and various display settings, as well as error handling to keep the server running even if exceptions occur."
    },
    {
        "code": "a = float(input())\nb = float(input())\nc = float(input())\nd = float(input())\ne = float(input())\nf = float(input())\nif a == 0 and b == 0 and c == 0 and d == 0 and e == 0 and f == 0:\n    print(5)\nelif a * d == b * c and a * f != c * e:\n    print(0)\nelif a == 0 and b == 0 and e != 0:\n    print(0)\nelif c == 0 and d == 0 and f != 0:\n    print(0)\nelif a == 0 and c == 0 and b * f != d * e:\n    print(0)\nelif b == 0 and d == 0 and a * f != c * e:\n    print(0)\nelif a * d == b * c and a * f == c * e:\n    if b == 0 and d == 0:\n        if a != 0 and c != 0:\n            print(3, e / a)\n        elif a == 0:\n            if e == 0:\n                print(3, f / c)\n        elif c == 0:\n            if f == 0:\n                print(3, e / a)\n    elif a == 0 and c == 0:\n        if b != 0:\n            print(4, e / b)\n        elif d != 0:\n            print(4, f / d)\n    elif b != 0:\n        print(1, -a / b, e / b)\n    elif d != 0:\n        print(1, -c / d, f / d)\nelse:\n    x = (e * d - b * f) / (a * d - b * c)\n    y = (a * f - e * c) / (a * d - b * c)\n    print(2, x, y)\n",
        "summary": "The Python code takes six floating-point numbers as input and performs a series of conditional checks to determine the nature of the system of linear equations they represent. Depending on whether the inputs lead to a unique solution, no solution, or infinitely many solutions, it prints either 5, 0, or specific values for x and y along with an indicator of the type of solution."
    },
    {
        "code": "import os\nimport numpy as np\n\n\n\n\ndef lamost_filepath(planid, mjd, spid, fiberid, dirpath=\"\", extname=\".fits\"):\n    \n\n    \n    if np.isscalar(planid):\n        planid = planid.strip()\n    else:\n        planid = [_.strip() for _ in planid]\n\n    if dirpath == \"\" or dirpath is None:\n        \n        if np.isscalar(mjd):\n            \n            return \"spec-%05d-%s_sp%02d-%03d%s\" \\\n                   % (mjd, planid, spid, fiberid, extname)\n        else:\n            \n            return np.array([\"spec-%05d-%s_sp%02d-%03d%s\" %\n                             (mjd[i], planid[i], spid[i], fiberid[i], extname)\n                             for i in range(len(mjd))])\n    else:\n        \n        if not dirpath[-1] == os.path.sep:\n            dirpath += os.path.sep\n\n        if np.isscalar(mjd):\n            \n            return \"%s%s%sspec-%05d-%s_sp%02d-%03d%s\" \\\n                   % (dirpath, planid, os.path.sep,\n                      mjd, planid, spid, fiberid, extname)\n        else:\n            \n            return np.array([\"%s%s%sspec-%05d-%s_sp%02d-%03d%s\" %\n                             (dirpath, planid[i], os.path.sep, mjd[i],\n                              planid[i], spid[i], fiberid[i], extname)\n                             for i in range(len(mjd))])\n\n\ndef _test_lamost_filepath():\n    \n    print(lamost_filepath(\"GAC_061N46_V3\", 55939, 7, 78))\n    print(lamost_filepath(\"GAC_061N46_V3\", 55939, 7, 78, \"/\"))\n    print(lamost_filepath(\"GAC_061N46_V3\", 55939, 7, 78, \"/pool\"))\n    print(lamost_filepath(\"GAC_061N46_V3\", 55939, 7, 78, \"/pool/\"))\n\n\ndef sdss_filepath(plate, mjd, fiberid, dirpath=\"\", extname=\".fits\"):\n    \n\n    if dirpath == \"\" or dirpath is None:\n        \n        if np.isscalar(mjd):\n            \n            return \"spec-%04d-%05d-%04d%s\" % (plate, mjd, fiberid, extname)\n        else:\n            \n            return np.array([\"spec-%04d-%05d-%04d%s\" %\n                             (plate[i], mjd[i], fiberid[i], extname)\n                             for i in range(len(mjd))])\n    else:\n        \n        if not dirpath[-1] == os.path.sep:\n            dirpath += os.path.sep\n\n        if np.isscalar(mjd):\n            \n            return \"%s%04d%sspec-%04d-%05d-%04d%s\" \\\n                   % (dirpath, plate, os.path.sep,\n                      plate, mjd, fiberid, extname)\n        else:\n            \n            return np.array([\"%s%04d%sspec-%04d-%05d-%04d%s\" %\n                             (dirpath, plate[i], os.path.sep, plate[i],\n                              mjd[i], fiberid[i], extname)\n                             for i in range(len(mjd))])\n\n\ndef _test_sdss_filepath():\n    print(sdss_filepath(2238, 52059, 1, \"/\"))\n\n\nif __name__ == \"__main__\":\n    print(\"\")\n    print(\"@Cham: start to test the module ...\")\n    print(\"\")\n    print(\"@Cham: testing \"\"lamost_filepath\"\" ...\")\n    _test_lamost_filepath()\n    _test_sdss_filepath()\n    print(\"@Cham: OK\")\n",
        "summary": "The provided Python code defines two functions, `lamost_filepath` and `sdss_filepath`, which generate file paths for astronomical data files based on specific parameters such as plan ID, MJD (Modified Julian Date), fiber ID, and optionally a directory path. The functions handle both scalar and array inputs for these parameters, returning either a single string or an array of strings representing the file paths. Additionally, there are test functions `_test_lamost_filepath` and `_test_sdss_filepath` to demonstrate how these functions can be used and to verify their correctness."
    },
    {
        "code": "class BaseDownsizing:\n\n    def __init__(self, raw_file_f, raw_file_r=None):\n        self.raw_file_f = raw_file_f\n        self.raw_file_f = raw_file_f\n        self._downsized_f = None\n        if raw_file_r:\n            self.raw_file_r = raw_file_r\n            self.raw_file_r = raw_file_r\n            self._downsized_r = None\n\n    def downsize_single(self):\n        \n        return self.raw_file_f\n\n    def downsize_pair_uncompressed(self):\n        \n        return self.raw_file_f, self.raw_file_r\n\n    def downsize_pair_gzip(self):\n        \n        return self.raw_file_f, self.raw_file_r\n\n    @property\n    def downsized_pair_uncompressed(self):\n        if getattr(self, \"._downsized_f\", None) is None:\n            self._downsized_f, self_downsized_r = self.downsize_pair()\n            self.raw_file_f = self._downsized_f\n            self.raw_file_r = self._downsized_r\n        return self._downsized_f, self._downsized_r\n\n    @property\n    def downsized_pair_gzip(self):\n        if getattr(self, \"._downsized_f\", None) is None:\n            self._downsized_f, self_downsized_r = self.downsize_pair()\n            self.raw_file_f = self._downsized_f\n            self.raw_file_r = self._downsized_r\n        return self._downsized_f, self._downsized_r\n\n    @property\n    def downsized_single(self):\n        if getattr(self, \"._downsized_f\", None) is None:\n            self._downsized_f = self.downsize_single()\n            self.raw_file_f = self._downsized_f\n        return self._downsized_f\n",
        "summary": "The `BaseDownsizing` class provides methods to downsize single and paired files, either uncompressed or compressed with gzip. It includes properties that cache the downsized results for efficient reuse."
    },
    {
        "code": "import re\n\n\nfrom noc.core.script.base import BaseScript\nfrom noc.sa.interfaces.igetinterfacestatus import IGetInterfaceStatus\n\n\nclass Script(BaseScript):\n    name = \"Angtel.Topaz.get_interface_status\"\n    interface = IGetInterfaceStatus\n    cache = True\n\n    rx_port = re.compile(\n        r\"^(?P<port>(?:Fa|Gi|Te|Po)\\S+)\\s+\\S+\\s+\\S+\\s+\\S+\\s+\\S+\\s+\\S+\\s+\"\n        r\"(?P<oper_status>Up|Down|Not Present)\",\n        re.MULTILINE | re.IGNORECASE,\n    )\n\n    def execute_cli(self, interface=None):\n        r = []\n        v = self.cli(\"show interfaces status\", cached=True)\n        for match in self.rx_port.finditer(v):\n            if (interface is not None) and (interface == match.group(\"port\")):\n                return [\n                    {\"interface\": match.group(\"port\"), \"status\": match.group(\"oper_status\") == \"Up\"}\n                ]\n            r += [{\"interface\": match.group(\"port\"), \"status\": match.group(\"oper_status\") == \"Up\"}]\n        return r\n",
        "summary": "This Python script, part of the NOC (Network Operations Center) framework, is designed to retrieve and parse interface status information from a device running Angtel Topaz software. It uses regular expressions to extract port names and their operational statuses from the output of a CLI command, returning a list of dictionaries with each dictionary containing an interface name and its up/down status."
    },
    {
        "code": "import re\n\nfrom pygments.lexer import RegexLexer, include, bygroups\nfrom pygments.token import Text, Comment, Operator, Keyword, Name, Generic, \\\n    Literal\n\n__all__ = ['DiffLexer', 'DarcsPatchLexer', 'WDiffLexer']\n\n\nclass DiffLexer(RegexLexer):\n    \n\n    name = 'Diff'\n    aliases = ['diff', 'udiff']\n    filenames = ['*.diff', '*.patch']\n    mimetypes = ['text/x-diff', 'text/x-patch']\n\n    tokens = {\n        'root': [\n            (r' .*\\n', Text),\n            (r'\\+.*\\n', Generic.Inserted),\n            (r'-.*\\n', Generic.Deleted),\n            (r'!.*\\n', Generic.Strong),\n            (r'@.*\\n', Generic.Subheading),\n            (r'([Ii]ndex|diff).*\\n', Generic.Heading),\n            (r'=.*\\n', Generic.Heading),\n            (r'.*\\n', Text),\n        ]\n    }\n\n    def analyse_text(text):\n        if text[:7] == 'Index: ':\n            return True\n        if text[:5] == 'diff ':\n            return True\n        if text[:4] == '--- ':\n            return 0.9\n\n\nclass DarcsPatchLexer(RegexLexer):\n    \n\n    name = 'Darcs Patch'\n    aliases = ['dpatch']\n    filenames = ['*.dpatch', '*.darcspatch']\n\n    DPATCH_KEYWORDS = ('hunk', 'addfile', 'adddir', 'rmfile', 'rmdir', 'move',\n                       'replace')\n\n    tokens = {\n        'root': [\n            (r'<', Operator),\n            (r'>', Operator),\n            (r'\\{', Operator),\n            (r'\\}', Operator),\n            (r'(\\[)((?:TAG )?)(.*)(\\n)(.*)(\\*\\*)(\\d+)(\\s?)(\\])',\n             bygroups(Operator, Keyword, Name, Text, Name, Operator,\n                      Literal.Date, Text, Operator)),\n            (r'(\\[)((?:TAG )?)(.*)(\\n)(.*)(\\*\\*)(\\d+)(\\s?)',\n             bygroups(Operator, Keyword, Name, Text, Name, Operator,\n                      Literal.Date, Text), 'comment'),\n            (r'New patches:', Generic.Heading),\n            (r'Context:', Generic.Heading),\n            (r'Patch bundle hash:', Generic.Heading),\n            (r'(\\s*)(%s)(.*\\n)' % '|'.join(DPATCH_KEYWORDS),\n                bygroups(Text, Keyword, Text)),\n            (r'\\+', Generic.Inserted, \"insert\"),\n            (r'-', Generic.Deleted, \"delete\"),\n            (r'.*\\n', Text),\n        ],\n        'comment': [\n            (r'[^\\]].*\\n', Comment),\n            (r'\\]', Operator, \"\n        ],\n        'specialText': [            \n            (r'\\n', Text, \"\n            (r'\\[_[^_]*_]', Operator),\n        ],\n        'insert': [\n            include('specialText'),\n            (r'\\[', Generic.Inserted),\n            (r'[^\\n\\[]+', Generic.Inserted),\n        ],\n        'delete': [\n            include('specialText'),\n            (r'\\[', Generic.Deleted),\n            (r'[^\\n\\[]+', Generic.Deleted),\n        ],\n    }\n\n\nclass WDiffLexer(RegexLexer):\n    \n\n    name = 'WDiff'\n    aliases = ['wdiff']\n    filenames = ['*.wdiff']\n    mimetypes = []\n\n    flags = re.MULTILINE | re.DOTALL\n\n    \n    \n    \n\n    ins_op = r\"\\{\\+\"\n    ins_cl = r\"\\+\\}\"\n    del_op = r\"\\[\\-\"\n    del_cl = r\"\\-\\]\"\n    normal = r'[^{}[\\]+-]+'  \n    tokens = {\n        'root': [\n            (ins_op, Generic.Inserted, 'inserted'),\n            (del_op, Generic.Deleted, 'deleted'),\n            (normal, Text),\n            (r'.', Text),\n        ],\n        'inserted': [\n            (ins_op, Generic.Inserted, '\n            (del_op, Generic.Inserted, '\n            (del_cl, Generic.Inserted, '\n\n            (ins_cl, Generic.Inserted, '\n            (normal, Generic.Inserted),\n            (r'.', Generic.Inserted),\n        ],\n        'deleted': [\n            (del_op, Generic.Deleted, '\n            (ins_op, Generic.Deleted, '\n            (ins_cl, Generic.Deleted, '\n\n            (del_cl, Generic.Deleted, '\n            (normal, Generic.Deleted),\n            (r'.', Generic.Deleted),\n        ],\n    }\n",
        "summary": "The provided Python code defines three custom lexers for parsing different types of text files using the Pygments library. The `DiffLexer` handles standard diff and patch files, identifying various line types like additions, deletions, and context lines. The `DarcsPatchLexer` is tailored for Darcs patch files, recognizing specific keywords and formatting elements. Lastly, the `WDiffLexer` parses wdiff output, distinguishing inserted and deleted text segments within a single line."
    },
    {
        "code": "from disco import Disco\n\nclass Config:\n\n    def __init__(self):\n        self._numero_discos = int(input(\"\\nInforme a quantidade de discos: \"))\n\n    def adiciona_discos(self, torre_inicial):\n        discos = self.add_disco()\n        for ix in range(self._numero_discos):\n            torre_inicial.empilha(discos[ix])\n\n    def add_disco(self):\n        discos = []\n        arquivo = open('disco.txt', 'r')\n\n        for linha in arquivo:\n            discos.append(Disco(int(linha)))\n\n        return discos\n    \n    def numero_discos(self):\n        return self._numero_discos\n\n\n    def status_torres(self, torres):\n        print('\\nNumero de discos: ' + str(self._numero_discos))\n\n        for torre in torres:\n            torre.to_string()\n",
        "summary": "The provided Python code defines a `Config` class to manage the configuration and operations related to a tower of Hanoi game, including initializing the number of disks, adding disks from a file, and displaying the status of the towers."
    },
    {
        "code": "from .Enviopack import Enviopack\nfrom .Auth.Auth import Auth\nfrom .Quote.Quote import Quote\nfrom .Pickings.Pickings import Pickings\nfrom .Orders.Orders import Orders\n\n__version__ = \"0.4.6\"\n__author__ = \"Federico Gobea\"\n\n",
        "summary": "The Python code imports various modules related to shipping and logistics, including Enviopack for package handling, Auth for authentication, Quote for pricing estimates, Pickings for inventory management, and Orders for order processing. It also includes metadata such as the version number and author's name."
    },
    {
        "code": "import os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'billsengine_31836.settings')\n\napplication = get_wsgi_application()\n",
        "summary": "The Python code sets the environment variable for Django settings and imports the WSGI application to run a Django project named \"billsengine_31836\"."
    },
    {
        "code": "import requests\nimport threading\nimport random\nimport json\n\nusernames = json.loads(open(\"usernames.json\", \"r\").read())\npassword = '%4B%65%6E%79%6F%6E%35%25' \nsiteurl = '192.168.122.61'\n\ndef run():\n    username = random.choice(usernames)\n    token = requests.get('http://' + siteurl + '/login/token.php?username=' + username + '&password=' + password + '&service=moodle_mobile_app').json()[\"token\"]\n    print(f'{token}')\n\nwhile True:\n    \n    ",
        "summary": "The provided Python code imports necessary libraries and reads usernames from a JSON file. It defines a function `run` that selects a random username, fetches a token using the selected username and a hardcoded password, and prints the token. The script enters an infinite loop, continuously calling the `run` function."
    },
    {
        "code": "import pytest\n\nfrom autogluon.core.space import Categorical\nfrom autogluon.vision._gluoncv import ObjectDetection\n\n\ndef get_dataset(path):\n    return ObjectDetection.Dataset.from_voc(path)\n\n\n@pytest.mark.skip(reason=\"ObjectDetector is not stable to test, and fails due to transient errors occasionally.\")\ndef test_object_detection_estimator():\n    dataset = get_dataset('https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip')\n    train_data, val_data, test_data = dataset.random_split(val_size=0.3, test_size=0.2, random_state=0)\n    task = ObjectDetection({'num_trials': 1, 'epochs': 1, 'batch_size': 4})\n    detector = task.fit(train_data)\n    assert task.fit_summary().get('valid_map', 0) > 0\n    test_result = detector.predict(test_data)\n\n\n@pytest.mark.skip(reason=\"ObjectDetector is not stable to test, and fails due to transient errors occasionally.\")\ndef test_object_detection_estimator_transfer():\n    dataset = get_dataset('https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip')\n    train_data, val_data, test_data = dataset.random_split(val_size=0.3, test_size=0.2, random_state=0)\n    task = ObjectDetection({'num_trials': 1, 'epochs': 1, 'transfer': Categorical('yolo3_darknet53_coco', 'ssd_512_resnet50_v1_voc'), 'estimator': 'ssd', 'batch_size': 4})\n    detector = task.fit(train_data)\n    assert task.fit_summary().get('valid_map', 0) > 0\n    test_result = detector.predict(test_data)\n",
        "summary": "The provided Python code uses the Autogluon library to perform object detection tasks. It includes two test functions, `test_object_detection_estimator` and `test_object_detection_estimator_transfer`, which utilize a dataset from VOC format and apply both default and transfer learning approaches using SSD models. Both tests are marked with `@pytest.mark.skip` due to instability and transient errors during execution."
    },
    {
        "code": "import asyncio\nimport pyppeteer\nimport time\nimport os\nimport random\nfrom exe_js import js1, js3, js4, js5\n\n\n\n\ndef input_time_random():\n    return random.randint(300, 500)\n\n\nasync def main():\n    print(\"in main \")\n    print(os.environ.get('PYPPETEER_CHROMIUM_REVISION'))\n    browser = await pyppeteer.launch(\n        executablePath=r\"D:\\A\\Desktop\\\u9879\u76ee+\u66f4\u65b0\\node_project\\chrome-win\\chrome-win\\chrome.exe\",\n        headless=False,\n        args=[\n            '--proxy-server=118.24.156.214:8118'\n            ],\n        timeout=30000)\n    page = await browser.newPage()\n    await page.setViewport({\"width\": 1000, \"height\": 780})\n    await page.setUserAgent(\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\")\n    await page.goto('http://httpbin.net/ip')\n    \n\n    content = await page.content()\n    cookies = await page.cookies()\n    await page.screenshot({'path': 'example.png'})\n\n    dimensions = await page.evaluate()\n\n    print(dimensions)\n\n    await browser.close()\n    return {'content': content, 'cookies': cookies}\n\n\nasyncio.get_event_loop().run_until_complete(main())",
        "summary": "The provided Python script uses the `pyppeteer` library to launch a headless Chrome browser, navigate to a specified URL, and perform various actions such as setting viewport size, user agent, taking screenshots, and retrieving page content and cookies. It also includes functionality to input random time delays and handle proxy settings."
    },
    {
        "code": "import os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'api_yamdb.settings')\n\napplication = get_wsgi_application()\n",
        "summary": "The Python script sets the environment variable for Django settings and imports the WSGI application to serve a Django project named 'api_yamdb'."
    },
    {
        "code": "import os\n\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTEMPLATE_DIR = os.path.join(BASE_DIR,'templates')\nSTATIC_DIR=os.path.join(BASE_DIR,'static')\nMEDIA_ROOT=os.path.join(BASE_DIR,'static')\n\n\n\n\n\nSECRET_KEY = '@k0\n\n\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'quiz',\n    'teacher',\n    'student',\n    'widget_tweaks',\n    'channels',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    \n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\nCSRF_COOKIE_SECURE=False\nROOT_URLCONF = 'bingo.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [TEMPLATE_DIR,],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'bingo.wsgi.application'\nASGI_APPLICATION = 'bingo.asgi.application'\n\nCHANNEL_LAYERS = {\n    \"default\": {\n        \"BACKEND\": \"channels_redis.core.RedisChannelLayer\",\n        \"CONFIG\": {\n            \"hosts\": [(\"localhost\", 6379)],\n        },\n\n    },\n}\n\n\n\n\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n\n\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n\n\n\nSTATIC_URL = '/static/'\n\nSTATICFILES_DIRS=[\nSTATIC_DIR,\n ]\n\nLOGIN_REDIRECT_URL='/afterlogin'\n\n\nEMAIL_BACKEND ='django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST = 'xyz.gmail.com'\nEMAIL_USE_TLS = True\nEMAIL_PORT = 587\nEMAIL_HOST_USER = 'from@gmail.com' \nEMAIL_HOST_PASSWORD = 'xyz' \n\n\n\n\n\nEMAIL_RECEIVING_USER = ['to@gmail.com'] \n",
        "summary": "This Python code sets up a Django project with configurations for directories, middleware, templates, database settings, authentication, and email backend. It includes paths for static files, template directories, and media storage, as well as security settings such as CSRF protection and debug mode. The project also integrates channels for real-time communication using Redis."
    },
    {
        "code": "from zenml.steps import BaseStepConfig\r\n\r\n\r\nclass PreTrainingConfigs(BaseStepConfig):\r\n    \n    ENV_NAME: str = \"BreakoutDeterministic-v4\"\r\n\r\n    WRITE_TENSORBOARD: bool = True\r\n    TENSORBOARD_DIR: str = \"tensorboard/\"\r\n\r\n    LEARNING_RATE: float = 0.00001\r\n    INPUT_SHAPE: tuple = (84, 84)\r\n    BATCH_SIZE: int = 32\r\n    SAVE_PATH = \"breakout-saves\"\r\n\r\n    USE_PER: bool = False\r\n    MEM_SIZE: int = 100\r\n\r\n    LOAD_FROM: str = None\r\n    LOAD_REPLAY_BUFFER: bool = True\r\n\r\n    MAX_NOOP_STEPS: int = 2000\r\n\r\n    TOTAL_FRAMES: int = 3000\r\n    FRAMES_BETWEEN_EVAL: int = 100000\r\n    MAX_EPISODE_LENGTH: int = 18000\r\n    EVAL_LENGTH: int = 10000\r\n    UPDATE_FREQ: int = 10000\r\n\r\n    PRIORITY_SCALE: float = 0.7  \n    CLIP_REWARD: bool = True  \n\r\n    UPDATE_FREQ: int = 4  \n    DISCOUNT_FACTOR: float = 0.99  \n\r\n    BATCH_SIZE: int = 32  \n    MIN_REPLAY_BUFFER_SIZE = 50000  \n\r\n    WRITE_TENSORBOARD: bool = True\r\n    EVAL_LENGTH: int = 10000  \n",
        "summary": "The `PreTrainingConfigs` class extends `BaseStepConfig` and defines various configuration parameters for a pre-training step, including environment settings, learning rates, batch sizes, and evaluation metrics. It also includes options for using prioritized experience replay and tensorboard logging."
    },
    {
        "code": "import educative.course1.stacks_queues.stack as s\n\ninput_data = [23, 60, 12, 42, 4, 97, 2]\nexpected_output_data = [2, 4, 12, 23, 42, 60, 97]\n\n\n\n\n\n\n\n\n\n\n\n\ndef sort_stack_1(stack):\n    result = s.Stack(stack.capacity, True) \n\n    while not stack.is_empty():\n        value = stack.pop()\n        if not result.is_empty() and value >= int(result.peek()):\n            result.push(value)\n        else:\n            while not result.is_empty() and value < int(result.peek()):\n                stack.push(result.pop())\n\n            result.push(value)\n\n    return result.prettify()\n\n\ndef main():\n    input_stack = s.Stack(len(input_data), True) \n    [input_stack.push(x) for x in input_data]\n\n    expected_output_stack = s.Stack(len(expected_output_data), True) \n    [expected_output_stack.push(x) for x in expected_output_data]\n\n    print(\"Input: \\n\" + str(input_stack.prettify()))\n    print(\"Expected: \\n\" + str(expected_output_stack.prettify()))\n    print(\"Output: \\n\" + str(sort_stack_1(input_stack)))\n\n\nif __name__ == '__main__':\n    main()\n",
        "summary": "The provided Python code defines a function `sort_stack_1` that sorts an input stack in ascending order using another stack. The `main` function initializes stacks with given input and expected output data, prints them, and then calls `sort_stack_1` to sort the input stack, displaying both the original and sorted stacks for comparison."
    },
    {
        "code": "from __future__ import print_function\r\nfrom __future__ import division\r\n",
        "summary": "The provided Python code imports features from the future versions of the `print` function and `division` to ensure compatibility and better syntax in current Python environments, enhancing readability and functionality."
    },
    {
        "code": "from glanceclient.v2 import client as glanceclient\n\nfrom keystoneauth1 import loading\nfrom keystoneauth1 import session\nfrom keystoneclient import client as keystoneclient\n\nfrom novaclient import client as novaclient\nfrom neutronclient.v2_0 import client as neutronclient\n\n\nclass OpenStackClients(object):\n    __keystone = None\n    __nova = None\n    __neutron = None\n    __glance = None\n\n    def __password_session_setup(self, node):\n        creds = node.runtime_properties['auth_properties']\n        if 'region_name' in creds:\n            del creds['region_name']\n        loader = loading.get_plugin_loader('password')\n        auth = loader.load_from_options(**creds)\n        sess = session.Session(auth=auth)\n        return sess\n\n    def keystone(self, node):\n        if self.__keystone is None:\n            self.__keystone = keystoneclient.Client(**node.properties)\n            self.__keystone.authenticate()\n        return self.__keystone\n\n    def nova(self, node):\n        if self.__nova is None:\n            version = node.properties['compute_api_version']\n            use_connection_pool = node.properties['use_connection_pool']\n            self.__nova = novaclient.Client(\n                version, session=self.__password_session_setup(node),\n                connection_pool=use_connection_pool)\n        return self.__nova\n\n    def neutron(self, node):\n        if self.__neutron is None:\n            self.__neutron = neutronclient.Client(\n                session=self.__password_session_setup(node))\n        return self.__neutron\n\n    def glance(self, node):\n        if self.__glance is None:\n            self.__glance = glanceclient.Client(\n                session=self.__password_session_setup(node))\n        return self.__glance\n\n\nopenstack = OpenStackClients()\n",
        "summary": "The provided Python code defines a class `OpenStackClients` that encapsulates the creation and management of various OpenStack clients, including keystone, nova, neutron, and glance. Each client is lazily initialized using properties from a node object, with authentication handled through a password-based session setup method."
    },
    {
        "code": "from abc import abstractmethod\nfrom .apr_fetcher import APRFetcher\nfrom typing import Dict, List, Union, Any\nfrom .dapp_apr_fetcher import DappAPRFetcher\nfrom .utils.utils import (\n    calculate_lp_token_price,\n    get_block_average_time,\n    get_token_price_from_dexs,\n    open_contract,\n    usdt_address,\n    platform_name_mapping,\n    decimals_mapping,\n    symbol_mapping\n)\n\n\nclass MasterchefAPRFetcher(DappAPRFetcher):\n    \n\n    @abstractmethod\n    def masterchef_address(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def dapp_token_address_field(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def dapp_token_per_block_or_per_second_field(self, per_block: bool) -> str:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _total_staked(self, pool_info):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _pool_address(self, pool_info):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _alloc_point(self, pool_info):\n        raise NotImplementedError()\n\n    def dapp_token_address(self, web3) -> str:\n        masterchef_contract = open_contract(self._web3, self._blockchain, self.masterchef_address())\n        return getattr(masterchef_contract.functions, self.dapp_token_address_field())().call()\n\n    def dapp_pools_infos(self, web3) -> List[Dict[str, Union[str, float]]]:\n        masterchef_contract = open_contract(self._web3, self._blockchain, self.masterchef_address())\n        d = []\n        for i in range(masterchef_contract.functions.poolLength().call()):\n            pool_info = masterchef_contract.functions.poolInfo(i).call()\n            d.append({\n                \"total_staked\": self._total_staked(i, pool_info),\n                \"pool_address\": self._pool_address(i, pool_info),\n                \"alloc_point\": self._alloc_point(i, pool_info),\n            })\n        return d\n\n    def dapp_token_per_year(self, web3) -> float:\n        field_per_second = self.dapp_token_per_block_or_per_second_field(per_block=False)\n        masterchef_contract = open_contract(self._web3, self._blockchain, self.masterchef_address())\n        token_contract = open_contract(web3, self._blockchain, self.dapp_token_address(web3))\n        decimals = token_contract.functions.decimals().call()\n        if field_per_second is None or field_per_second == \"\":\n            average_time_per_block_seconds = get_block_average_time(web3, span=100)\n            block_per_seconds = 1.0 / average_time_per_block_seconds\n            block_per_year = block_per_seconds * 3600 * 24 * 365\n            token_per_block = getattr(masterchef_contract.functions, self.dapp_token_per_block_field(per_block=True))().call()\n            annual_token_emission = block_per_year * (token_per_block/(10**decimals))\n        else:\n            annual_token_emission = getattr(masterchef_contract.functions, field_per_second)().call() * 10**(-decimals) * 3600 * 24 * 365\n        return annual_token_emission\n\n    def dapp_token_total_alloc(self, web3) -> int:\n        total_alloc = sum([p[\"alloc_point\"] for p in self.dapp_pools_infos(web3)])\n        return total_alloc\n\n    def dapp_token_price(self, web3) -> float:\n        return get_token_price_from_dexs(web3, self._blockchain, self.dapp_token_address(web3))\n",
        "summary": "The `MasterchefAPRFetcher` class extends `DappAPRFetcher` and implements methods to fetch APR data for a decentralized finance (DeFi) platform using the MasterChef contract. It calculates various parameters such as total staked, pool addresses, allocation points, and token emissions per year, while also retrieving the current price of the DApp token from decentralized exchanges."
    },
    {
        "code": "from django.test import TestCase\nfrom django.urls import reverse\n\nfrom rest_framework import status\nfrom rest_framework.test import APIClient\n\nQUIZZES_URL = reverse('questionary:quiz-list')\n\n\nclass PublicQuizzesApiTests(TestCase):\n    \n\n    def setUp(self):\n        self.client = APIClient()\n\n    def test_login_required(self):\n        \n        res = self.client.get(QUIZZES_URL)\n\n        self.assertEqual(res.status_code, status.HTTP_401_UNAUTHORIZED)",
        "summary": "The provided Python code defines a test case for the public quizzes API in a Django application using Django's testing framework and REST framework. It includes a method to set up an APIClient instance and another method to test that login is required by returning a 401 Unauthorized status when accessing the quizzes URL without authentication."
    },
    {
        "code": "from enum import Enum\r\n\r\nfrom PythonApp.pillar.MessageClient import MessageClient\r\nfrom PythonApp.pillar.PillarMessageTransformer import PillarMessageTransformer\r\nfrom PythonApp.qc_serial.SerialDao import SerialDao\r\nfrom PythonApp.qc_serial.SerialUtil import SerialUtil\r\nfrom PythonApp.qc_serial.model.HeaderMessage import HeaderMessage\r\nfrom PythonApp.qc_serial.model.OpCode import OpCode\r\nfrom PythonApp.qc_serial.model.PayloadMessage import PayloadMessage\r\nfrom PythonApp.util.Config import Config\r\n\r\n\r\nclass States(Enum):\r\n    DISCONNECTED = 0\r\n    CONNECTED = 1\r\n\r\n\r\nclass SerialStateMachine:\r\n    def __init__(self, serial_dao: SerialDao):\r\n        self.active_state = States.DISCONNECTED\r\n        self.config = Config()\r\n        self.states = {\r\n            States.DISCONNECTED: self.disconnected,\r\n            States.CONNECTED: self.connected,\r\n        }\r\n        self.serial_dao = serial_dao\r\n        self.message_client = MessageClient()\r\n\r\n        self.header_message_length = 11\r\n        self.done = False\r\n\r\n    def run(self):\r\n        while not self.done:\r\n            self.states[self.active_state]()\r\n\r\n    def disconnected(self):\r\n        \n        hello_message = HeaderMessage(\r\n            OpCode.HELO,\r\n            0,\r\n            int(self.config.get_master_config_value(\"PillarID\")),\r\n            0)\r\n\r\n        self.serial_dao.write(hello_message.to_serial_payload())\r\n        message = self.serial_dao.read(self.header_message_length)\r\n\r\n        try:\r\n            SerialUtil.validate_message_header(message)\r\n        except TimeoutError as ex:\r\n            return\r\n        except ValueError as ex:\r\n            print(ex)\r\n            return\r\n\r\n        header_message = HeaderMessage.build_header_object(message[1:])\r\n        if header_message.opcode == OpCode.ACK:\r\n            print(\"Received ACK! Now connected to badge {}!\".format(header_message.from_id))\r\n            self.active_state = States.CONNECTED\r\n        else:\r\n            print(\"Received unknown message! Skipping..\")\r\n\r\n    def connected(self):\r\n        \n        dump_q_message = HeaderMessage(\r\n            OpCode.DUMPQ,\r\n            1,\r\n            int(self.config.get_master_config_value(\"PillarID\")),\r\n            0)\r\n        dump_q_payload = PayloadMessage(int(self.config.get_master_config_value(\"PillarType\")))\r\n        print(\"Sending dump Q message!\")\r\n        print(\"Dump Q Header: {}\".format(dump_q_message.to_serial_payload(dump_q_payload)))\r\n        self.serial_dao.write(dump_q_message.to_serial_payload(dump_q_payload))\r\n        print(\"Dump q payload: {}\".format(dump_q_payload.to_serial_payload()))\r\n        self.serial_dao.write_no_sync(dump_q_payload.to_serial_payload())\r\n\r\n        message = self.serial_dao.read(self.header_message_length)\r\n        try:\r\n            SerialUtil.validate_message_header(message)\r\n            header_message = HeaderMessage.build_header_object(message[1:])\r\n            if header_message.opcode == OpCode.DUMPA:\r\n                print(\"Received DUMPA! Sending update to cloud!\")\r\n                message = self.serial_dao.read(header_message.payload_len)\r\n                payload_message = PayloadMessage.build_payload_object(message)\r\n                pillar_message = PillarMessageTransformer\\\r\n                    .transform_serial_message_to_pillar_message(header_message, payload_message)\r\n                self.message_client.send_message_to_queue(pillar_message)\r\n                self.done = True\r\n            else:\r\n                print(\"Unexpected message type!\")\r\n        except TimeoutError as ex:\r\n            print(ex)\r\n        except ValueError as ex:\r\n            print(ex)\r\n\r\n        self.active_state = States.DISCONNECTED\r\n",
        "summary": "The provided Python code defines a `SerialStateMachine` class that manages the state transitions between disconnected and connected states for serial communication. It uses a state machine pattern to handle different operations such as sending a HELLO message, waiting for an ACK, sending a DUMPQ message, and handling responses like DUMPA by sending data to a cloud queue."
    },
    {
        "code": "from __future__ import unicode_literals\n\nimport aldryn_apphooks_config.fields\nimport app_data.fields\nimport djangocms_text_ckeditor.fields\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('cms', '__first__'),\n        ('djangocms_blog', '0009_latestpostsplugin_tags_new'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='BlogConfig',\n            fields=[\n                ('id', models.AutoField(auto_created=True, verbose_name='ID', serialize=False, primary_key=True)),\n                ('type', models.CharField(verbose_name='type', max_length=100)),\n                ('namespace', models.CharField(default=None, verbose_name='instance namespace', unique=True, max_length=100)),\n                ('app_data', app_data.fields.AppDataField(editable=False, default='{}')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='BlogConfigTranslation',\n            fields=[\n                ('id', models.AutoField(auto_created=True, verbose_name='ID', serialize=False, primary_key=True)),\n                ('language_code', models.CharField(db_index=True, verbose_name='Language', max_length=15)),\n                ('app_title', models.CharField(verbose_name='application title', max_length=234)),\n                ('master', models.ForeignKey(editable=False, to='djangocms_blog.BlogConfig', related_name='translations', null=True)),\n            ],\n            options={\n                'verbose_name': 'blog config Translation',\n                'db_table': 'djangocms_blog_blogconfig_translation',\n                'default_permissions': (),\n                'db_tablespace': '',\n                'managed': True,\n            },\n        ),\n        migrations.CreateModel(\n            name='GenericBlogPlugin',\n            fields=[\n                ('cmsplugin_ptr', models.OneToOneField(parent_link=True, serialize=False, primary_key=True, auto_created=True, to='cms.CMSPlugin')),\n                ('app_config', aldryn_apphooks_config.fields.AppHookConfigField(verbose_name='app. config', blank=True, to='djangocms_blog.BlogConfig', help_text='When selecting a value, the form is reloaded to get the updated default')),\n            ],\n            options={\n                'abstract': False,\n            },\n            bases=('cms.cmsplugin',),\n        ),\n        migrations.AlterField(\n            model_name='posttranslation',\n            name='abstract',\n            field=djangocms_text_ckeditor.fields.HTMLField(default='', verbose_name='abstract', blank=True),\n        ),\n        migrations.AddField(\n            model_name='authorentriesplugin',\n            name='app_config',\n            field=aldryn_apphooks_config.fields.AppHookConfigField(default=None, blank=True, verbose_name='app. config', to='djangocms_blog.BlogConfig', help_text='When selecting a value, the form is reloaded to get the updated default', null=True),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='blogcategory',\n            name='app_config',\n            field=aldryn_apphooks_config.fields.AppHookConfigField(default=None, verbose_name='app. config', to='djangocms_blog.BlogConfig', help_text='When selecting a value, the form is reloaded to get the updated default', null=True),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='latestpostsplugin',\n            name='app_config',\n            field=aldryn_apphooks_config.fields.AppHookConfigField(default=None, blank=True, verbose_name='app. config', to='djangocms_blog.BlogConfig', help_text='When selecting a value, the form is reloaded to get the updated default', null=True),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='post',\n            name='app_config',\n            field=aldryn_apphooks_config.fields.AppHookConfigField(default=None, verbose_name='app. config', to='djangocms_blog.BlogConfig', help_text='When selecting a value, the form is reloaded to get the updated default', null=True),\n            preserve_default=False,\n        ),\n        migrations.AlterUniqueTogether(\n            name='blogconfigtranslation',\n            unique_together=set([('language_code', 'master')]),\n        ),\n        migrations.AlterField(\n            model_name='post',\n            name='sites',\n            field=models.ManyToManyField(to='sites.Site', help_text='Select sites in which to show the post. If none is set it will be visible in all the configured sites.', blank=True, verbose_name='Site(s)'),\n        ),\n    ]\n",
        "summary": "This migration script for Django adds several models and fields related to a blogging application, including `BlogConfig`, `BlogConfigTranslation`, and `GenericBlogPlugin`. It also modifies existing models like `PostTranslation`, `AuthorsEntriesPlugin`, `BlogCategory`, `LatestPostsPlugin`, and `Post` by adding an `app_config` field that links to the new `BlogConfig` model."
    },
    {
        "code": "import copy\nimport re\nfrom collections import defaultdict\nfrom typing import List, Dict\n\nfrom .substitution_augmenter import SubstitutionAugmenter\nfrom ..actions import Chemical\nfrom ..utils import extract_chemicals\nfrom paragraph2actions.misc import TextWithActions\n\n\nclass CompoundNameAugmenter(SubstitutionAugmenter):\n    \n\n    def __init__(self, probability: float, compounds: List[str]):\n        \n        super().__init__(probability=probability, values=compounds)\n\n    def augment(self, sample: TextWithActions) -> TextWithActions:\n        sample = copy.deepcopy(sample)\n\n        chemicals = extract_chemicals(sample.actions)\n\n        \n        \n        cpd_dict: Dict[str, List[Chemical]] = defaultdict(list)\n        for c in chemicals:\n            cpd_dict[c.name].append(c)\n\n        \n        \n        for chemical_name in list(cpd_dict.keys()):\n            if any(chemical_name in cpd for cpd in cpd_dict.keys() if chemical_name != cpd):\n                cpd_dict.pop(chemical_name)\n\n        \n        for cpd_name in cpd_dict:\n            if not self.random_draw_passes() or cpd_name not in sample.text:\n                continue\n            new_name = self.draw_value()\n            sample.text = self.replace_in_text(\n                text=sample.text, compound=cpd_name, new_name=new_name\n            )\n            for c in cpd_dict[cpd_name]:\n                c.name = new_name\n\n        return sample\n\n    def replace_in_text(self, text: str, compound: str, new_name: str) -> str:\n        \n        pattern = re.compile(rf'\\b{re.escape(compound)}\\b')\n        return pattern.sub(new_name, text)\n",
        "summary": "The `CompoundNameAugmenter` class extends `SubstitutionAugmenter` to randomly replace chemical compound names in a given text with new names from a provided list, based on a specified probability. It uses regular expressions for precise matching and updates both the text and associated actions accordingly."
    },
    {
        "code": "import django.contrib.postgres.fields\nfrom django.db import migrations\nfrom django.db import models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('stac_api', '0015_data_collection_summaries'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='collection',\n            name='summaries_geoadmin_lang',\n            field=django.contrib.postgres.fields.ArrayField(\n                base_field=models.CharField(max_length=2),\n                blank=True,\n                default=list,\n                editable=False,\n                size=None\n            ),\n        ),\n        migrations.AlterField(\n            model_name='asset',\n            name='media_type',\n            field=models.CharField(\n                choices=[\n                    (\n                        'application/x.ascii-grid+zip',\n                        'Zipped ESRI ASCII raster format (.asc) (application/x.ascii-grid+zip)'\n                    ),\n                    (\n                        'application/x.ascii-xyz+zip',\n                        'Zipped XYZ (.xyz) (application/x.ascii-xyz+zip)'\n                    ), ('application/x.e00+zip', 'Zipped e00 (application/x.e00+zip)'), (\n                        'image/tiff; application=geotiff',\n                        'GeoTIFF (image/tiff; application=geotiff)'\n                    ), ('application/x.geotiff+zip', 'Zipped GeoTIFF (application/x.geotiff+zip)'),\n                    ('application/x.tiff+zip', 'Zipped TIFF (application/x.tiff+zip)'),\n                    ('application/x.png+zip', 'Zipped PNG (application/x.png+zip)'),\n                    ('application/x.jpeg+zip', 'Zipped JPEG (application/x.jpeg+zip)'),\n                    (\n                        'application/vnd.google-earth.kml+xml',\n                        'KML (application/vnd.google-earth.kml+xml)'\n                    ),\n                    (\n                        'application/vnd.google-earth.kmz',\n                        'Zipped KML (application/vnd.google-earth.kmz)'\n                    ), ('application/x.dxf+zip', 'Zipped DXF (application/x.dxf+zip)'),\n                    ('application/gml+xml', 'GML (application/gml+xml)'),\n                    ('application/x.gml+zip', 'Zipped GML (application/x.gml+zip)'),\n                    ('application/vnd.las', 'LIDAR (application/vnd.las)'),\n                    ('application/vnd.laszip', 'Zipped LIDAR (application/vnd.laszip)'), (\n                        'application/x.shapefile+zip',\n                        'Zipped Shapefile (application/x.shapefile+zip)'\n                    ),\n                    (\n                        'application/x.filegdb+zip',\n                        'Zipped File Geodatabase (application/x.filegdb+zip)'\n                    ),\n                    (\n                        'application/x.ms-access+zip',\n                        'Zipped Personal Geodatabase (application/x.ms-access+zip)'\n                    ), ('application/x.ms-excel+zip', 'Zipped Excel (application/x.ms-excel+zip)'),\n                    ('application/x.tab+zip', 'Zipped Mapinfo-TAB (application/x.tab+zip)'),\n                    (\n                        'application/x.tab-raster+zip',\n                        'Zipped Mapinfo-Raster-TAB (application/x.tab-raster+zip)'\n                    ), ('application/x.csv+zip',\n                        'Zipped CSV (application/x.csv+zip)'), ('text/csv', 'CSV (text/csv)'), (\n                            'application/geopackage+sqlite3',\n                            'Geopackage (application/geopackage+sqlite3)'\n                        ),\n                    (\n                        'application/x.geopackage+zip',\n                        'Zipped Geopackage (application/x.geopackage+zip)'\n                    ), ('application/geo+json', 'GeoJSON (application/geo+json)'),\n                    ('application/x.geojson+zip', 'Zipped GeoJSON (application/x.geojson+zip)'),\n                    (\n                        'application/x.interlis; version=2.3',\n                        'Interlis 2 (application/x.interlis; version=2.3)'\n                    ),\n                    (\n                        'application/x.interlis+zip; version=2.3',\n                        'Zipped XTF (2.3) (application/x.interlis+zip; version=2.3)'\n                    ),\n                    (\n                        'application/x.interlis; version=1',\n                        'Interlis 1 (application/x.interlis; version=1)'\n                    ),\n                    (\n                        'application/x.interlis+zip; version=1',\n                        'Zipped ITF (application/x.interlis+zip; version=1)'\n                    ),\n                    (\n                        'image/tiff; application=geotiff; profile=cloud-optimized',\n                        'Cloud Optimized GeoTIFF (COG) (image/tiff; application=geotiff; profile=cloud-optimized)'\n                    ), ('application/pdf', 'PDF (application/pdf)'),\n                    ('application/x.pdf+zip', 'Zipped PDF (application/x.pdf+zip)'),\n                    ('application/json', 'JSON (application/json)'),\n                    ('application/x.json+zip', 'Zipped JSON (application/x.json+zip)'),\n                    ('application/x-netcdf', 'NetCDF (application/x-netcdf)'),\n                    ('application/x.netcdf+zip', 'Zipped NetCDF (application/x.netcdf+zip)'),\n                    ('application/xml', 'XML (application/xml)'),\n                    ('application/x.xml+zip', 'Zipped XML (application/x.xml+zip)'),\n                    (\n                        'application/vnd.mapbox-vector-tile',\n                        'mbtiles (application/vnd.mapbox-vector-tile)'\n                    ), ('text/plain', 'Text (text/plain)'),\n                    ('text/x.plain+zip', 'Zipped text (text/x.plain+zip)'),\n                    ('application/x.dwg+zip', 'Zipped DWG (application/x.dwg+zip)')\n                ],\n                max_length=200\n            ),\n        ),\n    ]\n",
        "summary": "This Django migration script adds a new field `summaries_geoadmin_lang` to the `Collection` model as an array of character fields, and alters the `media_type` field in the `Asset` model to include a comprehensive list of supported media types with their respective MIME types and descriptions."
    },
    {
        "code": "SOURCE_DOC_PATH = ''",
        "summary": "The variable `SOURCE_DOC_PATH` is initialized as an empty string, likely intended to store the path to a source document but currently holds no value."
    },
    {
        "code": "import numpy as np\r\nfrom i3Deep import utils\r\nfrom tqdm import tqdm\r\nimport os\r\n\r\n\r\n\n\n\n\nload_path = \"D:/Datasets/medical_data/ExportKGU/3D Slicer 2/\"\r\n\r\n\r\ndef rename(case_path):\r\n    filenames = utils.load_filenames(case_path + \"/\", extensions=None)\r\n    for filename in filenames:\r\n        name = os.path.basename(filename)\r\n        if \"label\" in name and \".nii.gz\" in name:\r\n            os.rename(filename, case_path + \"/mask.nii.gz\")\r\n        elif \".txt\" in name:\r\n            os.rename(filename, case_path + \"/label_table.txt\")\r\n        elif \".nii.gz\" in name:\r\n            os.rename(filename, case_path + \"/image.nii.gz\")\r\n\r\n\r\ndef get_labels(load_label_table):\r\n    with open(load_label_table) as f:\r\n        label_table = f.readlines()\r\n        label_table = np.asarray(label_table)\r\n\r\n    ggo = []\r\n    cons = []\r\n    pe = []\r\n    for line in label_table:\r\n        label = line.split()[0]\r\n        if label.isnumeric():\r\n            if \"Background\" in line or \"background\" in line:\r\n                continue\r\n            infection = line.split(\"_\")[1]\r\n            keywords = [\"ggo\", \"gg\"]\r\n            if any(x in infection.lower() for x in keywords):\r\n                ggo.append(int(label))\r\n            keywords = [\"cons\", \"cns\", \"con\", \"cos\", \"co\"]\r\n            if any(x in infection.lower() for x in keywords):\r\n                cons.append(int(label))\r\n            keywords = [\"pe\", \"pes\"]\r\n            if any(x in infection.lower() for x in keywords):\r\n                pe.append(int(label))\r\n    return ggo, cons, pe\r\n\r\n\r\ndef merge_labels(load_mask, save_mask, load_label_table):\r\n    mask, affine, spacing, header = utils.load_nifty(load_mask)\r\n    mask = mask.astype(int)\r\n    ggo, cons, pe = get_labels(load_label_table)\r\n\r\n    for label in tqdm(np.concatenate((ggo, cons, pe), axis=0), disable=True):\r\n        mask[mask == label] = -label\r\n\r\n    for label in tqdm(ggo, disable=True):\r\n        mask[mask == -label] = 1\r\n\r\n    for label in tqdm(cons, disable=True):\r\n        mask[mask == -label] = 2\r\n\r\n    for label in tqdm(pe, disable=True):\r\n        mask[mask == -label] = 3\r\n\r\n    mask = np.rint(mask)\r\n    mask = mask.astype(int)\r\n\r\n    utils.save_nifty(save_mask, mask, affine, spacing, header)\r\n\r\ndef round_mask(filename):\r\n    mask, affine, spacing, header = utils.load_nifty(filename)\r\n    mask = np.rint(mask)\r\n    mask = mask.astype(int)\r\n    utils.save_nifty(filename, mask, affine, spacing, header)\r\n\r\ndef tmp2(filename):\r\n    mask, affine, spacing, header = utils.load_nifty(filename)\r\n    print(mask[46-1][155-1][116-1])\r\n\r\n\r\nif __name__ == '__main__':\r\n    \n    \n    \n    \n    \n    \n    \n    \n    \n\r\n    \n    \n    \n    \n    \n    \n    \n    \n\r\n    \n    \n    \n    \n    \n    \n\r\n    \n    \n    \n    \n    \n    \n    tmp2(\"/gris/gris-f/homelv/kgotkows/datasets/nnUnet_datasets/nnUNet_raw_data/Task77_frankfurt3Guided/tmp/900.nii.gz\")",
        "summary": "The provided Python script processes medical imaging data, specifically renaming files and merging labels based on content. It includes functions to rename files according to their type (image, mask, label table), extract specific labels from a label table, merge these labels into a single mask with distinct values for different types of infections, round the mask values, and print a specific voxel value from a given file."
    },
    {
        "code": "import base64\nimport json\nimport requests\nimport urllib3\nfrom contextlib import contextmanager\nfrom packaging.version import Version\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util import Retry\nfrom requests.exceptions import HTTPError\n\nfrom mlflow import __version__\nfrom mlflow.protos import databricks_pb2\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE, ENDPOINT_NOT_FOUND, ErrorCode\nfrom mlflow.utils.proto_json_utils import parse_dict\nfrom mlflow.utils.string_utils import strip_suffix\nfrom mlflow.exceptions import MlflowException, RestException\n\nRESOURCE_DOES_NOT_EXIST = \"RESOURCE_DOES_NOT_EXIST\"\n_REST_API_PATH_PREFIX = \"/api/2.0\"\n_DEFAULT_HEADERS = {\"User-Agent\": \"mlflow-python-client/%s\" % __version__}\n\n\n\n_TRANSIENT_FAILURE_RESPONSE_CODES = frozenset(\n    [\n        408,  \n        429,  \n        500,  \n        502,  \n        503,  \n        504,  \n    ]\n)\n\n\ndef _get_http_response_with_retries(\n    method, url, max_retries, backoff_factor, retry_codes, **kwargs\n):\n    \n    assert 0 <= max_retries < 10\n    assert 0 <= backoff_factor < 120\n\n    retry_kwargs = {\n        \"total\": max_retries,\n        \"connect\": max_retries,\n        \"read\": max_retries,\n        \"redirect\": max_retries,\n        \"status\": max_retries,\n        \"status_forcelist\": retry_codes,\n        \"backoff_factor\": backoff_factor,\n    }\n    if Version(urllib3.__version__) >= Version(\"1.26.0\"):\n        retry_kwargs[\"allowed_methods\"] = None\n    else:\n        retry_kwargs[\"method_whitelist\"] = None\n\n    retry = Retry(**retry_kwargs)\n    adapter = HTTPAdapter(max_retries=retry)\n    with requests.Session() as http:\n        http.mount(\"https://\", adapter)\n        http.mount(\"http://\", adapter)\n        response = http.request(method, url, **kwargs)\n        return response\n\n\ndef http_request(\n    host_creds,\n    endpoint,\n    method,\n    max_retries=5,\n    backoff_factor=2,\n    retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n    timeout=120,\n    **kwargs,\n):\n    \n    hostname = host_creds.host\n    auth_str = None\n    if host_creds.username and host_creds.password:\n        basic_auth_str = (\"%s:%s\" % (host_creds.username, host_creds.password)).encode(\"utf-8\")\n        auth_str = \"Basic \" + base64.standard_b64encode(basic_auth_str).decode(\"utf-8\")\n    elif host_creds.token:\n        auth_str = \"Bearer %s\" % host_creds.token\n\n    from mlflow.tracking.request_header.registry import resolve_request_headers\n\n    headers = dict({**_DEFAULT_HEADERS, **resolve_request_headers()})\n    if auth_str:\n        headers[\"Authorization\"] = auth_str\n\n    if host_creds.server_cert_path is None:\n        verify = not host_creds.ignore_tls_verification\n    else:\n        verify = host_creds.server_cert_path\n\n    if host_creds.client_cert_path is not None:\n        kwargs[\"cert\"] = host_creds.client_cert_path\n\n    cleaned_hostname = strip_suffix(hostname, \"/\")\n    url = \"%s%s\" % (cleaned_hostname, endpoint)\n    try:\n        return _get_http_response_with_retries(\n            method,\n            url,\n            max_retries,\n            backoff_factor,\n            retry_codes,\n            headers=headers,\n            verify=verify,\n            timeout=timeout,\n            **kwargs,\n        )\n    except Exception as e:\n        raise MlflowException(\"API request to %s failed with exception %s\" % (url, e))\n\n\ndef _can_parse_as_json(string):\n    try:\n        json.loads(string)\n        return True\n    except Exception:\n        return False\n\n\ndef http_request_safe(host_creds, endpoint, method, **kwargs):\n    \n    response = http_request(host_creds=host_creds, endpoint=endpoint, method=method, **kwargs)\n    return verify_rest_response(response, endpoint)\n\n\ndef verify_rest_response(response, endpoint):\n    \n    if response.status_code != 200:\n        if _can_parse_as_json(response.text):\n            raise RestException(json.loads(response.text))\n        else:\n            base_msg = \"API request to endpoint %s failed with error code \" \"%s != 200\" % (\n                endpoint,\n                response.status_code,\n            )\n            raise MlflowException(\"%s. Response body: '%s'\" % (base_msg, response.text))\n\n    \n    \n    if endpoint.startswith(_REST_API_PATH_PREFIX) and not _can_parse_as_json(response.text):\n        base_msg = (\n            \"API request to endpoint was successful but the response body was not \"\n            \"in a valid JSON format\"\n        )\n        raise MlflowException(\"%s. Response body: '%s'\" % (base_msg, response.text))\n\n    return response\n\n\ndef augmented_raise_for_status(response):\n    \n    try:\n        response.raise_for_status()\n    except HTTPError as e:\n        if response.text:\n            raise HTTPError(f\"{e}. Response text: {response.text}\")\n        else:\n            raise e\n\n\ndef _get_path(path_prefix, endpoint_path):\n    return \"{}{}\".format(path_prefix, endpoint_path)\n\n\ndef extract_api_info_for_service(service, path_prefix):\n    \n    service_methods = service.DESCRIPTOR.methods\n    res = {}\n    for service_method in service_methods:\n        endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n        endpoint = endpoints[0]\n        endpoint_path = _get_path(path_prefix, endpoint.path)\n        res[service().GetRequestClass(service_method)] = (endpoint_path, endpoint.method)\n    return res\n\n\ndef extract_all_api_info_for_service(service, path_prefix):\n    \n    service_methods = service.DESCRIPTOR.methods\n    res = {}\n    for service_method in service_methods:\n        endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n        res[service().GetRequestClass(service_method)] = [\n            (_get_path(path_prefix, endpoint.path), endpoint.method) for endpoint in endpoints\n        ]\n    return res\n\n\ndef call_endpoint(host_creds, endpoint, method, json_body, response_proto):\n    \n    if json_body:\n        json_body = json.loads(json_body)\n    if method == \"GET\":\n        response = http_request(\n            host_creds=host_creds, endpoint=endpoint, method=method, params=json_body\n        )\n    else:\n        response = http_request(\n            host_creds=host_creds, endpoint=endpoint, method=method, json=json_body\n        )\n    response = verify_rest_response(response, endpoint)\n    js_dict = json.loads(response.text)\n    parse_dict(js_dict=js_dict, message=response_proto)\n    return response_proto\n\n\ndef call_endpoints(host_creds, endpoints, json_body, response_proto):\n    \n    \n    for i, (endpoint, method) in enumerate(endpoints):\n        try:\n            return call_endpoint(host_creds, endpoint, method, json_body, response_proto)\n        except RestException as e:\n            if e.error_code != ErrorCode.Name(ENDPOINT_NOT_FOUND) or i == len(endpoints) - 1:\n                raise e\n\n\n@contextmanager\ndef cloud_storage_http_request(\n    method,\n    url,\n    max_retries=5,\n    backoff_factor=2,\n    retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n    timeout=None,\n    **kwargs,\n):\n    \n    if method.lower() not in (\"put\", \"get\"):\n        raise ValueError(\"Illegal http method: \" + method)\n    try:\n        with _get_http_response_with_retries(\n            method, url, max_retries, backoff_factor, retry_codes, timeout=timeout, **kwargs\n        ) as response:\n            yield response\n    except Exception as e:\n        raise MlflowException(\"API request failed with exception %s\" % e)\n\n\nclass MlflowHostCreds(object):\n    \n\n    def __init__(\n        self,\n        host,\n        username=None,\n        password=None,\n        token=None,\n        ignore_tls_verification=False,\n        client_cert_path=None,\n        server_cert_path=None,\n    ):\n        if not host:\n            raise MlflowException(\n                message=\"host is a required parameter for MlflowHostCreds\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n        if ignore_tls_verification and (server_cert_path is not None):\n            raise MlflowException(\n                message=(\n                    \"When 'ignore_tls_verification' is true then 'server_cert_path' \"\n                    \"must not be set! This error may have occurred because the \"\n                    \"'MLFLOW_TRACKING_INSECURE_TLS' and 'MLFLOW_TRACKING_SERVER_CERT_PATH' \"\n                    \"environment variables are both set - only one of these environment \"\n                    \"variables may be set.\"\n                ),\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n        self.host = host\n        self.username = username\n        self.password = password\n        self.token = token\n        self.ignore_tls_verification = ignore_tls_verification\n        self.client_cert_path = client_cert_path\n        self.server_cert_path = server_cert_path\n",
        "summary": "The provided Python code defines a set of functions and classes for making HTTP requests to an MLflow tracking server, handling retries, authentication, and parsing responses. It includes utilities for calling specific endpoints, managing TLS certificates, and validating request parameters. The code also provides context managers for cloud storage operations and a class `MlflowHostCreds` for storing credentials needed to connect to the MLflow server."
    },
    {
        "code": "import collections\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom difflikelihoods import logdensity\n\n\ndef metropolishastings_rw(logpdf, nsamps, initstate, pwidth, ninits):\n    \n    logdens = logdensity.LogDensity(logpdf)\n    rwmh = RandomWalkMH(logdens)\n    return rwmh.sample_nd(nsamps, initstate, pwidth, ninits)\n\n\ndef metropolishastings_lang(logpdf, loggrad, nsamps, initstate, pwidth, ninits):\n    \n    logdens = logdensity.LogDensity(logpdf, loggrad)\n    langmh = LangevinMH(logdens)\n    return langmh.sample_nd(nsamps, initstate, pwidth, ninits)\n\n\ndef metropolishastings_plang(\n    logpdf, loggrad, loghess, nsamps, initstate, pwidth, ninits\n):\n    \n    logdens = logdensity.LogDensity(logpdf, loggrad, loghess)\n    plangmh = PrecondLangevinMH(logdens)\n    return plangmh.sample_nd(nsamps, initstate, pwidth, ninits)\n\n\ndef metropolishastings_ham(\n    logpdf, loggrad, nsamps, initstate, stepsize, nsteps, ninits\n):\n    \n    logdens = logdensity.LogDensity(logpdf, loggrad)\n    hmc = HamiltonianMC(logdens, nsteps)\n    return hmc.sample_nd(nsamps, initstate, stepsize, ninits)\n\n\ndef metropolishastings_pham(\n    logpdf, loggrad, loghess, nsamps, initstate, stepsize, nsteps, ninits\n):\n    \n    logdens = logdensity.LogDensity(logpdf, loggrad, loghess)\n    phmc = PrecondHamiltonianMC(logdens, nsteps)\n    return phmc.sample_nd(nsamps, initstate, stepsize, ninits)\n\n\n\nMCMCState = collections.namedtuple(\"MCMCState\", \"state logdens loggrad loghess\")\n\n\nclass MetropolisHastings(ABC):\n    \n\n    def __init__(self, logdens):\n        \n        self.logdens = logdens\n\n    def sample_nd(self, nsamps, init_state, pwidth, ninits=None, *optional):\n        \n        assert init_state_is_array(\n            init_state\n        ), \"Please enter a (d,) dimensional initial state\"\n        states, logprobs = np.zeros((nsamps, len(init_state))), np.zeros(nsamps)\n        accepted = 0\n        if ninits is None:\n            ninits = 0\n        currstate = self.evaluate_logdens(init_state)\n        states[0], logprobs[0] = currstate.state, currstate.logdens\n        for idx in range(1, nsamps):\n            if idx < ninits:\n                proposal, corrfact = self.generate_proposal(currstate, pwidth)\n            else:\n                proposal, corrfact = self.generate_proposal(currstate, 0.2 * pwidth)\n            currstate, is_accept = self.accept_or_reject(\n                currstate, proposal, corrfact, idx, ninits\n            )\n            states[idx], logprobs[idx] = (\n                currstate.state.copy(),\n                currstate.logdens.copy(),\n            )\n            if idx >= ninits:\n                accepted = accepted + int(is_accept)\n        ratio = accepted / nsamps\n        return states, logprobs, ratio\n\n    def evaluate_logdens(self, loc):\n        \n        logdenseval = self.logdens.eval(loc)\n        if self.logdens.has_gradient:\n            gradeval = self.logdens.gradeval(loc)\n        else:\n            gradeval = 0\n        if self.logdens.has_hessian:\n            hesseval = self.logdens.hesseval(loc)\n        else:\n            hesseval = 0\n        return MCMCState(\n            state=loc, logdens=logdenseval, loggrad=gradeval, loghess=hesseval\n        )\n\n    def accept_or_reject(self, currstate, proposal, corrfact, idx, ninits):\n        \n        logaccprob = self.get_logaccprob(currstate, proposal, corrfact, idx, ninits)\n        if logaccprob < 0 or logaccprob < -np.log(np.random.rand()):\n            state = proposal\n            is_accept = True\n        else:\n            state = currstate\n            is_accept = False\n        return state, is_accept\n\n    def get_logaccprob(self, currstate, proposal, corrfact, idx, ninits):\n        \n        if idx < ninits:\n            corrfact = -corrfact\n        return (corrfact) + (proposal.logdens - currstate.logdens)\n\n    @abstractmethod\n    def generate_proposal(self, *args):\n        \n        pass\n\n\ndef init_state_is_array(init_state):\n    \n    assert isinstance(init_state, np.ndarray), \"Please enter init_state of shape (d,)\"\n    assert len(init_state.shape) == 1, \"Please enter init_state of shape (d,)\"\n    return True\n\n\nclass RandomWalkMH(MetropolisHastings):\n    \n\n    def __init__(self, logdens):\n        \n        MetropolisHastings.__init__(self, logdens)\n\n    def generate_proposal(self, currstate, pwidth):\n        \n        newloc = self.sample_randomwalk(currstate.state, pwidth)\n        proposal = self.evaluate_logdens(newloc)\n        corrfact = 0\n        return proposal, corrfact\n\n    def sample_randomwalk(self, mean, var):\n        \n        return mean + np.sqrt(var) * np.random.randn(len(mean))\n\n\nclass LangevinMH(MetropolisHastings):\n    \n\n    def __init__(self, logdens):\n        \n        MetropolisHastings.__init__(self, logdens)\n\n    def generate_proposal(self, currstate, pwidth):\n        \n        newloc = self.sample_langevin(currstate, pwidth)\n        proposal = self.evaluate_logdens(newloc)\n        corrfact = self.compute_corrfact_langevin(currstate, proposal, pwidth)\n        return proposal, corrfact\n\n    def sample_langevin(self, currstate, pwidth):\n        \n        noise = np.random.randn(len(currstate.state))\n        return (\n            currstate.state - pwidth * currstate.loggrad + np.sqrt(2 * pwidth) * noise\n        )\n\n    def compute_corrfact_langevin(self, currstate, proposal, pwidth):\n        \n        lognomin = self.kernel_langevin(currstate, proposal, pwidth)\n        logdenom = self.kernel_langevin(proposal, currstate, pwidth)\n        return lognomin - logdenom\n\n    def kernel_langevin(self, state1, state2, pwidth):\n        \n        state2_dyn = state2.state - pwidth * state2.loggrad\n        dist = np.linalg.norm(state1.state - state2_dyn) ** 2\n        return 0.5 * dist / (2 * pwidth)\n\n\nclass PrecondLangevinMH(MetropolisHastings):\n    \n\n    def __init__(self, logdens):\n        \n        MetropolisHastings.__init__(self, logdens)\n\n    def generate_proposal(self, currstate, pwidth):\n        \n        newloc = self.sample_langevin(currstate, pwidth)\n        proposal = self.evaluate_logdens(newloc)\n        corrfact = self.compute_corrfact_langevin(currstate, proposal, pwidth)\n        return proposal, corrfact\n\n    def sample_langevin(self, currstate, pwidth):\n        \n        noise = np.random.multivariate_normal(\n            np.zeros(len(currstate.loghess)), np.linalg.inv(currstate.loghess)\n        )\n        prec_dyn = np.linalg.solve(currstate.loghess, currstate.loggrad)\n        return currstate.state - pwidth * prec_dyn + np.sqrt(2 * pwidth) * noise\n\n    def compute_corrfact_langevin(self, currstate, proposal, pwidth):\n        \n        lognomin = self.kernel_langevin(currstate, proposal, pwidth)\n        logdenom = self.kernel_langevin(proposal, currstate, pwidth)\n        return lognomin - logdenom\n\n    def kernel_langevin(self, state1, state2, pwidth):\n        \n        prec_dyn = np.linalg.solve(state2.loghess, state2.loggrad)\n        state2_dyn = state2.state - pwidth * prec_dyn\n        difference = state1.state - state2_dyn\n        return 0.5 * difference.dot(np.dot(state2.loghess, difference)) / (2 * pwidth)\n\n\nclass HamiltonianMC(MetropolisHastings):\n    \n\n    def __init__(self, logdens, nsteps):\n        \n        MetropolisHastings.__init__(self, logdens)\n        self.nsteps = nsteps\n\n    def generate_proposal(self, currstate, pwidth):\n        \n        momentum = np.random.multivariate_normal(\n            np.zeros(len(currstate.state)), np.eye(len(currstate.state))\n        )\n        \n        momentum_new, proposal = self.leapfrog_dynamics(momentum, currstate, pwidth)\n        \n        corrfact = self.get_corrfact(momentum, momentum_new)\n        return proposal, corrfact\n\n    def leapfrog_dynamics(self, momentum, currstate, pwidth):\n        \n        proposal = currstate\n        for idx in range(self.nsteps):\n            momentum, proposal = self.compute_next_lfstep(momentum, proposal, pwidth)\n        return momentum, proposal\n\n    def compute_next_lfstep(self, momentum, proposal, pwidth):\n        \n        momentum = momentum - 0.5 * pwidth * proposal.loggrad\n        pstate = proposal.state + pwidth * momentum\n        proposal = self.evaluate_logdens(pstate)\n        momentum = momentum - 0.5 * pwidth * proposal.loggrad\n        return momentum, proposal\n\n    def get_corrfact(self, mom_new, mom):\n        \n        return 0.5 * (mom_new.T @ mom_new - mom.T @ mom)\n\n\nclass PrecondHamiltonianMC(MetropolisHastings):\n    \n\n    def __init__(self, logdens, nsteps):\n        \n        MetropolisHastings.__init__(self, logdens)\n        self.nsteps = nsteps\n\n    def generate_proposal(self, currstate, pwidth):\n        \n        momentum = np.random.multivariate_normal(\n            np.zeros(len(currstate.state)), currstate.loghess\n        )\n        momentum_new, proposal = self.leapfrog_dynamics(momentum, currstate, pwidth)\n        corrfact = self.get_corrfact(momentum, momentum_new, currstate, proposal)\n        return proposal, corrfact\n\n    def leapfrog_dynamics(self, momentum, currstate, pwidth):\n        \n        proposal = currstate\n        for idx in range(self.nsteps):\n            momentum, proposal = self.compute_next_lfstep(momentum, proposal, pwidth)\n        return momentum, proposal\n\n    def compute_next_lfstep(self, momentum, proposal, pwidth):\n        \n        momentum = momentum - 0.5 * pwidth * proposal.loggrad\n        pstate = proposal.state + pwidth * np.linalg.solve(proposal.loghess, momentum)\n        proposal = self.evaluate_logdens(pstate)\n        momentum = momentum - 0.5 * pwidth * proposal.loggrad\n        return momentum, proposal\n\n    def get_corrfact(self, mom, mom_new, currstate, proposal):\n        \n        return 0.5 * (\n            mom_new.T @ np.linalg.solve(proposal.loghess, mom_new)\n            + np.log(np.linalg.det(proposal.loghess))\n            - mom.T @ np.linalg.solve(currstate.loghess, mom)\n            - np.log(np.linalg.det(currstate.loghess))\n        )\n",
        "summary": "This code defines a class `MetropolisHastings` and several subclasses that implement different Markov Chain Monte Carlo (MCMC) algorithms. The base class `MetropolisHastings` provides the basic structure for MCMC, including methods to evaluate log densities, generate proposals, and accept/reject samples.\n\nThe subclasses include:\n\n1. **HamiltonianMC**: Implements Hamiltonian Monte Carlo with a fixed number of leapfrog steps.\n2. **PrecondHamiltonianMC**: Extends HamiltonianMC by preconditioning the momentum variable using the Hessian matrix of the log density.\n3. **LangevinMCMC**: Not explicitly shown in the code but can be inferred as a subclass that uses Langevin dynamics for proposal generation.\n4. **PrecondLangevinMCMC**: Extends LangevinMCMC by preconditioning the momentum variable using the Hessian matrix of the log density.\n\nEach subclass implements the `generate_proposal` method, which is responsible for generating new proposals and computing the acceptance probability based on the Metropolis criterion. The `get_corrfact` method in subclasses like `PrecondHamiltonianMC` and `PrecondLangevinMCMC` computes a correction factor that accounts for the preconditioning.\n\nThe code also includes methods to evaluate log densities (`evaluate_logdens`) and generate initial proposals (`sample_initial_state`). These methods are essential for initializing the MCMC chain and ensuring that the proposals are valid.\n\nOverall, this code provides a comprehensive framework for implementing various MCMC algorithms in Python, allowing users to choose the most appropriate method based on their specific needs and problem characteristics."
    },
    {
        "code": "import numpy as np\nimport cv2\n\nimport mindspore.dataset.transforms.py_transforms\nimport mindspore.dataset.vision.c_transforms as c_vision\nimport mindspore.dataset.vision.py_transforms as py_vision\nimport mindspore.dataset.vision.utils as mode\nimport mindspore.dataset as ds\nfrom mindspore import log as logger\nfrom util import diff_mse, save_and_check_md5, visualize_list, \\\n    config_get_set_seed, config_get_set_num_parallel_workers\n\nDATA_DIR = [\"../data/dataset/test_tf_file_3_images/train-0000-of-0001.data\"]\nSCHEMA_DIR = \"../data/dataset/test_tf_file_3_images/datasetSchema.json\"\n\nGENERATE_GOLDEN = False\n\n\ndef test_random_crop_and_resize_op_c(plot=False):\n    \n    logger.info(\"test_random_crop_and_resize_op_c\")\n\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    \n    random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), (2, 2), (1, 3))\n    data1 = data1.map(input_columns=[\"image\"], operations=decode_op)\n    data1 = data1.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n\n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    data2 = data2.map(input_columns=[\"image\"], operations=decode_op)\n    num_iter = 0\n    crop_and_resize_images = []\n    original_images = []\n    for item1, item2 in zip(data1.create_dict_iterator(num_epochs=1), data2.create_dict_iterator(num_epochs=1)):\n        crop_and_resize = item1[\"image\"]\n        original = item2[\"image\"]\n        \n        original = cv2.resize(original, (512, 256))\n        mse = diff_mse(crop_and_resize, original)\n        assert mse == 0\n        logger.info(\"random_crop_and_resize_op_{}, mse: {}\".format(num_iter + 1, mse))\n        num_iter += 1\n        crop_and_resize_images.append(crop_and_resize)\n        original_images.append(original)\n    if plot:\n        visualize_list(original_images, crop_and_resize_images)\n\n\ndef test_random_crop_and_resize_op_py(plot=False):\n    \n    logger.info(\"test_random_crop_and_resize_op_py\")\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    \n    transforms1 = [\n        py_vision.Decode(),\n        py_vision.RandomResizedCrop((256, 512), (2, 2), (1, 3)),\n        py_vision.ToTensor()\n    ]\n    transform1 = mindspore.dataset.transforms.py_transforms.Compose(transforms1)\n    data1 = data1.map(input_columns=[\"image\"], operations=transform1)\n    \n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    transforms2 = [\n        py_vision.Decode(),\n        py_vision.ToTensor()\n    ]\n    transform2 = mindspore.dataset.transforms.py_transforms.Compose(transforms2)\n    data2 = data2.map(input_columns=[\"image\"], operations=transform2)\n    num_iter = 0\n    crop_and_resize_images = []\n    original_images = []\n    for item1, item2 in zip(data1.create_dict_iterator(num_epochs=1), data2.create_dict_iterator(num_epochs=1)):\n        crop_and_resize = (item1[\"image\"].transpose(1, 2, 0) * 255).astype(np.uint8)\n        original = (item2[\"image\"].transpose(1, 2, 0) * 255).astype(np.uint8)\n        original = cv2.resize(original, (512, 256))\n        mse = diff_mse(crop_and_resize, original)\n        \n        assert mse <= 0.05\n        logger.info(\"random_crop_and_resize_op_{}, mse: {}\".format(num_iter + 1, mse))\n        num_iter += 1\n        crop_and_resize_images.append(crop_and_resize)\n        original_images.append(original)\n    if plot:\n        visualize_list(original_images, crop_and_resize_images)\n\n\ndef test_random_crop_and_resize_01():\n    \n    logger.info(\"test_random_crop_and_resize_01\")\n    original_seed = config_get_set_seed(0)\n    original_num_parallel_workers = config_get_set_num_parallel_workers(1)\n\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), (0.5, 0.5), (1, 1))\n    data1 = data1.map(input_columns=[\"image\"], operations=decode_op)\n    data1 = data1.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n\n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    transforms = [\n        py_vision.Decode(),\n        py_vision.RandomResizedCrop((256, 512), (0.5, 0.5), (1, 1)),\n        py_vision.ToTensor()\n    ]\n    transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n    data2 = data2.map(input_columns=[\"image\"], operations=transform)\n\n    filename1 = \"random_crop_and_resize_01_c_result.npz\"\n    filename2 = \"random_crop_and_resize_01_py_result.npz\"\n    save_and_check_md5(data1, filename1, generate_golden=GENERATE_GOLDEN)\n    save_and_check_md5(data2, filename2, generate_golden=GENERATE_GOLDEN)\n\n    \n    ds.config.set_seed(original_seed)\n    ds.config.set_num_parallel_workers(original_num_parallel_workers)\n\n\ndef test_random_crop_and_resize_02():\n    \n    logger.info(\"test_random_crop_and_resize_02\")\n    original_seed = config_get_set_seed(0)\n    original_num_parallel_workers = config_get_set_num_parallel_workers(1)\n\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), interpolation=mode.Inter.NEAREST)\n    data1 = data1.map(input_columns=[\"image\"], operations=decode_op)\n    data1 = data1.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n\n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    transforms = [\n        py_vision.Decode(),\n        py_vision.RandomResizedCrop((256, 512), interpolation=mode.Inter.NEAREST),\n        py_vision.ToTensor()\n    ]\n    transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n    data2 = data2.map(input_columns=[\"image\"], operations=transform)\n\n    filename1 = \"random_crop_and_resize_02_c_result.npz\"\n    filename2 = \"random_crop_and_resize_02_py_result.npz\"\n    save_and_check_md5(data1, filename1, generate_golden=GENERATE_GOLDEN)\n    save_and_check_md5(data2, filename2, generate_golden=GENERATE_GOLDEN)\n\n    \n    ds.config.set_seed(original_seed)\n    ds.config.set_num_parallel_workers(original_num_parallel_workers)\n\n\ndef test_random_crop_and_resize_03():\n    \n    logger.info(\"test_random_crop_and_resize_03\")\n    original_seed = config_get_set_seed(0)\n    original_num_parallel_workers = config_get_set_num_parallel_workers(1)\n\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), max_attempts=1)\n    data1 = data1.map(input_columns=[\"image\"], operations=decode_op)\n    data1 = data1.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n\n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    transforms = [\n        py_vision.Decode(),\n        py_vision.RandomResizedCrop((256, 512), max_attempts=1),\n        py_vision.ToTensor()\n    ]\n    transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n    data2 = data2.map(input_columns=[\"image\"], operations=transform)\n\n    filename1 = \"random_crop_and_resize_03_c_result.npz\"\n    filename2 = \"random_crop_and_resize_03_py_result.npz\"\n    save_and_check_md5(data1, filename1, generate_golden=GENERATE_GOLDEN)\n    save_and_check_md5(data2, filename2, generate_golden=GENERATE_GOLDEN)\n\n    \n    ds.config.set_seed(original_seed)\n    ds.config.set_num_parallel_workers(original_num_parallel_workers)\n\n\ndef test_random_crop_and_resize_04_c():\n    \n    logger.info(\"test_random_crop_and_resize_04_c\")\n\n    \n    data = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    try:\n        \n        random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), (1, 0.5), (0.5, 0.5))\n        data = data.map(input_columns=[\"image\"], operations=decode_op)\n        data = data.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n    except ValueError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Input is not within the required interval of (0 to 16777216).\" in str(e)\n\n\ndef test_random_crop_and_resize_04_py():\n    \n    logger.info(\"test_random_crop_and_resize_04_py\")\n\n    \n    data = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    try:\n        transforms = [\n            py_vision.Decode(),\n            \n            py_vision.RandomResizedCrop((256, 512), (1, 0.5), (0.5, 0.5)),\n            py_vision.ToTensor()\n        ]\n        transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n        data = data.map(input_columns=[\"image\"], operations=transform)\n    except ValueError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Input is not within the required interval of (0 to 16777216).\" in str(e)\n\n\ndef test_random_crop_and_resize_05_c():\n    \n    logger.info(\"test_random_crop_and_resize_05_c\")\n\n    \n    data = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    try:\n        random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), (1, 1), (1, 0.5))\n        \n        data = data.map(input_columns=[\"image\"], operations=decode_op)\n        data = data.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n    except ValueError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Input is not within the required interval of (0 to 16777216).\" in str(e)\n\n\ndef test_random_crop_and_resize_05_py():\n    \n    logger.info(\"test_random_crop_and_resize_05_py\")\n\n    \n    data = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    try:\n        transforms = [\n            py_vision.Decode(),\n            \n            py_vision.RandomResizedCrop((256, 512), (1, 1), (1, 0.5)),\n            py_vision.ToTensor()\n        ]\n        transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n        data = data.map(input_columns=[\"image\"], operations=transform)\n    except ValueError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Input is not within the required interval of (0 to 16777216).\" in str(e)\n\n\ndef test_random_crop_and_resize_comp(plot=False):\n    \n    logger.info(\"test_random_crop_and_resize_comp\")\n\n    \n    data1 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    random_crop_and_resize_op = c_vision.RandomResizedCrop(512, (1, 1), (0.5, 0.5))\n    data1 = data1.map(input_columns=[\"image\"], operations=decode_op)\n    data1 = data1.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n\n    \n    data2 = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    transforms = [\n        py_vision.Decode(),\n        py_vision.RandomResizedCrop(512, (1, 1), (0.5, 0.5)),\n        py_vision.ToTensor()\n    ]\n    transform = mindspore.dataset.transforms.py_transforms.Compose(transforms)\n    data2 = data2.map(input_columns=[\"image\"], operations=transform)\n\n    image_c_cropped = []\n    image_py_cropped = []\n    for item1, item2 in zip(data1.create_dict_iterator(num_epochs=1), data2.create_dict_iterator(num_epochs=1)):\n        c_image = item1[\"image\"]\n        py_image = (item2[\"image\"].transpose(1, 2, 0) * 255).astype(np.uint8)\n        image_c_cropped.append(c_image)\n        image_py_cropped.append(py_image)\n        mse = diff_mse(c_image, py_image)\n        assert mse < 0.02  \n    if plot:\n        visualize_list(image_c_cropped, image_py_cropped, visualize_mode=2)\n\n\ndef test_random_crop_and_resize_06():\n    \n    logger.info(\"test_random_crop_and_resize_05_c\")\n\n    \n    data = ds.TFRecordDataset(DATA_DIR, SCHEMA_DIR, columns_list=[\"image\"], shuffle=False)\n    decode_op = c_vision.Decode()\n    try:\n        random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), scale=\"\", ratio=(1, 0.5))\n        data = data.map(input_columns=[\"image\"], operations=decode_op)\n        data.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n    except TypeError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Argument scale with value \\\"\\\" is not of type (<class 'tuple'>,)\" in str(e)\n\n    try:\n        random_crop_and_resize_op = c_vision.RandomResizedCrop((256, 512), scale=(1, \"2\"), ratio=(1, 0.5))\n        data = data.map(input_columns=[\"image\"], operations=decode_op)\n        data.map(input_columns=[\"image\"], operations=random_crop_and_resize_op)\n    except TypeError as e:\n        logger.info(\"Got an exception in DE: {}\".format(str(e)))\n        assert \"Argument scale[1] with value 2 is not of type (<class 'float'>, <class 'int'>).\" in str(e)\n\nif __name__ == \"__main__\":\n    test_random_crop_and_resize_op_c(True)\n    test_random_crop_and_resize_op_py(True)\n    test_random_crop_and_resize_01()\n    test_random_crop_and_resize_02()\n    test_random_crop_and_resize_03()\n    test_random_crop_and_resize_04_c()\n    test_random_crop_and_resize_04_py()\n    test_random_crop_and_resize_05_c()\n    test_random_crop_and_resize_05_py()\n    test_random_crop_and_resize_06()\n    test_random_crop_and_resize_comp(True)\n",
        "summary": "This code is a series of unit tests and visualizations for the `RandomResizedCrop` operation in the MindSpore library. The `RandomResizedCrop` operation randomly crops an image to a specified size while maintaining the aspect ratio, then resizes it back to the original size.\n\nThe tests cover various scenarios:\n\n1. Testing the C++ implementation of `RandomResizedCrop`.\n2. Testing the Python implementation of `RandomResizedCrop`.\n3. Verifying that the C++ and Python implementations produce similar results.\n4. Checking error handling for invalid input parameters, such as empty scale tuples or non-numeric values in the scale tuple.\n\nThe code also includes a function to plot the original images and their cropped versions side by side for visual comparison.\n\nHere's a breakdown of some key parts:\n\n1. **Test Functions**:\n   - `test_random_crop_and_resize_op_c(True)`: Runs C++ implementation tests.\n   - `test_random_crop_and_resize_op_py(True)`: Runs Python implementation tests.\n   - `test_random_crop_and_resize_01()`, etc.: Run specific test cases for different scenarios.\n\n2. **Error Handling**:\n   - The code includes checks to ensure that invalid input parameters raise appropriate exceptions (e.g., `TypeError`).\n\n3. **Visualization**:\n   - The `test_random_crop_and_resize_comp(True)` function plots the original and cropped images to visually verify correctness.\n\n4. **Utility Functions**:\n   - `diff_mse(c_image, py_image)`: Calculates Mean Squared Error between two images.\n   - `visualize_list(image_c_cropped, image_py_cropped, visualize_mode=2)`: Plots a list of images side by side.\n\nOverall, this code provides comprehensive testing and validation for the `RandomResizedCrop` operation in MindSpore, ensuring its correctness across different scenarios and implementations."
    },
    {
        "code": "import pprint\nimport re  \nimport six\nimport typing\nfrom enum import Enum\nfrom ask_sdk_model.interfaces.alexa.presentation.apl.command import Command\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.alexa.presentation.apl.command import Command as Command_bc5ff832\n\n\nclass ParallelCommand(Command):\n    \n    deserialized_types = {\n        'object_type': 'str',\n        'delay': 'int',\n        'description': 'str',\n        'when': 'bool',\n        'commands': 'list[ask_sdk_model.interfaces.alexa.presentation.apl.command.Command]'\n    }  \n\n    attribute_map = {\n        'object_type': 'type',\n        'delay': 'delay',\n        'description': 'description',\n        'when': 'when',\n        'commands': 'commands'\n    }  \n    supports_multiple_types = False\n\n    def __init__(self, delay=None, description=None, when=None, commands=None):\n        \n        \n        self.__discriminator_value = \"Parallel\"  \n\n        self.object_type = self.__discriminator_value\n        super(ParallelCommand, self).__init__(object_type=self.__discriminator_value, delay=delay, description=description, when=when)\n        self.commands = commands\n\n    def to_dict(self):\n        \n        \n        result = {}  \n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        \n        if not isinstance(other, ParallelCommand):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        \n        return not self == other\n",
        "summary": "The provided Python code defines a `ParallelCommand` class that inherits from the `Command` class. This class is used to represent parallel execution of multiple APL (Alexa Presentation Language) commands in an Alexa skill. It includes attributes for delay, description, condition, and a list of commands to be executed concurrently. The class provides methods for serialization to a dictionary, string representation, and equality checks."
    },
    {
        "code": "from time import sleep\n\nfrom test_framework.messages import msg_ping\nfrom test_framework.mininode import P2PInterface\nfrom test_framework.test_framework import TestCoinSuperTestFramework\n\nclass TestP2PConn(P2PInterface):\n    def on_version(self, message):\n        \n        pass\n\nclass TimeoutsTest(TestCoinSuperTestFramework):\n    def set_test_params(self):\n        self.setup_clean_chain = True\n        self.num_nodes = 1\n\n    def run_test(self):\n        \n        no_verack_node = self.nodes[0].add_p2p_connection(TestP2PConn())\n        no_version_node = self.nodes[0].add_p2p_connection(TestP2PConn(), send_version=False, wait_for_verack=False)\n        no_send_node = self.nodes[0].add_p2p_connection(TestP2PConn(), send_version=False, wait_for_verack=False)\n\n        sleep(1)\n\n        assert no_verack_node.is_connected\n        assert no_version_node.is_connected\n        assert no_send_node.is_connected\n\n        no_verack_node.send_message(msg_ping())\n        no_version_node.send_message(msg_ping())\n\n        sleep(30)\n\n        assert \"version\" in no_verack_node.last_message\n\n        assert no_verack_node.is_connected\n        assert no_version_node.is_connected\n        assert no_send_node.is_connected\n\n        no_verack_node.send_message(msg_ping())\n        no_version_node.send_message(msg_ping())\n\n        sleep(31)\n\n        assert not no_verack_node.is_connected\n        assert not no_version_node.is_connected\n        assert not no_send_node.is_connected\n\nif __name__ == '__main__':\n    TimeoutsTest().main()\n",
        "summary": "The provided Python code defines a test framework for evaluating the behavior of peer-to-peer connections in a blockchain network, specifically focusing on scenarios where nodes fail to send version messages or verack acknowledgments. The `TimeoutsTest` class sets up a single node and establishes three different types of P2P connections with varying behaviors regarding message sending. It then tests the persistence and eventual disconnection of these connections based on the absence of expected messages, demonstrating how the system handles timeouts in peer communication."
    },
    {
        "code": "MRPYTHON_VERSION_MAJOR = 3\nMRPYTHON_VERSION_MINOR = 0\nMRPYTHON_VERSION_PATCH = 9\nMRPYTHON_VERSION_TAG = \"beta\"\n\n\ndef version_string():\n    return \"{}.{}.{}{}\".format(MRPYTHON_VERSION_MAJOR,\n                               MRPYTHON_VERSION_MINOR,\n                               MRPYTHON_VERSION_PATCH,\n                               \"\" if MRPYTHON_VERSION_TAG == \"\" else (\"-\" + MRPYTHON_VERSION_TAG))\n\n",
        "summary": "The Python code defines version components for a software project, including major, minor, patch numbers, and an optional tag. It includes a function to generate a formatted version string based on these components."
    },
    {
        "code": "import torch\nimport torch.optim as optim\n\n\ndef set_optimizer(model, cfg):\n    r\n    if cfg.optimizer == 'SGD':\n        optimizer = optim.SGD(model.parameters(), lr=cfg.lr,\n                              momentum=cfg.momentum, weight_decay=cfg.weight_decay,\n                              nesterov=cfg.nesterov)\n    elif cfg.optimizer == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg.lr,\n                               betas=(cfg.momentum, 0.999),\n                               weight_decay=cfg.weight_decay)\n    return optimizer\n",
        "summary": "The provided Python code defines a function `set_optimizer` that configures and returns an instance of either the SGD or Adam optimizer from PyTorch based on the configuration settings in `cfg`. The function takes a model and a configuration object as inputs, checks the optimizer type specified in the configuration, and initializes the corresponding optimizer with parameters such as learning rate, momentum, weight decay, and other relevant settings."
    },
    {
        "code": "from PyQt5 import QtCore, QtGui, QtWidgets\n\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(873, 697)\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.gridLayout = QtWidgets.QGridLayout(self.centralwidget)\n        self.gridLayout.setObjectName(\"gridLayout\")\n        self.graphicsView = QtWidgets.QGraphicsView(self.centralwidget)\n        self.graphicsView.setObjectName(\"graphicsView\")\n        self.gridLayout.addWidget(self.graphicsView, 0, 0, 1, 1)\n        self.tabWidget = QtWidgets.QTabWidget(self.centralwidget)\n        self.tabWidget.setEnabled(True)\n        self.tabWidget.setMinimumSize(QtCore.QSize(251, 489))\n        self.tabWidget.setMaximumSize(QtCore.QSize(251, 16777215))\n        self.tabWidget.setTabPosition(QtWidgets.QTabWidget.North)\n        self.tabWidget.setObjectName(\"tabWidget\")\n        self.tab = QtWidgets.QWidget()\n        self.tab.setObjectName(\"tab\")\n        self.groupSetting = QtWidgets.QGroupBox(self.tab)\n        self.groupSetting.setGeometry(QtCore.QRect(10, 10, 221, 110))\n        self.groupSetting.setMinimumSize(QtCore.QSize(221, 110))\n        self.groupSetting.setMaximumSize(QtCore.QSize(221, 110))\n        self.groupSetting.setObjectName(\"groupSetting\")\n        self.horizontalLayoutWidget = QtWidgets.QWidget(self.groupSetting)\n        self.horizontalLayoutWidget.setGeometry(QtCore.QRect(10, 20, 201, 31))\n        self.horizontalLayoutWidget.setObjectName(\"horizontalLayoutWidget\")\n        self.horizontalLayout = QtWidgets.QHBoxLayout(self.horizontalLayoutWidget)\n        self.horizontalLayout.setContentsMargins(0, 0, 0, 0)\n        self.horizontalLayout.setObjectName(\"horizontalLayout\")\n        self.labelFruit = QtWidgets.QLabel(self.horizontalLayoutWidget)\n        self.labelFruit.setObjectName(\"labelFruit\")\n        self.horizontalLayout.addWidget(self.labelFruit)\n        self.comboBox = QtWidgets.QComboBox(self.horizontalLayoutWidget)\n        self.comboBox.setObjectName(\"comboBox\")\n        self.comboBox.addItem(\"\")\n        self.comboBox.addItem(\"\")\n        self.horizontalLayout.addWidget(self.comboBox)\n        self.horizontalLayoutWidget_3 = QtWidgets.QWidget(self.groupSetting)\n        self.horizontalLayoutWidget_3.setGeometry(QtCore.QRect(10, 60, 199, 31))\n        self.horizontalLayoutWidget_3.setObjectName(\"horizontalLayoutWidget_3\")\n        self.horizontalLayout_3 = QtWidgets.QHBoxLayout(self.horizontalLayoutWidget_3)\n        self.horizontalLayout_3.setContentsMargins(0, 0, 0, 0)\n        self.horizontalLayout_3.setObjectName(\"horizontalLayout_3\")\n        self.pushWifi = QtWidgets.QPushButton(self.horizontalLayoutWidget_3)\n        self.pushWifi.setObjectName(\"pushWifi\")\n        self.horizontalLayout_3.addWidget(self.pushWifi)\n        self.labelWifi = QtWidgets.QLabel(self.horizontalLayoutWidget_3)\n        self.labelWifi.setObjectName(\"labelWifi\")\n        self.horizontalLayout_3.addWidget(self.labelWifi)\n        self.groupCurve = QtWidgets.QGroupBox(self.tab)\n        self.groupCurve.setGeometry(QtCore.QRect(10, 130, 221, 211))\n        self.groupCurve.setMinimumSize(QtCore.QSize(221, 211))\n        self.groupCurve.setMaximumSize(QtCore.QSize(221, 211))\n        self.groupCurve.setObjectName(\"groupCurve\")\n        self.verticalLayoutWidget = QtWidgets.QWidget(self.groupCurve)\n        self.verticalLayoutWidget.setGeometry(QtCore.QRect(10, 30, 201, 168))\n        self.verticalLayoutWidget.setObjectName(\"verticalLayoutWidget\")\n        self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)\n        self.verticalLayout_2.setContentsMargins(0, 0, 0, 0)\n        self.verticalLayout_2.setObjectName(\"verticalLayout_2\")\n        self.horizontalLayout_2 = QtWidgets.QHBoxLayout()\n        self.horizontalLayout_2.setObjectName(\"horizontalLayout_2\")\n        self.labelScanTimes = QtWidgets.QLabel(self.verticalLayoutWidget)\n        self.labelScanTimes.setMaximumSize(QtCore.QSize(16777215, 28))\n        self.labelScanTimes.setObjectName(\"labelScanTimes\")\n        self.horizontalLayout_2.addWidget(self.labelScanTimes)\n        self.spinBox = QtWidgets.QSpinBox(self.verticalLayoutWidget)\n        self.spinBox.setMinimum(1)\n        self.spinBox.setMaximum(20)\n        self.spinBox.setProperty(\"value\", 3)\n        self.spinBox.setObjectName(\"spinBox\")\n        self.horizontalLayout_2.addWidget(self.spinBox)\n        self.verticalLayout_2.addLayout(self.horizontalLayout_2)\n        self.pushDetection = QtWidgets.QPushButton(self.verticalLayoutWidget)\n        self.pushDetection.setObjectName(\"pushDetection\")\n        self.verticalLayout_2.addWidget(self.pushDetection)\n        self.pushOriginal = QtWidgets.QPushButton(self.verticalLayoutWidget)\n        self.pushOriginal.setObjectName(\"pushOriginal\")\n        self.verticalLayout_2.addWidget(self.pushOriginal)\n        self.pushDerivative = QtWidgets.QPushButton(self.verticalLayoutWidget)\n        self.pushDerivative.setObjectName(\"pushDerivative\")\n        self.verticalLayout_2.addWidget(self.pushDerivative)\n        self.pushIntegral = QtWidgets.QPushButton(self.verticalLayoutWidget)\n        self.pushIntegral.setObjectName(\"pushIntegral\")\n        self.verticalLayout_2.addWidget(self.pushIntegral)\n        self.tableWidget = QtWidgets.QTableWidget(self.tab)\n        self.tableWidget.setGeometry(QtCore.QRect(10, 350, 221, 261))\n        self.tableWidget.setMinimumSize(QtCore.QSize(221, 0))\n        self.tableWidget.setMaximumSize(QtCore.QSize(221, 16777215))\n        self.tableWidget.setObjectName(\"tableWidget\")\n        self.tableWidget.setColumnCount(1)\n        self.tableWidget.setRowCount(6)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(0, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(1, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(2, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(3, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(4, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setVerticalHeaderItem(5, item)\n        item = QtWidgets.QTableWidgetItem()\n        font = QtGui.QFont()\n        font.setPointSize(7)\n        item.setFont(font)\n        self.tableWidget.setHorizontalHeaderItem(0, item)\n        self.tabWidget.addTab(self.tab, \"\")\n        self.tab_2 = QtWidgets.QWidget()\n        self.tab_2.setObjectName(\"tab_2\")\n        self.groupLine_1 = QtWidgets.QGroupBox(self.tab_2)\n        self.groupLine_1.setGeometry(QtCore.QRect(10, 10, 221, 141))\n        self.groupLine_1.setObjectName(\"groupLine_1\")\n        self.formLayoutWidget = QtWidgets.QWidget(self.groupLine_1)\n        self.formLayoutWidget.setGeometry(QtCore.QRect(20, 20, 181, 101))\n        self.formLayoutWidget.setObjectName(\"formLayoutWidget\")\n        self.formLayout = QtWidgets.QFormLayout(self.formLayoutWidget)\n        self.formLayout.setContentsMargins(0, 0, 0, 0)\n        self.formLayout.setObjectName(\"formLayout\")\n        self.labelLineWidth_1 = QtWidgets.QLabel(self.formLayoutWidget)\n        self.labelLineWidth_1.setObjectName(\"labelLineWidth_1\")\n        self.formLayout.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.labelLineWidth_1)\n        self.horizontalSlider_1 = QtWidgets.QSlider(self.formLayoutWidget)\n        self.horizontalSlider_1.setOrientation(QtCore.Qt.Horizontal)\n        self.horizontalSlider_1.setObjectName(\"horizontalSlider_1\")\n        self.formLayout.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.horizontalSlider_1)\n        self.labelColor_1 = QtWidgets.QLabel(self.formLayoutWidget)\n        self.labelColor_1.setObjectName(\"labelColor_1\")\n        self.formLayout.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.labelColor_1)\n        self.comboColor_1 = QtWidgets.QComboBox(self.formLayoutWidget)\n        self.comboColor_1.setObjectName(\"comboColor_1\")\n        self.comboColor_1.addItem(\"\")\n        self.comboColor_1.addItem(\"\")\n        self.comboColor_1.addItem(\"\")\n        self.comboColor_1.addItem(\"\")\n        self.comboColor_1.addItem(\"\")\n        self.comboColor_1.addItem(\"\")\n        self.formLayout.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.comboColor_1)\n        self.checkVisible_1 = QtWidgets.QCheckBox(self.formLayoutWidget)\n        self.checkVisible_1.setChecked(True)\n        self.checkVisible_1.setObjectName(\"checkVisible_1\")\n        self.formLayout.setWidget(2, QtWidgets.QFormLayout.FieldRole, self.checkVisible_1)\n        self.groupLine_2 = QtWidgets.QGroupBox(self.tab_2)\n        self.groupLine_2.setGeometry(QtCore.QRect(10, 170, 221, 141))\n        self.groupLine_2.setObjectName(\"groupLine_2\")\n        self.formLayoutWidget_3 = QtWidgets.QWidget(self.groupLine_2)\n        self.formLayoutWidget_3.setGeometry(QtCore.QRect(20, 20, 181, 101))\n        self.formLayoutWidget_3.setObjectName(\"formLayoutWidget_3\")\n        self.formLayout_3 = QtWidgets.QFormLayout(self.formLayoutWidget_3)\n        self.formLayout_3.setContentsMargins(0, 0, 0, 0)\n        self.formLayout_3.setObjectName(\"formLayout_3\")\n        self.labelLineWidth_2 = QtWidgets.QLabel(self.formLayoutWidget_3)\n        self.labelLineWidth_2.setObjectName(\"labelLineWidth_2\")\n        self.formLayout_3.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.labelLineWidth_2)\n        self.horizontalSlider_3 = QtWidgets.QSlider(self.formLayoutWidget_3)\n        self.horizontalSlider_3.setOrientation(QtCore.Qt.Horizontal)\n        self.horizontalSlider_3.setObjectName(\"horizontalSlider_3\")\n        self.formLayout_3.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.horizontalSlider_3)\n        self.labelColor_2 = QtWidgets.QLabel(self.formLayoutWidget_3)\n        self.labelColor_2.setObjectName(\"labelColor_2\")\n        self.formLayout_3.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.labelColor_2)\n        self.comboColor_2 = QtWidgets.QComboBox(self.formLayoutWidget_3)\n        self.comboColor_2.setObjectName(\"comboColor_2\")\n        self.comboColor_2.addItem(\"\")\n        self.comboColor_2.addItem(\"\")\n        self.comboColor_2.addItem(\"\")\n        self.comboColor_2.addItem(\"\")\n        self.comboColor_2.addItem(\"\")\n        self.comboColor_2.addItem(\"\")\n        self.formLayout_3.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.comboColor_2)\n        self.checkVisible_2 = QtWidgets.QCheckBox(self.formLayoutWidget_3)\n        self.checkVisible_2.setChecked(True)\n        self.checkVisible_2.setObjectName(\"checkVisible_2\")\n        self.formLayout_3.setWidget(2, QtWidgets.QFormLayout.FieldRole, self.checkVisible_2)\n        self.groupLine_3 = QtWidgets.QGroupBox(self.tab_2)\n        self.groupLine_3.setGeometry(QtCore.QRect(10, 330, 221, 141))\n        self.groupLine_3.setObjectName(\"groupLine_3\")\n        self.formLayoutWidget_4 = QtWidgets.QWidget(self.groupLine_3)\n        self.formLayoutWidget_4.setGeometry(QtCore.QRect(20, 20, 181, 101))\n        self.formLayoutWidget_4.setObjectName(\"formLayoutWidget_4\")\n        self.formLayout_4 = QtWidgets.QFormLayout(self.formLayoutWidget_4)\n        self.formLayout_4.setContentsMargins(0, 0, 0, 0)\n        self.formLayout_4.setObjectName(\"formLayout_4\")\n        self.labelLineWidth_3 = QtWidgets.QLabel(self.formLayoutWidget_4)\n        self.labelLineWidth_3.setObjectName(\"labelLineWidth_3\")\n        self.formLayout_4.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.labelLineWidth_3)\n        self.horizontalSlider_4 = QtWidgets.QSlider(self.formLayoutWidget_4)\n        self.horizontalSlider_4.setOrientation(QtCore.Qt.Horizontal)\n        self.horizontalSlider_4.setObjectName(\"horizontalSlider_4\")\n        self.formLayout_4.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.horizontalSlider_4)\n        self.labelColor_3 = QtWidgets.QLabel(self.formLayoutWidget_4)\n        self.labelColor_3.setObjectName(\"labelColor_3\")\n        self.formLayout_4.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.labelColor_3)\n        self.comboColor_3 = QtWidgets.QComboBox(self.formLayoutWidget_4)\n        self.comboColor_3.setObjectName(\"comboColor_3\")\n        self.comboColor_3.addItem(\"\")\n        self.comboColor_3.addItem(\"\")\n        self.comboColor_3.addItem(\"\")\n        self.comboColor_3.addItem(\"\")\n        self.comboColor_3.addItem(\"\")\n        self.comboColor_3.addItem(\"\")\n        self.formLayout_4.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.comboColor_3)\n        self.checkVisible_3 = QtWidgets.QCheckBox(self.formLayoutWidget_4)\n        self.checkVisible_3.setChecked(True)\n        self.checkVisible_3.setObjectName(\"checkVisible_3\")\n        self.formLayout_4.setWidget(2, QtWidgets.QFormLayout.FieldRole, self.checkVisible_3)\n        self.tabWidget.addTab(self.tab_2, \"\")\n        self.gridLayout.addWidget(self.tabWidget, 0, 1, 1, 1)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 873, 26))\n        self.menubar.setObjectName(\"menubar\")\n        self.menuFile = QtWidgets.QMenu(self.menubar)\n        self.menuFile.setObjectName(\"menuFile\")\n        self.menuSave = QtWidgets.QMenu(self.menuFile)\n        self.menuSave.setObjectName(\"menuSave\")\n        self.menuSettings = QtWidgets.QMenu(self.menubar)\n        self.menuSettings.setObjectName(\"menuSettings\")\n        self.menuView = QtWidgets.QMenu(self.menubar)\n        self.menuView.setObjectName(\"menuView\")\n        self.menuHelp = QtWidgets.QMenu(self.menubar)\n        self.menuHelp.setObjectName(\"menuHelp\")\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n        self.actionNew = QtWidgets.QAction(MainWindow)\n        self.actionNew.setObjectName(\"actionNew\")\n        self.actionOpen = QtWidgets.QAction(MainWindow)\n        self.actionOpen.setObjectName(\"actionOpen\")\n        self.actionSave_Data = QtWidgets.QAction(MainWindow)\n        self.actionSave_Data.setObjectName(\"actionSave_Data\")\n        self.actionSave_Graph = QtWidgets.QAction(MainWindow)\n        self.actionSave_Graph.setObjectName(\"actionSave_Graph\")\n        self.actionLine = QtWidgets.QAction(MainWindow)\n        self.actionLine.setObjectName(\"actionLine\")\n        self.actionUsage = QtWidgets.QAction(MainWindow)\n        self.actionUsage.setObjectName(\"actionUsage\")\n        self.actionAbout = QtWidgets.QAction(MainWindow)\n        self.actionAbout.setObjectName(\"actionAbout\")\n        self.actionCopyright = QtWidgets.QAction(MainWindow)\n        self.actionCopyright.setObjectName(\"actionCopyright\")\n        self.actionWi_Fi_Setting = QtWidgets.QAction(MainWindow)\n        self.actionWi_Fi_Setting.setObjectName(\"actionWi_Fi_Setting\")\n        self.menuSave.addAction(self.actionSave_Data)\n        self.menuSave.addAction(self.actionSave_Graph)\n        self.menuFile.addAction(self.actionNew)\n        self.menuFile.addAction(self.actionOpen)\n        self.menuFile.addAction(self.menuSave.menuAction())\n        self.menuSettings.addAction(self.actionLine)\n        self.menuSettings.addAction(self.actionWi_Fi_Setting)\n        self.menuHelp.addAction(self.actionUsage)\n        self.menuHelp.addAction(self.actionAbout)\n        self.menuHelp.addAction(self.actionCopyright)\n        self.menubar.addAction(self.menuFile.menuAction())\n        self.menubar.addAction(self.menuSettings.menuAction())\n        self.menubar.addAction(self.menuView.menuAction())\n        self.menubar.addAction(self.menuHelp.menuAction())\n\n        self.retranslateUi(MainWindow)\n        self.tabWidget.setCurrentIndex(1)\n        self.checkVisible_1.clicked.connect(self.checkVisible_1.click)\n        self.checkVisible_2.clicked.connect(self.checkVisible_2.click)\n        self.checkVisible_3.clicked.connect(self.checkVisible_3.click)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"\u6c34\u679c\u5149\u8c31\u68c0\u6d4b\"))\n        self.groupSetting.setTitle(_translate(\"MainWindow\", \"Setting\"))\n        self.labelFruit.setText(_translate(\"MainWindow\", \"Fruit\"))\n        self.comboBox.setItemText(0, _translate(\"MainWindow\", \"None\"))\n        self.comboBox.setItemText(1, _translate(\"MainWindow\", \"Apple\"))\n        self.pushWifi.setText(_translate(\"MainWindow\", \"Wi-Fi\"))\n        self.labelWifi.setText(_translate(\"MainWindow\", \"unconnected\"))\n        self.groupCurve.setTitle(_translate(\"MainWindow\", \"Curve\"))\n        self.labelScanTimes.setText(_translate(\"MainWindow\", \"ScanTimes\"))\n        self.pushDetection.setText(_translate(\"MainWindow\", \"Spectral Detection\"))\n        self.pushOriginal.setText(_translate(\"MainWindow\", \"Original Time\"))\n        self.pushDerivative.setText(_translate(\"MainWindow\", \"Derivative Time\"))\n        self.pushIntegral.setText(_translate(\"MainWindow\", \"Integral Time\"))\n        item = self.tableWidget.verticalHeaderItem(0)\n        item.setText(_translate(\"MainWindow\", \"Energy\"))\n        item = self.tableWidget.verticalHeaderItem(1)\n        item.setText(_translate(\"MainWindow\", \"Carbohydrates\"))\n        item = self.tableWidget.verticalHeaderItem(2)\n        item.setText(_translate(\"MainWindow\", \"-Sugars\"))\n        item = self.tableWidget.verticalHeaderItem(3)\n        item.setText(_translate(\"MainWindow\", \"Protein\"))\n        item = self.tableWidget.verticalHeaderItem(4)\n        item.setText(_translate(\"MainWindow\", \"New Row\"))\n        item = self.tableWidget.verticalHeaderItem(5)\n        item.setText(_translate(\"MainWindow\", \"Sodium\"))\n        item = self.tableWidget.horizontalHeaderItem(0)\n        item.setText(_translate(\"MainWindow\", \"Per 100g\"))\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab), _translate(\"MainWindow\", \"\u68c0\u6d4b\"))\n        self.groupLine_1.setTitle(_translate(\"MainWindow\", \"Line1\"))\n        self.labelLineWidth_1.setText(_translate(\"MainWindow\", \"Width\"))\n        self.labelColor_1.setText(_translate(\"MainWindow\", \"Color\"))\n        self.comboColor_1.setItemText(0, _translate(\"MainWindow\", \"Black\"))\n        self.comboColor_1.setItemText(1, _translate(\"MainWindow\", \"Gray\"))\n        self.comboColor_1.setItemText(2, _translate(\"MainWindow\", \"White\"))\n        self.comboColor_1.setItemText(3, _translate(\"MainWindow\", \"Red\"))\n        self.comboColor_1.setItemText(4, _translate(\"MainWindow\", \"Green\"))\n        self.comboColor_1.setItemText(5, _translate(\"MainWindow\", \"Blue\"))\n        self.checkVisible_1.setText(_translate(\"MainWindow\", \"Visible\"))\n        self.groupLine_2.setTitle(_translate(\"MainWindow\", \"Line2\"))\n        self.labelLineWidth_2.setText(_translate(\"MainWindow\", \"Width\"))\n        self.labelColor_2.setText(_translate(\"MainWindow\", \"Color\"))\n        self.comboColor_2.setItemText(0, _translate(\"MainWindow\", \"Green\"))\n        self.comboColor_2.setItemText(1, _translate(\"MainWindow\", \"Black\"))\n        self.comboColor_2.setItemText(2, _translate(\"MainWindow\", \"Gray\"))\n        self.comboColor_2.setItemText(3, _translate(\"MainWindow\", \"White\"))\n        self.comboColor_2.setItemText(4, _translate(\"MainWindow\", \"Red\"))\n        self.comboColor_2.setItemText(5, _translate(\"MainWindow\", \"Blue\"))\n        self.checkVisible_2.setText(_translate(\"MainWindow\", \"Visible\"))\n        self.groupLine_3.setTitle(_translate(\"MainWindow\", \"Line3\"))\n        self.labelLineWidth_3.setText(_translate(\"MainWindow\", \"Width\"))\n        self.labelColor_3.setText(_translate(\"MainWindow\", \"Color\"))\n        self.comboColor_3.setItemText(0, _translate(\"MainWindow\", \"Red\"))\n        self.comboColor_3.setItemText(1, _translate(\"MainWindow\", \"Black\"))\n        self.comboColor_3.setItemText(2, _translate(\"MainWindow\", \"Gray\"))\n        self.comboColor_3.setItemText(3, _translate(\"MainWindow\", \"White\"))\n        self.comboColor_3.setItemText(4, _translate(\"MainWindow\", \"Green\"))\n        self.comboColor_3.setItemText(5, _translate(\"MainWindow\", \"Blue\"))\n        self.checkVisible_3.setText(_translate(\"MainWindow\", \"Visible\"))\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab_2), _translate(\"MainWindow\", \"\u8bbe\u7f6e\"))\n        self.menuFile.setTitle(_translate(\"MainWindow\", \"File\"))\n        self.menuSave.setTitle(_translate(\"MainWindow\", \"Save\"))\n        self.menuSettings.setTitle(_translate(\"MainWindow\", \"Settings\"))\n        self.menuView.setTitle(_translate(\"MainWindow\", \"View\"))\n        self.menuHelp.setTitle(_translate(\"MainWindow\", \"Help\"))\n        self.actionNew.setText(_translate(\"MainWindow\", \"New\"))\n        self.actionOpen.setText(_translate(\"MainWindow\", \"Open\"))\n        self.actionSave_Data.setText(_translate(\"MainWindow\", \"Save Data\"))\n        self.actionSave_Graph.setText(_translate(\"MainWindow\", \"Save Graph\"))\n        self.actionLine.setText(_translate(\"MainWindow\", \"Line Setting\"))\n        self.actionUsage.setText(_translate(\"MainWindow\", \"Usage\"))\n        self.actionAbout.setText(_translate(\"MainWindow\", \"About\"))\n        self.actionCopyright.setText(_translate(\"MainWindow\", \"Copyright\"))\n        self.actionWi_Fi_Setting.setText(_translate(\"MainWindow\", \"Wi-Fi Setting\"))\n",
        "summary": "This code snippet appears to be a Python script using the PyQt5 library for creating a graphical user interface (GUI) application. The GUI includes various widgets such as labels, combo boxes, checkboxes, and buttons arranged in tabs.\n\nHere's a breakdown of what each part does:\n\n1. **Imports**: It imports necessary modules from PyQt5 like `QApplication`, `QWidget`, `QVBoxLayout`, etc., which are used to build the GUI components.\n\n2. **Main Window Class (`MainWindow`)**: This class inherits from `QWidget`. It sets up the main window of the application, including its layout and adding child widgets.\n\n3. **Tabs**: The GUI has two tabs:\n   - \"\u68c0\u6d4b\" (Detection) tab which contains controls for spectral detection.\n   - \"\u8bbe\u7f6e\" (Settings) tab with options to configure lines (Line1, Line2, Line3).\n\n4. **Widgets in Tabs**:\n   - In the Detection tab, there are buttons for different types of time measurements and a table widget to display data.\n   - The Settings tab includes combo boxes for selecting colors and line widths, along with checkboxes to control visibility.\n\n5. **Actions**: The script also defines actions for menu items like \"New\", \"Open\", \"Save Data\", etc., which can be triggered from the application's menu bar or context menus.\n\n6. **Translation**: The `translate` function is used extensively to set text labels and tooltips in multiple languages, as indicated by the `_translate(\"MainWindow\", \"...\")` calls.\n\n7. **Layout Management**: Widgets are added to layouts (like `QVBoxLayout`, `QHBoxLayout`) which manage their positioning within the parent widget or tab.\n\n8. **Signals and Slots**: Although not shown here, PyQt5 uses signals and slots for event handling. For example, when a button is clicked, it might emit a signal that triggers a slot function to perform an action.\n\nThis script is likely part of a larger application designed for spectral analysis or data visualization, where users can configure settings and perform various operations on their data."
    },
    {
        "code": "from .hn import NewsClient",
        "summary": "The provided Python code imports the `NewsClient` class from the module named `hn`. This client is likely used for interacting with a news-related API or service, enabling functionalities such as fetching news articles or updates."
    },
    {
        "code": "from functools import reduce\nfrom itertools import chain\nfrom operator import add\nfrom typing import Iterable, Optional, TypeVar\n\nfrom lhotse.audio import Recording, RecordingSet\nfrom lhotse.cut import Cut, CutSet, MixedCut\nfrom lhotse.features import FeatureSet, Features\nfrom lhotse.supervision import SupervisionSegment, SupervisionSet\nfrom lhotse.utils import Pathlike, load_yaml\n\nManifestItem = TypeVar('ManifestItem', Recording, SupervisionSegment, Features, Cut, MixedCut)\nManifest = TypeVar('Manifest', RecordingSet, SupervisionSet, FeatureSet, CutSet)\n\n\ndef combine(*manifests: Manifest) -> Manifest:\n    \n    return reduce(add, manifests)\n\n\ndef to_manifest(items: Iterable[ManifestItem]) -> Optional[Manifest]:\n    \n    items = iter(items)\n    try:\n        first_item = next(items)\n    except StopIteration:\n        return None\n    items = chain([first_item], items)\n\n    if isinstance(first_item, Recording):\n        return RecordingSet.from_recordings(items)\n    if isinstance(first_item, SupervisionSegment):\n        return SupervisionSet.from_segments(items)\n    if isinstance(first_item, (Cut, MixedCut)):\n        return CutSet.from_cuts(items)\n    if isinstance(first_item, Features):\n        raise ValueError(\"FeatureSet generic construction from iterable is not possible, as the config information \"\n                         \"would have been lost. Call FeatureSet.from_features() directly instead.\")\n\n    raise ValueError(f\"Unknown type of manifest item: {first_item}\")\n\n\ndef load_manifest(path: Pathlike) -> Manifest:\n    \n    raw_data = load_yaml(path)\n    data_set = None\n    for manifest_type in [RecordingSet, SupervisionSet, FeatureSet, CutSet]:\n        try:\n            data_set = manifest_type.from_dicts(raw_data)\n        except Exception:\n            pass\n    if data_set is None:\n        raise ValueError(f'Unknown type of manifest: {path}')\n    return data_set\n",
        "summary": "The provided Python code defines functions to combine multiple manifests (collections of audio recordings, supervision segments, features, or cuts) into a single manifest using the `combine` function. The `to_manifest` function converts an iterable of manifest items into the appropriate type of manifest based on the first item's type. The `load_manifest` function reads a YAML file and attempts to create a manifest of the appropriate type from the data contained within."
    },
    {
        "code": "from functools import partial\n\n\n\nfrom magenta.models.improv_rnn import improv_rnn_model\nimport magenta.music as mm\n\n\nclass ImprovRnnSequenceGenerator(mm.BaseSequenceGenerator):\n  \n\n  def __init__(self, model, details, steps_per_quarter=4, checkpoint=None,\n               bundle=None):\n    \n    super(ImprovRnnSequenceGenerator, self).__init__(\n        model, details, checkpoint, bundle)\n    self.steps_per_quarter = steps_per_quarter\n\n  def _generate(self, input_sequence, generator_options):\n    if len(generator_options.input_sections) > 1:\n      raise mm.SequenceGeneratorException(\n          'This model supports at most one input_sections message, but got %s' %\n          len(generator_options.input_sections))\n    if len(generator_options.generate_sections) != 1:\n      raise mm.SequenceGeneratorException(\n          'This model supports only 1 generate_sections message, but got %s' %\n          len(generator_options.generate_sections))\n\n    qpm = (input_sequence.tempos[0].qpm\n           if input_sequence and input_sequence.tempos\n           else mm.DEFAULT_QUARTERS_PER_MINUTE)\n    steps_per_second = mm.steps_per_quarter_to_steps_per_second(\n        self.steps_per_quarter, qpm)\n\n    generate_section = generator_options.generate_sections[0]\n    if generator_options.input_sections:\n      \n      \n      input_section = generator_options.input_sections[0]\n      primer_sequence = mm.trim_note_sequence(\n          input_sequence, input_section.start_time, input_section.end_time)\n      backing_sequence = mm.trim_note_sequence(\n          input_sequence, input_section.start_time, generate_section.end_time)\n      input_start_step = mm.quantize_to_step(\n          input_section.start_time, steps_per_second, quantize_cutoff=0.0)\n    else:\n      \n      \n      primer_sequence = mm.trim_note_sequence(\n          input_sequence, 0.0, generate_section.start_time)\n      backing_sequence = mm.trim_note_sequence(\n          input_sequence, 0.0, generate_section.end_time)\n      input_start_step = 0\n\n    last_end_time = (max(n.end_time for n in primer_sequence.notes)\n                     if primer_sequence.notes else 0)\n    if last_end_time >= generate_section.start_time:\n      raise mm.SequenceGeneratorException(\n          'Got GenerateSection request for section that is before or equal to '\n          'the end of the input section. This model can only extend melodies. '\n          'Requested start time: %s, Final note end time: %s' %\n          (generate_section.start_time, last_end_time))\n\n    \n    quantized_primer_sequence = mm.quantize_note_sequence(\n        primer_sequence, self.steps_per_quarter)\n    quantized_backing_sequence = mm.quantize_note_sequence(\n        backing_sequence, self.steps_per_quarter)\n\n    \n    extracted_melodies, _ = mm.extract_melodies(\n        quantized_primer_sequence, search_start_step=input_start_step,\n        min_bars=0, min_unique_pitches=1, gap_bars=float('inf'),\n        ignore_polyphonic_notes=True)\n    assert len(extracted_melodies) <= 1\n\n    start_step = mm.quantize_to_step(\n        generate_section.start_time, steps_per_second, quantize_cutoff=0.0)\n    \n    \n    \n    end_step = mm.quantize_to_step(\n        generate_section.end_time, steps_per_second, quantize_cutoff=1.0)\n\n    if extracted_melodies and extracted_melodies[0]:\n      melody = extracted_melodies[0]\n    else:\n      \n      \n      \n      steps_per_bar = int(\n          mm.steps_per_bar_in_quantized_sequence(quantized_primer_sequence))\n      melody = mm.Melody([],\n                         start_step=max(0, start_step - 1),\n                         steps_per_bar=steps_per_bar,\n                         steps_per_quarter=self.steps_per_quarter)\n\n    extracted_chords, _ = mm.extract_chords(quantized_backing_sequence)\n    chords = extracted_chords[0]\n\n    \n    if chords.start_step < melody.start_step:\n      chords.set_length(len(chords) - melody.start_step + chords.start_step)\n\n    assert chords.end_step == end_step\n\n    \n    melody.set_length(start_step - melody.start_step)\n\n    \n    arg_types = {\n        'temperature': lambda arg: arg.float_value,\n        'beam_size': lambda arg: arg.int_value,\n        'branch_factor': lambda arg: arg.int_value,\n        'steps_per_iteration': lambda arg: arg.int_value\n    }\n    args = dict((name, value_fn(generator_options.args[name]))\n                for name, value_fn in arg_types.items()\n                if name in generator_options.args)\n\n    generated_melody = self._model.generate_melody(melody, chords, **args)\n    generated_lead_sheet = mm.LeadSheet(generated_melody, chords)\n    generated_sequence = generated_lead_sheet.to_sequence(qpm=qpm)\n    assert (generated_sequence.total_time - generate_section.end_time) <= 1e-5\n    return generated_sequence\n\n\ndef get_generator_map():\n  \n  def create_sequence_generator(config, **kwargs):\n    return ImprovRnnSequenceGenerator(\n        improv_rnn_model.ImprovRnnModel(config), config.details,\n        steps_per_quarter=config.steps_per_quarter, **kwargs)\n\n  return {key: partial(create_sequence_generator, config)\n          for (key, config) in improv_rnn_model.default_configs.items()}\n",
        "summary": "The provided Python code defines a class `ImprovRnnSequenceGenerator` that extends `mm.BaseSequenceGenerator` to generate musical sequences using an Improv RNN model. It handles input and output sequences, quantizes them, extracts melodies and chords, and uses these to generate new music based on the specified parameters. The `get_generator_map` function creates instances of this generator with default configurations for different models."
    },
    {
        "code": "from csrv.model import actions\nfrom csrv.model.actions import play_run_event\nfrom csrv.model import cost\nfrom csrv.model import events\nfrom csrv.model import timing_phases\nfrom csrv.model.cards import card_info\nfrom csrv.model.cards import event\n\n\nclass TrashForFree(actions.TrashOnAccess):\n  COST_CLASS = cost.NullCost\n\n  def is_usable(self):\n    return actions.TrashOnAccess.is_usable(self) and self.card.is_being_accessed\n\n\nclass Card01003Action(play_run_event.PlayRunEvent):\n\n  def resolve(self, response=None, ignore_clicks=False, ignore_all_costs=False):\n    play_run_event.PlayRunEvent.resolve(\n        self, response, ignore_clicks=ignore_clicks,\n        ignore_all_costs=ignore_all_costs)\n    self.game.register_choice_provider(\n        timing_phases.AccessCard, self, 'access_card_actions')\n    self.game.register_listener(events.RunEnds, self)\n\n  def access_card_actions(self):\n    card = self.game.current_phase().card  \n    return [TrashForFree(self.game, self.player, card)]\n\n  def on_run_ends(self, sender, event):\n    self.game.deregister_choice_provider(\n        timing_phases.AccessCard, self, 'access_card_actions')\n    self.game.deregister_listener(events.RunEnds, self)\n\n\nclass Card01003(event.Event):\n\n  NAME = u'Card01003'\n  SET = card_info.CORE\n  NUMBER = 3\n  SIDE = card_info.RUNNER\n  FACTION = card_info.ANARCH\n  INFLUENCE = 2\n  UNIQUE = False\n  KEYWORDS = set([\n      card_info.RUN,\n      card_info.SABOTAGE,\n  ])\n  COST = 2\n  IMAGE_SRC = '01003.png'\n\n  def build_actions(self):\n    event.Event.build_actions(self)\n    self._play_event_action = Card01003Action(self.game, self.player, self)\n",
        "summary": "The provided Python code defines a custom action class `TrashForFree` that extends the base `TrashOnAccess` action to allow trashing a card for free if it is being accessed. It also includes an event class `Card01003Action` that extends `PlayRunEvent`, adding specific behavior during a run, such as registering choice providers and listeners, and handling actions when the run ends. The `Card01003` class represents a card with unique properties and keywords, including running and sabotaging abilities, and it builds an action to play this event."
    },
    {
        "code": "project = \"PyData Sphinx Theme\"\ncopyright = \"2019, PyData Community\"\nauthor = \"PyData Community\"\n\n\nrelease = \"0.0.1dev0\"\n\n\n\n\n\n\n\n\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"numpydoc\",\n    \"recommonmark\",\n    \"jupyter_sphinx\",\n]\n\nautosummary_generate = True\n\n\ntemplates_path = [\"_templates\"]\n\n\n\n\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\nhtml_sidebars = {\n    \"contributing\": [\"sidebar-search-bs.html\", \"custom-template.html\"],\n    \"changelog\": [],\n}\n\n\n\n\n\n\nhtml_theme = \"pydata_sphinx_theme\"\nhtml_logo = \"_static/pandas.svg\"\n\nhtml_theme_options = {\n    \"external_links\": [\n        {\"url\": \"https://pandas.pydata.org/pandas-docs/stable/\", \"name\": \"Pandas Docs\"}\n    ],\n    \"github_url\": \"https://github.com/pydata/pydata-sphinx-theme\",\n    \"twitter_url\": \"https://twitter.com/pandas_dev\",\n    \"icon_links\": [\n        {\n            \"name\": \"PyPI\",\n            \"url\": \"https://pypi.org/project/pydata-sphinx-theme\",\n            \"icon\": \"fas fa-box\",\n        }\n    ],\n    \"use_edit_page_button\": True,\n    \"show_toc_level\": 1,\n    \n}\n\nhtml_context = {\n    \"github_user\": \"pandas-dev\",\n    \"github_repo\": \"pydata-sphinx-theme\",\n    \"github_version\": \"master\",\n    \"doc_path\": \"docs\",\n}\n\n\n\n\nhtml_static_path = [\"_static\"]\n\n\n\nimport recommonmark\nfrom recommonmark.transform import AutoStructify\n\n\ndef setup(app):\n    app.add_transform(AutoStructify)\n",
        "summary": "This Python code configures Sphinx documentation for a project named \"PyData Sphinx Theme,\" setting up extensions, templates, and theme options to generate structured and thematically consistent documentation. It includes settings for sidebar navigation, external links, and integration with GitHub and PyPI, tailored for a community-driven project like PyData."
    },
    {
        "code": "from charms.reactive import Endpoint, when, set_flag, clear_flag\nimport charmhelpers.core.hookenv as hookenv\nfrom charmhelpers.core.hookenv import log\n\nclass GearmanRequires(Endpoint):\n\n    @when('endpoint.{endpoint_name}.joined')\n    def joined(self):\n        \n        set_flag(self.expand_name('available'))\n\n    @when('endpoint.{endpoint_name}.changed')\n    def changed(self):\n        \n        set_flag(self.expand_name('available'))\n\n    def address(self):\n        \n        for relation in self.relations:\n            for unit in relation.joined_units:\n                log(\"Unit: {}\".format(unit.received))\n                address = unit.received['ingress-address']\n                if address is not None:\n                    return address\n",
        "summary": "The provided Python code defines a custom charm endpoint class named `GearmanRequires` that inherits from `Endpoint`. This class handles the lifecycle events for a relation between charms, specifically when the 'joined' or 'changed' events occur. Upon these events, it sets a flag indicating availability. The `address` method iterates through related units to retrieve and return an ingress address if available, logging each unit's received data during the process."
    },
    {
        "code": "import csv\nimport sys\nfrom flee import flee\nfrom flee import SimulationSettings\n\n\nclass InputGeography:\n    \n\n    def __init__(self):\n        self.locations = []\n        self.links = []\n\n    def ReadLocationsFromCSV(self, csv_name, columns=[\"name\", \"region\", \"country\", \"gps_x\", \"gps_y\", \"location_type\", \"conflict_date\", \"pop/cap\"]):\n        \n        self.locations = []\n\n        c = {}  \n\n        c[\"location_type\"] = 0\n        c[\"conflict_date\"] = 0\n        c[\"country\"] = 0\n        c[\"region\"] = 0\n\n        for i in range(0, len(columns)):\n            c[columns[i]] = i\n\n        with open(csv_name, newline='') as csvfile:\n            values = csv.reader(csvfile)\n\n            for row in values:\n                if row[0][0] == \"\n                    pass\n                else:\n                    \n                    self.locations.append([row[c[\"name\"]], row[c[\"pop/cap\"]], row[c[\"gps_x\"]], row[c[\"gps_y\"]], row[\n                                          c[\"location_type\"]], row[c[\"conflict_date\"]], row[c[\"region\"]], row[c[\"country\"]]])\n\n    def ReadLinksFromCSV(self, csv_name, name1_col=0, name2_col=1, dist_col=2):\n        \n        self.links = []\n\n        with open(csv_name, newline='') as csvfile:\n            values = csv.reader(csvfile)\n\n            for row in values:\n                if row[0][0] == \"\n                    pass\n                else:\n                    \n                    self.links.append(\n                        [row[name1_col], row[name2_col], row[dist_col]])\n\n    def ReadClosuresFromCSV(self, csv_name):\n        \n        self.closures = []\n\n        with open(csv_name, newline='') as csvfile:\n            values = csv.reader(csvfile)\n\n            for row in values:\n                if row[0][0] == \"\n                    pass\n                else:\n                    \n                    self.closures.append(row)\n\n    def StoreInputGeographyInEcosystem(self, e):\n        \n        lm = {}\n\n        for l in self.locations:\n            \n            if len(l[1]) < 1:\n                l[1] = \"0\"\n            \n            if len(l[7]) < 1:\n                l[7] = \"unknown\"\n\n            \n            movechance = l[4]\n            if \"conflict\" in l[4].lower() and int(l[5]) > 0:\n                movechance = \"town\"\n\n            if \"camp\" in l[4].lower():\n                lm[l[0]] = e.addLocation(l[0], movechance=movechance, capacity=int(\n                    l[1]), x=l[2], y=l[3], country=l[7], region=l[6])\n            else:\n                lm[l[0]] = e.addLocation(l[0], movechance=movechance, pop=int(\n                    l[1]), x=l[2], y=l[3], country=l[7], region=l[6])\n\n        for l in self.links:\n            if (len(l) > 3):\n                if int(l[3]) == 1:\n                    e.linkUp(l[0], l[1], int(l[2]), True)\n                if int(l[3]) == 2:\n                    e.linkUp(l[1], l[0], int(l[2]), True)\n                else:\n                    e.linkUp(l[0], l[1], int(l[2]), False)\n            else:\n                e.linkUp(l[0], l[1], int(l[2]), False)\n\n        e.closures = []\n        for l in self.closures:\n            e.closures.append([l[0], l[1], l[2], int(l[3]), int(l[4])])\n\n        return e, lm\n\n    def AddNewConflictZones(self, e, time):\n        for l in self.locations:\n            if \"conflict\" in l[4].lower() and int(l[5]) == time:\n                print(\"Time = %s. Adding a new conflict zone [%s]\" % (\n                    time, l[0]), file=sys.stderr)\n                e.add_conflict_zone(l[0])\n",
        "summary": "The provided Python code defines a class `InputGeography` that reads geographical data from CSV files and stores it in an ecosystem simulation model using the FLEE library. It handles locations, links, and closures, and can add new conflict zones based on specified conditions."
    },
    {
        "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n\nexported_pipeline = KNeighborsClassifier(n_neighbors=2, p=1, weights=\"distance\")\n\nif hasattr(exported_pipeline, 'random_state'):\n    setattr(exported_pipeline, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n",
        "summary": "The provided Python code imports necessary libraries for data manipulation and machine learning, reads a dataset from a CSV file, splits it into training and testing sets, initializes a K-Nearest Neighbors classifier with specific parameters, fits the model to the training data, and then predicts outcomes on the test set."
    },
    {
        "code": "from salttesting import TestCase\nfrom salttesting.mock import MagicMock, patch\nfrom salttesting.helpers import ensure_in_syspath\n\nensure_in_syspath('../../')\n\n\nfrom salt.modules import brew\n\n\nbrew.__context__ = {}\nbrew.__salt__ = {}\n\nTAPS_STRING = 'homebrew/dupes\\nhomebrew/science\\nhomebrew/x11'\nTAPS_LIST = ['homebrew/dupes', 'homebrew/science', 'homebrew/x11']\nHOMEBREW_BIN = '/usr/local/bin/brew'\n\n\nclass BrewTestCase(TestCase):\n    \n\n    \n\n    def test_list_taps(self):\n        \n        mock_taps = MagicMock(return_value=TAPS_STRING)\n        with patch.dict(brew.__salt__, {'cmd.run': mock_taps}):\n            self.assertEqual(brew._list_taps(), TAPS_LIST)\n\n    \n\n    @patch('salt.modules.brew._list_taps', MagicMock(return_value=TAPS_LIST))\n    def test_tap_installed(self):\n        \n        self.assertTrue(brew._tap('homebrew/science'))\n\n    @patch('salt.modules.brew._list_taps', MagicMock(return_value={}))\n    def test_tap_failure(self):\n        \n        mock_failure = MagicMock(return_value=1)\n        with patch.dict(brew.__salt__, {'cmd.retcode': mock_failure}):\n            self.assertFalse(brew._tap('homebrew/test'))\n\n    @patch('salt.modules.brew._list_taps', MagicMock(return_value=TAPS_LIST))\n    def test_tap(self):\n        \n        mock_success = MagicMock(return_value=0)\n        with patch.dict(brew.__salt__, {'cmd.retcode': mock_success}):\n            self.assertTrue(brew._tap('homebrew/test'))\n\n    \n\n    def test_homebrew_bin(self):\n        \n        mock_path = MagicMock(return_value='/usr/local')\n        with patch.dict(brew.__salt__, {'cmd.run': mock_path}):\n            self.assertEqual(brew._homebrew_bin(), '/usr/local/bin/brew')\n\n    \n    \n    \n\n    def test_list_pkgs_removed(self):\n        \n        self.assertEqual(brew.list_pkgs(removed=True), {})\n\n    def test_list_pkgs_versions_true(self):\n        \n        mock_context = {'foo': ['bar']}\n        with patch.dict(brew.__context__, {'pkg.list_pkgs': mock_context}):\n            self.assertEqual(brew.list_pkgs(versions_as_list=True),\n                             mock_context)\n\n    \n\n    def test_version(self):\n        \n        mock_version = MagicMock(return_value='0.1.5')\n        with patch.dict(brew.__salt__, {'pkg_resource.version': mock_version}):\n            self.assertEqual(brew.version('foo'), '0.1.5')\n\n    \n    \n\n    \n    \n    \n\n    @patch('salt.modules.brew.list_pkgs',\n           MagicMock(return_value={'test': '0.1.5'}))\n    def test_remove(self):\n        \n        mock_params = MagicMock(return_value=({'foo': None}, 'repository'))\n        with patch.dict(brew.__salt__,\n                        {'pkg_resource.parse_targets': mock_params}):\n            self.assertEqual(brew.remove('foo'), {})\n\n    \n\n    @patch('salt.modules.brew._homebrew_bin',\n           MagicMock(return_value=HOMEBREW_BIN))\n    def test_refresh_db_failure(self):\n        \n        mock_user = MagicMock(return_value='foo')\n        mock_failure = MagicMock(return_value=1)\n        with patch.dict(brew.__salt__, {'file.get_user': mock_user,\n                                        'cmd.retcode': mock_failure}):\n            self.assertFalse(brew.refresh_db())\n\n    @patch('salt.modules.brew._homebrew_bin',\n           MagicMock(return_value=HOMEBREW_BIN))\n    def test_refresh_db(self):\n        \n        mock_user = MagicMock(return_value='foo')\n        mock_success = MagicMock(return_value=0)\n        with patch.dict(brew.__salt__, {'file.get_user': mock_user,\n                                        'cmd.retcode': mock_success}):\n            self.assertTrue(brew.refresh_db())\n\n    \n    \n    \n\n    def test_install(self):\n        \n        mock_params = MagicMock(return_value=[None, None])\n        with patch.dict(brew.__salt__,\n                        {'pkg_resource.parse_targets': mock_params}):\n            self.assertEqual(brew.install('name=foo'), {})\n\n\nif __name__ == '__main__':\n    from integration import run_tests\n    run_tests(BrewTestCase, needs_daemon=False)\n",
        "summary": "This Python code defines a test case class `BrewTestCase` that inherits from `salttesting.TestCase`. It tests various functions within the `brew` module, including listing taps, checking if a tap is installed, handling tap operations, retrieving Homebrew binary paths, listing packages with removed versions, getting package versions, removing packages, refreshing the database, and installing packages. The tests use mocking to simulate external commands and return values, ensuring that the functions behave as expected under different conditions."
    },
    {
        "code": "import torch\nfrom torch.nn import functional as F\n\nfrom dassl.optim import build_optimizer, build_lr_scheduler\nfrom dassl.utils import count_num_param\nfrom dassl.engine import TRAINER_REGISTRY, TrainerX\nfrom dassl.engine.trainer import SimpleNet\n\n\n@TRAINER_REGISTRY.register()\nclass CrossGrad(TrainerX):\n    \n\n    def __init__(self, cfg):\n        super().__init__(cfg)\n        self.eps_f = cfg.TRAINER.CG.EPS_F\n        self.eps_d = cfg.TRAINER.CG.EPS_D\n        self.alpha_f = cfg.TRAINER.CG.ALPHA_F\n        self.alpha_d = cfg.TRAINER.CG.ALPHA_D\n\n    def build_model(self):\n        cfg = self.cfg\n\n        print('Building F')\n        self.F = SimpleNet(cfg, cfg.MODEL, self.num_classes)\n        self.F.to(self.device)\n        print('\n        self.optim_F = build_optimizer(self.F, cfg.OPTIM)\n        self.sched_F = build_lr_scheduler(self.optim_F, cfg.OPTIM)\n        self.register_model('F', self.F, self.optim_F, self.sched_F)\n\n        print('Building D')\n        self.D = SimpleNet(cfg, cfg.MODEL, self.dm.num_source_domains)\n        self.D.to(self.device)\n        print('\n        self.optim_D = build_optimizer(self.D, cfg.OPTIM)\n        self.sched_D = build_lr_scheduler(self.optim_D, cfg.OPTIM)\n        self.register_model('D', self.D, self.optim_D, self.sched_D)\n\n    def forward_backward(self, batch):\n        input, label, domain = self.parse_batch_train(batch)\n\n        input.requires_grad = True\n\n        \n        loss_d = F.cross_entropy(self.D(input), domain)\n        loss_d.backward()\n        grad_d = torch.clamp(input.grad.data, min=-0.1, max=0.1)\n        input_d = input.data + self.eps_f * grad_d\n\n        \n        input.grad.data.zero_()\n        loss_f = F.cross_entropy(self.F(input), label)\n        loss_f.backward()\n        grad_f = torch.clamp(input.grad.data, min=-0.1, max=0.1)\n        input_f = input.data + self.eps_d * grad_f\n\n        input = input.detach()\n\n        \n        loss_f1 = F.cross_entropy(self.F(input), label)\n        loss_f2 = F.cross_entropy(self.F(input_d), label)\n        loss_f = (1 - self.alpha_f) * loss_f1 + self.alpha_f * loss_f2\n        self.model_backward_and_update(loss_f, 'F')\n\n        \n        loss_d1 = F.cross_entropy(self.D(input), domain)\n        loss_d2 = F.cross_entropy(self.D(input_f), domain)\n        loss_d = (1 - self.alpha_d) * loss_d1 + self.alpha_d * loss_d2\n        self.model_backward_and_update(loss_d, 'D')\n\n        output_dict = {\n            'loss_f': loss_f.item(),\n            'loss_d': loss_d.item(),\n            'lr': self.optim_F.param_groups[0]['lr']\n        }\n\n        if (self.batch_idx + 1) == self.num_batches:\n            self.update_lr()\n\n        return output_dict\n\n    def model_inference(self, input):\n        return self.F(input)\n",
        "summary": "The provided Python code defines a custom PyTorch trainer class named `CrossGrad` that extends the base `TrainerX`. This trainer is designed for training a cross-graduated network, which involves two models (`F` and `D`) trained in an alternating manner. The forward-backward method alternates between updating model `F` using gradients from model `D`, and vice versa, with specific perturbations applied to the input based on these gradients."
    },
    {
        "code": "from aliyunsdkcore.request import RpcRequest\nfrom aliyunsdkvpc.endpoint import endpoint_data\n\nclass DeleteGlobalAccelerationInstanceRequest(RpcRequest):\n\n\tdef __init__(self):\n\t\tRpcRequest.__init__(self, 'Vpc', '2016-04-28', 'DeleteGlobalAccelerationInstance','vpc')\n\t\tif hasattr(self, \"endpoint_map\"):\n\t\t\tsetattr(self, \"endpoint_map\", endpoint_data.getEndpointMap())\n\t\tif hasattr(self, \"endpoint_regional\"):\n\t\t\tsetattr(self, \"endpoint_regional\", endpoint_data.getEndpointRegional())\n\r\n\r\n\tdef get_ResourceOwnerId(self):\r\n\t\treturn self.get_query_params().get('ResourceOwnerId')\r\n\r\n\tdef set_ResourceOwnerId(self,ResourceOwnerId):\r\n\t\tself.add_query_param('ResourceOwnerId',ResourceOwnerId)\r\n\r\n\tdef get_GlobalAccelerationInstanceId(self):\r\n\t\treturn self.get_query_params().get('GlobalAccelerationInstanceId')\r\n\r\n\tdef set_GlobalAccelerationInstanceId(self,GlobalAccelerationInstanceId):\r\n\t\tself.add_query_param('GlobalAccelerationInstanceId',GlobalAccelerationInstanceId)\r\n\r\n\tdef get_ResourceOwnerAccount(self):\r\n\t\treturn self.get_query_params().get('ResourceOwnerAccount')\r\n\r\n\tdef set_ResourceOwnerAccount(self,ResourceOwnerAccount):\r\n\t\tself.add_query_param('ResourceOwnerAccount',ResourceOwnerAccount)\r\n\r\n\tdef get_OwnerAccount(self):\r\n\t\treturn self.get_query_params().get('OwnerAccount')\r\n\r\n\tdef set_OwnerAccount(self,OwnerAccount):\r\n\t\tself.add_query_param('OwnerAccount',OwnerAccount)\r\n\r\n\tdef get_OwnerId(self):\r\n\t\treturn self.get_query_params().get('OwnerId')\r\n\r\n\tdef set_OwnerId(self,OwnerId):\r\n\t\tself.add_query_param('OwnerId',OwnerId)",
        "summary": "This Python code defines a class `DeleteGlobalAccelerationInstanceRequest` that inherits from `RpcRequest`, used for deleting a global acceleration instance in the Alibaba Cloud VPC service. The class includes methods to get and set various parameters such as resource owner ID, global acceleration instance ID, and owner account, which are necessary for making the deletion request through the API."
    },
    {
        "code": "import time\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport itertools\r\nimport collections\r\nimport matplotlib.pyplot as plt\r\n\r\n\ndf = pd.read_csv(\"Chinese_Names_Corpus_Gender\uff08120W\uff09.txt\", header=2)\r\ndf = df[df.sex != \"\u672a\u77e5\"]\r\nnames = df[\"dict\"].values\r\n\r\n\nchars = [list(name) for name in names]\r\nchars_flatten = list(itertools.chain(*chars))\r\nfreq = collections.Counter(chars_flatten)\r\nfreq = pd.DataFrame(freq.items(), columns=[\"char\", \"freq\"])\r\nfreq = freq.sort_values(by=\"freq\", ascending=False)\r\n\r\n\nchar_rank = np.arange(freq.shape[0])\r\nchar_freq = freq[\"freq\"].values\r\nplt.plot(char_rank, char_freq)\r\nplt.plot(np.log(1.0 + char_rank), np.log(char_freq))\r\n\r\n\ndict_size = 500\r\ndict = list(freq[\"char\"].values[:dict_size])\r\ndict_set = set(dict)\r\nfiltered = list(filter(lambda item: set(item[1]).issubset(dict_set), enumerate(names)))\r\nind = [idx for idx, name in filtered]\r\ndat = df.iloc[ind]\r\ndat[\"y\"] = np.where(dat[\"sex\"] == \"\u7537\", 0, 1)\r\n\r\n\n\n\ntrain = dat.sample(n=10000, random_state=123)\r\ntest = dat.sample(n=1000, random_state=321)\r\n\r\n\ndef char2index(char):\r\n    return dict.index(char)\r\n\r\ndef name2index(name):\r\n    return [char2index(char) for char in name]\r\n\r\ndef name2tensor(name):\r\n    tensor = torch.zeros(len(name), 1, dict_size)\r\n    for i, char in enumerate(name):\r\n        tensor[i, 0, char2index(char)] = 1\r\n    return tensor\r\n\r\nchar2index(\"\u674e\")\r\nname2index(\"\u674e\u5174\")\r\nname2tensor(\"\u674e\u5174\")\r\n\r\n\r\n\r\n\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(RNN, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\r\n        self.h2o = nn.Linear(hidden_size, 1)\r\n\r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), dim=1)\r\n        hidden = torch.tanh(self.i2h(combined))\r\n        output = torch.sigmoid(self.h2o(hidden))\r\n        return output, hidden\r\n\r\n    def init_hidden(self):\r\n        return torch.zeros(1, self.hidden_size)\r\n\r\n\n\n\n\n\n\r\n\r\n\r\nnp.random.seed(123)\r\ntorch.random.manual_seed(123)\r\n\r\nn = train.shape[0]\r\nn_hidden = 64\r\nnepoch = 5\r\nbs = 100\r\n\r\nrnn = RNN(dict_size, n_hidden)\r\nopt = torch.optim.Adam(rnn.parameters(), lr=0.001)\r\ntrain_ind = np.arange(n)\r\nlosses = []\r\n\r\nt1 = time.time()\r\nfor k in range(nepoch):\r\n    np.random.shuffle(train_ind)\r\n    \n    for j in range(0, n, bs):\r\n        \n        mb = train.iloc[train_ind[j:(j + bs)]]\r\n        mb_size = mb.shape[0]\r\n        loss = 0.0\r\n        \n        for i in range(mb_size):\r\n            name = mb[\"dict\"].values[i]\r\n            input = name2tensor(name)\r\n            hidden = rnn.init_hidden()\r\n            y = mb[\"y\"].values[i]\r\n            for s in range(input.shape[0]):\r\n                output, hidden = rnn(input[s], hidden)\r\n            loss = loss - y * torch.log(output) - (1.0 - y) * torch.log(1.0 - output)\r\n\r\n        loss = loss / mb_size\r\n        opt.zero_grad()\r\n        loss.backward()\r\n        opt.step()\r\n\r\n        losses.append(loss.item())\r\n        if j // bs % 10 == 0:\r\n            print(f\"epoch {k}, batch {j // bs}, loss = {loss.item()}\")\r\nt2 = time.time()\r\nprint(t2 - t1)\r\n\r\nplt.plot(losses)\r\n\r\n\nntest = test.shape[0]\r\ntrue_label = test[\"y\"].values\r\npred = np.zeros(ntest)\r\nrnn.eval()\r\nfor i in range(ntest):\r\n    input = name2tensor(test[\"dict\"].values[i])\r\n    hidden = rnn.init_hidden()\r\n    with torch.no_grad():\r\n        for s in range(input.shape[0]):\r\n            output, hidden = rnn(input[s], hidden)\r\n    pred[i] = output.item()\r\n    if i % 100 == 0:\r\n        print(f\"processed {i}\")\r\nloss = -np.mean(true_label * np.log(pred) + (1.0 - true_label) * np.log(1.0 - pred))\r\nprint(loss)\r\npred_label = (pred > 0.5).astype(int)\r\nprint(np.mean(pred_label == true_label))\r\n\r\n\nnp.random.seed(123)\r\ntorch.random.manual_seed(123)\r\nind = np.random.choice(ntest, 10)\r\nypred = 1 * (pred[ind] > 0.5)\r\nprint(test.iloc[ind])\r\nprint(test[\"y\"].values[ind])\r\nprint(ypred)\r\n\r\n\r\n\r\nnames = [\"\u674e\", \"\u674e\u96ea\", \"\u674e\u96ea\u5cf0\"]\r\nfor name in names:\r\n    input = name2tensor(name)\r\n    hidden = rnn.init_hidden()\r\n    with torch.no_grad():\r\n        for s in range(input.shape[0]):\r\n            output, hidden = rnn(input[s], hidden)\r\n    pred = output.item()\r\n    print(f\"namae: {name}, P(female) = {pred}\")\r\n",
        "summary": "The provided Python code processes a dataset of Chinese names to predict gender using a Recurrent Neural Network (RNN). It first analyzes the frequency of characters in the names, filters out names based on these frequencies, and then trains an RNN model to classify names as male or female. The script includes data preprocessing, model training, evaluation, and prediction functionalities."
    },
    {
        "code": "import sys\n\nimport numpy as np\nimport pennylane as qml\nimport pytest\nimport qiskit\n\nfrom pennylane_qiskit import AerDevice, BasicAerDevice\n\nfrom conftest import state_backends\n\npldevices = [(\"qiskit.aer\", qiskit.Aer), (\"qiskit.basicaer\", qiskit.BasicAer)]\n\n\nclass TestDeviceIntegration:\n    \n\n    @pytest.mark.parametrize(\"d\", pldevices)\n    def test_load_device(self, d, backend):\n        \n        dev = qml.device(d[0], wires=2, backend=backend, shots=1024)\n        assert dev.num_wires == 2\n        assert dev.shots == 1024\n        assert dev.short_name == d[0]\n        assert dev.provider == d[1]\n\n    def test_incorrect_backend(self):\n        \n        with pytest.raises(ValueError, match=\"Backend 'none' does not exist\"):\n            qml.device(\"qiskit.aer\", wires=2, backend=\"none\")\n\n    def test_incorrect_backend_wires(self):\n        \n        with pytest.raises(ValueError, match=r\"Backend 'statevector\\_simulator' supports maximum\"):\n            qml.device(\"qiskit.aer\", wires=100, backend=\"statevector_simulator\")\n\n    def test_args(self):\n        \n        with pytest.raises(TypeError, match=\"missing 1 required positional argument\"):\n            qml.device(\"qiskit.aer\")\n\n        with pytest.raises(qml.DeviceError, match=\"specified number of shots needs to be at least 1\"):\n            qml.device(\"qiskit.aer\", backend=\"qasm_simulator\", wires=1, shots=0)\n\n    @pytest.mark.parametrize(\"d\", pldevices)\n    @pytest.mark.parametrize(\"analytic\", [True, False])\n    @pytest.mark.parametrize(\"shots\", [8192])\n    def test_one_qubit_circuit(self, shots, analytic, d, backend, tol):\n        \n        if backend not in state_backends and analytic:\n            pytest.skip(\"Hardware simulators do not support analytic mode\")\n\n        dev = qml.device(d[0], wires=1, backend=backend, shots=shots, analytic=analytic)\n\n        a = 0.543\n        b = 0.123\n        c = 0.987\n\n        @qml.qnode(dev)\n        def circuit(x, y, z):\n            \n            qml.BasisState(np.array([1]), wires=0)\n            qml.Hadamard(wires=0)\n            qml.Rot(x, y, z, wires=0)\n            return qml.expval(qml.PauliZ(0))\n\n        assert np.allclose(circuit(a, b, c), np.cos(a) * np.sin(b), **tol)\n\n    @pytest.mark.parametrize(\"d\", pldevices)\n    @pytest.mark.parametrize(\"analytic\", [False])\n    @pytest.mark.parametrize(\"shots\", [8192])\n    def test_one_qubit_circuit(self, shots, analytic, d, backend, tol):\n        \n        dev = qml.device(d[0], wires=1, backend=backend, shots=shots, analytic=analytic)\n\n        a = 0\n        b = 0\n        c = np.pi\n        expected = 1\n\n        @qml.qnode(dev)\n        def circuit(x, y, z):\n            \n            qml.BasisState(np.array([0]), wires=0)\n            qml.Rot(x, y, z, wires=0)\n            return qml.expval(qml.PauliZ(0))\n\n        assert np.allclose(circuit(a, b, c), expected, **tol)\n\n    def test_gradient_for_tensor_product(self):\n        \n        n_qubits = 2\n        depth = 2\n\n        def ansatz(weights):\n            weights = weights.reshape(depth, n_qubits)\n            qml.RX(weights[0][0], wires=[0])\n            qml.RZ(weights[0][1], wires=[0])\n            qml.RX(weights[1][0], wires=[0])\n            qml.RZ(weights[1][1], wires=[0])\n            return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1))\n\n        dev_qsk = qml.device(\n                    \"qiskit.aer\",\n                    wires=n_qubits,\n                    shots=1000,\n                    backend=\"qasm_simulator\",\n                )\n\n        weights = np.random.random((depth, n_qubits)).flatten()\n\n        \n        exp_sampled = qml.QNode(ansatz, dev_qsk, diff_method=\"parameter-shift\")\n        grad_shift = qml.grad(exp_sampled, argnum=0)\n        exp_sampled(weights)\n        grad_shift(weights)\n\nclass TestKeywordArguments:\n    \n\n    @pytest.mark.parametrize(\"d\", pldevices)\n    def test_compile_backend(self, d):\n        \n        dev = qml.device(d[0], wires=2, compile_backend=\"test value\")\n        assert dev.compile_backend == \"test value\"\n\n    def test_noise_model(self):\n        \n        dev = qml.device(\"qiskit.aer\", wires=2, noise_model=\"test value\")\n        assert dev.noise_model == \"test value\"\n\n    def test_invalid_noise_model(self):\n        \n        with pytest.raises(ValueError, match=\"does not support noisy simulations\"):\n            dev = qml.device(\"qiskit.basicaer\", wires=2, noise_model=\"test value\")\n\n    def test_overflow_kwargs(self):\n        \n        dev = qml.device('qiskit.aer', wires=2, k1=\"v1\", k2=\"v2\")\n        assert dev.run_args[\"k1\"] == \"v1\"\n        assert dev.run_args[\"k2\"] == \"v2\"\n\n\nclass TestLoadIntegration:\n    \n\n    hadamard_qasm = 'OPENQASM 2.0;' \\\n                    'include \"qelib1.inc\";' \\\n                    'qreg q[1];' \\\n                    'h q[0];'\n\n    def test_load_qiskit_circuit(self):\n        \n        theta = qiskit.circuit.Parameter('\u03b8')\n\n        qc = qiskit.QuantumCircuit(2)\n        qc.rx(theta, 0)\n\n        my_template = qml.load(qc, format='qiskit')\n\n        dev = qml.device('default.qubit', wires=2)\n\n        angles = np.array([0.53896774, 0.79503606, 0.27826503, 0.])\n\n        @qml.qnode(dev)\n        def loaded_quantum_circuit(angle):\n            my_template({theta: angle})\n            return qml.expval(qml.PauliZ(0))\n\n        @qml.qnode(dev)\n        def quantum_circuit(angle):\n            qml.RX(angle, wires=[0])\n            return qml.expval(qml.PauliZ(0))\n\n        for x in angles:\n            assert np.allclose(loaded_quantum_circuit(x), quantum_circuit(x))\n\n    def test_load_from_qasm_string(self):\n        \n\n        dev = qml.device('default.qubit', wires=2)\n\n        @qml.qnode(dev)\n        def loaded_quantum_circuit():\n            qml.from_qasm(TestLoadIntegration.hadamard_qasm)(wires=[0])\n            return qml.expval(qml.PauliZ(0))\n\n        @qml.qnode(dev)\n        def quantum_circuit():\n            qml.Hadamard(wires=[0])\n            return qml.expval(qml.PauliZ(0))\n\n        assert np.allclose(loaded_quantum_circuit(), quantum_circuit())\n\n    @pytest.mark.skipif(sys.version_info < (3, 6), reason=\"tmpdir fixture requires Python >=3.6\")\n    def test_load_qasm_from_file(self, tmpdir):\n        \n        apply_hadamard = tmpdir.join(\"hadamard.qasm\")\n\n        with open(apply_hadamard, \"w\") as f:\n            f.write(TestLoadIntegration.hadamard_qasm)\n\n        hadamard = qml.from_qasm_file(apply_hadamard)\n\n        dev = qml.device('default.qubit', wires=2)\n\n        @qml.qnode(dev)\n        def loaded_quantum_circuit():\n            hadamard(wires=[0])\n            return qml.expval(qml.PauliZ(0))\n\n        @qml.qnode(dev)\n        def quantum_circuit():\n            qml.Hadamard(wires=[0])\n            return qml.expval(qml.PauliZ(0))\n\n        assert np.allclose(loaded_quantum_circuit(), quantum_circuit())\n\n\nclass TestPLOperations:\n    \n\n    @pytest.mark.parametrize(\"shots\", [1000])\n    @pytest.mark.parametrize(\"analytic\", [True, False])\n    def test_rotation(self, init_state, state_vector_device, shots, analytic, tol):\n        \n\n        dev = state_vector_device(1)\n\n        if dev.backend_name == \"unitary_simulator\":\n            pytest.skip(\"Test only runs for backends that are not the unitary simulator.\")\n\n        state = init_state(1)\n\n        a = 0.542\n        b = 1.3432\n        c = -0.654\n\n        I = np.eye(2)\n        Y = np.array([[0, -1j], [1j, 0]])  \n        Z = np.array([[1, 0], [0, -1]])  \n\n        def ry(theta):\n            return np.cos(theta / 2) * I + 1j * np.sin(-theta / 2) * Y\n\n        def rz(theta):\n            return np.cos(theta / 2) * I + 1j * np.sin(-theta / 2) * Z\n\n        @qml.qnode(dev)\n        def qubitstatevector_and_rot():\n            qml.QubitStateVector(state, wires=[0])\n            qml.Rot(a, b, c, wires=[0])\n            return qml.expval(qml.Identity(0))\n\n        qubitstatevector_and_rot()\n\n        assert np.allclose(np.abs(dev.state) ** 2, np.abs(rz(c) @ ry(b) @ rz(a) @ state) ** 2, **tol)\n\n    @pytest.mark.parametrize(\"shots\", [1000])\n    @pytest.mark.parametrize(\"analytic\", [True, False])\n    def test_basisstate(self, init_state, state_vector_device, shots, analytic, tol):\n        \n\n        dev = state_vector_device(2)\n        state = np.array([1, 0])\n\n        @qml.qnode(dev)\n        def basisstate():\n            qml.BasisState(state, wires=[0, 1])\n            return qml.expval(qml.Identity(0))\n\n        basisstate()\n\n        expected_state = np.zeros(2**dev.num_wires)\n        expected_state[2] = 1\n\n        assert np.allclose(np.abs(dev.state) ** 2, np.abs(expected_state) ** 2, **tol)\n\n    @pytest.mark.parametrize(\"shots\", [1000])\n    @pytest.mark.parametrize(\"analytic\", [True, False])\n    def test_basisstate_init_all_zero_states(self, init_state, state_vector_device, shots, analytic, tol):\n        \n\n        dev = state_vector_device(4)\n        state = np.array([0, 0, 0, 0])\n\n        @qml.qnode(dev)\n        def basisstate():\n            qml.BasisState(state, wires=[0, 1, 2, 3])\n            return qml.expval(qml.Identity(0))\n\n        basisstate()\n\n        expected_state = np.zeros(2**dev.num_wires)\n        expected_state[0] = 1\n\n        assert np.allclose(np.abs(dev.state) ** 2, np.abs(expected_state) ** 2, **tol)\n\n\nclass TestInverses:\n    \n\n    def test_inverse_of_operation(self):\n        \n        dev = qml.device('default.qubit', wires=2)\n\n        dev2 = qml.device('qiskit.aer', backend='statevector_simulator', shots=5, wires=2, analytic=True)\n\n        angles = np.array([0.53896774, 0.79503606, 0.27826503, 0.])\n\n        @qml.qnode(dev)\n        def circuit_with_inverses(angle):\n            qml.Hadamard(0).inv()\n            qml.RX(angle, wires=0).inv()\n            return qml.expval(qml.PauliZ(0))\n\n        @qml.qnode(dev2)\n        def circuit_with_inverses_default_qubit(angle):\n            qml.Hadamard(0).inv()\n            qml.RX(angle, wires=0).inv()\n            return qml.expval(qml.PauliZ(0))\n\n        for x in angles:\n            assert np.allclose(circuit_with_inverses(x), circuit_with_inverses_default_qubit(x))\n",
        "summary": "This code snippet is a comprehensive test suite for the Qiskit library, specifically focusing on operations and devices related to quantum computing. It includes tests for various quantum gates, state preparation, and inverse operations. Here's a breakdown of what each part does:\n\n1. **Imports**: The script starts by importing necessary modules from Qiskit, including `qnode`, `QubitStateVector`, `BasisState`, `Rot`, `Hadamard`, `RZ`, `RY`, `PauliZ`, and `Identity`.\n\n2. **Device Setup**: It defines a function to set up the device based on whether it's a state vector device or a Qiskit Aer device.\n\n3. **Tests for Rotations**:\n   - The script tests the `Rot` gate, which applies three rotations (X, Y, Z) in sequence.\n   - It uses both unitary and statevector simulators to verify that the final state is correct after applying these rotations.\n\n4. **Tests for Basis States**:\n   - It tests the `BasisState` operation, which prepares a quantum state from a given basis vector.\n   - The script checks if the prepared state matches the expected state for both single and multiple qubits.\n\n5. **Inverse Operations**:\n   - The script includes tests to verify that inverse operations (`.inv()`) are correctly applied to gates like `Hadamard` and `RX`.\n   - It compares the results of circuits with inverses using Qiskit Aer's statevector simulator against a default Qiskit device.\n\n6. **Parameterization**:\n   - The tests are parameterized over different shot numbers, analytical settings, and initial states to ensure robustness across various scenarios.\n\n7. **Tolerance and Assertions**:\n   - It uses a tolerance (`tol`) for floating-point comparisons and asserts that the results from different devices match within this tolerance.\n\nThis test suite is crucial for ensuring that Qiskit's quantum operations are implemented correctly and consistently across different backends, which is essential for developing reliable quantum applications."
    },
    {
        "code": "from tensorflow_datasets.core.utils.image_utils import *\nfrom tensorflow_datasets.core.utils.py_utils import *\nfrom tensorflow_datasets.core.utils.tf_utils import *\nfrom tensorflow_datasets.core.utils.tqdm_utils import *\nfrom tensorflow_datasets.core.utils.version import Experiment\nfrom tensorflow_datasets.core.utils.version import Version\n\n",
        "summary": "The Python code imports various utility functions and classes from the TensorFlow Datasets library, including image processing, general utilities, TensorFlow-specific tools, progress bar handling, and versioning mechanisms."
    },
    {
        "code": "from contextlib import contextmanager\nfrom threading import Lock\n\n\n\n\n\n\nclass RWLock(object):\n    \n\n    def __init__(self):\n\n        self.w_lock = Lock()\n        self.num_r_lock = Lock()\n        self.num_r = 0\n\n        \n        \n        \n        \n        \n        self.d_lock = Lock()\n\n    \n    \n\n    def r_acquire(self):\n        self.d_lock.acquire()\n        self.num_r_lock.acquire()\n        self.num_r += 1\n\n        if self.num_r == 1:\n            self.w_lock.acquire()\n\n        self.num_r_lock.release()\n        self.d_lock.release()\n\n    def r_release(self):\n        assert self.num_r > 0\n        self.num_r_lock.acquire()\n        self.num_r -= 1\n        if self.num_r == 0:\n            self.w_lock.release()\n\n        self.num_r_lock.release()\n\n    @contextmanager\n    def r_locked(self):\n        \n        try:\n            self.r_acquire()\n            yield\n        finally:\n            self.r_release()\n\n    \n    \n\n    def w_acquire(self):\n        self.d_lock.acquire()\n        self.w_lock.acquire()\n\n    def w_acquire_non_blocking(self):\n        \n        \n        if self.d_lock.acquire(blocking=False):\n            if self.w_lock.acquire(blocking=False):\n                return True\n            else:\n                self.d_lock.release()\n        return False\n\n    def w_release(self):\n        self.w_lock.release()\n        self.d_lock.release()\n\n    def w_demote(self):\n        \n\n        \n        \n        \n        self.num_r_lock.acquire()\n        self.num_r += 1\n        self.num_r_lock.release()\n        self.d_lock.release()\n\n    @contextmanager\n    def w_locked(self):\n        \n        try:\n            self.w_acquire()\n            yield\n        finally:\n            self.w_release()\n",
        "summary": "The provided Python code defines a class `RWLock` that implements a reader-writer lock using threading locks. This lock allows multiple readers to access the resource simultaneously but restricts access when a writer is present, ensuring exclusive write access and preventing writers from blocking other writers or readers unnecessarily. The class provides methods for acquiring and releasing read and write locks, as well as context managers for managing these locks in a safe manner."
    },
    {
        "code": "import numpy as np\r\nimport tensorflow as tf\r\ndef net_u(self, x, t):  \r\n        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\r\n        return u",
        "summary": "The provided Python code defines a method `net_u` within a class that takes input variables `x` and `t`, concatenates them, and then passes the result through a neural network defined by `self.neural_net`, using weights and biases stored in `self.weights` and `self.biases`, respectively. The output of this process is returned as `u`."
    },
    {
        "code": "import time\nimport asyncio\n\n\nimport aiopg\nfrom psycopg2 import extras\n\n\nfrom ddtrace.constants import ANALYTICS_SAMPLE_RATE_KEY\nfrom ddtrace.contrib.aiopg.patch import patch, unpatch\nfrom ddtrace import Pin\n\n\nfrom tests.opentracer.utils import init_tracer\nfrom tests.contrib.config import POSTGRES_CONFIG\nfrom tests.test_tracer import get_dummy_tracer\nfrom tests.contrib.asyncio.utils import AsyncioTestCase, mark_asyncio\n\n\nTEST_PORT = str(POSTGRES_CONFIG['port'])\n\n\nclass AiopgTestCase(AsyncioTestCase):\n    \n    TEST_SERVICE = 'postgres'\n\n    def setUp(self):\n        super().setUp()\n        self._conn = None\n        patch()\n\n    def tearDown(self):\n        super().tearDown()\n        if self._conn and not self._conn.closed:\n            self._conn.close()\n\n        unpatch()\n\n    @asyncio.coroutine\n    def _get_conn_and_tracer(self):\n        conn = self._conn = yield from aiopg.connect(**POSTGRES_CONFIG)\n        Pin.get_from(conn).clone(tracer=self.tracer).onto(conn)\n\n        return conn, self.tracer\n\n    @asyncio.coroutine\n    def assert_conn_is_traced(self, tracer, db, service):\n\n        \n        \n        try:\n            yield from db.execute('select \\'foobar\\'')\n        except AttributeError:\n            pass\n\n        writer = tracer.writer\n        \n        q = 'select \\'foobarblah\\''\n        start = time.time()\n        cursor = yield from db.cursor()\n        yield from cursor.execute(q)\n        rows = yield from cursor.fetchall()\n        end = time.time()\n        assert rows == [('foobarblah',)]\n        assert rows\n        spans = writer.pop()\n        assert spans\n        assert len(spans) == 1\n        span = spans[0]\n        assert span.name == 'postgres.query'\n        assert span.resource == q\n        assert span.service == service\n        assert span.meta['sql.query'] == q\n        assert span.error == 0\n        assert span.span_type == 'sql'\n        assert start <= span.start <= end\n        assert span.duration <= end - start\n\n        \n        ot_tracer = init_tracer('aiopg_svc', tracer)\n        with ot_tracer.start_active_span('aiopg_op'):\n            cursor = yield from db.cursor()\n            yield from cursor.execute(q)\n            rows = yield from cursor.fetchall()\n            assert rows == [('foobarblah',)]\n        spans = writer.pop()\n        assert len(spans) == 2\n        ot_span, dd_span = spans\n        \n        assert ot_span.parent_id == None\n        assert dd_span.parent_id == ot_span.span_id\n        assert ot_span.name == 'aiopg_op'\n        assert ot_span.service == 'aiopg_svc'\n        assert dd_span.name == 'postgres.query'\n        assert dd_span.resource == q\n        assert dd_span.service == service\n        assert dd_span.meta['sql.query'] == q\n        assert dd_span.error == 0\n        assert dd_span.span_type == 'sql'\n\n        \n        q = 'select * from some_non_existant_table'\n        cur = yield from db.cursor()\n        try:\n            yield from cur.execute(q)\n        except Exception:\n            pass\n        else:\n            assert 0, 'should have an error'\n        spans = writer.pop()\n        assert spans, spans\n        assert len(spans) == 1\n        span = spans[0]\n        assert span.name == 'postgres.query'\n        assert span.resource == q\n        assert span.service == service\n        assert span.meta['sql.query'] == q\n        assert span.error == 1\n        \n        assert span.meta['out.port'] == TEST_PORT\n        assert span.span_type == 'sql'\n\n    @mark_asyncio\n    def test_disabled_execute(self):\n        conn, tracer = yield from self._get_conn_and_tracer()\n        tracer.enabled = False\n        \n        yield from (yield from conn.cursor()).execute(query='select \\'blah\\'')\n        yield from (yield from conn.cursor()).execute('select \\'blah\\'')\n        assert not tracer.writer.pop()\n\n    @mark_asyncio\n    def test_manual_wrap_extension_types(self):\n        conn, _ = yield from self._get_conn_and_tracer()\n        \n        \n        \n        extras.register_uuid(conn_or_curs=conn)\n\n    @mark_asyncio\n    def test_connect_factory(self):\n        tracer = get_dummy_tracer()\n\n        services = ['db', 'another']\n        for service in services:\n            conn, _ = yield from self._get_conn_and_tracer()\n            Pin.get_from(conn).clone(service=service, tracer=tracer).onto(conn)\n            yield from self.assert_conn_is_traced(tracer, conn, service)\n            conn.close()\n\n        \n        service_meta = tracer.writer.pop_services()\n        expected = {}\n        assert service_meta == expected\n\n    @mark_asyncio\n    def test_patch_unpatch(self):\n        tracer = get_dummy_tracer()\n        writer = tracer.writer\n\n        \n        patch()\n        patch()\n\n        service = 'fo'\n\n        conn = yield from aiopg.connect(**POSTGRES_CONFIG)\n        Pin.get_from(conn).clone(service=service, tracer=tracer).onto(conn)\n        yield from (yield from conn.cursor()).execute('select \\'blah\\'')\n        conn.close()\n\n        spans = writer.pop()\n        assert spans, spans\n        assert len(spans) == 1\n\n        \n        unpatch()\n\n        conn = yield from aiopg.connect(**POSTGRES_CONFIG)\n        yield from (yield from conn.cursor()).execute('select \\'blah\\'')\n        conn.close()\n\n        spans = writer.pop()\n        assert not spans, spans\n\n        \n        patch()\n\n        conn = yield from aiopg.connect(**POSTGRES_CONFIG)\n        Pin.get_from(conn).clone(service=service, tracer=tracer).onto(conn)\n        yield from (yield from conn.cursor()).execute('select \\'blah\\'')\n        conn.close()\n\n        spans = writer.pop()\n        assert spans, spans\n        assert len(spans) == 1\n\n\nclass AiopgAnalyticsTestCase(AiopgTestCase):\n    @asyncio.coroutine\n    def trace_spans(self):\n        service = 'db'\n        conn, _ = yield from self._get_conn_and_tracer()\n\n        Pin.get_from(conn).clone(service='db', tracer=self.tracer).onto(conn)\n\n        cursor = yield from conn.cursor()\n        yield from cursor.execute('select \\'foobar\\'')\n        rows = yield from cursor.fetchall()\n        assert rows\n\n        return self.get_spans()\n\n    @mark_asyncio\n    def test_analytics_default(self):\n        spans = yield from self.trace_spans()\n        self.assertEqual(len(spans), 1)\n        self.assertIsNone(spans[0].get_metric(ANALYTICS_SAMPLE_RATE_KEY))\n\n    @mark_asyncio\n    def test_analytics_with_rate(self):\n        with self.override_config(\n            'aiopg',\n            dict(analytics_enabled=True, analytics_sample_rate=0.5)\n        ):\n            spans = yield from self.trace_spans()\n            self.assertEqual(len(spans), 1)\n            self.assertEqual(spans[0].get_metric(ANALYTICS_SAMPLE_RATE_KEY), 0.5)\n\n    @mark_asyncio\n    def test_analytics_without_rate(self):\n        with self.override_config(\n            'aiopg',\n            dict(analytics_enabled=True)\n        ):\n            spans = yield from self.trace_spans()\n            self.assertEqual(len(spans), 1)\n            self.assertEqual(spans[0].get_metric(ANALYTICS_SAMPLE_RATE_KEY), 1.0)\n",
        "summary": "The provided Python code defines test cases for the `aiopg` library, which is an asynchronous PostgreSQL client for Python using asyncio. The tests include functionality to trace database queries, handle exceptions, manage connections, and configure analytics sampling rates."
    },
    {
        "code": "import torchvision\n\n\nclass VisionDatasetWithIndices(torchvision.datasets.vision.VisionDataset):\n  \n\n  def __init__(self, dataset):\n    super(VisionDatasetWithIndices, self).__init__(None)\n    self.dataset = dataset\n\n  def __getitem__(self, index):\n    data, target = self.dataset.__getitem__(index)\n    return data, target, index\n\n  def __len__(self):\n    return len(self.dataset)\n",
        "summary": "The `VisionDatasetWithIndices` class extends `torchvision.datasets.vision.VisionDataset`, overriding the `__getitem__` method to include the sample index in the returned tuple, thereby providing access to indices along with data and targets. This allows for additional functionality that requires knowing the position of each sample within the dataset."
    },
    {
        "code": "import os\r\nimport argparse\r\nimport subprocess\r\nimport shutil\r\n\r\nparser = argparse.ArgumentParser(description=\"Compile the Mount&Blade Warband shaders.\")\r\nparser.add_argument(\"-b\", \"--compile-b\", action=\"store_true\", help=\"compile the ps_2_b profile as well\")\r\nargs = parser.parse_args()\r\n\r\nif not os.access(os.path.join(\"shaders\", \"fxc.exe\"), os.R_OK|os.X_OK):\r\n  print \"You must copy fxc.exe from the TaleWorlds Warband shader package to the shaders subdirectory.\"\r\n  exit(1)\r\n\r\nimport module_info\r\n\r\ndef compile_profile(profile, name):\r\n  command_list = [\"./fxc.exe\", \"/nologo\", \"/T\", \"fx_2_0\", \"/D\", \"PS_2_X=%s\" % profile, \"/Fo\", \"mb.fxo\", \"mb.fx\"]\r\n  exit_code = subprocess.call(command_list, cwd=\"shaders\")\r\n  output_fxo = os.path.join(\"shaders\", \"mb.fxo\")\r\n  if exit_code == 0:\r\n    module_fxo = module_info.export_path(name)\r\n    try:\r\n      os.remove(module_fxo)\r\n    except Exception:\r\n      pass\r\n    shutil.move(output_fxo, module_fxo)\r\n  else:\r\n    try:\r\n      os.remove(output_fxo)\r\n    except Exception:\r\n      pass\r\n    exit(exit_code)\r\n\r\ncompile_profile(\"ps_2_a\", \"mb.fx\")\r\n\r\nif args.compile_b:\r\n  compile_profile(\"ps_2_b\", \"mb_2b.fx\")\r\n",
        "summary": "This Python script compiles Mount&Blade Warband shaders using the fxc.exe compiler, handling both ps_2_a and optionally ps_2_b profiles based on user input. It checks for the presence of fxc.exe, defines a function to compile shader profiles, and moves the compiled files to appropriate locations while cleaning up temporary outputs."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations performed are not specified, but they involve reading, manipulating, and writing data, likely in a text format such as CSV or JSON."
    },
    {
        "code": "from QcloudApi.modules import base\n\n\nclass Trade(base.Base):\n    requestHost = 'trade.api.qcloud.com'\n",
        "summary": "The provided Python code defines a class named `Trade` that inherits from the `Base` class imported from the `QcloudApi.modules` module. The `Trade` class sets the `requestHost` attribute to 'trade.api.qcloud.com', indicating that it is configured for communication with the Tencent Cloud Trade API endpoint."
    },
    {
        "code": "import copy\nimport logging\nfrom datetime import datetime, timedelta\nfrom urllib.error import ContentTooShortError, HTTPError, URLError\nfrom urllib.request import (\n    HTTPPasswordMgrWithDefaultRealm,\n    HTTPBasicAuthHandler,\n    HTTPDigestAuthHandler,\n    build_opener,\n    install_opener,\n    urlopen,\n)\n\nimport voluptuous as vol\nfrom homeassistant.components.calendar import (\n    ENTITY_ID_FORMAT,\n    PLATFORM_SCHEMA,\n    CalendarEventDevice,\n    calculate_offset,\n    is_offset_reached,\n)\nfrom homeassistant.const import CONF_NAME, CONF_PASSWORD, CONF_URL, CONF_USERNAME\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.util import Throttle\nfrom .icalendarparser import ICalendarParser\n\nVERSION = \"2.0.0\"\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_DEVICE_ID = \"device_id\"\nCONF_CALENDARS = \"calendars\"\nCONF_CALENDAR = \"calendar\"\nCONF_INCLUDE_ALL_DAY = \"includeAllDay\"\nCONF_PARSER = \"parser\"\n\nOFFSET = \"!!\"\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend(\n    {\n        \n        vol.Optional(CONF_CALENDARS, default=[]): vol.All(\n            cv.ensure_list,\n            vol.Schema(\n                [\n                    vol.Schema(\n                        {\n                            vol.Required(CONF_URL): vol.Url(),\n                            vol.Required(CONF_NAME): cv.string,\n                            vol.Optional(\n                                CONF_INCLUDE_ALL_DAY, default=False\n                            ): cv.boolean,\n                            vol.Optional(CONF_USERNAME, default=\"\"): cv.string,\n                            vol.Optional(CONF_PASSWORD, default=\"\"): cv.string,\n                            vol.Optional(CONF_PARSER, default=\"icalevents\"): cv.string,\n                        }\n                    )\n                ]\n            ),\n        )\n    }\n)\n\nMIN_TIME_BETWEEN_UPDATES = timedelta(minutes=15)\n\n\n\n\nMIN_TIME_BETWEEN_DOWNLOADS = timedelta(minutes=10)\n\n\ndef setup_platform(hass, config, add_entities, _=None):\n    \n    _LOGGER.debug(\"Setting up ics calendars\")\n    calendar_devices = []\n    for calendar in config.get(CONF_CALENDARS):\n        device_data = {\n            CONF_NAME: calendar.get(CONF_NAME),\n            CONF_URL: calendar.get(CONF_URL),\n            CONF_INCLUDE_ALL_DAY: calendar.get(CONF_INCLUDE_ALL_DAY),\n            CONF_USERNAME: calendar.get(CONF_USERNAME),\n            CONF_PASSWORD: calendar.get(CONF_PASSWORD),\n            CONF_PARSER: calendar.get(CONF_PARSER),\n        }\n        device_id = \"{}\".format(device_data[CONF_NAME])\n        entity_id = generate_entity_id(ENTITY_ID_FORMAT, device_id, hass=hass)\n        calendar_devices.append(ICSCalendarEventDevice(entity_id, device_data))\n\n    add_entities(calendar_devices)\n\n\nclass ICSCalendarEventDevice(CalendarEventDevice):\n    \n\n    def __init__(self, entity_id, device_data):\n        _LOGGER.debug(\"Initializing calendar: %s\", device_data[CONF_NAME])\n        self.data = ICSCalendarData(device_data)\n        self.entity_id = entity_id\n        self._event = None\n        self._name = device_data[CONF_NAME]\n        self._offset_reached = False\n        self._last_call = None\n        self._last_event_list = None\n\n    @property\n    def device_state_attributes(self):\n        \n        return {\"offset_reached\": self._offset_reached}\n\n    @property\n    def event(self):\n        \n        return self._event\n\n    @property\n    def name(self):\n        \n        return self._name\n\n    async def async_get_events(self, hass, start_date, end_date):\n        \n        if (\n            self._last_event_list is None\n            or self._last_call is None\n            or (datetime.now() - self._last_call) > MIN_TIME_BETWEEN_UPDATES\n        ):\n            self._last_call = datetime.now()\n            self._last_event_list = await self.data.async_get_events(\n                hass, start_date, end_date\n            )\n        return self._last_event_list\n\n    def update(self):\n        \n        self.data.update()\n        event = copy.deepcopy(self.data.event)\n        if event is None:\n            self._event = event\n            return\n        event = calculate_offset(event, OFFSET)\n        self._offset_reached = is_offset_reached(event)\n        self._event = event\n\n\nclass ICSCalendarData:\n    \n\n    def __init__(self, device_data):\n        \n        self.name = device_data[CONF_NAME]\n        self.url = device_data[CONF_URL]\n        self.include_all_day = device_data[CONF_INCLUDE_ALL_DAY]\n        self.parser = ICalendarParser.get_instance(device_data[CONF_PARSER])\n        self.event = None\n        self._calendar_data = None\n        self._last_download = None\n\n        if device_data[CONF_USERNAME] != \"\" and device_data[CONF_PASSWORD] != \"\":\n            passman = HTTPPasswordMgrWithDefaultRealm()\n            passman.add_password(\n                None, self.url, device_data[CONF_USERNAME], device_data[CONF_PASSWORD]\n            )\n            basic_auth_handler = HTTPBasicAuthHandler(passman)\n            digest_auth_handler = HTTPDigestAuthHandler(passman)\n            opener = build_opener(digest_auth_handler, basic_auth_handler)\n            install_opener(opener)\n\n    def _download_calendar(self):\n        if (\n            self._calendar_data is None\n            or self._last_download is None\n            or (datetime.now() - self._last_download) > MIN_TIME_BETWEEN_DOWNLOADS\n        ):\n            self._last_download = datetime.now()\n            self._calendar_data = None\n            try:\n                with urlopen(self.url) as conn:\n                    self._calendar_data = conn.read().decode().replace(\"\\0\", \"\")\n            except HTTPError as http_error:\n                _LOGGER.error(f\"{self.name}: Failed to open url: {http_error.reason}\")\n            except ContentTooShortError as content_too_short_error:\n                _LOGGER.error(\n                    f\"{self.name}: Could not download calendar data: {content_too_short_error.reason}\"\n                )\n            except URLError as url_error:\n                _LOGGER.error(f\"{self.name}: Failed to open url: {url_error.reason}\")\n            except:\n                _LOGGER.error(f\"{self.name}: Failed to open url!\")\n        return\n\n    async def async_get_events(self, hass, start_date, end_date):\n        \n        event_list = []\n        await hass.async_add_job(self._download_calendar)\n        try:\n            events = self.parser.get_event_list(\n                content=self._calendar_data,\n                start=start_date,\n                end=end_date,\n                include_all_day=self.include_all_day,\n            )\n            event_list = list(map(self.format_dates, events))\n        except:\n            _LOGGER.error(f\"{self.name}: Failed to parse ICS!\")\n            event_list = []\n\n        return event_list\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \n        self._download_calendar()\n        try:\n            self.event = self.parser.get_current_event(\n                content=self._calendar_data, include_all_day=self.include_all_day\n            )\n            self.event[\"start\"] = self.get_hass_date(\n                self.event[\"start\"], self.event[\"all_day\"]\n            )\n            self.event[\"end\"] = self.get_hass_date(\n                self.event[\"end\"], self.event[\"all_day\"]\n            )\n            return True\n        except:\n            _LOGGER.error(f\"{self.name}: Failed to parse ICS!\")\n\n        return False\n\n    def format_dates(self, event):\n        event[\"start\"] = self.get_date_formatted(event[\"start\"], event[\"all_day\"])\n        event[\"end\"] = self.get_date_formatted(event[\"end\"], event[\"all_day\"])\n        return event\n\n    def get_date_formatted(self, dt, is_all_day):\n        \n        \n        \n        if is_all_day:\n            return dt.strftime(\"%Y-%m-%d\")\n\n        return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n\n    def get_hass_date(self, dt, is_all_day):\n        \n        if is_all_day:\n            return {\"date\": self.parser.get_date_formatted(dt, is_all_day)}\n        return {\"dateTime\": self.parser.get_date_formatted(dt, is_all_day)}\n",
        "summary": "This Python code defines a custom calendar component for Home Assistant that integrates with iCalendar feeds. It handles authentication, data downloading, and event parsing to provide calendar events in the Home Assistant interface."
    },
    {
        "code": "from arm.logicnode.arm_nodes import *\n\nclass SetTransformNode(ArmLogicTreeNode):\n    \n    bl_idname = 'LNSetTransformNode'\n    bl_label = 'Set Object Transform'\n    arm_version = 1\n\n    def init(self, context):\n        super(SetTransformNode, self).init(context)\n        self.add_input('ArmNodeSocketAction', 'In')\n        self.add_input('ArmNodeSocketObject', 'Object')\n        self.add_input('NodeSocketShader', 'Transform')\n        self.add_output('ArmNodeSocketAction', 'Out')\n\nadd_node(SetTransformNode, category=PKG_AS_CATEGORY)\n",
        "summary": "The provided Python code defines a custom node class `SetTransformNode` that inherits from `ArmLogicTreeNode`. This node is designed to set the transform of an object in a 3D scene using Armory Logic Nodes. It includes inputs for an action signal, an object, and a transformation matrix, and outputs an action signal upon completion. The node is then registered under the category \"PKG_AS_CATEGORY\"."
    },
    {
        "code": "from setuptools import setup, find_packages\n\n\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n\nwith open(path.join(here, \"README.rst\"), encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetup(\n    \n    name=\"circuitpython-displayio-cartesian\",\n    use_scm_version=True,\n    setup_requires=[\"setuptools_scm\"],\n    description=\"A cartesian plane widget for displaying graphical information.\",\n    long_description=long_description,\n    long_description_content_type=\"text/x-rst\",\n    \n    url=\"https://github.com/circuitpython/CircuitPython_Org_DisplayIO_Cartesian.git\",\n    \n    author=\"Jose David M.\",\n    author_email=\"\",\n    install_requires=[\n        \"Adafruit-Blinka\",\n        \"adafruit-circuitpython-display-text\",\n        \"adafruit-circuitpython-displayio-layout\",\n    ],\n    \n    license=\"MIT\",\n    \n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: System :: Hardware\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n    ],\n    \n    keywords=\"adafruit blinka circuitpython micropython displayio_cartesian displayio widget \"\n    \"graphics gui graph chart graphic\",\n    \n    \n    \n    \n    py_modules=[\"displayio_cartesian\"],\n)\n",
        "summary": "This Python script uses `setuptools` to package and distribute a CircuitPython library named `circuitpython-displayio-cartesian`, which provides a cartesian plane widget for displaying graphical information. The setup includes metadata such as the author, license, dependencies, and classifiers, along with a long description read from a README file."
    },
    {
        "code": "from ssmpfwd.helpers import verify_plugin_version, verbose_debug_quiet, time_decorator\r\nfrom unittest.mock import MagicMock, patch\r\nimport unittest\r\n\r\n\r\nclass TestVerifyPluginVersion(unittest.TestCase):\r\n    @patch(\"ssmpfwd.helpers.subprocess\")\r\n    def test_verify_plugin_version_success(self, mock_subprocess):\r\n        result = mock_subprocess.run()\r\n        result.stdout = b\"9.8.3\"\r\n        self.assertTrue(verify_plugin_version(\"9.8.3\"))\r\n\r\n    @patch(\"ssmpfwd.helpers.subprocess\")\r\n    def test_verify_plugin_version_fail(self, mock_subprocess):\r\n        with self.assertLogs(\"ssmpfwd.helpers\", level=\"INFO\") as cm:\r\n            result = mock_subprocess.run()\r\n            result.stdout = b\"1.8.1\"\r\n            self.assertFalse(verify_plugin_version(\"9.2.3\"))\r\n            self.assertEqual(cm.output[0], \"ERROR:ssmpfwd.helpers:session-manager-plugin version 1.8.1 is installed, 9.2.3 is required\")\r\n\r\n\r\nclass TestVerboseDebugQuiet(unittest.TestCase):\r\n    import logging\r\n\r\n    def setUp(self):\r\n        @verbose_debug_quiet\r\n        def test_func():\r\n            pass\r\n\r\n        self.vdq = test_func\r\n        self.vdq()\r\n\r\n    def test_quiet(self):\r\n        option_name = \"quiet\"\r\n        self.assertTrue(any([p.name == option_name for p in self.vdq.__click_params__]), msg=f\"Can not find {option_name} in option parameters\")\r\n\r\n    def test_debug(self):\r\n        flag_value = self.logging.DEBUG\r\n        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\r\n\r\n    def test_verbose(self):\r\n        flag_value = self.logging.INFO\r\n        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\r\n\r\n    def test_default_loglevel(self):\r\n        flag_value = self.logging.WARN\r\n        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\r\n\r\n\r\nclass TestTimeDecorator(unittest.TestCase):\r\n    from time import sleep\r\n\r\n    def setUp(self):\r\n        @time_decorator\r\n        def test_func():\r\n            self.sleep(0.5)\r\n\r\n        self.time_decorated_method = test_func\r\n\r\n    def test_time_decorartor(self):\r\n        with self.assertLogs(\"ssmpfwd.helpers\", level=\"INFO\") as cm:\r\n            self.time_decorated_method()\r\n            self.assertEqual(cm.output[0], \"INFO:ssmpfwd.helpers:[*] starting test_func\")\r\n",
        "summary": "The provided Python code includes unit tests for three functions: `verify_plugin_version`, `verbose_debug_quiet`, and `time_decorator`. The tests use the `unittest` framework and mock objects to simulate function behavior, ensuring that the functions operate as expected under various conditions."
    },
    {
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pint\n\nfrom main import ureg\nureg.setup_matplotlib(True)\nfrom uncertainties import ufloat, umath, unumpy\nimport pandas as pd\nfrom scipy.signal import find_peaks\nfrom scipy.integrate import simpson\nfrom scipy.optimize import curve_fit\nplt.rcParams['text.usetex'] = True\n\namp = 700*ureg.mV\nR=ufloat(0.82, 0.82*0.1)*ureg.ohm\n\ndf = pd.read_csv(\"./ESRB.csv\")\n\n\n\n    \n    \n\n\n\nI0_modulation = (unumpy.uarray(np.linspace(\n    df['V_modulation_raw'].min(),\n    df['V_modulation_raw'].max(),\n    len(df)\n), df['V_modulation_err'].mean())*ureg.mV/R).to('ampere')\n\nptp_Y = unumpy.uarray(\n    df['ptp_Y_raw'].values*df['phase_sign'].values,\n    df['ptp_Y_err'].values\n)*ureg.mV\nptp_X_modulation = ufloat(3.09, 0.01)*ureg.mV\n\nfig, ax = plt.subplots()\nI0_modulation_err = np.array([val.m.s for val in I0_modulation])\nI0_modulation_raw = np.array([val.m.n for val in I0_modulation])\nptp_ratio = ptp_Y/ptp_X_modulation\nabsorption_deriviative = ptp_ratio/max(ptp_ratio)\nabsorption_deriviative_raw = np.array([val.m.n for val in absorption_deriviative])\nabsorption_deriviative_err = np.array([val.m.s for val in absorption_deriviative])\nax.errorbar(\n    I0_modulation_raw*ureg.ampere,\n    absorption_deriviative_raw, \n    fmt='.',\n    yerr=absorption_deriviative_err,\n    \n    \n    \n    label='Absorption Deriviative'\n)\n\ndef lorentzian_dif_fit(I, I0, gamma, amplitude):\n    return amplitude*(-2*(gamma**2)*(I - I0))/ \\\n            (gamma**2 + (I - I0)**2)**2\ndef lorentzian_fit(I, I0, gamma, amplitude):\n    return amplitude*gamma**2/\\\n            (gamma**2 + (I - I0)**2)**2\n\n\n\n\n\n\n\nmatlab_p0    = [0.5479, 0.03847, 0.05554]\nmatlab_bounds=((0.547,  0.03672,  0.05304),\n               (0.5488, 0.04021,   0.05805))\nI_rf = ufloat(matlab_p0[0], abs(matlab_bounds[0][0] - matlab_p0[0]))*ureg.ampere\nI_hwhm = ufloat(matlab_p0[1], abs(matlab_bounds[0][1] - matlab_p0[1]))*ureg.ampere\n\nfrom main import g_times_bohr\n\nH_RF = ufloat(34.914, 0.009)*ureg.gauss\nk = H_RF/I_rf\n\ndef I2f(I):\n    return (I*k*g_times_bohr/ureg.planck_constant).to('megahertz')\n\nf0_modulation = I2f(I0_modulation)\nf_rf = I2f(I_rf)\nf_hwhm = I2f(I_hwhm)\nT2 = (1/f_hwhm).to('nanosecond')\n\n\n\n\n    \n    \n\n\n\n    \n    \n    \n\n\nI0_modulation_seq = np.linspace(\n    I0_modulation.min().m.n,\n    I0_modulation.max().m.n,\n    len(I0_modulation)*100\n)\nax.plot(\n    I0_modulation_seq*ureg.ampere,\n    lorentzian_dif_fit(I0_modulation_seq, I_rf.m.n, I_hwhm.m.n, matlab_p0[2]),\n    label=\"Matlab fit\"\n)\nax.set_yticks([])\naxt = ax.twiny()\naxt.grid(linestyle='--')\naxt.set_yticks([])\nf0_modulation_seq = np.linspace(\n    f0_modulation.min().m.n,\n    f0_modulation.max().m.n,\n    len(f0_modulation)*100\n)\ndef lorentzian_wrapper(f0):\n    \n    \n    return lorentzian_fit(f0, f_rf.m.n, f_hwhm.m.n, matlab_p0[2]*800)\naxt.plot(\n    f0_modulation_seq*ureg.megahertz,\n    lorentzian_wrapper(f0_modulation_seq),\n    label = \"Lorenzian fit\", color='green'\n)\naxt.set_xticks(\n    [(f_rf - f_hwhm).m.n, f_rf.m.n, (f_rf + f_hwhm).m.n],\n    ['', '$f_{rf}$', '']\n)\naxt.set_xlabel('')\naxt.arrow(\n    length_includes_head = True,\n    x = (f_rf - f_hwhm).m.n*ureg.megahertz,\n    y = lorentzian_wrapper((f_rf - f_hwhm).m.n),\n    dx = 2*f_hwhm.m.n*ureg.megahertz,\n    dy = 0,\n    head_length = f_hwhm.m.n/10,\n    head_width = matlab_p0[2],\n    label=\"Full Width Half Max\",\n)\naxt.arrow(\n    length_includes_head = True,\n    x = (f_rf + f_hwhm).m.n*ureg.megahertz,\n    y = lorentzian_wrapper((f_rf + f_hwhm).m.n),\n    dx = -2*f_hwhm.m.n*ureg.megahertz,\n    head_length = f_hwhm.m.n/10,\n    head_width = matlab_p0[2],\n    dy = 0,\n)\naxt.text(\n    0.5, 0.63,\n    \n    \n    \"FWHM\",\n    transform=ax.transAxes,\n    \n)\nax.legend(loc='upper right')\n\nplt.show()\nfig.savefig(\"ESRB.pgf\")\nfig.savefig(\"ESRB.png\")\n\n\n\n\n\n",
        "summary": "The Python script processes data from a CSV file using pandas and numpy, performs uncertainty calculations with uncertainties library, fits the data to a Lorentzian function, and plots the results with matplotlib, including error bars and annotations for full width at half maximum (FWHM). The plot is saved in both PGF and PNG formats."
    },
    {
        "code": "from flask import Flask, request, redirect\nfrom twilio.twiml.messaging_response import MessagingResponse\nfrom get_secrets import *\n\ndef main():\n    resp = MessagingResponse()\n\n    resp.message (\"You have reached the DogBot. Thanks for contacting us :)\")\n\n    return str(resp)\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "The provided Python code sets up a simple Flask application that responds to incoming SMS messages with a predefined message, \"You have reached the DogBot. Thanks for contacting us :)\", using Twilio's MessagingResponse class. When executed as the main program, it runs indefinitely, listening for and processing incoming SMS requests."
    },
    {
        "code": "from __future__ import annotations\n\nfrom enum import IntEnum\n\n\nclass Algorithm(IntEnum):\n    \n\n    RSA_PKCS1_SHA1 = 1\n    RSA_PKCS1_SHA256 = 2\n    RSA_PKCS1_SHA384 = 3\n    RSA_PKCS1_SHA512 = 4\n    RSA_PSS_SHA1 = 5\n    RSA_PSS_SHA256 = 6\n    RSA_PSS_SHA384 = 7\n    RSA_PSS_SHA512 = 8\n    RSA_2048 = 9\n    RSA_3072 = 10\n    RSA_4096 = 11\n    EC_P256 = 12\n    EC_P384 = 13\n    EC_P521 = 14\n    EC_K256 = 15\n    EC_BP256 = 16\n    EC_BP384 = 17\n    EC_BP512 = 18\n    HMAC_SHA1 = 19\n    HMAC_SHA256 = 20\n    HMAC_SHA384 = 21\n    HMAC_SHA512 = 22\n    ECDSA_SHA1 = 23\n    EC_ECDH = 24\n    RSA_OAEP_SHA1 = 25\n    RSA_OAEP_SHA256 = 26\n    RSA_OAEP_SHA384 = 27\n    RSA_OAEP_SHA512 = 28\n    AES128_CCM_WRAP = 29\n    Opaque_Data = 30\n    Opaque_X509_Certificate = 31\n    MGF1_SHA1 = 32\n    MGF1_SHA256 = 33\n    MGF1_SHA384 = 34\n    MGF1_SHA512 = 35\n    SSH_Template = 36\n    Yubico_OTP_AES128 = 37\n    Yubico_AES_Authentication = 38\n    Yubico_OTP_AES192 = 39\n    Yubico_OTP_AES256 = 40\n    AES192_CCM_WRAP = 41\n    AES256_CCM_WRAP = 42\n    ECDSA_SHA256 = 43\n    ECDSA_SHA384 = 44\n    ECDSA_SHA512 = 45\n    ED25519 = 46\n    EC_P224 = 47\n\n\nclass Capability(IntEnum):\n    \n\n    GetOpaque = 0\n    PutOpaque = 1\n    PutAuthenticationKey = 2\n    PutAsymmetricKey = 3\n    GenerateAsymmetricKey = 4\n    SignPkcs = 5\n    SignPss = 6\n    SignEcdsa = 7\n    SignEddsa = 8\n    DecryptPkcs = 9\n    DecryptOaep = 10\n    DeriveEcdh = 11\n    ExportWrapped = 12\n    ImportWrapped = 13\n    PutWrapKey = 14\n    GenerateWrapKey = 15\n    ExportableUnderWrap = 16\n    SetOption = 17\n    GetOption = 18\n    GetPseudoRandom = 19\n    PutMacKey = 20\n    GenerateHmacKey = 21\n    SignHmac = 22\n    VerifyHmac = 23\n    GetLogEntries = 24\n    SignSshCertificate = 25\n    GetTemplate = 26\n    PutTemplate = 27\n    ResetDevice = 28\n    DecryptOtp = 29\n    CreateOtpAead = 30\n    RandomizeOtpAead = 31\n    RewrapFromOtpAeadKey = 32\n    RewrapToOtpAeadKey = 33\n    SignAttestationCertificate = 34\n    PutOtpAeadKey = 35\n    GenerateOtpAeadKey = 36\n    WrapData = 37\n    UnwrapData = 38\n    DeleteOpaque = 39\n    DeleteAuthenticationKey = 40\n    DeleteAsymmetricKey = 41\n    DeleteWrapKey = 42\n    DeleteHmacKey = 43\n    DeleteTemplate = 44\n    DeleteOtpAeadKey = 45\n    ChangeAuthenticationKey = 46\n\n\nclass Command(IntEnum):\n    \n\n    Echo = 0x01\n    CreateSession = 0x03\n    AuthenticateSession = 0x04\n    SessionMessage = 0x05\n    GetDeviceInfo = 0x06\n    ResetDevice = 0x08\n    CloseSession = 0x40\n    GetStorageInfo = 0x41\n    PutOpaque = 0x42\n    GetOpaque = 0x43\n    PutAuthenticationKey = 0x44\n    PutAsymmetricKey = 0x45\n    GenerateAsymmetricKey = 0x46\n    SignPkcs1 = 0x47\n    ListObjects = 0x48\n    DecryptPkcs1 = 0x49\n    ExportWrapped = 0x4A\n    ImportWrapped = 0x4B\n    PutWrapKey = 0x4C\n    GetLogEntries = 0x4D\n    GetObjectInfo = 0x4E\n    SetOption = 0x4F\n    GetOption = 0x50\n    GetPseudoRandom = 0x51\n    PutHmacKey = 0x52\n    SignHmac = 0x53\n    GetPublicKey = 0x54\n    SignPss = 0x55\n    SignEcdsa = 0x56\n    DeriveEcdh = 0x57\n    DeleteObject = 0x58\n    DecryptOaep = 0x59\n    GenerateHmacKey = 0x5A\n    GenerateWrapKey = 0x5B\n    VerifyHmac = 0x5C\n    SignSshCertificate = 0x5D\n    PutTemplate = 0x5E\n    GetTemplate = 0x5F\n    DecryptOtp = 0x60\n    CreateOtpAead = 0x61\n    RandomizeOtpAead = 0x62\n    RewrapOtpAead = 0x63\n    SignAttestationCertificate = 0x64\n    PutOtpAeadKey = 0x65\n    GenerateOtpAeadKey = 0x66\n    SetLogIndex = 0x67\n    WrapData = 0x68\n    UnwrapData = 0x69\n    SignEddsa = 0x6A\n    BlinkDevice = 0x6B\n    ChangeAuthenticationKey = 0x6C\n    Error = 0x7F\n\n\nclass Error(IntEnum):\n    \n\n    OK = 0x00\n    INVALID_COMMAND = 0x01\n    INVALID_DATA = 0x02\n    INVALID_SESSION = 0x03\n    AUTHENTICATION_FAILED = 0x04\n    SESSIONS_FULL = 0x05\n    SESSION_FAILED = 0x06\n    STORAGE_FAILED = 0x07\n    WRONG_LENGTH = 0x08\n    INSUFFICIENT_PERMISSIONS = 0x09\n    LOG_FULL = 0x0A\n    OBJECT_NOT_FOUND = 0x0B\n    INVALID_ID = 0x0C\n    SSH_CA_CONSTRAINT_VIOLATION = 0x0E\n    INVALID_OTP = 0x0F\n    DEMO_MODE = 0x10\n    OBJECT_EXISTS = 0x11\n\n\nclass ObjectType(IntEnum):\n    \n\n    Opaque = 0x01\n    AuthenticationKey = 0x02\n    AsymmetricKey = 0x03\n    WrapKey = 0x04\n    HmacKey = 0x05\n    Template = 0x06\n    OtpAeadKey = 0x07\n\n\nclass Option(IntEnum):\n    \n\n    ForceAudit = 0x01\n    CommandAudit = 0x03\n",
        "summary": "The provided Python code defines several enumerations using the `IntEnum` class from the `enum` module, each representing different types of algorithms, capabilities, commands, errors, object types, and options used in a security or cryptographic context. These enumerations serve as standardized identifiers for various operations and data types within a system that handles secure communications and storage."
    },
    {
        "code": "from __future__ import annotations\n\nfrom mhr_api.models import utils as model_utils\nfrom mhr_api.utils.base import BaseEnum\n\nfrom .db import db\n\n\nclass EventTracking(db.Model):  \n    \n\n    class EventTrackingTypes(BaseEnum):\n        \n\n        SEARCH_REPORT = 'SEARCH_REPORT'\n        API_NOTIFICATION = 'API_NOTIFICATION'\n        EMAIL = 'EMAIL'\n        SURFACE_MAIL = 'SURFACE_MAIL'\n        EMAIL_REPORT = 'EMAIL_REPORT'\n        REGISTRATION_REPORT = 'REGISTRATION_REPORT'\n\n    __tablename__ = 'event_tracking'\n\n    id = db.Column('id', db.Integer, db.Sequence('event_tracking_id_seq'), primary_key=True)\n    key_id = db.Column('key_id', db.Integer, nullable=False, index=True)\n    event_ts = db.Column('event_ts', db.DateTime, nullable=False, index=True)\n    event_tracking_type = db.Column('event_tracking_type', db.String(20),\n                                    db.ForeignKey('event_tracking_types.event_tracking_type'),\n                                    nullable=False, index=True)\n    status = db.Column('status', db.Integer, nullable=True)\n    message = db.Column('message', db.String(2000), nullable=True)\n    email_id = db.Column('email_address', db.String(250), nullable=True)\n\n    \n    tracking_type = db.relationship('EventTrackingType', foreign_keys=[event_tracking_type],\n                                    back_populates='event_tracking', cascade='all, delete', uselist=False)\n\n    def save(self):\n        \n        db.session.add(self)\n        db.session.commit()\n\n    @property\n    def json(self) -> dict:\n        \n        event_tracking = {\n            'eventTrackingId': self.id,\n            'keyId': self.key_id,\n            'type': self.event_tracking_type,\n            'createDateTime': model_utils.format_ts(self.event_ts)\n        }\n        if self.status:\n            event_tracking['status'] = self.status\n        if self.message:\n            event_tracking['message'] = self.message\n        if self.email_id:\n            event_tracking['emailAddress'] = self.email_id\n\n        return event_tracking\n\n    @classmethod\n    def find_by_id(cls, event_id: int):\n        \n        if event_id:\n            return cls.query.get(event_id)\n\n        return None\n\n    @classmethod\n    def find_by_key_id(cls, key_id: int):\n        \n        event_tracking = None\n        if key_id:\n            event_tracking = cls.query.filter(EventTracking.key_id == key_id) \\\n                                      .order_by(EventTracking.id).all()\n\n        return event_tracking\n\n    @classmethod\n    def find_by_key_id_type(cls, key_id: int, event_tracking_type: str, extra_key: str = None):\n        \n        event_tracking = None\n        if key_id and event_tracking_type:\n            event_tracking = cls.query.filter(EventTracking.key_id == key_id,\n                                              EventTracking.event_tracking_type == event_tracking_type) \\\n                                      .order_by(EventTracking.id).all()\n\n            if event_tracking is not None and extra_key:\n                events = []\n                for event in event_tracking:\n                    if event.message and event.message.find(extra_key) > 0:\n                        events.append(event)\n                return events\n        return event_tracking\n\n    @staticmethod\n    def create(key_id: int, event_type: str, status: int = None, message: str = None):\n        \n        event_tracking = EventTracking(key_id=key_id, event_tracking_type=event_type, status=status, message=message)\n        event_tracking.event_ts = model_utils.now_ts()\n        event_tracking.save()\n\n        return event_tracking\n",
        "summary": "The `EventTracking` class in Python defines a database model for tracking events with various types such as search reports, API notifications, and emails. It includes methods for saving instances to the database, retrieving events by ID or key ID, and creating new events with specified details."
    },
    {
        "code": "from riaps.run.comp import Component\nimport logging\n\nclass Hello(Component):\n    def __init__(self):\n        super(Hello, self).__init__()\n        \n    def on_clock(self):\n        now = self.clock.recv_pyobj()   \n        self.logger.info('on_clock(): %s' % str(now))\n    \n\n\n",
        "summary": "The provided Python code defines a component named `Hello` that inherits from the `Component` class in the RIAPS framework. This component includes an initialization method and a callback function `on_clock`, which is triggered by receiving an object via the `clock` port, logging the received time using Python's `logging` module."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union\nfrom ... import _utilities, _tables\nfrom ._enums import *\n\n__all__ = ['DataConnector']\n\n\nclass DataConnector(pulumi.CustomResource):\n    def __init__(__self__,\n                 resource_name: str,\n                 opts: Optional[pulumi.ResourceOptions] = None,\n                 data_connector_id: Optional[pulumi.Input[str]] = None,\n                 etag: Optional[pulumi.Input[str]] = None,\n                 kind: Optional[pulumi.Input[Union[str, 'DataConnectorKind']]] = None,\n                 resource_group_name: Optional[pulumi.Input[str]] = None,\n                 workspace_name: Optional[pulumi.Input[str]] = None,\n                 __props__=None,\n                 __name__=None,\n                 __opts__=None):\n        \n        if __name__ is not None:\n            warnings.warn(\"explicit use of __name__ is deprecated\", DeprecationWarning)\n            resource_name = __name__\n        if __opts__ is not None:\n            warnings.warn(\"explicit use of __opts__ is deprecated, use 'opts' instead\", DeprecationWarning)\n            opts = __opts__\n        if opts is None:\n            opts = pulumi.ResourceOptions()\n        if not isinstance(opts, pulumi.ResourceOptions):\n            raise TypeError('Expected resource options to be a ResourceOptions instance')\n        if opts.version is None:\n            opts.version = _utilities.get_version()\n        if opts.id is None:\n            if __props__ is not None:\n                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')\n            __props__ = dict()\n\n            __props__['data_connector_id'] = data_connector_id\n            __props__['etag'] = etag\n            if kind is None and not opts.urn:\n                raise TypeError(\"Missing required property 'kind'\")\n            __props__['kind'] = kind\n            if resource_group_name is None and not opts.urn:\n                raise TypeError(\"Missing required property 'resource_group_name'\")\n            __props__['resource_group_name'] = resource_group_name\n            if workspace_name is None and not opts.urn:\n                raise TypeError(\"Missing required property 'workspace_name'\")\n            __props__['workspace_name'] = workspace_name\n            __props__['name'] = None\n            __props__['type'] = None\n        alias_opts = pulumi.ResourceOptions(aliases=[pulumi.Alias(type_=\"azure-nextgen:securityinsights:DataConnector\"), pulumi.Alias(type_=\"azure-nextgen:securityinsights/latest:DataConnector\")])\n        opts = pulumi.ResourceOptions.merge(opts, alias_opts)\n        super(DataConnector, __self__).__init__(\n            'azure-nextgen:securityinsights/v20200101:DataConnector',\n            resource_name,\n            __props__,\n            opts)\n\n    @staticmethod\n    def get(resource_name: str,\n            id: pulumi.Input[str],\n            opts: Optional[pulumi.ResourceOptions] = None) -> 'DataConnector':\n        \n        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))\n\n        __props__ = dict()\n\n        return DataConnector(resource_name, opts=opts, __props__=__props__)\n\n    @property\n    @pulumi.getter\n    def etag(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"etag\")\n\n    @property\n    @pulumi.getter\n    def kind(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"kind\")\n\n    @property\n    @pulumi.getter\n    def name(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"name\")\n\n    @property\n    @pulumi.getter\n    def type(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"type\")\n\n    def translate_output_property(self, prop):\n        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop\n\n    def translate_input_property(self, prop):\n        return _tables.SNAKE_TO_CAMEL_CASE_TABLE.get(prop) or prop\n\n",
        "summary": "The provided Python code defines a `DataConnector` class that extends `pulumi.CustomResource`, representing a data connector in Azure Security Insights. It includes methods for initialization and retrieval of the resource, as well as properties to manage its attributes such as `etag`, `kind`, `name`, and `type`."
    },
    {
        "code": "from keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import  Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense\nfrom keras import backend as K\n\nclass LeNet:\n    @staticmethod\n    def build(width, height, depth, classes):\n        model = Sequential()\n        inputShape = (height, width, depth)\n\n        if K.image_data_format() == \"channels_first\":\n            inputShape = (depth, height, width)\n\n        \n        model.add(Conv2D(20, (5, 5), padding='same', input_shape=inputShape))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n\n        \n        model.add(Conv2D(50, (5, 5), padding='same'))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        \n        model.add(Flatten())\n        model.add(Dense(500))\n        model.add(Activation('relu'))\n\n        \n        model.add(Dense(classes))\n        model.add(Activation('softmax'))\n\n        return model\n",
        "summary": "The provided Python code defines a class `LeNet` with a static method `build` that constructs a convolutional neural network architecture using Keras, specifically tailored for image classification tasks. The network consists of two convolutional layers followed by max pooling layers, a fully connected layer, and an output layer with softmax activation for multi-class classification."
    },
    {
        "code": "from complatecpp import Stream\n\n\nclass TestStream(Stream):\n    __test__ = False\n\n    def __init__(self, *args, **kwargs):\n        \n        Stream.__init__(self, *args, **kwargs)\n        self.data = str()\n\n    def write(self, string, length):\n        self.data += string[0:length]\n\n    def writeln(self, string, length):\n        self.data += string[0:length]\n        self.data += '\\n'\n\n    def flush(self):\n        pass\n\n    def str(self):\n        return self.data\n",
        "summary": "The `TestStream` class extends the `Stream` class from the `complatecpp` module and overrides its methods to provide custom behavior for writing strings, handling line breaks, and returning the accumulated data as a string. The class is marked with `__test__ = False`, indicating it may not be used for testing purposes."
    },
    {
        "code": "from CyberSource import *\nimport os\nimport json\nfrom importlib.machinery import SourceFileLoader\n\nconfig_file = os.path.join(os.getcwd(), \"data\", \"Configuration.py\")\nconfiguration = SourceFileLoader(\"module.name\", config_file).load_module()\n\n\ndef del_none(d):\n    for key, value in list(d.items()):\n        if value is None:\n            del d[key]\n        elif isinstance(value, dict):\n            del_none(value)\n    return d\n\ndef multiple_line_items():\n    clientReferenceInformationCode = \"addressEg\"\n    clientReferenceInformationComments = \"dav-All fields\"\n    clientReferenceInformation = Riskv1decisionsClientReferenceInformation(\n        code = clientReferenceInformationCode,\n        comments = clientReferenceInformationComments\n    )\n\n    orderInformationBillToAddress1 = \"12301 research st\"\n    orderInformationBillToAddress2 = \"1\"\n    orderInformationBillToAddress3 = \"2\"\n    orderInformationBillToAddress4 = \"3\"\n    orderInformationBillToAdministrativeArea = \"TX\"\n    orderInformationBillToCountry = \"US\"\n    orderInformationBillToLocality = \"Austin\"\n    orderInformationBillToPostalCode = \"78759\"\n    orderInformationBillTo = Riskv1addressverificationsOrderInformationBillTo(\n        address1 = orderInformationBillToAddress1,\n        address2 = orderInformationBillToAddress2,\n        address3 = orderInformationBillToAddress3,\n        address4 = orderInformationBillToAddress4,\n        administrative_area = orderInformationBillToAdministrativeArea,\n        country = orderInformationBillToCountry,\n        locality = orderInformationBillToLocality,\n        postal_code = orderInformationBillToPostalCode\n    )\n\n    orderInformationShipToAddress1 = \"PO Box 9088\"\n    orderInformationShipToAddress2 = \"\"\n    orderInformationShipToAddress3 = \"\"\n    orderInformationShipToAddress4 = \"\"\n    orderInformationShipToAdministrativeArea = \"CA\"\n    orderInformationShipToCountry = \"US\"\n    orderInformationShipToLocality = \"San Jose\"\n    orderInformationShipToPostalCode = \"95132\"\n    orderInformationShipTo = Riskv1addressverificationsOrderInformationShipTo(\n        address1 = orderInformationShipToAddress1,\n        address2 = orderInformationShipToAddress2,\n        address3 = orderInformationShipToAddress3,\n        address4 = orderInformationShipToAddress4,\n        administrative_area = orderInformationShipToAdministrativeArea,\n        country = orderInformationShipToCountry,\n        locality = orderInformationShipToLocality,\n        postal_code = orderInformationShipToPostalCode\n    )\n\n\n    orderInformationLineItems = []\n    orderInformationLineItems1 = Riskv1addressverificationsOrderInformationLineItems(\n        unit_price = \"120.50\",\n        quantity = 3,\n        product_sku = \"9966223\",\n        product_name = \"headset\",\n        product_code = \"electronix\"\n    )\n\n    orderInformationLineItems.append(orderInformationLineItems1.__dict__)\n\n    orderInformationLineItems2 = Riskv1addressverificationsOrderInformationLineItems(\n        unit_price = \"10.50\",\n        quantity = 2,\n        product_sku = \"9966226\",\n        product_name = \"wwrdf\",\n        product_code = \"electronic\"\n    )\n\n    orderInformationLineItems.append(orderInformationLineItems2.__dict__)\n\n    orderInformation = Riskv1addressverificationsOrderInformation(\n        bill_to = orderInformationBillTo.__dict__,\n        ship_to = orderInformationShipTo.__dict__,\n        line_items = orderInformationLineItems\n    )\n\n    buyerInformationMerchantCustomerId = \"QWERTY\"\n    buyerInformation = Riskv1addressverificationsBuyerInformation(\n        merchant_customer_id = buyerInformationMerchantCustomerId\n    )\n\n    requestObj = VerifyCustomerAddressRequest(\n        client_reference_information = clientReferenceInformation.__dict__,\n        order_information = orderInformation.__dict__,\n        buyer_information = buyerInformation.__dict__\n    )\n\n\n    requestObj = del_none(requestObj.__dict__)\n    requestObj = json.dumps(requestObj)\n\n\n    try:\n        config_obj = configuration.Configuration()\n        client_config = config_obj.get_configuration()\n        api_instance = VerificationApi(client_config)\n        return_data, status, body = api_instance.verify_customer_address(requestObj)\n\n        print(\"\\nAPI RESPONSE CODE : \", status)\n        print(\"\\nAPI RESPONSE BODY : \", body)\n\n        return return_data\n    except Exception as e:\n        print(\"\\nException when calling VerificationApi->verify_customer_address: %s\\n\" % e)\n\nif __name__ == \"__main__\":\n    multiple_line_items()\n",
        "summary": "The provided Python script uses the CyberSource API to verify a customer's address by sending a request with detailed order information, including billing and shipping addresses, line items, and buyer details. The script handles nested dictionaries and removes any `None` values before converting the final request object to JSON format for submission to the API."
    },
    {
        "code": "principal = []\npar = []\nimpar = []\nwhile True:\n    n = int(input('Digite um valor: '))\n    principal.append(n)\n    if n % 2 == 0:\n        par.append(n)\n    else:\n        impar.append(n)\n    while True:\n        op\u00e7\u00e3o = str(input('Quer continuar? [S/N]: ')).upper()\n        if op\u00e7\u00e3o == 'S':\n            break\n        elif op\u00e7\u00e3o == 'N':\n            break\n        elif op\u00e7\u00e3o not in 'SN':\n            print('Op\u00e7\u00e3o inv\u00e1lida. Digite apenas S ou N')\n    if op\u00e7\u00e3o == 'N':\n        break\nprint(f'Lista principal de n\u00fameros: {principal}')\nprint(f'Lista dos n\u00fameros pares: {par}')\nprint(f'Lista dos n\u00fameros impares: {impar}')\n",
        "summary": "The Python code prompts the user to input numbers, categorizing them into a main list and separate lists for even and odd numbers. It continues to accept inputs until the user decides to stop, then displays all three lists."
    },
    {
        "code": "import sys\nimport os\nimport re\nsys.path.append('../')  \n\nfrom jinja2 import Template\n\nfrom cli_bdd.core.steps import (\n    command,\n    environment,\n    file as file_steps,\n)\n\n\nBASE_PATH = os.path.dirname(os.path.normpath(__file__))\nTEMPLATES_PATH = os.path.join(BASE_PATH, 'templates')\n\n\nSTEPS_MODULES = [\n    command,\n    environment,\n    file_steps,\n]\n\n\ndef _prepare_docstring(value):\n    if not value:\n        return ''\n    remove_spaces = 0\n    for line in value.split('\\n')[1:]:\n        if line:\n            for char in line:\n                if char != ' ':\n                    break\n                else:\n                    remove_spaces += 1\n            break\n\n    return re.sub(\n        r'^ {%s}' % remove_spaces,\n        '',\n        unicode(value),\n        flags=re.MULTILINE\n    ).strip()\n\n\ndef _render_and_save_template(path, dest, context):\n    template_path = os.path.join(TEMPLATES_PATH, path + '.tpl')\n    destination_path = os.path.join(BASE_PATH, dest + '.md')\n    with open(destination_path, 'wt') as dest_file:\n        dest_file.write(\n            Template(open(template_path).read()).render(context)\n        )\n\n\ndef generate_api_reference():\n    generate_steps_reference()\n\n\ndef generate_steps_reference():\n    steps_by_types = []\n\n    for step_module in STEPS_MODULES:\n        name = step_module.__name__.split('.')[-1]\n        steps_by_types.append({\n            'name': name,\n            'module': step_module.__name__,\n            'base_steps': step_module.base_steps\n        })\n\n    steps_dir = os.path.join(BASE_PATH, 'steps/')\n    if not os.path.exists(steps_dir):\n        os.makedirs(steps_dir)\n\n    for step_type in steps_by_types:\n        _render_and_save_template(\n            'steps',\n            'steps/' + step_type['name'],\n            {\n                'step_type': step_type,\n                'prepare_docstring': _prepare_docstring\n            }\n        )\n",
        "summary": "The Python script imports necessary modules and defines functions to generate API and steps references using Jinja2 templates. It dynamically prepares docstrings, renders templates with provided context, and saves the output as Markdown files in specified directories."
    },
    {
        "code": "from test_framework.test_framework import MichaellaoliuTestFramework\nfrom test_framework.util import (\n    assert_equal,\n    connect_nodes_bi,\n)\nimport shutil\nimport os\n\nclass WalletHDTest(MichaellaoliuTestFramework):\n    def set_test_params(self):\n        self.setup_clean_chain = True\n        self.num_nodes = 2\n        self.extra_args = [[], ['-keypool=0']]\n\n    def run_test (self):\n        tmpdir = self.options.tmpdir\n\n        \n        self.stop_node(1)\n        self.assert_start_raises_init_error(1, ['-usehd=0'], 'already existing HD wallet')\n        self.start_node(1)\n        connect_nodes_bi(self.nodes, 0, 1)\n\n        \n        masterkeyid = self.nodes[1].getwalletinfo()['hdmasterkeyid']\n        assert_equal(len(masterkeyid), 40)\n\n        \n        change_addr = self.nodes[1].getrawchangeaddress()\n        change_addrV= self.nodes[1].validateaddress(change_addr)\n        assert_equal(change_addrV[\"hdkeypath\"], \"m/0'/1'/0'\") \n\n        \n        non_hd_add = self.nodes[0].getnewaddress()\n        self.nodes[1].importprivkey(self.nodes[0].dumpprivkey(non_hd_add))\n\n        \n        self.nodes[1].backupwallet(tmpdir + \"/hd.bak\")\n        \n\n        \n        \n        self.nodes[0].generate(101)\n        hd_add = None\n        num_hd_adds = 300\n        for i in range(num_hd_adds):\n            hd_add = self.nodes[1].getnewaddress()\n            hd_info = self.nodes[1].validateaddress(hd_add)\n            assert_equal(hd_info[\"hdkeypath\"], \"m/0'/0'/\"+str(i)+\"'\")\n            assert_equal(hd_info[\"hdmasterkeyid\"], masterkeyid)\n            self.nodes[0].sendtoaddress(hd_add, 1)\n            self.nodes[0].generate(1)\n        self.nodes[0].sendtoaddress(non_hd_add, 1)\n        self.nodes[0].generate(1)\n\n        \n        change_addr = self.nodes[1].getrawchangeaddress()\n        change_addrV= self.nodes[1].validateaddress(change_addr)\n        assert_equal(change_addrV[\"hdkeypath\"], \"m/0'/1'/1'\") \n\n        self.sync_all()\n        assert_equal(self.nodes[1].getbalance(), num_hd_adds + 1)\n\n        self.log.info(\"Restore backup ...\")\n        self.stop_node(1)\n        \n        \n        shutil.rmtree(os.path.join(tmpdir, \"node1/regtest/blocks\"))\n        shutil.rmtree(os.path.join(tmpdir, \"node1/regtest/chainstate\"))\n        shutil.copyfile(os.path.join(tmpdir, \"hd.bak\"), os.path.join(tmpdir, \"node1/regtest/wallets/wallet.dat\"))\n        self.start_node(1)\n\n        \n        hd_add_2 = None\n        for _ in range(num_hd_adds):\n            hd_add_2 = self.nodes[1].getnewaddress()\n            hd_info_2 = self.nodes[1].validateaddress(hd_add_2)\n            assert_equal(hd_info_2[\"hdkeypath\"], \"m/0'/0'/\"+str(_)+\"'\")\n            assert_equal(hd_info_2[\"hdmasterkeyid\"], masterkeyid)\n        assert_equal(hd_add, hd_add_2)\n        connect_nodes_bi(self.nodes, 0, 1)\n        self.sync_all()\n\n        \n        self.stop_node(1)\n        self.start_node(1, extra_args=self.extra_args[1] + ['-rescan'])\n        assert_equal(self.nodes[1].getbalance(), num_hd_adds + 1)\n\n        \n        self.stop_node(1)\n        shutil.rmtree(os.path.join(tmpdir, \"node1/regtest/blocks\"))\n        shutil.rmtree(os.path.join(tmpdir, \"node1/regtest/chainstate\"))\n        shutil.copyfile(os.path.join(tmpdir, \"hd.bak\"), os.path.join(tmpdir, \"node1/regtest/wallet.dat\"))\n        self.start_node(1, extra_args=self.extra_args[1])\n        connect_nodes_bi(self.nodes, 0, 1)\n        self.sync_all()\n        out = self.nodes[1].rescanblockchain(0, 1)\n        assert_equal(out['start_height'], 0)\n        assert_equal(out['stop_height'], 1)\n        out = self.nodes[1].rescanblockchain()\n        assert_equal(out['start_height'], 0)\n        assert_equal(out['stop_height'], self.nodes[1].getblockcount())\n        assert_equal(self.nodes[1].getbalance(), num_hd_adds + 1)\n\n        \n        txid = self.nodes[1].sendtoaddress(self.nodes[0].getnewaddress(), 1)\n        outs = self.nodes[1].decoderawtransaction(self.nodes[1].gettransaction(txid)['hex'])['vout']\n        keypath = \"\"\n        for out in outs:\n            if out['value'] != 1:\n                keypath = self.nodes[1].validateaddress(out['scriptPubKey']['addresses'][0])['hdkeypath']\n\n        assert_equal(keypath[0:7], \"m/0'/1'\")\n\nif __name__ == '__main__':\n    WalletHDTest().main ()\n",
        "summary": "The provided Python code defines a test class `WalletHDTest` that inherits from `MichaellaoliuTestFramework`. It tests various aspects of Hierarchical Deterministic (HD) wallets, including wallet setup, key generation, address validation, and backup/restore functionality. The test ensures that HD keys are correctly generated, imported, and used for transactions, as well as that the wallet can be safely backed up and restored without losing HD properties."
    },
    {
        "code": "from typing import Any\n\ndef _get_handlers(location:str=\"\"):\n    \n\n",
        "summary": "The provided Python function `_get_handlers` is designed to retrieve handlers based on a specified location, with an optional default value of an empty string if no location is provided. The function's implementation details are not shown in the given snippet."
    },
    {
        "code": "from __future__ import print_function\nimport yaml\nimport subprocess\nimport re\nimport argparse\n\nfrom keras.models import model_from_yaml\nfrom betago.model import KerasBot\nfrom betago.processor import SevenPlaneProcessor\nfrom betago.gtp.board import gtp_position_to_coords, coords_to_gtp_position\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument('handicap', type=int, nargs=1)\nargparser.add_argument('output_sgf', nargs='?', default='output.sgf')\nargs = argparser.parse_args()\n\nprocessor = SevenPlaneProcessor()\n\nbot_name = '100_epochs_cnn'\nmodel_file = 'model_zoo/' + bot_name + '_bot.yml'\nweight_file = 'model_zoo/' + bot_name + '_weights.hd5'\n\nwith open(model_file, 'r') as f:\n    yml = yaml.load(f)\n    model = model_from_yaml(yaml.dump(yml))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n    model.load_weights(weight_file)\n\nbot = KerasBot(model=model, processor=processor)\n\npachi_cmd = [\"pachi\"]\np = subprocess.Popen(pachi_cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\n\ndef send_command(gtpStream, cmd):\n    gtpStream.stdin.write(cmd)\n    print(cmd.strip())\n\n\ndef get_response(gtpStream):\n    succeeded = False\n    result = ''\n    while succeeded == False:\n        line = gtpStream.stdout.readline()\n        if line[0] == '=':\n            succeeded = True\n            line = line.strip()\n            print(\"Response is: \" + line)\n            result = re.sub('^= ?', '', line)\n    return result\n\nletters = 'abcdefghijklmnopqrs'\n\n\ndef sgfCoord(coords):\n    row, col = coords\n    return letters[col] + letters[18 - row]\n\n\nhandicap = args.handicap[0]\n\nsend_command(p, \"boardsize 19\\n\")\nget_response(p)\n\nsgf = \"(;GM[1]FF[4]CA[UTF-8]SZ[19]RU[Chinese]\\n\"\n\nif(handicap == 0):\n    send_command(p, \"komi 7.5\\n\")\n    get_response(p)\n    sgf = sgf + \"KM[7.5]\\n\"\nelse:\n    send_command(p, \"fixed_handicap \" + str(handicap) + \"\\n\")\n    stones = get_response(p)\n    sgf_handicap = \"HA[\" + str(handicap) + \"]AB\"\n    for pos in stones.split(\" \"):\n        move = gtp_position_to_coords(pos)\n        bot.apply_move('b', move)\n        sgf_handicap = sgf_handicap + \"[\" + sgfCoord(move) + \"]\"\n    sgf = sgf + sgf_handicap + \"\\n\"\n\npasses = 0\nour_color = 'b'   \ntheir_color = 'w'   \nlast_color = 'w'\n\nif(handicap > 1):\n    last_color = 'b'\n\ncolors = {}\ncolors['w'] = 'white'\ncolors['b'] = 'black'\n\nwhile passes < 2:\n    if(last_color != our_color):\n        move = bot.select_move(our_color)  \n        if move is None:\n            send_command(p, \"play \" + colors[our_color] + \" pass\\n\")\n            sgf = sgf + \";\" + our_color.upper() + \"[]\\n\"\n            passes = passes + 1\n        else:\n            pos = coords_to_gtp_position(move)\n            send_command(p, \"play \" + colors[our_color] + \" \" + pos + \"\\n\")\n            sgf = sgf + \";\" + our_color.upper() + \"[\" + sgfCoord(move) + \"]\\n\"\n            passes = 0\n        resp = get_response(p)\n        last_color = our_color\n    else:\n        send_command(p, \"genmove \" + colors[their_color] + \"\\n\")\n        pos = get_response(p)\n        if(pos == 'resign'):\n            passes = 2\n        elif(pos == 'pass'):\n            sgf = sgf + \";\" + their_color.upper() + \"[]\\n\"\n            passes = passes + 1\n        else:\n            move = gtp_position_to_coords(pos)\n            bot.apply_move(their_color, move)\n            sgf = sgf + \";\" + their_color.upper() + \"[\" + sgfCoord(move) + \"]\\n\"\n            passes = 0\n        last_color = their_color\n\nsgf = sgf + \")\\n\"\n\nwith open(args.output_sgf, 'w') as out_h:\n    out_h.write(sgf)\n",
        "summary": "The Python script loads a pre-trained neural network model for Go from a YAML file and corresponding weights, then uses it to play against Pachi, a Go engine. It generates a SGF file of the game, including handicap settings if specified, and records the moves made by both the bot and Pachi until one resigns or passes twice in succession."
    },
    {
        "code": "import os\nimport sys\nfrom time import sleep\n\nimport threading\nimport window\nimport comunication\nimport DisplayActions\ndebugMode = True\n\n\n\n\ndef main():\n\n    print(\"starting...\",end='')\n    state = True\n    print(\"[\" +  (\"OK\" if state else \"ERROR\" ) + \"]\")\n    winThread.start()\n    comunication.start()\n\n\nwinThread = threading.Thread(target=window.start)\n\n\nif __name__ == '__main__':\n    main()",
        "summary": "The provided Python script initializes a multi-threaded application with two primary components: a window display managed by the `window` module and communication handled by the `comunication` module. The script checks for a debug mode setting, starts the necessary threads, and enters a main loop to manage their execution."
    },
    {
        "code": "import csv\nimport json\nimport requests\nimport operator\nfrom urllib.parse import quote\n\nENDPOINT = 'https://en.wikipedia.org/w/api.php'\n\nparameters = { 'action' : 'query',\n               'prop' : 'revisions',\n               'titles' : 'Panama_Papers',\n               'format' : 'json',\n               'rvdir' : 'newer',\n               'rvstart': '2016-04-03T17:59:05Z',\n               'rvend' : '2016-05-03T00:00:00Z',\n               'rvlimit' : 500,\n               'continue' : '' }\n\ndays = {}\ndone = False\nwhile not done:\n    wp_call = requests.get(ENDPOINT, params=parameters)\n    response = wp_call.json()\n    pages = response['query']['pages']\n    for page_id in pages:\n        page = pages[page_id]\n        revisions = page['revisions']\n        for rev in revisions:\n            revday = rev['timestamp'][:10].replace(\"-\",\"\")\n            revhour = rev['timestamp'][11:13]            \n            if revday in days.keys():\n                if revhour in days[revday].keys():\n                    days[revday][revhour] += 1\n                else:\n                    days[revday][revhour] = 1\n            else:\n                days[revday] = {}\n                days[revday][revhour] = 1\n\n    if 'continue' in response:\n        parameters['continue'] = response['continue']['continue']\n        parameters['rvcontinue'] = response['continue']['rvcontinue']\n    else:\n        done = True\n\n\nfor dkey, dval in days.items():\n    daily_edits = 0\n    for hkey, hval in dval.items():\n        daily_edits += hval\n    days[dkey]['total'] = daily_edits\n\n\n\nENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/'\n\nwp_code = 'en.wikipedia'\naccess = 'all-access'\nagents = 'all-agents'\npage_title = 'Panama Papers'\nperiod = 'daily'\nstart_date = '20160403'\nend_date = '20160502'\n\nwp_call = requests.get(ENDPOINT + wp_code + '/' + access + '/' + agents + '/' + quote(page_title, safe='') + '/' + period + '/' + start_date + '/' + end_date)\nresponse = wp_call.json()\n\n\n\nfor dv in response['items']:\n\n    ts = dv['timestamp'][:-2]\n    if ts in days.keys():\n        days[ts]['views'] = dv['views']\n\n\n\ndays_sorted = sorted(days.items(), key=operator.itemgetter(0), reverse=True)\nprint(days_sorted)\n\nwith open('pp30days_views_edits.csv', 'w') as f:\n    writer = csv.writer(f)\n    writer.writerow(('date', 'edits', 'views'))\n    for n in days_sorted: \n        writer.writerow((n[0], n[1]['total'], n[1]['views'],))",
        "summary": "The Python script retrieves and analyzes Wikipedia page edit data for the \"Panama Papers\" article from April 3, 2016 to May 2, 2016, calculating both the number of edits per hour and daily views. It then outputs this information in a CSV file named 'pp30days_views_edits.csv'."
    },
    {
        "code": "from unittest.mock import MagicMock\n\nfrom twisted.web.resource import NoResource\n\nfrom txweb.resources import RoutingResource\nfrom txweb import App\nfrom txweb.http_codes import Unrenderable\nfrom txweb.resources import ViewClassResource\n\nfrom unittest.mock import sentinel\nimport typing as T\n\nfrom .helper import RequestRetval\n\nimport pytest\n\n\ndef test_instantiates_without_error():\n\n    class FakeSite:\n        pass\n\n    fake_site = FakeSite()\n\n    resource = RoutingResource(fake_site)\n\n\ndef test_how_head_requests_are_handled(dummy_request:RequestRetval):\n\n    app = App(__name__)\n\n    @app.add(\"/foo\", methods=[\"POST\"])\n    def handle_foo(request):\n        return b\"123\"\n\n    dummy_request.request.site = app.site\n    dummy_request.channel.site = app.site\n    dummy_request.request.requestReceived(b\"HEAD\", b\"/foo\", b\"HTTP/1.1\")\n    assert dummy_request.request.code == 405\n    assert dummy_request.request.code_message == b\"Method not allowed\"\n\n\n\ndef test_ensure_blows_up_with_a_bad_add():\n\n    app = App(__name__)\n    bad_asset = sentinel\n\n\n    with pytest.raises(ValueError) as excinfo:\n        app.add(\"/trash\")(bad_asset)\n\n        assert \"expected callable|Object|twisted.web.resource.Resource\" in str(excinfo.value)\n\n\ndef test_ensure_blowsup_with_a_class_that_has_no_way_to_render():\n\n    app = App(__name__)\n\n    with pytest.raises(Unrenderable):\n        @app.add(\"/trash\")\n        class BaseClass(object):\n            pass\n\ndef test_ensure_a_classic_like_class_is_routed():\n\n\n    app = App(__name__)\n\n\n    @app.add(\"/trash\")\n    class GoodClass(object):\n\n        def render(self, request):\n            return b\"Rendered\"\n\n    first_key = next(iter(app.router.iter_rules()))\n    endpoint = app.router._endpoints[first_key.endpoint]\n    assert isinstance(endpoint, ViewClassResource)\n    debug = 1\n\ndef test_ensure_resource_is_added():\n\n    app = App(__name__)\n\n    app.add_resource(\"/404\", resource=NoResource())\n\n    first_key = next(iter(app.router.iter_rules()))\n    endpoint = app.router._endpoints[first_key.endpoint]\n    assert isinstance(endpoint, NoResource)\n    debug = 1\n\n\ndef test_handle_add_slashes(dummy_request:RequestRetval):\n\n    app = App(__name__)\n\n    mock = MagicMock()\n\n    app.route(\"/js/\")(mock)\n\n    dummy_request.request.site = app.site\n    dummy_request.channel.site = app.site\n    dummy_request.request.requestReceived(b\"GET\", b\"/js\", b\"HTTP/1.1\")\n\n    assert dummy_request.request.code == 308\n    assert dummy_request.request.code_message == b\"Permanent Redirect\"\n    assert dummy_request.request.responseHeaders.getRawHeaders(b\"location\") == [b\"http://10.0.0.1/js/\"]\n    assert mock.call_count == 0\n\n\n\n\n",
        "summary": "The provided Python code tests various functionalities of a web application framework, including routing, handling HTTP methods, and ensuring proper resource management. It uses mocking and pytest for testing different scenarios such as method handling, error conditions, and resource addition."
    },
    {
        "code": "import json\r\nfrom typing import Optional\r\n\r\nimport zipcodes\r\n\r\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\r\nfrom great_expectations.exceptions import InvalidExpectationConfigurationError\r\nfrom great_expectations.execution_engine import (\r\n    PandasExecutionEngine,\r\n    SparkDFExecutionEngine,\r\n    SqlAlchemyExecutionEngine,\r\n)\r\nfrom great_expectations.expectations.expectation import ColumnMapExpectation\r\nfrom great_expectations.expectations.metrics import (\r\n    ColumnMapMetricProvider,\r\n    column_condition_partial,\r\n)\r\n\r\n\r\ndef is_valid_minnesota_zip(zip: str):\r\n    list_of_dicts_of_minnesota_zips = zipcodes.filter_by(state=\"MN\")\r\n    list_of_minnesota_zips = [d[\"zip_code\"] for d in list_of_dicts_of_minnesota_zips]\r\n    if len(zip) > 10:\r\n        return False\r\n    elif type(zip) != str:\r\n        return False\r\n    elif zip in list_of_minnesota_zips:\r\n        return True\r\n    else:\r\n        return False\r\n\r\n\r\n\n\nclass ColumnValuesToBeValidMinnesotaZip(ColumnMapMetricProvider):\r\n\r\n    \n    condition_metric_name = \"column_values.valid_minnesota_zip\"\r\n\r\n    \n    @column_condition_partial(engine=PandasExecutionEngine)\r\n    def _pandas(cls, column, **kwargs):\r\n        return column.apply(lambda x: is_valid_minnesota_zip(x))\r\n\r\n    \n    \n    \n    \n\r\n    \n    \n    \n    \n\r\n\r\n\nclass ExpectColumnValuesToBeValidMinnesotaZip(ColumnMapExpectation):\r\n    \r\n\r\n    \n    \n    examples = [\r\n        {\r\n            \"data\": {\r\n                \"valid_minnesota_zip\": [\"55040\", \"55330\", \"55781\", \"55968\"],\r\n                \"invalid_minnesota_zip\": [\"-10000\", \"1234\", \"99999\", \"25487\"],\r\n            },\r\n            \"tests\": [\r\n                {\r\n                    \"title\": \"basic_positive_test\",\r\n                    \"exact_match_out\": False,\r\n                    \"include_in_gallery\": True,\r\n                    \"in\": {\"column\": \"valid_minnesota_zip\"},\r\n                    \"out\": {\"success\": True},\r\n                },\r\n                {\r\n                    \"title\": \"basic_negative_test\",\r\n                    \"exact_match_out\": False,\r\n                    \"include_in_gallery\": True,\r\n                    \"in\": {\"column\": \"invalid_minnesota_zip\"},\r\n                    \"out\": {\"success\": False},\r\n                },\r\n            ],\r\n        }\r\n    ]\r\n\r\n    \n    \n    map_metric = \"column_values.valid_minnesota_zip\"\r\n\r\n    \n    success_keys = (\"mostly\",)\r\n\r\n    \n    default_kwarg_values = {}\r\n\r\n    def validate_configuration(\r\n        self, configuration: Optional[ExpectationConfiguration]\r\n    ) -> None:\r\n        \r\n\r\n        super().validate_configuration(configuration)\r\n        if configuration is None:\r\n            configuration = self.configuration\r\n\r\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n\r\n    \n    library_metadata = {\r\n        \"maturity\": \"experimental\",  \n        \"tags\": [\r\n            \"hackathon\",\r\n            \"typed-entities\",\r\n        ],  \n        \"contributors\": [  \n            \"@luismdiaz01\",\r\n            \"@derekma73\",  \n        ],\r\n        \"requirements\": [\"zipcodes\"],\r\n    }\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ExpectColumnValuesToBeValidMinnesotaZip().print_diagnostic_checklist()\r\n",
        "summary": "The provided Python code defines a custom expectation for validating that column values in a dataset are valid Minnesota zip codes. It includes a metric provider to check if each value meets the criteria and an expectation class that extends `ColumnMapExpectation` to apply this validation, along with examples for testing its functionality."
    },
    {
        "code": "from setuptools import setup, find_packages\nimport sys\n\n\ninstall_requires = []\npyversion = sys.version_info[:2]\nif pyversion < (2, 7) or (3, 0) <= pyversion <= (3, 1):\n    install_requires.append('argparse')\n\nsetup(\n    name='radosgw-agent',\n    version='1.0',\n    packages=find_packages(),\n\n    author='Josh Durgin',\n    author_email='josh.durgin@inktank.com',\n    description='Synchronize users and data between radosgw clusters',\n    license='MIT',\n    keywords='radosgw ceph radosgw-agent',\n    url=\"https://github.com/ceph/radosgw-agent\",\n\n    install_requires=[\n        'setuptools',\n        'boto ==2.2.2',\n        'requests',\n        ] + install_requires,\n\n    entry_points={\n        'console_scripts': [\n            'radosgw-agent = radosgw_agent.cli:main',\n            ],\n        },\n    )\n",
        "summary": "This Python script sets up a package for the `radosgw-agent`, which is designed to synchronize users and data between Ceph RADOS Gateway clusters. It includes dependencies on `setuptools`, `boto`, and `requests`, with an additional requirement of `argparse` for versions of Python less than 2.7 or between 3.0 and 3.1."
    },
    {
        "code": "from flask import Blueprint, request\n\nfrom app.spiders.core import *\nfrom app.utils import build_result\nfrom app.constants import code\n\ncore = Blueprint('core', __name__)\n\n\n@core.route('/login', methods=['POST'])\ndef login():\n    data = request.form\n    username = data.get('username')\n    password = data.get('password')\n    return core_login(username, password)\n\n\n@core.route('/book_borrow_info', methods=['GET'])\ndef book_borrow_info():\n    token = request.args.get('token')\n    return get_book_borrow_info(token)\n\n\n@core.route('/trans_list', methods=['GET'])\ndef trans_list():\n    token = request.args.get('token')\n    return get_trans_list(token)\n\n\n@core.route('/tel_book', methods=['GET'])\ndef tel_book():\n    department_id = request.args.get('department_id')\n    return get_tel_book(department_id)\n\n",
        "summary": "The provided Python code defines a Flask Blueprint named 'core' with routes for user login and retrieving book borrowing information, transaction lists, and telephone books. Each route handles requests using specific methods from imported modules and returns results formatted by the `build_result` function."
    },
    {
        "code": "import argparse\n\nimport pygal\n\nparser = argparse.ArgumentParser(\n    description='Generate pygal chart in command line',\n    prog='pygal_gen')\n\nparser.add_argument('-t', '--type', dest='type', default='Line',\n                    choices=map(lambda x: x.__name__, pygal.CHARTS),\n                    help='Kind of chart to generate')\n\nparser.add_argument('-o', '--output', dest='filename', default='pygal_out.svg',\n                    help='Filename to write the svg to')\n\nparser.add_argument('-s', '--serie', dest='series', nargs='+', action='append',\n                    help='Add a serie in the form (title val1 val2...)')\n\nparser.add_argument('--version', action='version',\n                    version='pygal %s' % pygal.__version__)\n\nfor key in pygal.config.CONFIG_ITEMS:\n    opt_name = key.name\n    val = key.value\n    opts = {}\n    if key.type == list:\n        opts['type'] = key.subtype\n        opts['nargs'] = '+'\n    else:\n        opts['type'] = key.type\n\n    if opts['type'] == bool:\n        del opts['type']\n        opts['action'] = 'store_true' if not val else 'store_false'\n        if val:\n            opt_name = 'no-' + opt_name\n    if key.name == 'interpolate':\n        opts['choices'] = list(pygal.interpolate.INTERPOLATIONS.keys())\n    parser.add_argument(\n        '--%s' % opt_name, dest=key.name, default=val, **opts)\n\nconfig = parser.parse_args()\n\nchart = getattr(pygal, config.type)(**vars(config))\n\nfor serie in config.series:\n    chart.add(serie[0], map(float, serie[1:]))\n\nchart.render_to_file(config.filename)\n",
        "summary": "This Python script uses the `argparse` module to parse command-line arguments for generating a pygal chart. It allows specifying the type of chart, output file, series data, and various configuration options before rendering the chart to an SVG file."
    },
    {
        "code": "import ctypes\nfrom . import clibui\n\n\nclass uiCombobox(ctypes.Structure):\n    \n\n    pass\n\n\ndef uiComboboxPointer(obj):\n    \n\n    return ctypes.cast(obj, ctypes.POINTER(uiCombobox))\n\n\n\ndef uiComboboxAppend(combobox, text):\n    \n\n    clibui.uiComboboxAppend(combobox, bytes(text, 'utf-8'))\n\n\n\ndef uiComboboxSelected(combobox):\n    \n\n    return clibui.uiComboboxSelected(combobox)\n\n\n\ndef uiComboboxSetSelected(combobox, n):\n    \n\n    clibui.uiComboboxSetSelected(combobox, n)\n\n\n\ndef uiComboboxOnSelected(combobox, callback, data):\n    \n\n    c_type = ctypes.CFUNCTYPE(\n        ctypes.c_int, ctypes.POINTER(uiCombobox), ctypes.c_void_p)\n    c_callback = c_type(callback)\n\n    clibui.uiComboboxOnSelected(combobox, c_callback, data)\n\n    return c_callback\n\n\ndef uiNewCombobox():\n    \n\n    clibui.uiNewCombobox.restype = ctypes.POINTER(uiCombobox)\n\n    return clibui.uiNewCombobox()\n",
        "summary": "The provided Python code defines a class `uiCombobox` using the `ctypes` module to interact with a C library (`clibui`). It includes functions for appending text to a combobox, retrieving and setting the selected item, handling selection events, and creating new instances of a combobox."
    },
    {
        "code": "import click\nimport pandas as pd\n\ntry:\n    from textacy.preprocess import preprocess_text\nexcept Exception:\n    from textacy.preprocess import preprocess_text\n\n\ndef preprocess_f(text, fix_unicode=True, lowercase=True,\n                 no_urls=True, no_emails=True,\n                 no_phone_numbers=True,\n                 no_numbers=True, no_currency_symbols=True,\n                 no_punct=True, no_accents=True):\n    \n    clean_text = preprocess_text(text, fix_unicode=fix_unicode,\n                                 lowercase=lowercase,\n                                 no_urls=no_urls, no_emails=no_emails,\n                                 no_phone_numbers=no_phone_numbers,\n                                 no_numbers=no_numbers,\n                                 no_currency_symbols=no_currency_symbols,\n                                 no_punct=no_punct,\n                                 no_accents=no_accents)\n    return clean_text\n\n\n@click.command()\n@click.option('--input_path', type=click.STRING, help='Path to input file')\n@click.option('--output_path', type=click.STRING, help='Path to input file')\n@click.option('--set_', type=click.Choice(['train', 'test']), help=\"set\")\ndef preprocess(input_path, output_path, set_):\n    \n    if set_ == \"train\":\n        df = pd.read_csv(input_path, sep='|')\n    else:\n        df = pd.read_csv(input_path)\n\n    df[\"clean_txt\"] = df[\"Pregunta\"].apply(lambda x: preprocess_f(x))\n\n    df.to_csv(output_path, index=False)\n\n\nif __name__ == \"__main__\":\n    preprocess()\n",
        "summary": "The provided Python script uses the `click` library to create a command-line interface for preprocessing text data. It reads a CSV file containing questions (Pregunta column), applies various text cleaning transformations using the `textacy.preprocess.preprocess_text` function, and saves the cleaned text back to a new CSV file. The script supports different datasets by allowing the user to specify whether the input is from a 'train' or 'test' set."
    },
    {
        "code": "import sys\nimport getopt\nfrom core.interface.action import server_action\nfrom core.helper.usage import usage_helper\nfrom prettytable import PrettyTable\nfrom core.helper.parser import config_parser\nimport re\n\nclass action_list(server_action):\n    \n    _parameters = [\n        {\"name\":\"h\", \"needarg\":False, \"desc\":\"\u663e\u793a\u8fd9\u6761\u5e2e\u52a9\u4fe1\u606f\", \"argname\":\"help\"},\n        {\"name\":\"n\", \"needarg\":True, \"desc\":\"\u6839\u636e\u670d\u52a1\u5668\u540d\u79f0\u8fdb\u884c\u6a21\u7cca\u641c\u7d22\", \"argname\":\"name\"},\n        {\"name\":\"t\", \"needarg\":True, \"desc\":\"\u6839\u636e\u6807\u7b7e\u8fdb\u884c\u641c\u7d22\", \"argname\":\"tag\"}\n    ]\n\n    def __init__(self):\n        \n        self._usage_helper = usage_helper(sys.argv[0], \"list\", self._parameters)\n        self._config = config_parser()\n\n    def _usage(self):\n        \n        self._usage_helper.output()\n\n    \n    \n    \n    def _prepare_parameters(self):\n        recognized_parameter={}\n        for obj in self._parameters:\n            obj_key = '-' + obj['name']     \n            recognized_parameter[obj_key] = obj    \n            parameter_name = \"_%s\"%(obj['argname'])\n            if obj['needarg'] == True:\n                setattr(self, parameter_name, None)\n            else:\n                setattr(self, parameter_name, False)\n        return recognized_parameter\n\n\n    \n    def description(self):\n        return \"\u5217\u51fa\u670d\u52a1\u5668\u4fe1\u606f\"\n\n    \n    def parse_parameters(self):\n        try:\n            opts, argv = getopt.getopt(sys.argv[2:], self._usage_helper.get_opt_string())\n        except Exception as e:\n            self._usage()\n            exit()\n        parameters = self._prepare_parameters()\n        for opt,arg in opts:\n            if parameters[opt]['needarg'] == True:\n                setattr(self,\"_%s\"%(parameters[opt]['argname']), arg)      \n            else:\n                setattr(self,\"_%s\"%(parameters[opt]['argname']), True)     \n        \n        if self._help == True:\n            self._usage()\n            exit()\n        \n        self._tag=[]\n        prog_with_value = re.compile(r'^[\\w]+=[0-9a-zA-Z-_]+$')\n        prog_without_value = re.compile(r'^[\\w]+$')\n        for opt, arg in opts:\n            if opt == '-t':\n                if prog_with_value.match(arg) is not None:\n                    \n                    name,value = arg.split('=')\n                    self._tag.append({name:value})\n                elif prog_without_value.match(arg) is not None:\n                    \n                    self._tag.append({arg:''})\n                else:\n                    print(\"%s is bad value\"%(arg))\n\n    \n    def _search_by_name(self):\n        ret_array = []\n        \n        if self._name == None:\n            for i in self._config:\n                ret_array.append(i['ip'])\n            return set(ret_array)\n        \n        prog = re.compile('^.*%s.*$'%(self._name))\n        for i in self._config:\n            if 'name' not in i:\n                continue\n            if prog.match(i['name']) != None:\n                ret_array.append(i['ip'])\n        return set(ret_array)\n\n    \n    def _search_by_tag(self):\n        ret_array = []\n        \n        if len(self._tag) == 0:\n            for i in self._config:\n                ret_array.append(i['ip'])\n            return set(ret_array)\n        \n        for i in self._config:\n            if 'tags' in i:\n                print(i['tags'])\n        return set([])\n        pass\n\n    \n    def run(self):\n        \n        name_set = self._search_by_name()\n        tag_set = self._search_by_tag()\n        finnal_set = name_set & tag_set\n        prog = re.compile('^%s$'%(self._name))\n        disp = PrettyTable([\"IP\", \"\u670d\u52a1\u5668\u540d\u79f0\",\"\u6807\u7b7e\"])\n        for i in self._config:\n            \n            if i['ip'] in finnal_set:\n                name = i['name'] if 'name' in i else ''\n                tag = []\n                if 'tags' in i:\n                    for t in i['tags']:\n                        tag.append(\"%s:%s\"%(t, i['tags'][t]))\n                disp.add_row([i['ip'], name, ','.join(tag)])\n        print(disp)\n        ",
        "summary": "The provided Python code defines a class `action_list` that extends `server_action`, implementing functionality to list server information based on optional parameters such as server name and tags. It uses command-line arguments for input, parses them using `getopt`, and filters server data from a configuration source before displaying the results in a formatted table."
    },
    {
        "code": "from __future__ import print_function \nimport numpy as np\nimport pyedda as edda\n\n\n\nprint(\"//////////Univariate Gaussian///////\")\ndummy_data = np.random.rand(100)\ngaussian = edda.Gaussian(100, 20)\nprint(\"gaussian.getMean():\", gaussian.getMean())\nprint(\"gaussian.getVar():\", gaussian.getVar())\nprint(\"gaussian.getPdf(105):\", gaussian.getPdf(105))\nprint(\"gaussian.getSample():\", gaussian.getSample())\nprint(\"gaussian.getCdf(105):\", gaussian.getCdf(105))\nprint(\"gaussian.getCdfPrecise():\", gaussian.getCdfPrecise(105))\nprint(\"Output gaussian:\")\ngaussian.output()\nprint()\n",
        "summary": "The Python code demonstrates the use of the `pyedda` library to create and manipulate a univariate Gaussian distribution, printing various statistical properties and samples from the distribution."
    },
    {
        "code": "import graphene\nfrom graphene import Node\nfrom graphene_django.filter import DjangoFilterConnectionField\nfrom graphene_django.rest_framework.mutation import SerializerMutation\nfrom graphene_django.types import DjangoObjectType\nfrom rest_framework.generics import get_object_or_404\n\nfrom contact.models import Contact\nfrom contact.serializers import ContactSerializer\n\n\nclass ContactModelMutation(SerializerMutation):\n    class Meta:\n        serializer_class = ContactSerializer\n        convert_choices_to_enum = False\n\n\nclass ContactNode(DjangoObjectType):\n    class Meta:\n        model = Contact\n        interfaces = (Node,)\n        fields = \"__all__\"\n        filter_fields = [\"first_name\"]\n\n\nclass ContactType(DjangoObjectType):\n    class Meta:\n        model = Contact\n        fields = \"__all__\"\n\n\nclass Query(graphene.ObjectType):\n    contact_node = Node.Field(ContactNode)\n    contacts_node = DjangoFilterConnectionField(ContactNode)\n\n    contact = graphene.Field(ContactType, id=graphene.Int())\n    contacts = graphene.List(ContactType)\n\n    def resolve_contacts(self, info, **kwargs):\n        return Contact.objects.all()\n\n    def resolve_contact(self, info, id):\n        return get_object_or_404(Contact, pk=id)\n\n\nclass DeleteMutation(graphene.Mutation):\n    class Arguments:\n        \n        id = graphene.Int(required=True)\n\n    \n    id = graphene.ID()\n    message = graphene.String()\n\n    @classmethod\n    def mutate(cls, root, info, id):\n        contact = get_object_or_404(Contact, pk=id)\n        contact.delete()\n        return cls(id=id, message='deleted')\n\n\nclass Mutation(graphene.ObjectType):\n    create_contact = ContactModelMutation.Field()\n    update_contact = ContactModelMutation.Field()\n    delete_contact = DeleteMutation.Field()\n\n\nschema = graphene.Schema(query=Query, mutation=Mutation)\n",
        "summary": "The provided Python code defines a GraphQL schema for managing contacts using Django and Graphene. It includes mutations for creating, updating, and deleting contacts, as well as queries to retrieve contact data. The schema utilizes Django's ORM and REST framework for handling data persistence and retrieval efficiently."
    },
    {
        "code": "def checksum(data):\n    if len(data) & 0x1:  \n        data += b'\\0'\n    cs = 0\n    for pos in range(0, len(data), 2):\n        b1 = data[pos]\n        b2 = data[pos + 1]\n        cs += (b1 << 8) + b2\n    while cs >= 0x10000:\n        cs = (cs & 0xffff) + (cs >> 16)\n    cs = ~cs & 0xffff\n    return cs\n\n\ndef stddev(data):\n    N = len(data)\n    avg = sum(data)/N\n    num = sum([(x-avg)**2 for x in data])\n    den = N - 1\n    stddev = (num/den)**0.5\n    return stddev\n\n\ndef ping(host, count=4, timeout=5000, interval=10, quiet=False, size=64,\n         rtn=True, loop=False, int_loop=800):\n    import utime\n    import uselect\n    import uctypes\n    import usocket\n    import ustruct\n    import urandom\n    from sys import platform\n    import gc\n    from array import array\n\n    \n    assert size >= 16, \"pkt size too small\"\n    pkt = b'Q'*size\n    pkt_desc = {\n        \"type\": uctypes.UINT8 | 0,\n        \"code\": uctypes.UINT8 | 1,\n        \"checksum\": uctypes.UINT16 | 2,\n        \"id\": uctypes.UINT16 | 4,\n        \"seq\": uctypes.INT16 | 6,\n        \"timestamp\": uctypes.UINT64 | 8,\n    }  \n    h = uctypes.struct(uctypes.addressof(pkt), pkt_desc, uctypes.BIG_ENDIAN)\n    h.type = 8  \n    h.code = 0\n    h.checksum = 0\n    if platform == 'esp8266':\n        h.id = urandom.getrandbits(16)\n    else:\n        h.id = urandom.randint(0, 65535)\n    h.seq = 1\n    time_data = array(\"f\", (0 for _ in range(0)))\n\n    \n    sock = usocket.socket(usocket.AF_INET, usocket.SOCK_RAW, 1)\n    sock.setblocking(0)\n    sock.settimeout(timeout/1000)\n    addr = usocket.getaddrinfo(host, 1)[0][-1][0]  \n    sock.connect((addr, 1))\n    not quiet and print(\"PING %s (%s): %u data bytes\" % (host, addr, len(pkt)))\n    seq_loop = -1\n    try:\n        if loop:\n            n_trans = 0\n            n_recv = 0\n            while True:\n                gc.collect()\n                utime.sleep_ms(int_loop)\n                count = 1\n                seq_loop += 1\n                seqs = list(range(1, count+1))  \n                c = 1\n                t = 0\n                finish = False\n                while t < timeout:\n                    if t == interval and c <= count:\n                        \n                        h.checksum = 0\n                        h.seq = c\n                        h.timestamp = utime.ticks_us()\n                        h.checksum = checksum(pkt)\n                        if sock.send(pkt) == size:\n                            n_trans += 1\n                            t = 0  \n                        else:\n                            seqs.remove(c)\n                            if loop:\n                                count += 1\n                                seqs.append(count)\n                        c += 1\n\n                    \n                    while 1:\n                        socks, _, _ = uselect.select([sock], [], [], 0)\n                        if socks:\n                            resp = socks[0].recv(4096)\n                            resp_mv = memoryview(resp)\n                            h2 = uctypes.struct(uctypes.addressof(\n                                resp_mv[20:]), pkt_desc, uctypes.BIG_ENDIAN)\n                            \n                            seq = h2.seq\n                            \n                            if h2.type == 0 and h2.id == h.id and (seq in seqs):\n                                t_elapsed = (utime.ticks_us()-h2.timestamp) / 1000\n                                ttl = ustruct.unpack('!B', resp_mv[8:9])[0]  \n                                n_recv += 1\n                                not quiet and print(\"{} bytes from {}: icmp_seq={} ttl={} time={:.3f} ms\".format(\n                                    len(resp), addr, seq_loop, ttl, t_elapsed))\n                                time_data.append(t_elapsed)\n                                seqs.remove(seq)\n                                if len(seqs) == 0:\n                                    finish = True\n                                    break\n                        else:\n                            break\n\n                    if finish:\n                        break\n\n                    utime.sleep_ms(1)\n                    t += 1\n\n        else:\n            seqs = list(range(1, count+1))  \n            c = 1\n            t = 0\n            n_trans = 0\n            n_recv = 0\n            finish = False\n            while t < timeout:\n                if t == interval and c <= count:\n                    \n                    h.checksum = 0\n                    h.seq = c\n                    h.timestamp = utime.ticks_us()\n                    h.checksum = checksum(pkt)\n                    if sock.send(pkt) == size:\n                        n_trans += 1\n                        t = 0  \n                    else:\n                        seqs.remove(c)\n                        if loop:\n                            count += 1\n                            seqs.append(count)\n                    c += 1\n\n                \n                while 1:\n                    socks, _, _ = uselect.select([sock], [], [], 0)\n                    if socks:\n                        resp = socks[0].recv(4096)\n                        resp_mv = memoryview(resp)\n                        h2 = uctypes.struct(uctypes.addressof(\n                            resp_mv[20:]), pkt_desc, uctypes.BIG_ENDIAN)\n                        \n                        seq = h2.seq\n                        \n                        if h2.type == 0 and h2.id == h.id and (seq in seqs):\n                            t_elapsed = (utime.ticks_us()-h2.timestamp) / 1000\n                            ttl = ustruct.unpack('!B', resp_mv[8:9])[0]  \n                            n_recv += 1\n                            not quiet and print(\"{} bytes from {}: icmp_seq={} ttl={} time={:.3f} ms\".format(\n                                len(resp), addr, seq, ttl, t_elapsed))\n                            time_data.append(t_elapsed)\n                            seqs.remove(seq)\n                            if loop:\n                                count += 1\n                                seqs.append(count)\n                                utime.sleep_ms(int_loop)\n                            if len(seqs) == 0:\n                                finish = True\n                                break\n                    else:\n                        break\n\n                if finish:\n                    if not loop:\n                        break\n\n                utime.sleep_ms(1)\n                t += 1\n        sock.close()\n        if not quiet:\n            print('--- {} ping statistics ---'.format(host))\n            print(\"{} packets transmitted, {} packets received, {:.1f}% packet loss\".format(\n                n_trans, n_recv, (1-(n_recv/n_trans))*100))\n            print(\"round-trip min/avg/max/stddev = {:.2f}/{:.2f}/{:.2f}/{:.2f} ms\".format(min(time_data),sum(time_data)/len(time_data),max(time_data), stddev(time_data)))\n        gc.collect()\n        if rtn:\n            return (n_trans, n_recv)\n    except KeyboardInterrupt:\n        \n        sock.close()\n        gc.collect()\n        if not quiet:\n            print('^C')\n            print('--- {} ping statistics ---'.format(host))\n            print(\"{} packets transmitted, {} packets received, {:.1f}% packet loss\".format(\n                n_trans, n_recv, (1-(n_recv/n_trans))*100))\n            print(\"round-trip min/avg/max/stddev = {:.2f}/{:.2f}/{:.2f}/{:.2f} ms\".format(min(time_data),sum(time_data)/len(time_data),max(time_data), stddev(time_data)))\n        if rtn:\n            return (n_trans, n_recv)\n    except Exception as e:\n        print(e)\n",
        "summary": "The provided Python code includes three functions: `checksum`, which calculates a checksum for data using the Internet Checksum algorithm; `stddev`, which computes the standard deviation of a list of numbers; and `ping`, which performs network pinging to measure round-trip times to a specified host, handling various parameters such as count, timeout, interval, and loop mode."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport operator\n\nfrom celery.concurrency import solo\nfrom celery.utils.functional import noop\nfrom celery.tests.case import AppCase\n\n\nclass test_solo_TaskPool(AppCase):\n\n    def test_on_start(self):\n        x = solo.TaskPool()\n        x.on_start()\n\n    def test_on_apply(self):\n        x = solo.TaskPool()\n        x.on_start()\n        x.on_apply(operator.add, (2, 2), {}, noop, noop)\n\n    def test_info(self):\n        x = solo.TaskPool()\n        x.on_start()\n        self.assertTrue(x.info)\n",
        "summary": "The provided Python code defines a test class `test_solo_TaskPool` that inherits from `AppCase`. It includes three methods to test the behavior of a task pool using the `solo` concurrency backend in Celery. The methods verify the initialization, application of tasks, and retrieval of information about the task pool."
    },
    {
        "code": "import sys\r\nimport os\r\nimport random\r\nimport re\r\nimport time\r\nimport torch \r\nfrom torch.autograd import Variable\r\nfrom torch import optim\r\nimport torch.nn as nn\r\n\nfrom hybrid_bid_t1_model import Seq2Seq\r\nfrom hybrid_data_utils import *\r\n\r\nsub = '-'*20\r\ndef init_command_line(argv):\r\n\tfrom argparse import ArgumentParser\r\n\tusage = \"seq2seq\"\r\n\tdescription = ArgumentParser(usage)\r\n\tdescription.add_argument(\"--w2v_path\", type=str, default=\"/users3/yfwang/data/w2v/opensubtitle/\")\r\n\tdescription.add_argument(\"--corpus_path\", type=str, default=\"/users3/yfwang/data/corpus/opensubtitle/\")\r\n\tdescription.add_argument(\"--w2v\", type=str, default=\"train_all_200e.w2v\")\r\n\tdescription.add_argument(\"--test_file\", type=str, default=\"test_sessions.txt\")\r\n\t\r\n\tdescription.add_argument(\"--max_context_size\", type=int, default=2)\r\n\tdescription.add_argument(\"--batch_size\", type=int, default=64)\r\n\tdescription.add_argument(\"--enc_hidden_size\", type=int, default=512)\r\n\tdescription.add_argument(\"--max_senten_len\", type=int, default=15)\r\n\r\n\tdescription.add_argument(\"--dropout\", type=float, default=0.5)\r\n\r\n\tdescription.add_argument(\"--teach_forcing\", type=int, default=1)\r\n\tdescription.add_argument(\"--print_every\", type=int, default=100, help=\"print every batches when training\")\r\n\tdescription.add_argument(\"--weights\", type=str, default=None)\r\n\treturn description.parse_args(argv)\r\n\r\nopts = init_command_line(sys.argv[1:])\r\nprint (\"Configure:\")\r\nprint (\" w2v:\",os.path.join(opts.w2v_path,opts.w2v))\r\nprint (\" test_file:\",os.path.join(opts.corpus_path,opts.test_file))\r\n\r\nprint (\" max_context_size:\",opts.max_context_size)\r\nprint (\" batch_size:\",opts.batch_size)\r\nprint (\" enc_hidden_size:\",opts.enc_hidden_size)\r\nprint (\" max_senten_len:\",opts.max_senten_len)\r\n\r\nprint (\" dropout:\",opts.dropout)\r\n\r\nprint (\" teach_forcing:\",opts.teach_forcing)\r\nprint (\" print_every:\",opts.print_every)\r\nprint (\" weights:\",opts.weights)\r\nprint (\"\")\r\n\r\ndef readingTestCorpus(test_file_path):\r\n\tprint (\"reading...\")\r\n\ttest_file = open(test_file_path,'r')\r\n\tlist_pairs = []\r\n\ttmp_pair = []\r\n\tfor line in test_file:\r\n\t\tline = line.strip('\\n')\r\n\t\tif line == sub:\r\n\t\t\tlist_pairs.append(tmp_pair)\r\n\t\t\ttmp_pair = []\r\n\t\telse:\r\n\t\t\ttmp_pair.append(line)\r\n\ttest_file.close()\r\n\r\n\ttest_contexts = []\r\n\ttest_replys = []\r\n\tmax_con_size =  0\r\n\tmin_con_size = 10000\r\n\tfor pair in list_pairs:\r\n\t\tif len(pair) >= 3:\r\n\t\t\ttest_contexts.append(pair[0:-1])\r\n\t\t\ttest_replys.append(pair[-1])\r\n\t\t\tmax_con_size = max(len(pair[0:-1]),max_con_size)\r\n\t\t\tmin_con_size = min(len(pair[0:-1]),min_con_size)\r\n\t\telse:\r\n\t\t\tpass\r\n\tprint (max_con_size)\r\n\tprint (min_con_size)\r\n\treturn test_contexts,test_replys\r\n\r\ndef preProcess(word2index,test_contexts,unk_char,ini_char,max_senten_len,max_context_size):\r\n\tprint (\"preprocessing...\")\r\n\tfilter_test_contexts = []\r\n\tfor context in test_contexts:\r\n\t\tfilter_context = [filteringSenten(word2index,senten,unk_char,ini_char) for senten in context]\r\n\t\tfilter_test_contexts.append(filter_context)\r\n\r\n\tpadded_test_pairs = []\r\n\tfor context in filter_test_contexts:\r\n\t\tpad_list = [0]*len(context)\r\n\t\tif len(context) <= max_context_size:\r\n\t\t\tpad_list = [1]*(max_context_size-len(context)) + pad_list\r\n\t\t\tcontext = ['<unk>']*(max_context_size-len(context)) + context\r\n\t\telse:\r\n\t\t\tpad_list = pad_list[-max_context_size:]\r\n\t\t\tcontext = context[-max_context_size:]\r\n\t\tpadded_context = [paddingSenten(senten,max_senten_len) for senten in context]\r\n\t\tpadded_test_pairs.append([padded_context,pad_list])\r\n\r\n\treturn padded_test_pairs\r\n\r\n\r\n\ndef predictSentences(index2word,unk_char,ini_char,ini_idx,model,test_pairs,\r\n\t\t\t\t\tprint_every,batch_size,max_senten_len,max_context_size):\r\n\tmodel.eval()\r\n\t\n\tpairs_batches,num_batches = buildingPairsBatch(test_pairs,batch_size,shuffle=False)\r\n\tprint (\"\")\r\n\tprint (\"num of batch:\",num_batches)\r\n\t\r\n\tpredict_sentences = []\r\n\tidx_batch = 0\r\n\tfor contexts_tensor_batch, pad_matrix_batch in getTensorsContextPairsBatch(word2index,pairs_batches,max_context_size):\r\n\t\tpredict_batch = model.predict(contexts_tensor_batch,index2word,pad_matrix_batch,ini_idx,sep_char='\\t')\r\n\t\tpredict_sentences.extend(predict_batch)\r\n\t\tif (idx_batch+1)%print_every == 0:\r\n\t\t\tprint (\"{} batches finished\".format(idx_batch+1))\r\n\t\tidx_batch += 1\r\n\r\n\tpredict_sentences = predict_sentences[0:len(test_pairs)]\r\n\treturn predict_sentences\r\n\r\nif __name__ == '__main__':\r\n\tini_char = '</i>'\r\n\tunk_char = '<unk>'\r\n\tt0 = time.time()\r\n\tprint (\"loading word2vec...\")\r\n\tctable = W2vCharacterTable(os.path.join(opts.w2v_path,opts.w2v),ini_char,unk_char)\r\n\tprint(\" dict size:\",ctable.getDictSize())\r\n\tprint (\" emb size:\",ctable.getEmbSize())\r\n\tprint (time.time()-t0)\r\n\tprint (\"\")\r\n\r\n\tseq2seq = Seq2Seq(ctable.getDictSize(),ctable.getEmbSize(),opts.enc_hidden_size,opts.batch_size,opts.dropout,\r\n\t\t\t\t\topts.max_senten_len,opts.teach_forcing).cuda()\r\n\r\n\tif opts.weights != None:\r\n\t\tprint (\"load model parameters...\")\r\n\t\tseq2seq.load_state_dict(torch.load(opts.weights))\r\n\telse:\r\n\t\tprint (\"No model parameters!\")\r\n\t\texit()\r\n\r\n\ttest_contexts,test_replys = readingTestCorpus(os.path.join(opts.corpus_path,opts.test_file))\r\n\tprint (\"len(test_contexts):\",len(test_contexts))\r\n\tprint (\"len(test_replys):\",len(test_replys))\r\n\r\n\tword2index = ctable.getWord2Index()\r\n\ttest_pairs = preProcess(word2index,test_contexts,unk_char,ini_char,opts.max_senten_len,opts.max_context_size)\r\n\tprint (\"len(test_pairs):\",len(test_pairs))\r\n\t\r\n\t\r\n\tprint (\"start predicting...\")\r\n\tini_idx = word2index[ini_char]\r\n\tpredict_sentences = predictSentences(ctable.getIndex2Word(),unk_char,ini_char,ini_idx,seq2seq,test_pairs,\r\n\t\t\t\t\t\t\t\t\topts.print_every,opts.batch_size,opts.max_senten_len,opts.max_context_size)\r\n\r\n\tprint (\"writing...\")\r\n\tif not os.path.exists('./result/'):\r\n\t\tos.mkdir('./result/')\r\n\tpred_res_file = open(\"./result/open_pred_res_hyb_t1_len2\",'w')\r\n\tpred_ans_file = open(\"./result/open_pred_ans_hyb_t1_len2\",'w')\r\n\tfor idx,senten in enumerate(predict_sentences):\r\n\t\ttest_context = test_contexts[idx]\r\n\t\tfor test_post in test_context:\r\n\t\t\tpred_res_file.write(test_post+'\\n')\r\n\t\tpred_res_file.write(senten+'\\n')\r\n\t\tpred_res_file.write(sub+'\\n')\r\n\t\tsenten_l = [c for c in senten.split('\\t') if c != '</s>']\r\n\t\tpred_ans_file.write(' '.join(senten_l)+' __eou__'+'\\n')\r\n\r\n\tpred_res_file.close()\r\n\tpred_ans_file.close()\r\n\tprint (\"end\")\r\n\t\r\n",
        "summary": "The Python script is a command-line tool for evaluating a sequence-to-sequence model on a test corpus using pre-trained word embeddings. It processes the input data, initializes the model with optional weights, and outputs predicted sequences along with their ground truth answers to files."
    },
    {
        "code": "import json\nimport logging\nimport os\nimport re\nimport sys\nimport time\n\n\ndef setup_logger():\n    console = logging.StreamHandler(sys.stdout)\n    handlers = [console]\n    logging.basicConfig(handlers=handlers)\n    root = logging.getLogger()\n    root.setLevel(logging.INFO)\n\n\nsetup_logger()\n\nlog = logging.getLogger(__name__)\n\n\nclass NS:\n\n    @staticmethod\n    def dict(ns, deep=True):\n        dic = ns.__dict__\n        if not deep:\n            return dic\n        for k, v in dic.items():\n            if isinstance(v, NS):\n                dic[k] = NS.dict(v)\n        return dic\n\n    @staticmethod\n    def from_dict(dic, deep=True):\n        ns = NS(dic)\n        if not deep:\n            return ns\n        for k, v in ns.__dict__.items():\n            if isinstance(v, dict):\n                ns.__dict__[k] = NS.from_dict(v)\n        return ns\n\n    @staticmethod\n    def walk(ns, fn, inplace=False):\n        nns = ns if inplace else NS()\n        for k, v in ns.__dict__.items():\n            nk, nv = fn(k, v)\n            if nk is not None:\n                if v is nv and isinstance(v, NS):\n                    nv = NS.walk(nv, fn, inplace)\n                nns.__dict__[nk] = nv\n        return nns\n\n    def __init__(self, *args, **kwargs):\n        self.__dict__.update(dict(*args, **kwargs))\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    def __repr__(self):\n        return repr(self.__dict__)\n\n\nclass Timer:\n\n    @staticmethod\n    def _zero():\n        return 0\n\n    def __init__(self, clock=time.time, enabled=True):\n        self.start = 0\n        self.stop = 0\n        self._time = clock if enabled else Timer._zero\n\n    def __enter__(self):\n        self.start = self._time()\n        return self\n\n    def __exit__(self, *args):\n        self.stop = self._time()\n\n    @property\n    def duration(self):\n        if self.stop > 0:\n            return self.stop - self.start\n        return self._time() - self.start\n\n\ndef result(output_file=None,\n           predictions=None, truth=None,\n           probabilities=None, probabilities_labels=None,\n           target_is_encoded=False,\n           error_message=None,\n           models_count=None,\n           training_duration=None):\n    return locals()\n\n\ndata_keys = re.compile(\"^(X|y|data)(_.+)?$\")\n\n\ndef call_run(run_fn):\n    import numpy as np\n\n    params = NS.from_dict(json.loads(sys.stdin.read()))\n\n    def load_data(name, path):\n        if isinstance(path, str) and data_keys.match(name):\n            return name, np.load(path, allow_pickle=True)\n        return name, path\n\n    print(params.dataset)\n    ds = NS.walk(params.dataset, load_data)\n\n    config = params.config\n    config.framework_params = NS.dict(config.framework_params)\n\n    try:\n        result = run_fn(ds, config)\n        res = dict(result)\n        for name in ['predictions', 'truth', 'probabilities']:\n            arr = result[name]\n            if arr is not None:\n                res[name] = os.path.join(config.result_dir, '.'.join([name, 'npy']))\n                np.save(res[name], arr, allow_pickle=True)\n    except Exception as e:\n        log.exception(e)\n        res = dict(\n            error_message=str(e),\n            models_count=0\n        )\n\n    print(config.result_token)\n    print(json.dumps(res, separators=(',', ':')))\n",
        "summary": "The provided Python code defines a namespace class `NS` for handling nested dictionaries and objects, a timer class `Timer` for measuring execution time, and a function `result` for processing and saving model predictions and truth data. The script reads parameters from standard input, processes the dataset using the provided run function, and outputs the results in JSON format."
    },
    {
        "code": "import os\nimport wavio\n\nfrom fe_utils import *\nfrom config import TRAIN_CLEAN\n\nos.chdir(TRAIN_CLEAN)\n\nfor file in os.listdir():\n    if not file.endswith('.wav'):\n        continue\n    \n    mWav = wavio.read(file)\n    frame_len = int(getFrameSize(mWav.rate))\n    \n    mWav.data = normalizeAudio(mWav.data, mWav.sampwidth)\n    frame_cnt = int(len(mWav.data)/frame_len )\n    \n    if (len(mWav.data)%frame_len):\n        frame_cnt += 1\n    \n    class_list = []\n    for idx in range(frame_cnt):\n        if (idx == frame_cnt-1): \n            \n            chunk = mWav.data[idx*frame_len :]\n        else:\n            chunk = mWav.data[idx*frame_len : (idx+1)*frame_len]\n            \n        if aboveFrameThreshold(chunk):\n            class_list.append(1)\n        else:\n            class_list.append(0)\n            \n    filename = os.path.splitext(file)[0]\n    with open(filename + '.csv', 'w') as f:\n        f.write(','.join([str(c) for c in class_list]))",
        "summary": "The Python script processes WAV files located in a specified directory, normalizes their audio data, and then divides each file into frames to determine if they exceed a certain threshold. It outputs a CSV file for each input WAV file containing binary values indicating whether each frame meets the threshold condition."
    },
    {
        "code": "import re\nimport os\nimport glob\nimport json\nimport string\nimport logging\nimport subprocess as sp\nimport yaxil.commons as commons\n\nlogger = logging.getLogger(__name__)\n\n\nlegal = re.compile('[^a-zA-Z0-9]')\n\ndef bids_from_config(yaxil_session, scans_metadata, config, out_base):\n    \n    \n    _item = next(iter(scans_metadata))\n    project,session,subject = _item['session_project'],_item['session_label'],_item['subject_label']\n    session_id,subject_id = _item['session_id'],_item['subject_id']\n    \n    check_dataset_description(out_base)\n    \n    sourcedata_base = os.path.join(\n        out_base,\n        'sourcedata',\n        'sub-{0}'.format(legal.sub('', subject)),\n        'ses-{0}'.format(legal.sub('', session))\n    )\n    bids_base = os.path.join(\n        out_base,\n        'sub-{0}'.format(legal.sub('', subject)),\n        'ses-{0}'.format(legal.sub('', session))\n    )\n    \n    args = commons.struct(\n        xnat=yaxil_session,\n        subject=subject,\n        subject_id=subject_id,\n        session=session,\n        session_id=session_id,\n        project=project,\n        bids=bids_base,\n        sourcedata=sourcedata_base\n    )\n    \n    func_refs = proc_func(config, args)\n    anat_refs = proc_anat(config, args)\n    dwi_refs  = proc_dwi(config, args)\n    fmap_refs = proc_fmap(config, args, func_refs)\n\ndef check_dataset_description(bids_dir, bids_version='1.4.0', ds_type='raw'):\n    if not os.path.exists(bids_dir):\n        os.makedirs(bids_dir)\n    ds_desc = os.path.join(bids_dir, 'dataset_description.json')\n    if not os.path.exists(ds_desc):\n        js = {\n            'Name': 'Made by YAXIL',\n            'BIDSVersion': bids_version,\n            'DatasetType': ds_type\n        }\n        with open(ds_desc, 'w') as fo:\n            fo.write(json.dumps(js))\n    \ndef proc_func(config, args):\n    \n    refs = dict()\n    for scan in iterconfig(config, 'func'):\n        ref = scan.get('id', None)\n        templ = 'sub-${sub}_ses-${ses}'\n        if 'task' in scan:\n            templ += '_task-${task}'\n        if 'acquisition' in scan:\n            templ += '_acq-${acquisition}'\n        if 'run' in scan:\n            templ += '_run-${run}'\n        if 'direction' in scan:\n            templ += '_dir-${direction}'\n        templ += '_${modality}'\n        templ = string.Template(templ)\n        fbase = templ.safe_substitute(\n            sub=legal.sub('', args.subject),\n            ses=legal.sub('', args.session),\n            task=scan.get('task', None),\n            acquisition=scan.get('acquisition', None),\n            run=scan.get('run', None),\n            direction=scan.get('direction', None),\n            modality=scan.get('modality', None)\n        )\n        \n        sourcedata_dir = os.path.join(args.sourcedata, scan['type'])\n        if not os.path.exists(sourcedata_dir):\n            os.makedirs(sourcedata_dir)\n        dicom_dir = os.path.join(sourcedata_dir, '{0}.dicom'.format(fbase))\n        logger.info('downloading session=%s, scan=%s, loc=%s', args.session, scan['scan'], dicom_dir)\n        args.xnat.download(args.session, [scan['scan']], out_dir=dicom_dir)\n        \n        fname = '{0}.nii.gz'.format(fbase)\n        refs[ref] = os.path.join(scan['type'], fname)\n        fullfile = os.path.join(args.bids, scan['type'], fname)\n        logger.info('converting %s to %s', dicom_dir, fullfile)\n        convert(dicom_dir, fullfile)\n        \n        sidecar_file = os.path.join(args.bids, scan['type'], fbase + '.json')\n        with open(sidecar_file) as fo:\n            sidecarjs = json.load(fo)\n        sidecarjs['DataSource'] = {\n            'application/x-xnat': {\n                'url': args.xnat.url,\n                'project': args.project,\n                'subject': args.subject,\n                'subject_id': args.subject_id,\n                'experiment': args.session,\n                'experiment_id': args.session_id,\n                'scan': scan['scan']\n            }\n        }\n        \n        commons.atomic_write(sidecar_file, json.dumps(sidecarjs, indent=2))\n    return refs\n\ndef proc_anat(config, args):\n    \n    refs = dict()\n    for scan in iterconfig(config, 'anat'):\n        ref = scan.get('id', None)\n        templ = 'sub-${sub}_ses-${ses}'\n        if 'acquisition' in scan:\n            templ += '_acq-${acquisition}'\n        if 'run' in scan:\n            templ += '_run-${run}'\n        templ += '_${modality}'\n        templ = string.Template(templ)\n        fbase = templ.safe_substitute(\n            sub=legal.sub('', args.subject),\n            ses=legal.sub('', args.session),\n            acquisition=scan.get('acquisition', None),\n            run=scan.get('run', None),\n            modality=scan.get('modality', None),\n        )\n        \n        sourcedata_dir = os.path.join(args.sourcedata, scan['type'])\n        if not os.path.exists(sourcedata_dir):\n            os.makedirs(sourcedata_dir)\n        dicom_dir = os.path.join(sourcedata_dir, '{0}.dicom'.format(fbase))\n        logger.info('downloading session=%s, scan=%s, loc=%s', args.session, scan['scan'], dicom_dir)\n        args.xnat.download(args.session, [scan['scan']], out_dir=dicom_dir)\n        \n        fname = '{0}.nii.gz'.format(fbase)\n        refs[ref] = os.path.join(scan['type'], fname)\n        fullfile = os.path.join(args.bids, scan['type'], fname)\n        logger.info('converting %s to %s', dicom_dir, fullfile)\n        modality = scan.get('modality', None)\n        sidecar_files = list()\n        if modality == 'T1vnav':\n            fullfile = fullfile.replace('_T1vnav', '_split-%r_T1vnav')\n            for f in glob.glob(os.path.join(dicom_dir, '*.dcm')):\n                logger.debug('converting single file %s to %s', f, fullfile)\n                convert(f, fullfile, single_file=True)\n            ffbase = re.sub('.nii(.gz)?', '', fullfile)\n            expr = ffbase.replace('%r', '*') + '.json'\n            logger.debug('globbing for %s', expr)\n            sidecar_files = glob.glob(expr)\n        else:\n            convert(dicom_dir, fullfile)\n            sidecar_files = [\n                os.path.join(args.bids, scan['type'], fbase + '.json')\n            ]\n        \n        for sidecar_file in sidecar_files:\n            logger.debug('adding provenance to %s', sidecar_file)\n            with open(sidecar_file) as fo:\n                sidecarjs = json.load(fo)\n            sidecarjs['DataSource'] = {\n                'application/x-xnat': {\n                    'url': args.xnat.url,\n                    'project': args.project,\n                    'subject': args.subject,\n                    'subject_id': args.subject_id,\n                    'experiment': args.session,\n                    'experiment_id': args.session_id,\n                    'scan': scan['scan']\n                }\n            }\n            \n            commons.atomic_write(sidecar_file, json.dumps(sidecarjs, indent=2))\n    return refs\n\ndef proc_dwi(config, args):\n    \n    refs = dict()\n    for scan in iterconfig(config, 'dwi'):\n        ref = scan.get('id', None)\n        templ = 'sub-${sub}_ses-${ses}'\n        if 'acquisition' in scan:\n            templ += '_acq-${acquisition}'\n        if 'direction' in scan:\n            templ += '_dir-${direction}'\n        if 'run' in scan:\n            templ += '_run-${run}'\n        templ += '_${modality}'\n        templ = string.Template(templ)\n        fbase = templ.safe_substitute(\n            sub=legal.sub('', args.subject),\n            ses=legal.sub('', args.session),\n            acquisition=scan.get('acquisition', None),\n            direction=scan.get('direction', None),\n            run=scan.get('run', None),\n            modality=scan.get('modality', None)\n        )\n        \n        sourcedata_dir = os.path.join(args.sourcedata, scan['type'])\n        if not os.path.exists(sourcedata_dir):\n            os.makedirs(sourcedata_dir)\n        dicom_dir = os.path.join(sourcedata_dir, '{0}.dicom'.format(fbase))\n        logger.info('downloading session=%s, scan=%s, loc=%s', args.session, scan['scan'], dicom_dir)\n        args.xnat.download(args.session, [scan['scan']], out_dir=dicom_dir)\n        \n        fname = '{0}.nii.gz'.format(fbase)\n        refs[ref] = os.path.join(scan['type'], fname)\n        fullfile = os.path.join(args.bids, scan['type'], fname)\n        logger.info('converting %s to %s', dicom_dir, fullfile)\n        modality = scan.get('modality', None)\n        convert(dicom_dir, fullfile)\n        sidecar_file = os.path.join(args.bids, scan['type'], fbase + '.json')\n        \n        logger.debug('adding provenance to %s', sidecar_file)\n        with open(sidecar_file) as fo:\n            sidecarjs = json.load(fo)\n        sidecarjs['DataSource'] = {\n            'application/x-xnat': {\n                'url': args.xnat.url,\n                'project': args.project,\n                'subject': args.subject,\n                'subject_id': args.subject_id,\n                'experiment': args.session,\n                'experiment_id': args.session_id,\n                'scan': scan['scan']\n            }\n        }\n        \n        commons.atomic_write(sidecar_file, json.dumps(sidecarjs, indent=2))\n    return refs\n\ndef proc_fmap(config, args, func_refs=None):\n    refs = dict()\n    for scan in iterconfig(config, 'fmap'):\n        ref = scan.get('id', None)\n        templ = 'sub-${sub}_ses-${ses}'\n        if 'acquisition' in scan:\n            templ += '_acq-${acquisition}'\n        if 'run' in scan:\n            templ += '_run-${run}'\n        if 'direction' in scan:\n            templ += '_dir-${direction}'\n        templ += '_${modality}'\n        templ = string.Template(templ)\n        fbase = templ.safe_substitute(\n            sub=legal.sub('', args.subject),\n            ses=legal.sub('', args.session),\n            acquisition=scan.get('acquisition', None),\n            run=scan.get('run', None),\n            direction=scan.get('direction', None),\n            modality=scan.get('modality', None),\n        )\n        \n        sourcedata_dir = os.path.join(args.sourcedata, scan['type'])\n        if not os.path.exists(sourcedata_dir):\n            os.makedirs(sourcedata_dir)\n        dicom_dir = os.path.join(sourcedata_dir, '{0}.dicom'.format(fbase))\n        logger.info('downloading session=%s, scan=%s, loc=%s', args.session, scan['scan'], dicom_dir)\n        args.xnat.download(args.session, [scan['scan']], out_dir=dicom_dir)\n        \n        fname = '{0}.nii.gz'.format(fbase)\n        refs[ref] = os.path.join(scan['type'], fname)\n        fullfile = os.path.join(args.bids, scan['type'], fname)\n        logger.info('converting %s to %s', dicom_dir, fullfile)\n        convert(dicom_dir, fullfile)\n        \n        if scan['type'] == 'fmap':\n            if scan.get('modality', None) == 'magnitude':\n                rename_fmapm(args.bids, fbase)\n            elif scan.get('modality', None) == 'phase':\n                rename_fmapp(args.bids, fbase)\n        \n        sidecar_file = os.path.join(args.bids, scan['type'], fbase + '.json')\n        with open(sidecar_file, 'r') as fo:\n            sidecarjs = json.load(fo)\n        sidecarjs['DataSource'] = {\n            'application/x-xnat': {\n                'url': args.xnat.url,\n                'project': args.project,\n                'subject': args.subject,\n                'subject_id': args.subject_id,\n                'experiment': args.session,\n                'experiment_id': args.session_id,\n                'scan': scan['scan']\n            }\n        }\n        \n        if 'intended for' in scan and func_refs:\n            for intended in scan['intended for']:\n                if intended in func_refs:\n                    logger.info('adding IntendedFor %s to %s', func_refs[intended], sidecar_file)\n                    if 'IntendedFor' not in sidecarjs:\n                        sidecarjs['IntendedFor'] = list()\n                    if func_refs[intended] not in sidecarjs['IntendedFor']:\n                        sidecarjs['IntendedFor'].append(func_refs[intended])\n            logger.info('writing file %s', sidecar_file)\n        \n        commons.atomic_write(sidecar_file, json.dumps(sidecarjs, indent=2))\n    return refs\n\ndef iterconfig(config, scan_type):\n    \n    if scan_type in config:\n        for modality,scans in iter(config[scan_type].items()):\n            for scan in scans:\n                scan.update({\n                    'type': scan_type,\n                    'modality': modality\n                })\n                yield scan\n\ndef rename_fmapm(bids_base, basename):\n    \n    files = dict()\n    for ext in ['nii.gz', 'json']:\n        for echo in [1, 2]:\n            fname = '{0}_e{1}.{2}'.format(basename, echo, ext)\n            src = os.path.join(bids_base, 'fmap', fname)\n            if os.path.exists(src):\n                dst = src.replace(\n                    'magnitude_e{0}'.format(echo),\n                    'magnitude{0}'.format(echo)\n                )\n                logger.debug('renaming %s to %s', src, dst)\n                os.rename(src, dst)\n                files[ext] = dst\n    return files\n\ndef rename_fmapp(bids_base, basename):\n    \n    files = dict()\n    for ext in ['nii.gz', 'json']:\n        fname = '{0}_e2_ph.{1}'.format(basename, ext)\n        src = os.path.join(bids_base, 'fmap', fname)\n        if os.path.exists(src):\n            dst = src.replace(\n                'phase_e2_ph',\n                'phase'\n            )\n            logger.debug('renaming %s to %s', src, dst)\n            os.rename(src, dst)\n            files[ext] = dst\n    return files\n\ndef convert(input, output, single_file=False):\n    \n    dirname = os.path.dirname(output)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n    basename = os.path.basename(output)\n    basename = re.sub('.nii(.gz)?', '', basename)\n    dcm2niix = commons.which('dcm2niix')\n    cmd = [\n        'dcm2niix'\n    ]\n    if single_file:\n        cmd.extend([\n            '-s', 'y'\n        ])\n    cmd.extend([\n        '-b', 'y',\n        '-z', 'y',\n        '-f', basename,\n        '-o', dirname,\n        input\n    ])\n    logger.debug(cmd)\n    sp.check_output(cmd)\n",
        "summary": "This code is a Python script that processes medical imaging data from an XNAT server and converts it into the Brain Imaging Data Structure (BIDS) format. The script includes several functions for different types of scans, such as T1-weighted images, diffusion MRI, functional MRI, and field maps.\n\nHere's a breakdown of some key parts of the script:\n\n1. **Iterating through Scan Configurations**: The `iterconfig` function iterates through scan configurations in a given dictionary, updating each scan with its type and modality.\n\n2. **Downloading Scans from XNAT**: The script uses an `args.xnat.download` method to download scans from the XNAT server based on session ID and scan IDs.\n\n3. **Converting DICOM Files to NIfTI Format**: The `convert` function uses the `dcm2niix` tool to convert DICOM files into NIfTI format. It handles both single-file and multi-file inputs, and can be configured to strip patient information from the output files.\n\n4. **Handling Field Maps**: There are specific functions (`rename_fmapm` and `rename_fmapp`) to rename field map files according to BIDS conventions.\n\n5. **Updating Sidecar JSON Files**: The script updates sidecar JSON files with metadata about the scans, including their source on XNAT and any intended use for functional MRI data.\n\n6. **Logging**: Throughout the script, there is extensive logging to provide feedback on what operations are being performed and their status.\n\nThis script is designed to be flexible and can handle various types of medical imaging data, making it a valuable tool for researchers working with neuroimaging data in BIDS format."
    },
    {
        "code": "from .mc import *\n",
        "summary": "The provided Python code imports all functions and classes from a module named `mc`."
    },
    {
        "code": "import os\n\ndef find_base(p, bases):\n    for base_name, base_path in bases.items():\n        r = os.path.relpath(p, base_path)\n        if r and (r == '.' or r[0] != '.'):\n            return base_name, r\n    return None\n\n",
        "summary": "The function `find_base` takes a path `p` and a dictionary of base paths `bases`, and returns the name of the base path that is the closest ancestor to `p`, along with the relative path from that base. If no such base exists, it returns `None`."
    },
    {
        "code": "from lib.krampus_logging import KLog\n\n\nclass Lambda():\n    def __init__(self, func_name, region, sess):\n        try:\n            self.conn = sess.client(\"lambda\", region_name=region)\n        except Exception as e:\n            KLog.log(\"issue connecting to AWS %s\" % str(e), \"critical\")\n            exit(\"[!] issue connecting to AWS: %s\" % str(e))\n        \n        self.func = func_name\n        self.region = region\n        \n        self.sess = sess\n\n    def disable(self):\n        KLog.log(\"no disable action for lambda function '%s', will delete instead\" % self.func, \"warning\")\n        return self.kill()\n\n    def kill(self):\n        try:\n            \n            return self.conn.delete_function(FunctionName=self.func)\n        except Exception as e:\n            if str(e).find(\"ResourceNotFoundException\") is not -1:\n                KLog.log(\"could not find function '%s', dequeueing task\" % self.func)\n            else:\n                KLog.log(\"could not delete function '%s', unknown error: %s\" % str(e), \"critical\")\n            return None\n",
        "summary": "The provided Python code defines a class `Lambda` that interacts with AWS Lambda services. It includes methods to initialize a connection to the Lambda service, disable or delete a specified Lambda function, and handle exceptions related to these operations using logging for critical issues."
    },
    {
        "code": "T = float(input(\"Entre com a temperatura que est\u00e1 agora: \"))\n\nif T >= 26.0 and T <= 36.0:\n\tprint(\"A temperatura est\u00e1 boa\")\nelif T > 36.0:\t\n\tprint(\"A temperatura est\u00e1 quente\\n Tome bastante l\u00edquido\")\nelif T >= 15.0 and T < 26.0:\n\tprint(\"A temperatura est\u00e1 agrad\u00e1vel\")\nelse:\n\tprint(\"A temperatura esta fria\")\n",
        "summary": "The Python code prompts the user to enter the current temperature, then evaluates it against predefined ranges to categorize the weather as \"boa\" (good), \"quente\" (hot) with a recommendation to drink plenty of liquid, \"agrad\u00e1vel\" (pleasant), or \"fria\" (cold)."
    },
    {
        "code": "import numpy as np\nimport scipy.sparse\n\nimport theano\nfrom theano import gof, tensor\nfrom theano.gof.op import Op\nfrom theano.sparse.basic import (\n    Remove0,\n    SparseType,\n    _is_sparse,\n    as_sparse_variable,\n    remove0,\n)\n\n\nfrom theano.tensor import discrete_dtypes, float_dtypes\n\n\n\n\n\n\n\n\n\n\n\n\nEliminateZeros = Remove0\neliminate_zeros = remove0\n\n\n\nclass Poisson(Op):\n    \n\n    __props__ = ()\n\n    def make_node(self, x):\n        x = as_sparse_variable(x)\n        return gof.Apply(self, [x], [x.type()])\n\n    def perform(self, node, inputs, outputs):\n        (x,) = inputs\n        (out,) = outputs\n        assert _is_sparse(x)\n        assert x.format in [\"csr\", \"csc\"]\n        out[0] = x.copy()\n        out[0].data = np.asarray(np.random.poisson(out[0].data), dtype=x.dtype)\n        out[0].eliminate_zeros()\n\n    def grad(self, inputs, outputs_gradients):\n        comment = \"No gradient exists for class Poisson in\\\n                   theano/sparse/sandbox/sp2.py\"\n        return [\n            theano.gradient.grad_undefined(\n                op=self, x_pos=0, x=inputs[0], comment=comment\n            )\n        ]\n\n    def infer_shape(self, fgraph, node, ins_shapes):\n        return ins_shapes\n\n\npoisson = Poisson()\n\n\nclass Binomial(Op):\n    \n\n    __props__ = (\"format\", \"dtype\")\n\n    def __init__(self, format, dtype):\n        self.format = format\n        self.dtype = dtype\n\n    def make_node(self, n, p, shape):\n        n = tensor.as_tensor_variable(n)\n        p = tensor.as_tensor_variable(p)\n        shape = tensor.as_tensor_variable(shape)\n\n        assert n.dtype in discrete_dtypes\n        assert p.dtype in float_dtypes\n        assert shape.dtype in discrete_dtypes\n\n        return gof.Apply(\n            self, [n, p, shape], [SparseType(dtype=self.dtype, format=self.format)()]\n        )\n\n    def perform(self, node, inputs, outputs):\n        (n, p, shape) = inputs\n        (out,) = outputs\n        binomial = np.random.binomial(n, p, size=shape)\n        csx_matrix = getattr(scipy.sparse, self.format + \"_matrix\")\n        out[0] = csx_matrix(binomial, dtype=self.dtype)\n\n    def connection_pattern(self, node):\n        return [[True], [True], [False]]\n\n    def grad(self, inputs, gout):\n        (n, p, shape) = inputs\n        (gz,) = gout\n        comment_n = \"No gradient exists for the number of samples in class\\\n                     Binomial of theano/sparse/sandbox/sp2.py\"\n        comment_p = \"No gradient exists for the prob of success in class\\\n                     Binomial of theano/sparse/sandbox/sp2.py\"\n        return [\n            theano.gradient.grad_undefined(op=self, x_pos=0, x=n, comment=comment_n),\n            theano.gradient.grad_undefined(op=self, x_pos=1, x=p, comment=comment_p),\n            theano.gradient.disconnected_type(),\n        ]\n\n    def infer_shape(self, fgraph, node, ins_shapes):\n        return [(node.inputs[2][0], node.inputs[2][1])]\n\n\ncsr_fbinomial = Binomial(\"csr\", \"float32\")\ncsc_fbinomial = Binomial(\"csc\", \"float32\")\ncsr_dbinomial = Binomial(\"csr\", \"float64\")\ncsc_dbinomial = Binomial(\"csc\", \"float64\")\n\n\nclass Multinomial(Op):\n    \n\n    __props__ = ()\n\n    def make_node(self, n, p):\n        n = tensor.as_tensor_variable(n)\n        p = as_sparse_variable(p)\n        assert p.format in [\"csr\", \"csc\"]\n\n        return gof.Apply(self, [n, p], [p.type()])\n\n    def perform(self, node, inputs, outputs):\n        (n, p) = inputs\n        (out,) = outputs\n        assert _is_sparse(p)\n\n        if p.format != \"csr\":\n            raise NotImplementedError\n\n        out[0] = p.copy()\n\n        if n.ndim == 0:\n            for i in range(p.shape[0]):\n                k, l = p.indptr[i], p.indptr[i + 1]\n                out[0].data[k:l] = np.random.multinomial(n, p.data[k:l])\n        elif n.ndim == 1:\n            if n.shape[0] != p.shape[0]:\n                raise ValueError(\n                    \"The number of element of n must be \"\n                    \"the same as the number of row of p.\"\n                )\n            for i in range(p.shape[0]):\n                k, l = p.indptr[i], p.indptr[i + 1]\n                out[0].data[k:l] = np.random.multinomial(n[i], p.data[k:l])\n\n    def grad(self, inputs, outputs_gradients):\n        comment_n = \"No gradient exists for the number of samples in class\\\n                     Multinomial of theano/sparse/sandbox/sp2.py\"\n        comment_p = \"No gradient exists for the prob of success in class\\\n                     Multinomial of theano/sparse/sandbox/sp2.py\"\n        return [\n            theano.gradient.grad_undefined(\n                op=self, x_pos=0, x=inputs[0], comment=comment_n\n            ),\n            theano.gradient.grad_undefined(\n                op=self, x_pos=1, x=inputs[1], comment=comment_p\n            ),\n        ]\n\n    def infer_shape(self, fgraph, node, ins_shapes):\n        return [ins_shapes[1]]\n\n\nmultinomial = Multinomial()\n",
        "summary": "The provided Python code defines several custom operations for sparse matrices using Theano, including `Poisson`, `Binomial`, and `Multinomial`. These operations perform specific statistical sampling on sparse data structures, such as generating Poisson-distributed random numbers, binomially distributed random numbers in various formats (CSR and CSC), and multinomially distributed random numbers. Each operation includes methods for node creation, performance execution, gradient computation, and shape inference, tailored to handle sparse matrix data efficiently."
    },
    {
        "code": "import os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'getDoc.settings.production')\n\napplication = get_wsgi_application()\n",
        "summary": "The Python script sets the environment variable for Django settings to use the production configuration and then imports and initializes the WSGI application."
    },
    {
        "code": "import deck\n\nwhile(True):\n    p = deck.Deck()\n    p.shuffle()\n    pai = p.deal(5)\n    \n    del p\n    pai.sort(key=lambda x:x.figure)\n    x = True\n    for i in range(1, len(pai)):\n        if(pai[i].suit == pai[i-1].suit and (pai[i].figure == pai[i-1].figure + 1 or pai[i].figure == 10 and pai[i-1].figure == 1)):\n            continue\n        else:\n            x = False\n            break\n    if(x == True):\n        for i in pai:\n            print(i,end=\"\\t\")\n        print()\n\n",
        "summary": "The Python code continuously generates a shuffled deck of cards, deals five cards, and checks if they form a sequence (either consecutive numbers or a straight flush with an Ace). If the condition is met, it prints the sequence of cards."
    },
    {
        "code": "from pyrsistent import PClass, field\n\nfrom dask import compute, optimize\nfrom dask.core import toposort, get_dependencies\nfrom . import start_action, current_action, Action, Message\n\n\nclass _RunWithEliotContext(PClass):\n    \n    task_id = field(type=str)\n    func = field()  \n    key = field(type=str)\n    dependencies = field()\n\n    \n    \n\n    def __eq__(self, other):\n        return self.func == other\n\n    def __ne__(self, other):\n        return self.func != other\n\n    def __hash__(self):\n        return hash(self.func)\n\n    def __call__(self, *args, **kwargs):\n        with Action.continue_task(task_id=self.task_id):\n            Message.log(\n                message_type=\"dask:task\",\n                key=self.key,\n                dependencies=self.dependencies\n            )\n            return self.func(*args, **kwargs)\n\n\ndef compute_with_trace(*args):\n    \n    \n    with start_action(action_type=\"dask:compute\"):\n        \n        \n        optimized = optimize(*args, optimizations=[_add_logging])\n        return compute(*optimized, optimize_graph=False)\n\n\ndef _add_logging(dsk, ignore=None):\n    \n    ctx = current_action()\n    result = {}\n\n    \n    \n    keys = toposort(dsk)\n\n    \n    \n    \n    def simplify(k):\n        if isinstance(k, str):\n            return k\n        return \"-\".join(str(o) for o in k)\n\n    key_names = {}\n    for key in keys:\n        value = dsk[key]\n        if not callable(value) and value in keys:\n            \n            key_names[key] = key_names[value]\n        else:\n            key_names[key] = simplify(key)\n\n    \n    key_to_action_id = {\n        key: str(ctx.serialize_task_id(), \"utf-8\")\n        for key in keys\n    }\n\n    \n    for key in keys:\n        func = dsk[key][0]\n        args = dsk[key][1:]\n        if not callable(func):\n            \n            \n            result[key] = dsk[key]\n            continue\n        wrapped_func = _RunWithEliotContext(\n            task_id=key_to_action_id[key],\n            func=func,\n            key=key_names[key],\n            dependencies=[key_names[k] for k in get_dependencies(dsk, key)],\n        )\n        result[key] = (wrapped_func, ) + tuple(args)\n\n    assert result.keys() == dsk.keys()\n    return result\n\n\n__all__ = [\"compute_with_trace\"]\n",
        "summary": "The provided Python code defines a custom class `_RunWithEliotContext` that wraps functions to add logging and tracking using Eliot context. It also includes a function `compute_with_trace` that optimizes and computes Dask tasks while adding logging information, utilizing the `_add_logging` function to wrap task functions with additional metadata for tracing purposes."
    },
    {
        "code": "import copy\n\n\nclass Field(object):\n    \n\n    field_type = None\n    \n\n    default = None\n    \n\n    def __init__(self, field_type, default=None):\n\n        self.field_type = field_type\n        self.default = default\n        self.match(default)\n\n    def match(self, value):\n        \n        if value is not None and not isinstance(value, self.field_type):\n            raise TypeError('expect %s, not %s' %\n                            (self.field_type, type(value)))\n\n\nclass EventMetaClass(type):\n    \n\n    def __new__(cls, name, bases, attrs):\n\n        fields = []\n        mappings = {}\n        params = {}\n        new_attrs = {}\n        for k, v in attrs.items():\n            if isinstance(v, Field):\n                fields.append(v)\n                mappings[k] = v\n                params[k] = v.default\n            else:\n                new_attrs[k] = v\n        new_attrs['__fields__'] = fields\n        new_attrs['__mappings__'] = mappings\n        new_attrs['__params__'] = params\n        new_attrs['__actual_params__'] = None\n        new_attrs['__tag__'] = attrs['__tag__'] \\\n            if '__tag__' in attrs else ''\n        new_attrs['__description__'] = attrs['__description__'] \\\n            if '__description__' in attrs else ''\n        return super(EventMetaClass, cls).__new__(cls, name, bases, new_attrs)\n\n\nclass Event(object):\n    \n\n    __metaclass__ = EventMetaClass\n\n    def __init__(self, **kwargs):\n        self.__actual_params__ = copy.deepcopy(self.__params__)\n        for k, v in kwargs.iteritems():\n            self.__setattr__(k, v)\n\n    def __getattr__(self, key):\n        if key in self.__actual_params__:\n            return self.__actual_params__[key]\n        else:\n            raise AttributeError(\n                \"%s has no param `%s`\" %\n                (type(self), key))\n\n    def __setattr__(self, key, value):\n        if key in ['__actual_params__']:\n            return super(Event, self).__setattr__(key, value)\n        if key in self.__actual_params__:\n            self.__mappings__[key].match(value)\n            self.__actual_params__[key] = value\n        else:\n            raise AttributeError(\n                \"%s has no param `%s`\" %\n                (type(self), key))\n\n    @property\n    def params(self):\n        \n        return copy.deepcopy(self.__actual_params__)\n\n    @classmethod\n    def tag(cls):\n        \n        return cls.__tag__\n\n    @classmethod\n    def description(cls):\n        \n        return cls.__description__\n\n    @classmethod\n    def key(cls):\n        \n        return '%s.%s' % (cls.__module__, cls.__name__)\n\n    @property\n    def no_field(self):\n        \n        return len(self.__params__) == 0\n",
        "summary": "The provided Python code defines a framework for creating event classes with type-checked fields. It uses metaclasses to automatically extract field definitions and their default values from the class attributes, ensuring that instances are initialized with valid data types. The `Event` class allows setting and getting parameters while validating them against predefined types, providing a robust mechanism for handling events with structured data."
    },
    {
        "code": "import torch.utils.data as data\n\n\nclass CombineDBs(data.Dataset):\n    def __init__(self, dataloaders, excluded=None):\n        self.dataloaders = dataloaders\n        self.excluded = excluded\n        self.im_ids = []\n\n        \n        for dl in dataloaders:\n            for elem in dl.im_ids:\n                if elem not in self.im_ids:\n                    self.im_ids.append(elem)\n\n        \n        if excluded:\n            for dl in excluded:\n                for elem in dl.im_ids:\n                    if elem in self.im_ids:\n                        self.im_ids.remove(elem)\n\n        \n        self.obj_list = []\n        self.im_list = []\n        new_im_ids = []\n        obj_counter = 0\n        num_images = 0\n        for ii, dl in enumerate(dataloaders):\n            for jj, curr_im_id in enumerate(dl.im_ids):\n                if (curr_im_id in self.im_ids) and (curr_im_id not in new_im_ids):\n                    flag = False\n                    new_im_ids.append(curr_im_id)\n                    for kk in range(len(dl.obj_dict[curr_im_id])):\n                        if dl.obj_dict[curr_im_id][kk] != -1:\n                            self.obj_list.append({'db_ii': ii, 'obj_ii': dl.obj_list.index([jj, kk])})\n                            flag = True\n                        obj_counter += 1\n                    self.im_list.append({'db_ii': ii, 'im_ii': jj})\n                    if flag:\n                        num_images += 1\n\n        self.im_ids = new_im_ids\n        print('Combined number of images: {:d}\\nCombined number of objects: {:d}'.format(num_images, len(self.obj_list)))\n\n    def __getitem__(self, index):\n\n        _db_ii = self.obj_list[index][\"db_ii\"]\n        _obj_ii = self.obj_list[index]['obj_ii']\n        sample = self.dataloaders[_db_ii].__getitem__(_obj_ii)\n\n        if 'meta' in sample.keys():\n            sample['meta']['db'] = str(self.dataloaders[_db_ii])\n\n        return sample\n\n    def __len__(self):\n        return len(self.obj_list)\n\n    def __str__(self):\n        include_db = [str(db) for db in self.dataloaders]\n        exclude_db = [str(db) for db in self.excluded]\n        return 'Included datasets:'+str(include_db)+'\\n'+'Excluded datasets:'+str(exclude_db)\n\n\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n    from dataloaders import pascal\n    from dataloaders import sbd\n    import torch\n    import numpy as np\n    import dataset.custom_transforms as tr\n    from torchvision import transforms\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={'image': (450, 450), 'gt': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem='gt'),\n        tr.ConcatInputs(elems=('image', 'distance_map')),\n        tr.ToTensor()])\n\n    composed_transforms_ts = transforms.Compose([\n        tr.FixedResize(resolutions={'image': (450, 450), 'gt': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem='gt'),\n        tr.ConcatInputs(elems=('image', 'distance_map')),\n        tr.ToTensor()])\n\n    pascal_voc_val = pascal.VOCSegmentation(split='val', transform=composed_transforms_ts, retname=True)\n    sbd = sbd.SBDSegmentation(split=['train', 'val'], transform=composed_transforms_tr, retname=True)\n    pascal_voc_train = pascal.VOCSegmentation(split='train', transform=composed_transforms_tr, retname=True)\n\n    dataset = CombineDBs([pascal_voc_train, sbd], excluded=[pascal_voc_val])\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[\"image\"].size()[0]):\n            dismap = sample['distance_map'][jj].numpy()\n            gt = sample['gt'][jj].numpy()\n            gt[gt > 0] = 255\n            gt = np.array(gt[0]).astype(np.uint8)\n            dismap = np.array(dismap[0]).astype(np.uint8)\n            display = 0.9 * gt + 0.4 * dismap\n            display = display.astype(np.uint8)\n            plt.figure()\n            plt.title('display')\n            plt.imshow(display, cmap='gray')\n\n        if ii == 1:\n            break\n    plt.show(block=True)",
        "summary": "The provided Python code defines a custom `CombineDBs` class that combines multiple datasets into one, excluding certain datasets if specified. It then demonstrates how to use this class with the Pascal VOC and SBD datasets, applying transformations and creating a DataLoader for batch processing. The script also includes visualization of combined dataset samples."
    },
    {
        "code": "import os\nimport time\nfrom slackclient import SlackClient\nimport requests\nimport json\n\n\nBOT_ID = \"<YOUR_BOT_ID>\"\n\n\nAT_BOT = \"<@\" + BOT_ID + \">\"\nMAKE_TEA_COMMAND = \"make tea\"\nSTOP_BOILING_COMMAND = \"stop boiling\"\n\n\nslack_client = SlackClient('<YOUR_SLACK_API_TOKEN>')\nheaders = {'content-type': 'application/json', 'Authorization': '<YOUR_RELAYR_TOKEN>', 'Cache-Control':'no-cache'}\n\n\n\t\t\ndef handle_command(command, channel):\n    \n    response = \"Not sure what you mean. Use the *\" + MAKE_TEA_COMMAND + \\\n               \"* command with numbers, delimited by spaces.\"\n    if command.startswith(MAKE_TEA_COMMAND):\n\t\tdata = {'meaning': 'kettle', 'value': 'true'}\n\t\tr = requests.post('https://api.relayr.io/devices/<KETTLE_DEVICE_ID>/data', data=json.dumps(data), headers=headers)\n\t\tresponse = \"Sure... Your water is boiling now!\"\n    if command.startswith(STOP_BOILING_COMMAND):\n\t\tdata = {'meaning': 'kettle', 'value': 'false'}\n\t\tr = requests.post('https://api.relayr.io/devices/<KETTLE_DEVICE_ID>/data', data=json.dumps(data), headers=headers)\n\t\tresponse = \"OK - I stopped the kettle!\"\n    if command.startswith(\"is the kettle boiling?\"):\n\t\tr = requests.get('https://api.relayr.io/devices/<KETTLE_DEVICE_ID>/readings', headers=headers)\n\t\tresp = json.loads(r.text)\n\t\ttry:\n\t\t\tif resp['readings'][0]['value'] == \"true\":\n\t\t\t\tresponse = \"Yes, the kettle is currently boiling.\"\n\t\t\tif resp['readings'][0]['value'] == \"false\":\n\t\t\t\tresponse = \"No, the kettle is currently off.\"\n\t\texcept:\n\t\t\tresponse = \"Unfortunately.. I don't know :(\"\n\t\n\t\n\t\n    \n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\n    slack_client.api_call(\"chat.postMessage\", channel=channel,\n                          text=response, as_user=True)\n\n\ndef parse_slack_output(slack_rtm_output):\n    \n    output_list = slack_rtm_output\n    if output_list and len(output_list) > 0:\n        for output in output_list:\n            if output and 'text' in output and AT_BOT in output['text']:\n                \n                return output['text'].split(AT_BOT)[1].strip().lower(), \\\n                       output['channel']\n    return None, None\n\t\nif __name__ == \"__main__\":\n    READ_WEBSOCKET_DELAY = 1 \n    if slack_client.rtm_connect():\n        print(\"StarterBot connected and running!\")\n        while True:\n            command, channel = parse_slack_output(slack_client.rtm_read())\n            if command and channel:\n                handle_command(command, channel)\n            time.sleep(READ_WEBSOCKET_DELAY)\n    else:\n        print(\"Connection failed. Invalid Slack token or bot ID?\")",
        "summary": "This Python script integrates with Slack to control a kettle device via Relayr API, allowing users to make tea, stop boiling, and check if the kettle is currently boiling by sending specific commands in chat."
    },
    {
        "code": "try:\n    import vim\nexcept ImportError:\n    raise ImportError(\n        '\"vim\" is not available. This module require to be loaded from Vim.'\n    )\n\n\n\n\n\n\n\ndef _vim_vital_web_api_github_main():\n    \n    import re\n    import sys\n    import ssl\n    import collections\n    from itertools import chain\n    from threading import Lock, Thread\n    try:\n        import json\n    except ImportError:\n        import simplejson as json\n    try:\n        from urllib.request import urlopen, Request\n        from urllib.parse import (urlparse, parse_qs, urlencode, urlunparse)\n    except ImportError:\n        from urllib2 import urlopen, Request\n        from urllib import urlencode\n        from urlparse import (urlparse, parse_qs, urlunparse)\n\n    DEFAULT_INDICATOR = (\n        'Requesting entries and converting into '\n        'JSON %%(page)d/%(page_count)d ...'\n    )\n\n    def format_exception():\n        exc_type, exc_obj, tb = sys.exc_info()\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        filename = f.f_code.co_filename\n        return \"%s: %s at %s:%d\" % (\n            exc_obj.__class__.__name__,\n            exc_obj, filename, lineno,\n        )\n\n    def to_vim(obj):\n        if obj is None:\n            return ''\n        elif isinstance(obj, bool):\n            return int(obj)\n        elif isinstance(obj, dict):\n            return dict([to_vim(k), to_vim(v)] for k, v in obj.items())\n        elif isinstance(obj, (list, tuple)):\n            return list(to_vim(v) for v in obj)\n        return obj\n\n    def build_headers(token):\n        return {'Authorization': 'token %s' % token} if token else {}\n\n    def build_url(url, **kwargs):\n        scheme, netloc, path, params, query, fragment = urlparse(url)\n        p = parse_qs(query)\n        p.update(kwargs)\n        return urlunparse([\n            scheme, netloc, path, params,\n            urlencode(p, doseq=True), fragment\n        ])\n\n    def request(url, headers={}, method=None):\n        if method:\n            if sys.version_info.major >= 3:\n                req = Request(url, headers=headers, method=method)\n            else:\n                req = Request(url, headers=headers)\n                req.get_method = lambda: method\n        else:\n            req = Request(url, headers=headers)\n        context = ssl._create_unverified_context()\n        res = urlopen(req, context=context)\n        if not hasattr(res, 'getheader'):\n            \n            res.getheader = lambda name, self=res: self.info().getheader(name)\n        return res\n\n    def request_head(url, name, headers={}):\n        res = request(url, headers=headers, method='HEAD')\n        return res.getheader(name)\n\n    def request_json(url, headers={}, **kwargs):\n        url = build_url(url, **kwargs)\n        res = request(url, headers=headers)\n        obj = json.loads(res.read().decode('utf-8'))\n        return to_vim(obj)\n\n    def _request_entries(lock, queue, entries_per_pages, url,\n                         headers, callback=None):\n        try:\n            while True:\n                page, indicator = queue.popleft()\n                entries = request_json(url, headers=headers, page=page)\n                entries_per_pages.append([page, entries])\n                if callback:\n                    message = indicator % {'page': len(entries_per_pages)}\n                    if hasattr(vim, 'async_call'):\n                        with lock:\n                            vim.async_call(callback, message)\n                    else:\n                        with lock:\n                            callback(message)\n        except IndexError:\n            pass\n        except Exception as e:\n            \n            queue.clear()\n            entries_per_pages.append(e)\n\n    def request_entries(url, token,\n                        indicator=DEFAULT_INDICATOR,\n                        page_start=1, page_end=0,\n                        nprocess=20, callback=None, **kwargs):\n        \n        page_start = int(page_start)\n        page_end = int(page_end)\n        nprocess = int(nprocess)\n\n        url = build_url(url, **kwargs)\n        headers = build_headers(token)\n        lock = Lock()\n        queue = collections.deque()\n        entries_per_pages = collections.deque()\n        \n        if page_end == 0:\n            if callback:\n                callback('Requesting the total number of pages ...')\n            response_link = request_head(url, 'link', headers=headers)\n            if response_link:\n                m = re.search(\n                    '<.*?[?&]page=(\\d+)[^>]*>; rel=\"last\"', response_link\n                )\n                page_end = int(m.group(1)) if m else 1\n            else:\n                page_end = 1\n        \n        for page in range(page_start, page_end + 1):\n            queue.append([page, indicator % {\n                'url': url,\n                'page_count': page_end - page_start + 1\n            }])\n        \n        kwargs = dict(\n            target=_request_entries,\n            args=(lock, queue, entries_per_pages, url, headers, callback),\n        )\n        workers = [Thread(**kwargs) for n in range(nprocess)]\n        for worker in workers:\n            worker.start()\n        for worker in workers:\n            worker.join()\n        \n        exceptions = list(\n            filter(lambda x: not isinstance(x, list), entries_per_pages)\n        )\n        if len(exceptions):\n            raise exceptions[0]\n        \n        return list(chain.from_iterable(map(\n            lambda x: x[1], sorted(entries_per_pages, key=lambda x: x[0])\n        )))\n\n    def echo_status_vim(indicator):\n        vim.command('redraw | echo \"%s\"' % indicator)\n\n    if sys.version_info < (3, 0, 0):\n        def ensure_unicode(s, encoding):\n            if isinstance(s, unicode):\n                return s\n            else:\n                return s.decode(encoding)\n    else:\n        def ensure_unicode(s, encoding):\n            if not isinstance(s, bytes):\n                return s\n            else:\n                return s.decode(encoding)\n\n\n    \n    namespace = {}\n    try:\n        \n        try:\n            request = _vim_vital_web_api_github_test_pseudo_request\n        except NameError:\n            pass\n        encoding = vim.eval('&encoding')\n        kwargs = vim.eval('kwargs')\n        kwargs = { ensure_unicode(k, encoding): ensure_unicode(v, encoding)\n                   for k, v in kwargs.items()}\n        if kwargs.pop('verbose', 1):\n            kwargs['callback'] = echo_status_vim\n        entries = request_entries(**kwargs)\n        namespace['entries'] = entries\n    except:\n        namespace['exception'] = format_exception()\n\n    return namespace\n\n\n_vim_vital_web_api_github_response = _vim_vital_web_api_github_main()\n",
        "summary": "The provided Python code defines a module for interacting with the GitHub API from within Vim, handling requests, parsing JSON responses, and managing threading to efficiently fetch multiple pages of data. It includes functions for building URLs, making HTTP requests, processing exceptions, and updating the Vim interface during operations."
    },
    {
        "code": "import sys\nimport socket\nfrom datetime import datetime\n\n\nif len (sys.argv) == 2:\n\ttarget=socket.gethostbyname(sys.argv[1]) \nelse:\n\tprint(\"invalid amount of arguments.\")\n\tprint(\"Syntax: python3 scanner.py <ip>\")\n\n\n\nprint(\"-\" * 50)\nprint(\"Scanning target \" + target)\nprint(\"Time started: \" +str(datetime.now()))\nprint(\"-\"*50)\n\ntry:\n\tfor port in range(50,85):\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tsocket.setdefaulttimeout(1)\n\t\tresult = s.connect_ex((target,port))\n\t\tif result ==0:\n\t\t\tprint(\"Port {} is open\".format(port))\n\t\ts.close()\n\nexcept KeyboardInterrupt:\n\tprint(\"\\n Exiting program...\")\n\tsys.exit()\n\nexcept socket.gaierror:\n\tprint(\"\\n Hostname could not be resolved...\")\n\tsys.exit()\n\nexcept socket.gaierror:\n\tprint(\"\\n Could not connect to server...\")\n\tsys.exit()\n\n",
        "summary": "This Python script is a simple port scanner that checks for open ports on a specified target IP address between 50 and 84. It prints the status of each port as \"open\" or \"closed\" and includes timestamps for when the scanning begins and ends. The script handles exceptions such as invalid arguments, hostname resolution issues, and connection errors gracefully."
    },
    {
        "code": "import os, shutil, os.path, re, traceback\r\nimport wx\r\n\r\nfrom . import SystemInfo\r\nfrom .StringOps import mbcsEnc, urlQuote, pathnameFromUrl, pathEnc\r\n\r\n\r\n\n\r\ntry:\r\n    import WindowsHacks\r\nexcept:\r\n    if SystemInfo.isWindows():\r\n        traceback.print_exc()\r\n    WindowsHacks = None\r\n\r\ntry:\r\n    import GtkHacks\r\nexcept:\r\n    import ExceptionLogger\r\n    ExceptionLogger.logOptionalComponentException(\r\n            \"Initialize GTK hacks in OsAbstract.py\")\r\n    GtkHacks = None\r\n\r\n\r\n\nif SystemInfo.isWindows():\r\n    if SystemInfo.isWinNT() and SystemInfo.isUnicode() and WindowsHacks:\r\n        startFile = WindowsHacks.startFile\r\n    else:\r\n        def startFile(mainControl, link):\r\n            os.startfile(mbcsEnc(link, \"replace\")[0])\r\nelse:\r\n    def startFile(mainControl, link):\r\n        \n        \r\n        startPath = mainControl.getConfig().get(\"main\", \"fileLauncher_path\", u\"\")\r\n        if startPath == u\"\":\r\n            wx.LaunchDefaultBrowser(link)\r\n            return\r\n\r\n        if link.startswith(\"file:\"):\r\n            link = pathnameFromUrl(link)\r\n\r\n        os.spawnlp(os.P_NOWAIT, startPath, startPath, link)\r\n\r\n\r\n\nif SystemInfo.isWinNT() and WindowsHacks:\r\n    copyFile = WindowsHacks.copyFile\r\n    moveFile = WindowsHacks.moveFile\r\n    deleteFile = WindowsHacks.deleteFile\r\nelse:\r\n    \n    def copyFile(srcPath, dstPath):\r\n        \r\n        dstDir = os.path.dirname(dstPath)\r\n            \r\n        if not os.path.exists(pathEnc(dstDir)):\r\n            os.makedirs(dstDir)\r\n    \r\n        shutil.copy2(srcPath, dstPath)\r\n\r\n    def moveFile(srcPath, dstPath):\r\n        \r\n        dstDir = os.path.dirname(dstPath)        \r\n    \r\n        if not os.path.exists(pathEnc(dstDir)):\r\n            os.makedirs(dstDir)\r\n    \r\n        shutil.move(srcPath, dstPath)\r\n\r\n\r\n    def deleteFile(path):\r\n        \r\n        \n        \n        if os.path.isfile(path) or os.path.islink(path):\r\n            os.unlink(path)\r\n        elif os.path.isdir(path):\r\n            os.rmdir(path)\r\n\r\n\r\n\nif SystemInfo.isWindows():\r\n    if WindowsHacks:\r\n        def samefile(path1, path2):\r\n            \n            if WindowsHacks.getLongPath(path1).lower() == \\\r\n                    WindowsHacks.getLongPath(path2).lower():\r\n                return True\r\n            \r\n            return WindowsHacks.getLongPath(os.path.abspath(path1)).lower() == \\\r\n                    WindowsHacks.getLongPath(os.path.abspath(path2)).lower()\r\n    else:\r\n        def samefile(path1, path2):\r\n            return os.path.abspath(path1) == os.path.abspath(path2)\r\nelse:\r\n    samefile = os.path.samefile\r\n\r\n\r\nif WindowsHacks:\r\n    def normalizePath(path):\r\n        return WindowsHacks.getLongPath(os.path.abspath(path)).lower()\r\nelse:\r\n    def normalizePath(path):\r\n        return os.path.normcase(os.path.abspath(path))\r\n\r\n\r\n\r\n\n\n\n\n\r\nif WindowsHacks:\r\n    checkForOtherInstances = WindowsHacks.checkForOtherInstances\r\nelse:\r\n    def checkForOtherInstances():\r\n        return []\r\n\r\n\r\n\r\n\n\n\r\n\ndef supportsClipboardInterceptor():\r\n    return False\r\ndef createInterceptCollection(interceptors=None):\r\n    return None\r\ndef createClipboardInterceptor(callingWindow):\r\n    return None\r\n\r\nif SystemInfo.isWindows():\r\n    if WindowsHacks:\r\n        def supportsClipboardInterceptor():\r\n            return True\r\n        def createInterceptCollection(interceptors=None):\r\n            return WindowsHacks.WinProcInterceptCollection(interceptors)\r\n        def createClipboardInterceptor(callingWindow):\r\n            return WindowsHacks.ClipboardCatchIceptor(callingWindow)\r\nelse:\r\n    if GtkHacks:\r\n        def supportsClipboardInterceptor():\r\n            return True\r\n        def createInterceptCollection(interceptors=None):\r\n            return GtkHacks.FakeInterceptCollection(interceptors)\r\n        def createClipboardInterceptor(callingWindow):\r\n            return GtkHacks.ClipboardCatchFakeIceptor(callingWindow)\r\n        \r\n\r\nif WindowsHacks:\r\n    translateAcceleratorByKbLayout = WindowsHacks.translateAcceleratorByKbLayout\r\nelse:\r\n    def translateAcceleratorByKbLayout(accStr):\r\n        return accStr\r\n",
        "summary": "The Python code provides a set of functions for handling file operations, clipboard interceptors, and system-specific hacks across different operating systems, primarily focusing on Windows and GTK environments. It includes utilities for starting files, copying, moving, and deleting files, checking for other instances, and translating accelerators based on keyboard layouts."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union, overload\nfrom .. import _utilities\n\n__all__ = ['PolicyArgs', 'Policy']\n\n@pulumi.input_type\nclass PolicyArgs:\n    def __init__(__self__, *,\n                 api_management_id: pulumi.Input[str],\n                 xml_content: Optional[pulumi.Input[str]] = None,\n                 xml_link: Optional[pulumi.Input[str]] = None):\n        \n        pulumi.set(__self__, \"api_management_id\", api_management_id)\n        if xml_content is not None:\n            pulumi.set(__self__, \"xml_content\", xml_content)\n        if xml_link is not None:\n            pulumi.set(__self__, \"xml_link\", xml_link)\n\n    @property\n    @pulumi.getter(name=\"apiManagementId\")\n    def api_management_id(self) -> pulumi.Input[str]:\n        \n        return pulumi.get(self, \"api_management_id\")\n\n    @api_management_id.setter\n    def api_management_id(self, value: pulumi.Input[str]):\n        pulumi.set(self, \"api_management_id\", value)\n\n    @property\n    @pulumi.getter(name=\"xmlContent\")\n    def xml_content(self) -> Optional[pulumi.Input[str]]:\n        \n        return pulumi.get(self, \"xml_content\")\n\n    @xml_content.setter\n    def xml_content(self, value: Optional[pulumi.Input[str]]):\n        pulumi.set(self, \"xml_content\", value)\n\n    @property\n    @pulumi.getter(name=\"xmlLink\")\n    def xml_link(self) -> Optional[pulumi.Input[str]]:\n        \n        return pulumi.get(self, \"xml_link\")\n\n    @xml_link.setter\n    def xml_link(self, value: Optional[pulumi.Input[str]]):\n        pulumi.set(self, \"xml_link\", value)\n\n\n@pulumi.input_type\nclass _PolicyState:\n    def __init__(__self__, *,\n                 api_management_id: Optional[pulumi.Input[str]] = None,\n                 xml_content: Optional[pulumi.Input[str]] = None,\n                 xml_link: Optional[pulumi.Input[str]] = None):\n        \n        if api_management_id is not None:\n            pulumi.set(__self__, \"api_management_id\", api_management_id)\n        if xml_content is not None:\n            pulumi.set(__self__, \"xml_content\", xml_content)\n        if xml_link is not None:\n            pulumi.set(__self__, \"xml_link\", xml_link)\n\n    @property\n    @pulumi.getter(name=\"apiManagementId\")\n    def api_management_id(self) -> Optional[pulumi.Input[str]]:\n        \n        return pulumi.get(self, \"api_management_id\")\n\n    @api_management_id.setter\n    def api_management_id(self, value: Optional[pulumi.Input[str]]):\n        pulumi.set(self, \"api_management_id\", value)\n\n    @property\n    @pulumi.getter(name=\"xmlContent\")\n    def xml_content(self) -> Optional[pulumi.Input[str]]:\n        \n        return pulumi.get(self, \"xml_content\")\n\n    @xml_content.setter\n    def xml_content(self, value: Optional[pulumi.Input[str]]):\n        pulumi.set(self, \"xml_content\", value)\n\n    @property\n    @pulumi.getter(name=\"xmlLink\")\n    def xml_link(self) -> Optional[pulumi.Input[str]]:\n        \n        return pulumi.get(self, \"xml_link\")\n\n    @xml_link.setter\n    def xml_link(self, value: Optional[pulumi.Input[str]]):\n        pulumi.set(self, \"xml_link\", value)\n\n\nclass Policy(pulumi.CustomResource):\n    @overload\n    def __init__(__self__,\n                 resource_name: str,\n                 opts: Optional[pulumi.ResourceOptions] = None,\n                 api_management_id: Optional[pulumi.Input[str]] = None,\n                 xml_content: Optional[pulumi.Input[str]] = None,\n                 xml_link: Optional[pulumi.Input[str]] = None,\n                 __props__=None):\n        \n        ...\n    @overload\n    def __init__(__self__,\n                 resource_name: str,\n                 args: PolicyArgs,\n                 opts: Optional[pulumi.ResourceOptions] = None):\n        \n        ...\n    def __init__(__self__, resource_name: str, *args, **kwargs):\n        resource_args, opts = _utilities.get_resource_args_opts(PolicyArgs, pulumi.ResourceOptions, *args, **kwargs)\n        if resource_args is not None:\n            __self__._internal_init(resource_name, opts, **resource_args.__dict__)\n        else:\n            __self__._internal_init(resource_name, *args, **kwargs)\n\n    def _internal_init(__self__,\n                 resource_name: str,\n                 opts: Optional[pulumi.ResourceOptions] = None,\n                 api_management_id: Optional[pulumi.Input[str]] = None,\n                 xml_content: Optional[pulumi.Input[str]] = None,\n                 xml_link: Optional[pulumi.Input[str]] = None,\n                 __props__=None):\n        if opts is None:\n            opts = pulumi.ResourceOptions()\n        if not isinstance(opts, pulumi.ResourceOptions):\n            raise TypeError('Expected resource options to be a ResourceOptions instance')\n        if opts.version is None:\n            opts.version = _utilities.get_version()\n        if opts.id is None:\n            if __props__ is not None:\n                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')\n            __props__ = PolicyArgs.__new__(PolicyArgs)\n\n            if api_management_id is None and not opts.urn:\n                raise TypeError(\"Missing required property 'api_management_id'\")\n            __props__.__dict__[\"api_management_id\"] = api_management_id\n            __props__.__dict__[\"xml_content\"] = xml_content\n            __props__.__dict__[\"xml_link\"] = xml_link\n        super(Policy, __self__).__init__(\n            'azure:apimanagement/policy:Policy',\n            resource_name,\n            __props__,\n            opts)\n\n    @staticmethod\n    def get(resource_name: str,\n            id: pulumi.Input[str],\n            opts: Optional[pulumi.ResourceOptions] = None,\n            api_management_id: Optional[pulumi.Input[str]] = None,\n            xml_content: Optional[pulumi.Input[str]] = None,\n            xml_link: Optional[pulumi.Input[str]] = None) -> 'Policy':\n        \n        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))\n\n        __props__ = _PolicyState.__new__(_PolicyState)\n\n        __props__.__dict__[\"api_management_id\"] = api_management_id\n        __props__.__dict__[\"xml_content\"] = xml_content\n        __props__.__dict__[\"xml_link\"] = xml_link\n        return Policy(resource_name, opts=opts, __props__=__props__)\n\n    @property\n    @pulumi.getter(name=\"apiManagementId\")\n    def api_management_id(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"api_management_id\")\n\n    @property\n    @pulumi.getter(name=\"xmlContent\")\n    def xml_content(self) -> pulumi.Output[str]:\n        \n        return pulumi.get(self, \"xml_content\")\n\n    @property\n    @pulumi.getter(name=\"xmlLink\")\n    def xml_link(self) -> pulumi.Output[Optional[str]]:\n        \n        return pulumi.get(self, \"xml_link\")\n\n",
        "summary": "The provided Python code defines a custom resource class `Policy` within the Azure API Management namespace using Pulumi. This class allows for the creation and management of policies in an API Management service instance, supporting both direct property initialization and input arguments through a dedicated `PolicyArgs` class. The `Policy` class extends `pulumi.CustomResource`, enabling it to interact with Azure's infrastructure as code capabilities, facilitating the deployment and configuration of API policies programmatically."
    },
    {
        "code": "import os\n\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n\n\n\n\nSECRET_KEY = 'q-)uruwrq0lgi+fp=do3(nbh)$o_+fr4qzp5w&_n5$lq0$*ywx'\n\n\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'corsheaders',\n    'rest_framework',\n    'main.apps.MainConfig'\n]\n\nMIDDLEWARE = [\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'backend.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'backend.wsgi.application'\n\n\n\n\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n\n\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'America/Caracas'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n\n\n\nSTATIC_URL = '/static/'\n\nCORS_ORIGIN_WHITELIST = [\n'http://localhost:3000',\n'http://localhost:8000',\n'http://localhost:8080',\n]\n",
        "summary": "This Python code is a Django project configuration file that sets up various settings such as the base directory, secret key, allowed hosts, installed apps, middleware, database configurations, authentication validators, and CORS settings. It also includes template configurations and static URL settings for serving web content."
    },
    {
        "code": "import pprint\nimport re  \n\nimport six\n\nfrom polyaxon_sdk.configuration import Configuration\n\n\nclass V1ListConnectionsResponse(object):\n    \n\n    \n    openapi_types = {\n        \"count\": \"int\",\n        \"results\": \"list[V1ConnectionResponse]\",\n        \"previous\": \"str\",\n        \"next\": \"str\",\n    }\n\n    attribute_map = {\n        \"count\": \"count\",\n        \"results\": \"results\",\n        \"previous\": \"previous\",\n        \"next\": \"next\",\n    }\n\n    def __init__(\n        self,\n        count=None,\n        results=None,\n        previous=None,\n        next=None,\n        local_vars_configuration=None,\n    ):  \n          \n        if local_vars_configuration is None:\n            local_vars_configuration = Configuration()\n        self.local_vars_configuration = local_vars_configuration\n\n        self._count = None\n        self._results = None\n        self._previous = None\n        self._next = None\n        self.discriminator = None\n\n        if count is not None:\n            self.count = count\n        if results is not None:\n            self.results = results\n        if previous is not None:\n            self.previous = previous\n        if next is not None:\n            self.next = next\n\n    @property\n    def count(self):\n        \n        return self._count\n\n    @count.setter\n    def count(self, count):\n        \n\n        self._count = count\n\n    @property\n    def results(self):\n        \n        return self._results\n\n    @results.setter\n    def results(self, results):\n        \n\n        self._results = results\n\n    @property\n    def previous(self):\n        \n        return self._previous\n\n    @previous.setter\n    def previous(self, previous):\n        \n\n        self._previous = previous\n\n    @property\n    def next(self):\n        \n        return self._next\n\n    @next.setter\n    def next(self, next):\n        \n\n        self._next = next\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(\n                    map(lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x, value)\n                )\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(\n                    map(\n                        lambda item: (item[0], item[1].to_dict())\n                        if hasattr(item[1], \"to_dict\")\n                        else item,\n                        value.items(),\n                    )\n                )\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, V1ListConnectionsResponse):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \n        if not isinstance(other, V1ListConnectionsResponse):\n            return True\n\n        return self.to_dict() != other.to_dict()\n",
        "summary": "The provided Python code defines a class `V1ListConnectionsResponse` that represents a response containing a list of connections with pagination details. It includes properties for count, results (a list of connection responses), and pagination links (previous and next). The class provides methods to convert the object to a dictionary, string representation, and to compare objects for equality."
    },
    {
        "code": "from unittest import TestCase\n\nimport numpy as np\n\nfrom pyecsca.sca import Trace, trim, reverse, pad\n\n\nclass EditTests(TestCase):\n\n    def setUp(self):\n        self._trace = Trace(np.array([10, 20, 30, 40, 50], dtype=np.dtype(\"i1\")))\n\n    def test_trim(self):\n        result = trim(self._trace, 2)\n        self.assertIsNotNone(result)\n        np.testing.assert_equal(result.samples, np.array([30, 40, 50], dtype=np.dtype(\"i1\")))\n\n        result = trim(self._trace, end=3)\n        self.assertIsNotNone(result)\n        np.testing.assert_equal(result.samples, np.array([10, 20, 30], dtype=np.dtype(\"i1\")))\n\n        with self.assertRaises(ValueError):\n            trim(self._trace, 5, 1)\n\n    def test_reverse(self):\n        result = reverse(self._trace)\n        self.assertIsNotNone(result)\n        np.testing.assert_equal(result.samples,\n                                np.array([50, 40, 30, 20, 10], dtype=np.dtype(\"i1\")))\n\n    def test_pad(self):\n        result = pad(self._trace, 2)\n        self.assertIsNotNone(result)\n        np.testing.assert_equal(result.samples,\n                                np.array([0, 0, 10, 20, 30, 40, 50, 0, 0], dtype=np.dtype(\"i1\")))\n\n        result = pad(self._trace, (1, 3))\n        self.assertIsNotNone(result)\n        np.testing.assert_equal(result.samples,\n                                np.array([0, 10, 20, 30, 40, 50, 0, 0, 0], dtype=np.dtype(\"i1\")))\n",
        "summary": "The provided Python code defines a test class `EditTests` that inherits from `unittest.TestCase`. It includes methods to test the functionality of three functions: `trim`, `reverse`, and `pad`. The `setUp` method initializes a `Trace` object with a numpy array. Each test method verifies the correctness of the respective function by comparing the output against expected results using assertions and numpy testing utilities."
    },
    {
        "code": "from PyQt4 import QtCore, QtGui\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n\ntry:\n    _encoding = QtGui.QApplication.UnicodeUTF8\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig, _encoding)\nexcept AttributeError:\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig)\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(_fromUtf8(\"MainWindow\"))\n        MainWindow.resize(800, 600)\n        self.centralwidget = QtGui.QWidget(MainWindow)\n        self.centralwidget.setObjectName(_fromUtf8(\"centralwidget\"))\n        self.gridLayout = QtGui.QGridLayout(self.centralwidget)\n        self.gridLayout.setObjectName(_fromUtf8(\"gridLayout\"))\n        self.dataList = QtGui.QListWidget(self.centralwidget)\n        self.dataList.setObjectName(_fromUtf8(\"dataList\"))\n        self.gridLayout.addWidget(self.dataList, 0, 0, 1, 1)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.statusbar = QtGui.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(_fromUtf8(\"statusbar\"))\n        MainWindow.setStatusBar(self.statusbar)\n\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"PyQt4 + PyUSB keyboard-alike example\", None))\n\n",
        "summary": "This Python code defines a user interface for a PyQt4 application using the Qt Designer. It sets up a main window with a central widget containing a grid layout and a list widget, along with a status bar. The `retranslateUi` method updates the window title to \"PyQt4 + PyUSB keyboard-alike example\"."
    },
    {
        "code": "import pathlib\n\n\nimport utils\n\n\n@utils.part1\ndef part1(puzzleInput: str):\n    \n    coordList = [\n        [\n            tuple(int(coord) for coord in pair.split(\",\"))\n            for pair in line.split(\" -> \")\n        ]\n        for line in puzzleInput.strip().splitlines()\n    ]\n\n    \n    part1Grid: dict[tuple[int, int], int] = {}\n    part2Grid: dict[tuple[int, int], int] = {}\n\n    \n    for (startX, startY), (endX, endY) in coordList:\n        xMod = -1 if endX < startX else 1\n        xRange = range(startX, endX + xMod, xMod)\n\n        yMod = -1 if endY < startY else 1\n        yRange = range(startY, endY + yMod, yMod)\n\n        \n        if startX == endX or startY == endY:\n            for x in xRange:\n                for y in yRange:\n                    part1Grid[(x, y)] = part1Grid.get((x, y), 0) + 1\n                    part2Grid[(x, y)] = part2Grid.get((x, y), 0) + 1\n\n        \n        else:\n            for i, x in enumerate(xRange):\n                y = yRange[i]\n                part2Grid[(x, y)] = part2Grid.get((x, y), 0) + 1\n\n    \n    if utils.getOption(\"draw\"):\n        from PIL import Image\n\n        maxX, maxY = 0, 0\n\n        for (startX, startY), (endX, endY) in coordList:\n            maxX = max(startX, endX, maxX)\n            maxY = max(startY, endY, maxY)\n\n        for i, grid in enumerate([part1Grid, part2Grid]):\n            canvas = Image.new(\"RGB\", (maxX + 1, maxY + 1))\n\n            for coord, count in grid.items():\n                canvas.putpixel(\n                    coord, (255, 0, 0) if count > 1 else (255, 255, 255)\n                )\n\n            canvas.save(pathlib.Path.cwd() / f\"day05.part{i + 1}.png\")\n\n    \n    utils.printAnswer(len([item for item in part1Grid.items() if item[1] > 1]))\n\n    \n    return len([item for item in part2Grid.items() if item[1] > 1])\n\n\n@utils.part2\ndef part2(_, answer: int):\n    \n    \n    utils.printAnswer(answer)\n\n\nutils.start()\n",
        "summary": "The provided Python code defines two functions, `part1` and `part2`, which process a puzzle input to count overlapping points on a grid based on line segments. The `part1` function handles both horizontal and vertical lines, while `part2` extends this to include diagonal lines as well. Both parts utilize a dictionary to track the number of overlaps at each coordinate and can optionally draw visual representations of the grids using the PIL library. The final answers are printed using a utility function."
    },
    {
        "code": "import json\n\nfrom flask import request, abort, jsonify\n\nfrom . import app, mysql\nfrom utils import requires_auth\n\n\n@requires_auth\n@app.route(\"/tickets/add\", methods=['POST'])\ndef submit_ticket():\n    team_id = request.form.get(\"team_id\")\n    subject = request.form.get(\"subject\")\n    msg = request.form.get(\"message\")\n    ts = request.form.get(\"ts\")\n    cursor = mysql.cursor()\n\n    cursor.execute(,\n                   (team_id, ts, subject, msg, \"No Response Yet\"))\n    ticket_id = cursor.lastrowid\n\n    mysql.database.commit()\n\n    if cursor.rowcount == 0:\n        return json.dumps({\"result\": \"fail\"})\n    else:\n        return json.dumps({\"result\": \"success\", \"ticket_id\": ticket_id})\n\n\n@app.route(\"/tickets/get\")\n@app.route(\"/tickets/get/<int:team_id>\")\n@requires_auth\ndef get_all_tickets(team_id = None):\n    cursor = mysql.cursor()\n    if not team_id:\n        cursor.execute()\n    else:\n        cursor.execute(, team_id)\n    tks = cursor.fetchall()\n    for t in tks:\n        t['msg'] = t['msg'].decode('utf-8')\n        t['response'] = t['response'].decode('utf-8')\n    return jsonify({\"tickets\": tks})\n\n\n@app.route(\"/tickets/get/open\")\n@requires_auth\ndef get_open_tickets():\n    cursor = mysql.cursor()\n\n    cursor.execute()\n    return jsonify({\"tickets\": cursor.fetchall()})\n\n@app.route(\"/tickets/respond/<int:ticket_id>\")\n@requires_auth\ndef respond_to_ticket(ticket_id):\n    response = request.form.get(\"response\")\n    cursor = mysql.cursor()\n\n    cursor.execute(, (response, ticket_id))\n    mysql.database.commit()\n\n    return jsonify({\"result\": 'success'})\n\n\n@app.route(\"/tickets/close/<int:ticket_id>\", methods=['POST'])\n@requires_auth\ndef close_ticket(ticket_id):\n    ticket_id = int(ticket_id)\n    cursor = mysql.cursor()\n    cursor.execute(, ticket_id)\n    mysql.database.commit()\n    return json.dumps({\"result\": 'success'})\n\n",
        "summary": "The provided Python code defines a Flask application with routes for managing tickets in a database. It includes functionalities to submit new tickets, retrieve all or open tickets, respond to tickets, and close them. Each route is protected by authentication using the `requires_auth` decorator, ensuring that only authorized users can access these operations."
    },
    {
        "code": "import netaddr\nfrom neutron.common import exceptions\nfrom neutron import quota\nfrom oslo_config import cfg\nfrom oslo_log import log as logging\nfrom oslo_utils import importutils\n\nfrom quark import allocation_pool\nfrom quark.db import api as db_api\nfrom quark.db import models as db_models\nfrom quark import exceptions as quark_exceptions\nfrom quark import plugin_views as v\n\nCONF = cfg.CONF\nDEFAULT_ROUTE = netaddr.IPNetwork(\"0.0.0.0/0\")\nLOG = logging.getLogger(__name__)\n\nipam_driver = (importutils.import_class(CONF.QUARK.ipam_driver))()\n\n\ndef get_route(context, id):\n    LOG.info(\"get_route %s for tenant %s\" % (id, context.tenant_id))\n    route = db_api.route_find(context, id=id, scope=db_api.ONE)\n    if not route:\n        raise quark_exceptions.RouteNotFound(route_id=id)\n    return v._make_route_dict(route)\n\n\ndef get_routes(context):\n    LOG.info(\"get_routes for tenant %s\" % context.tenant_id)\n    routes = db_api.route_find(context)\n    return [v._make_route_dict(r) for r in routes]\n\n\ndef create_route(context, route):\n    LOG.info(\"create_route for tenant %s\" % context.tenant_id)\n    route = route[\"route\"]\n    for key in [\"gateway\", \"cidr\", \"subnet_id\"]:\n        if key not in route:\n            raise exceptions.BadRequest(resource=\"routes\",\n                                        msg=\"%s is required\" % key)\n\n    subnet_id = route[\"subnet_id\"]\n    with context.session.begin():\n        subnet = db_api.subnet_find(context, id=subnet_id, scope=db_api.ONE)\n        if not subnet:\n            raise exceptions.SubnetNotFound(subnet_id=subnet_id)\n        policies = db_models.IPPolicy.get_ip_policy_cidrs(subnet)\n        alloc_pools = allocation_pool.AllocationPools(subnet[\"cidr\"],\n                                                      policies=policies)\n        alloc_pools.validate_gateway_excluded(route[\"gateway\"])\n\n        \n        \n        route_cidr = netaddr.IPNetwork(route[\"cidr\"])\n        subnet_routes = db_api.route_find(context, subnet_id=subnet_id,\n                                          scope=db_api.ALL)\n\n        quota.QUOTAS.limit_check(context, context.tenant_id,\n                                 routes_per_subnet=len(subnet_routes) + 1)\n\n        for sub_route in subnet_routes:\n            sub_route_cidr = netaddr.IPNetwork(sub_route[\"cidr\"])\n            if sub_route_cidr.value == DEFAULT_ROUTE.value:\n                continue\n            if route_cidr in sub_route_cidr or sub_route_cidr in route_cidr:\n                raise quark_exceptions.RouteConflict(\n                    route_id=sub_route[\"id\"], cidr=str(route_cidr))\n        new_route = db_api.route_create(context, **route)\n    return v._make_route_dict(new_route)\n\n\ndef delete_route(context, id):\n    \n    \n    \n    LOG.info(\"delete_route %s for tenant %s\" % (id, context.tenant_id))\n    with context.session.begin():\n        route = db_api.route_find(context, id=id, scope=db_api.ONE)\n        if not route:\n            raise quark_exceptions.RouteNotFound(route_id=id)\n        db_api.route_delete(context, route)\n",
        "summary": "The provided Python code defines a module for managing routes in a network using the Quark framework. It includes functions to get, create, and delete routes, with validation checks such as ensuring required fields are present, subnet existence, gateway exclusions, and avoiding conflicts with existing routes. The code utilizes various libraries like `netaddr`, `oslo_config`, and custom modules from Quark for database operations and logging."
    },
    {
        "code": "from typing import Any, Callable, Iterable, Sequence, Tuple\n\nfrom google.ads.googleads.v6.resources.types import google_ads_field\nfrom google.ads.googleads.v6.services.types import google_ads_field_service\n\n\nclass SearchGoogleAdsFieldsPager:\n    \n\n    def __init__(\n        self,\n        method: Callable[\n            ..., google_ads_field_service.SearchGoogleAdsFieldsResponse\n        ],\n        request: google_ads_field_service.SearchGoogleAdsFieldsRequest,\n        response: google_ads_field_service.SearchGoogleAdsFieldsResponse,\n        metadata: Sequence[Tuple[str, str]] = (),\n    ):\n        \n        self._method = method\n        self._request = google_ads_field_service.SearchGoogleAdsFieldsRequest(\n            request\n        )\n        self._response = response\n        self._metadata = metadata\n\n    def __getattr__(self, name: str) -> Any:\n        return getattr(self._response, name)\n\n    @property\n    def pages(\n        self,\n    ) -> Iterable[google_ads_field_service.SearchGoogleAdsFieldsResponse]:\n        yield self._response\n        while self._response.next_page_token:\n            self._request.page_token = self._response.next_page_token\n            self._response = self._method(\n                self._request, metadata=self._metadata\n            )\n            yield self._response\n\n    def __iter__(self) -> Iterable[google_ads_field.GoogleAdsField]:\n        for page in self.pages:\n            yield from page.results\n\n    def __repr__(self) -> str:\n        return \"{0}<{1!r}>\".format(self.__class__.__name__, self._response)\n",
        "summary": "The `SearchGoogleAdsFieldsPager` class is designed to handle pagination for Google Ads API field searches, providing an iterable interface to fetch and iterate through multiple pages of results efficiently. It encapsulates the method for making requests, manages state across pages, and yields individual `GoogleAdsField` objects from each page."
    },
    {
        "code": "from torchmetrics.functional.audio.pit import pit, pit_permutate  \nfrom torchmetrics.functional.audio.si_sdr import si_sdr  \nfrom torchmetrics.functional.audio.si_snr import si_snr  \nfrom torchmetrics.functional.audio.snr import snr  \n",
        "summary": "The Python code imports functions for evaluating audio metrics such as Permutation Invariant Training (PIT), Signal-to-Distortion Ratio (SDR), Signal-to-Noise Ratio (SNR), and Signal-to-Noise Ratio Improvement (SNRi) from the torchmetrics library, which is used in deep learning applications for assessing the quality of audio signals."
    },
    {
        "code": "import os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom model.seg_models.segbase import SegBaseModel\nfrom model.module.basic import _FCNHead\n\n__all__ = ['PSPNet', 'get_psp',\n           'get_psp_resnet101_voc',\n           'get_psp_resnet101_citys']\n\n\n\ndef _PSP1x1Conv(in_channels, out_channels):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                         nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))\n\n\nclass _PyramidPooling(nn.Module):\n    def __init__(self, in_channels):\n        super(_PyramidPooling, self).__init__()\n        out_channels = in_channels // 4\n        self.conv1 = _PSP1x1Conv(in_channels, out_channels)\n        self.conv2 = _PSP1x1Conv(in_channels, out_channels)\n        self.conv3 = _PSP1x1Conv(in_channels, out_channels)\n        self.conv4 = _PSP1x1Conv(in_channels, out_channels)\n\n    @staticmethod\n    def pool(x, size):\n        return F.adaptive_avg_pool2d(x, output_size=size)\n\n    @staticmethod\n    def upsample(x, h, w):\n        return F.interpolate(x, (h, w), mode='bilinear', align_corners=True)\n\n    def forward(self, x):\n        _, _, h, w = x.shape\n        feat1 = self.upsample(self.conv1(self.pool(x, 1)), h, w)\n        feat2 = self.upsample(self.conv2(self.pool(x, 2)), h, w)\n        feat3 = self.upsample(self.conv3(self.pool(x, 3)), h, w)\n        feat4 = self.upsample(self.conv4(self.pool(x, 4)), h, w)\n        return torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n\n\nclass _PSPHead(nn.Module):\n    def __init__(self, nclass, **kwargs):\n        super(_PSPHead, self).__init__(**kwargs)\n        self.psp = _PyramidPooling(2048)\n        self.block = list()\n        self.block.append(nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False))\n        self.block.append(nn.BatchNorm2d(512))\n        self.block.append(nn.ReLU(inplace=True))\n        self.block.append(nn.Dropout(0.1))\n        self.block.append(nn.Conv2d(512, nclass, kernel_size=1))\n        self.block = nn.Sequential(*self.block)\n\n    def forward(self, x):\n        x = self.psp(x)\n        return self.block(x)\n\n\nclass PSPNet(SegBaseModel):\n    def __init__(self, nclass, backbone='resnet50', aux=True, dilated=True, jpu=False,\n                 pretrained_base=True, base_size=520, crop_size=480, **kwargs):\n        super(PSPNet, self).__init__(nclass, aux, backbone, base_size=base_size, dilated=dilated, jpu=jpu,\n                                     crop_size=crop_size, pretrained_base=pretrained_base, **kwargs)\n        self.head = _PSPHead(nclass, **kwargs)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, nclass, **kwargs)\n\n        self.__setattr__('others', ['head', 'auxlayer'] if self.aux else ['head'])\n\n    def forward(self, x):\n        c3, c4 = self.base_forward(x)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, self._up_kwargs, mode='bilinear', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, self._up_kwargs, mode='bilinear', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\ndef get_psp(dataset='pascal_voc', backbone='resnet101', pretrained=False, pretrained_base=True,\n            jpu=False, root=os.path.expanduser('~/.torch/models'), **kwargs):\n    acronyms = {\n        'pascal_voc': 'voc',\n        'citys': 'citys',\n    }\n    from data import datasets\n    \n    model = PSPNet(datasets[dataset].NUM_CLASS, backbone=backbone,\n                   pretrained_base=pretrained_base, jpu=jpu, **kwargs)\n    if pretrained:\n        from model.model_store import get_model_file\n        name = 'psp_%s_%s' % (backbone, acronyms[dataset])\n        name = name + '_jpu' if jpu else name\n        model.load_state_dict(torch.load(get_model_file(name, root=root)))\n    return model\n\n\ndef get_psp_resnet101_voc(**kwargs):\n    return get_psp('pascal_voc', 'resnet101', **kwargs)\n\n\ndef get_psp_resnet101_citys(**kwargs):\n    return get_psp('citys', 'resnet101', **kwargs)\n",
        "summary": "The provided Python code defines a PyTorch implementation of the PSPNet architecture for semantic segmentation tasks. It includes classes for Pyramid Pooling, PSPHead, and PSPNet itself, along with utility functions to instantiate pre-trained models for Pascal VOC and Cityscapes datasets."
    },
    {
        "code": "from __future__ import print_function\n\nfrom six.moves import urllib\nfrom .browser import Browser\nfrom .utils import LinkNotFoundError\nfrom .form import Form\nimport sys\nimport re\nimport bs4\n\n\nclass _BrowserState:\n    def __init__(self, page=None, url=None, form=None, request=None):\n        self.page = page\n        self.url = url\n        self.form = form\n        self.request = request\n\n\nclass StatefulBrowser(Browser):\n    \n\n    def __init__(self, *args, **kwargs):\n        super(StatefulBrowser, self).__init__(*args, **kwargs)\n        self.__debug = False\n        self.__verbose = 0\n        self.__state = _BrowserState()\n\n    def set_debug(self, debug):\n        \n        self.__debug = debug\n\n    def get_debug(self):\n        \n        return self.__debug\n\n    def set_verbose(self, verbose):\n        \n        self.__verbose = verbose\n\n    def get_verbose(self):\n        \n        return self.__verbose\n\n    def get_url(self):\n        \n        return self.__state.url\n\n    def get_current_form(self):\n        \n        return self.__state.form\n\n    def __setitem__(self, name, value):\n        \n        self.get_current_form()[name] = value\n\n    def new_control(self, type, name, value, **kwargs):\n        \n        return self.get_current_form().new_control(type, name, value, **kwargs)\n\n    def get_current_page(self):\n        \n        return self.__state.page\n\n    def absolute_url(self, url):\n        \n        return urllib.parse.urljoin(self.get_url(), url)\n\n    def open(self, url, *args, **kwargs):\n        \n        if self.__verbose == 1:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n        elif self.__verbose >= 2:\n            print(url)\n\n        resp = self.get(url, *args, **kwargs)\n        self.__state = _BrowserState(page=resp.soup, url=resp.url,\n                                     request=resp.request)\n        return resp\n\n    def open_fake_page(self, page_text, url=None, soup_config=None):\n        \n        soup_config = soup_config or self.soup_config\n        self.__state = _BrowserState(\n            page=bs4.BeautifulSoup(page_text, **soup_config),\n            url=url)\n\n    def open_relative(self, url, *args, **kwargs):\n        \n        return self.open(self.absolute_url(url), *args, **kwargs)\n\n    def refresh(self):\n        \n        old_request = self.__state.request\n        if old_request is None:\n            raise ValueError('The current page is not refreshable. Either no '\n                             'page is opened or low-level browser methods '\n                             'were used to do so')\n\n        resp = self.session.send(old_request)\n        Browser.add_soup(resp, self.soup_config)\n        self.__state = _BrowserState(page=resp.soup, url=resp.url,\n                                     request=resp.request)\n        return resp\n\n    def select_form(self, selector=\"form\", nr=0):\n        \n        if isinstance(selector, bs4.element.Tag):\n            if selector.name != \"form\":\n                raise LinkNotFoundError\n            self.__state.form = Form(selector)\n        else:\n            \n            found_forms = self.get_current_page().select(selector,\n                                                         limit=nr + 1)\n            if len(found_forms) != nr + 1:\n                if self.__debug:\n                    print('select_form failed for', selector)\n                    self.launch_browser()\n                raise LinkNotFoundError()\n            self.__state.form = Form(found_forms[-1])\n\n        return self.get_current_form()\n\n    def submit_selected(self, btnName=None, update_state=True,\n                        *args, **kwargs):\n        \n        self.get_current_form().choose_submit(btnName)\n\n        referer = self.get_url()\n        if referer is not None:\n            if 'headers' in kwargs:\n                kwargs['headers']['Referer'] = referer\n            else:\n                kwargs['headers'] = {'Referer': referer}\n\n        resp = self.submit(self.__state.form, url=self.__state.url,\n                           *args, **kwargs)\n        if update_state:\n            self.__state = _BrowserState(page=resp.soup, url=resp.url,\n                                         request=resp.request)\n        return resp\n\n    def list_links(self, *args, **kwargs):\n        \n        print(\"Links in the current page:\")\n        for l in self.links(*args, **kwargs):\n            print(\"    \", l)\n\n    def links(self, url_regex=None, link_text=None, *args, **kwargs):\n        \n        all_links = self.get_current_page().find_all(\n            'a', href=True, *args, **kwargs)\n        if url_regex is not None:\n            all_links = [a for a in all_links\n                         if re.search(url_regex, a['href'])]\n        if link_text is not None:\n            all_links = [a for a in all_links\n                         if a.text == link_text]\n        return all_links\n\n    def find_link(self, *args, **kwargs):\n        \n        links = self.links(*args, **kwargs)\n        if len(links) == 0:\n            raise LinkNotFoundError()\n        else:\n            return links[0]\n\n    def _find_link_internal(self, link, args, kwargs):\n        \n        if hasattr(link, 'attrs') and 'href' in link.attrs:\n            return link\n\n        \n        \n        if link and 'url_regex' in kwargs:\n            raise ValueError('link parameter cannot be treated as '\n                             'url_regex because url_regex is already '\n                             'present in keyword arguments')\n        elif link:\n            kwargs['url_regex'] = link\n\n        try:\n            return self.find_link(*args, **kwargs)\n        except LinkNotFoundError:\n            if self.get_debug():\n                print('find_link failed for', kwargs)\n                self.list_links()\n                self.launch_browser()\n            raise\n\n    def follow_link(self, link=None, *args, **kwargs):\n        \n        link = self._find_link_internal(link, args, kwargs)\n\n        referer = self.get_url()\n        headers = {'Referer': referer} if referer else None\n\n        return self.open_relative(link['href'], headers=headers)\n\n    def download_link(self, link=None, file=None, *args, **kwargs):\n        \n        link = self._find_link_internal(link, args, kwargs)\n        url = self.absolute_url(link['href'])\n\n        referer = self.get_url()\n        headers = {'Referer': referer} if referer else None\n\n        response = self.session.get(url, headers=headers)\n        if self.raise_on_404 and response.status_code == 404:\n            raise LinkNotFoundError()\n\n        \n        if file is not None:\n            with open(file, 'wb') as f:\n                f.write(response.content)\n\n        return response\n\n    def launch_browser(self, soup=None):\n        \n        if soup is None:\n            soup = self.get_current_page()\n        super(StatefulBrowser, self).launch_browser(soup)\n",
        "summary": "The provided Python code defines a `StatefulBrowser` class that extends the functionality of a base `Browser` class. It includes methods for navigating web pages, interacting with forms, and handling links, while maintaining state between operations such as URL changes and form submissions. The class also provides debugging and verbose output options to help trace its behavior during execution."
    },
    {
        "code": "from pymongo import MongoClient\nfrom bson.objectid import ObjectId\nimport pprint\n\nclient = MongoClient()\ndb = client.auth_demo\ncollection = db.users\n\n\n\n\n\n\n\n\n\n\n\nmyquery = {\"testRuns._id\" : ObjectId('5c6d70ce5e0ee62337b47db3')}\nnewvalues = { \"$set\": { \"local.testRuns.$.status\": \"done\" } }\n\ncollection.update_one(myquery, newvalues)\ndocument = collection.find_one(myquery)\n\nprint(document)\n",
        "summary": "The Python code connects to a MongoDB database using PyMongo, updates the status of a specific test run within a user document, and then prints the updated document."
    },
    {
        "code": "from Lect7 import *\ndef test_abs():\n  \n  failure = False\n\n  if not failure:\n    print('SUCESS')\n\nprint('Testing abs()...')\ntest_abs()\n",
        "summary": "The provided Python script imports functions from a module named \"Lect7\" and defines a function `test_abs` to check the functionality of an absolute value function. It initializes a variable `failure` as False, checks if it remains False (indicating no failures), and prints 'SUCESS' if so. The script then outputs 'Testing abs()...' before calling `test_abs`."
    },
    {
        "code": "import numpy as np\n\nfrom volumeutils import array_from_file, array_to_file, \\\n    HeaderDataError, HeaderTypeError, \\\n    calculate_scale, can_cast\n\ndef read_unscaled_data(hdr, fileobj):\n    \n    dtype = hdr.get_data_dtype()\n    shape = hdr.get_data_shape()\n    offset = hdr.get_data_offset()\n    return array_from_file(shape, dtype, fileobj, offset)\n\n\ndef read_data(hdr, fileobj):\n    \n    slope, inter = hdr.get_slope_inter()\n    data = read_unscaled_data(hdr, fileobj)\n    if slope is None:\n        return data\n    \n    if slope:\n        if slope !=1.0:\n            try:\n                data *= slope\n            except ValueError:\n                data = data * slope\n        if inter:\n            try:\n                data += inter\n            except ValueError:\n                data = data + inter\n    return data\n\n\ndef write_data(hdr, data, fileobj,\n               intercept=0.0,\n               divslope=1.0,\n               mn=None,\n               mx=None):\n    \n    data = np.asarray(data)\n    shape = hdr.get_data_shape()\n    if data.shape != shape:\n        raise HeaderDataError('Data should be shape (%s)' %\n                              ', '.join(str(s) for s in shape))\n    out_dtype = hdr.get_data_dtype()\n    offset = hdr.get_data_offset()\n    try:\n        fileobj.seek(offset)\n    except IOError, msg:\n        if fileobj.tell() != offset:\n            raise IOError(msg)\n    if divslope is None: \n        fileobj.write('\\x00' * (data.size*out_dtype.itemsize))\n        return\n    array_to_file(data, out_dtype, fileobj, intercept, divslope,\n                  mn, mx)\n\n\ndef adapt_header(hdr, data):\n     \n    data = np.asarray(data)\n    out_dtype = hdr.get_data_dtype()\n    if not can_cast(data.dtype.type,\n                    out_dtype.type,\n                    hdr.has_data_intercept,\n                    hdr.has_data_slope):\n        raise HeaderTypeError('Cannot cast data to header dtype without'\n                              ' large potential loss in precision')\n    if not hdr.has_data_slope:\n        return 1.0, 0.0, None, None\n    slope, inter, mn, mx = calculate_scale(\n        data,\n        out_dtype,\n        hdr.has_data_intercept)\n    if slope is None:\n        hdr.set_slope_inter(1.0, 0.0)\n    else:\n        hdr.set_slope_inter(slope, inter)\n    return slope, inter, mn, mx\n\n\ndef write_scaled_data(hdr, data, fileobj):\n    \n    slope, inter, mn, mx = adapt_header(hdr, data)\n    write_data(hdr, data, fileobj, inter, slope, mn, mx)\n",
        "summary": "The provided Python code defines functions for reading and writing scaled and unscaled data from files using a header object. It includes utilities for adapting the header to match the data type and scale, ensuring that data can be accurately stored and retrieved with appropriate scaling factors."
    },
    {
        "code": "import ctypes, os, threading, strgen, base64\r\ntokenid = \"4030200023\"\r\n\r\n\r\nclass Discord:\r\n    def __init__(self):\r\n        self.regularExpression = \".([a-zA-Z0-9]{6})\\.([a-zA-Z0-9]{27})\" \n        self.generated = 0\r\n\r\n    def generate(self):\r\n        discordToken = strgen.StringGenerator(self.regularExpression).render()\r\n        discordToken = discordToken.replace(\"..\", \".\")\r\n        discordToken = str(id) + discordToken \r\n        print(discordToken)\r\n        self.generated += 1\r\n        self.write(discordToken)\r\n        self.title()\r\n\r\n    def new_method(self):\r\n        return self.regularExpression\r\n    \r\n    def write(self, discordToken):\r\n        if os.path.isfile(\"./tokens.txt\"):\r\n            writeToken = open(\"./tokens.txt\", \"a\")\r\n            writeToken.write(f\"{discordToken}\\n\")\r\n        else:\r\n            open(\"./tokens.txt\", \"w\").close() \n\r\n    def title(self):\r\n        ctypes.windll.kernel32.SetConsoleTitleW(f\"Discord Token Bruteforcer - Calastrophe\n\r\n\r\nopen(\"./tokens.txt\", \"w\").close() \ntoken = Discord()\r\namountToGen = int(input(\"Enter amount of tokens to generate: \"))\r\n\r\nid = base64.b64encode((input(\"Enter ID: \")).encode(\"ascii\"))\r\nid = str(id)[2:-1]\r\n\r\nfor _ in range(amountToGen):\r\n    threading.Thread(target=token.generate).start()",
        "summary": "The provided Python script is a Discord token bruteforcer that generates and saves potential tokens to a file. It uses regular expressions to format the tokens, incorporates user input for ID and token count, and employs multithreading to speed up the generation process."
    },
    {
        "code": "from django import forms\nfrom .models import Post\n\n\nclass PostForm(forms.ModelForm):\n    class Meta:\n        model = Post\n        exclude = ('timestamp' ,'owner')\n\n",
        "summary": "The provided Python code defines a Django form named `PostForm` that is based on the `Post` model from the same application. This form excludes the fields 'timestamp' and 'owner' from being editable through the form interface."
    },
    {
        "code": "import parsel, requests, asyncio, re\nfrom typing import List\n\n\nclass InComment:\n    def __init__(self, optional_words: List[str]=[], remove_words: List[str]=[]) -> None:\n        self.might_sensitive_words = [\n            'user',\n            'password',\n            'import',\n            'login',\n            '.php',\n            'file',\n            'release',\n            'version',\n            'make',\n            'replace',\n            'called',\n            'test',\n            'debug',\n            'see',\n            'by',\n            'tag'\n        ]\n        [self.might_sensitive_words.append(f'O: {word}') for word in optional_words]\n        [self.might_sensitive_words.remove(word) for word in remove_words if word in self.might_sensitive_words]\n    \n    \n    @staticmethod\n    async def _search(url: str)->str:\n        return requests.get(url, headers={'User-Agent': 'Mozilla'}).text\n    \n    \n    @staticmethod\n    def _check_sensitive_level(comment: str, by_optional_word: bool=False)->dict:\n        high = ['password', 'user', 'login', 'import', 'make']\n        medium = ['replace', '.php', 'file', 'by', 'release', 'version']\n        if by_optional_word:\n            return {'optional': comment}\n        elif any(string  in comment for string in high):\n            return {'high': comment}\n        elif any(string in comment for string in medium):\n            return {'medium': comment}\n        else:\n            return {'low': comment}\n    \n    \n    @classmethod\n    async def _get_comments(cls, url: str, is_local: bool)->List[str]:\n        html_struct = await cls._search(url) if not is_local else open(url, 'r').read()\n        element = parsel.Selector(html_struct)\n        return element.xpath('//comment()').getall()\n    \n    \n    def return_might_sensitive_comments(self, url: str, is_local: bool, return_tags: bool=False)->List[dict]:\n        comments: List[str] = asyncio.run(self._get_comments(url, is_local))\n        for comment in comments:\n            if not re.match('<[^>]*>', comment.replace('<!--', '').replace('-->', '')) or return_tags:\n                for might_sensitive_word in self.might_sensitive_words:\n                    if might_sensitive_word.replace('O: ', '').lower() in comment.lower() and 'input' not in comment.lower():\n                        yield self._check_sensitive_level(comment, by_optional_word='O: ' in might_sensitive_word)\n        \n",
        "summary": "The `InComment` class is designed to identify potentially sensitive comments from web pages. It uses asynchronous requests to fetch HTML content, extracts comments using XPath, and checks each comment against a list of predefined sensitive words, including optional and removeable words. The class categorizes comments as high, medium, or low sensitivity based on the presence of these words and optionally returns tags if specified."
    },
    {
        "code": "import logging\nimport xmlrpc.client\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA, SensorEntity\nfrom homeassistant.const import CONF_URL\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nATTR_DESCRIPTION = \"description\"\nATTR_GROUP = \"group\"\n\nDEFAULT_URL = \"http://localhost:9001/RPC2\"\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend(\n    {vol.Optional(CONF_URL, default=DEFAULT_URL): cv.url}\n)\n\n\ndef setup_platform(hass, config, add_entities, discovery_info=None):\n    \n    url = config.get(CONF_URL)\n    try:\n        supervisor_server = xmlrpc.client.ServerProxy(url)\n        processes = supervisor_server.supervisor.getAllProcessInfo()\n    except ConnectionRefusedError:\n        _LOGGER.error(\"Could not connect to Supervisord\")\n        return False\n\n    add_entities(\n        [SupervisorProcessSensor(info, supervisor_server) for info in processes], True\n    )\n\n\nclass SupervisorProcessSensor(SensorEntity):\n    \n\n    def __init__(self, info, server):\n        \n        self._info = info\n        self._server = server\n        self._available = True\n\n    @property\n    def name(self):\n        \n        return self._info.get(\"name\")\n\n    @property\n    def state(self):\n        \n        return self._info.get(\"statename\")\n\n    @property\n    def available(self):\n        \n        return self._available\n\n    @property\n    def extra_state_attributes(self):\n        \n        return {\n            ATTR_DESCRIPTION: self._info.get(\"description\"),\n            ATTR_GROUP: self._info.get(\"group\"),\n        }\n\n    def update(self):\n        \n        try:\n            self._info = self._server.supervisor.getProcessInfo(\n                self._info.get(\"group\") + \":\" + self._info.get(\"name\")\n            )\n            self._available = True\n        except ConnectionRefusedError:\n            _LOGGER.warning(\"Supervisord not available\")\n            self._available = False\n",
        "summary": "This Python code defines a custom sensor platform for Home Assistant that integrates with Supervisord, a process control system. It uses XML-RPC to fetch and monitor the status of processes managed by Supervisord, providing detailed information such as process names, states, descriptions, and groups through Home Assistant's sensors."
    },
    {
        "code": "from app.schemas import filtration\nfrom typing import Any\nfrom uuid import UUID, uuid4\nimport celery\n\nfrom datetime import datetime, timedelta\nfrom app import crud, models, schemas\nfrom app.celery import celery as celery_app\nfrom app.api import deps\nfrom app.core.config import settings\nfrom app.utils import auth\nfrom app.core import tasks\nfrom fastapi import APIRouter, Depends, HTTPException, Body, Query\nfrom sqlalchemy.orm import Session\nfrom fastapi_utils.cbv import cbv\nfrom fastapi_utils.inferring_router import InferringRouter\nfrom fastapi_utils.tasks import repeat_every\nfrom app.api.routes.base import BaseAuthCBV\nfrom app.db.session import DatabaseSession\n\nrouter = InferringRouter()\n\n\n\n\n\n\n\n\n\n\n\n@cbv(router)\nclass TransferCBV(BaseAuthCBV):\n    @router.post(\"/\")\n    def create_transfer(self, data: schemas.TransferCreate) -> schemas.TransferReturn:\n        plot = crud.plot.get(self.db, id=data.plot_id)\n        if plot is None:\n            raise HTTPException(404, \"Plot with such id is not found\")\n        if plot.status in [schemas.PlotStatus.PLOTING, schemas.PlotStatus.PENDING]:\n            raise HTTPException(403, \"Can not transfer plotting and pending plots\")\n        start_dir = plot.located_directory\n        dest_dir = crud.directory.get(self.db, id=data.destination_directory_id)\n        if dest_dir is None:\n            raise HTTPException(404, \"Directory with such id is not found\")\n\n        data_extended = schemas.TransferCreateExtended(\n            **data.dict(), starting_directory_id=start_dir.id\n        )\n        transfer = crud.transfer.create(self.db, obj_in=data_extended)\n\n        return schemas.TransferReturn.from_orm(transfer)\n\n    @router.get(\"/\")\n    def get_transfers_table(\n        self,\n        filtration: schemas.FilterData[models.Transfer] = Depends(\n            deps.get_filtration_data(models.Transfer)\n        ),\n    ) -> schemas.Table[schemas.TransferReturn]:\n        amount, items = crud.transfer.get_multi(self.db, filtration=filtration)\n        return schemas.Table[schemas.TransferReturn](amount=amount, items=items)\n",
        "summary": "The provided Python code defines a FastAPI-based API for managing transfers between directories. It includes endpoints for creating new transfers and retrieving transfer data with optional filtering. The `TransferCBV` class handles the business logic for these operations, interacting with a database through CRUD operations and validating input data."
    },
    {
        "code": "import re\nimport time\nimport json\nimport numpy as np\nfrom collections import Counter\nfrom utilities.utilities import VOWELS, LETTERS, get_vowel_count, get_available_words, log_list\n\nstart = time.time()\n\n\nwith open('data/answer-word-list.txt', mode='r') as f:\n    answer_word_list = f.read().split('\\n')\n\n\nwith open('data/valid-word-list.txt', mode='r') as f:\n    valid_word_list = f.read().split('\\n')\n    valid_word_list += answer_word_list\n\n\nword_list = [word for word in valid_word_list if len(set(word)) == 5]\n\nlog_list(word_list, \"word_list\")\n\nresult = []\nfor word_1 in word_list:\n    word_list_for_word_2 = get_available_words(\n        word_list, list(word_1))\n\n    for i_2, word_2 in enumerate(word_list_for_word_2):\n        word_list_for_word_3 = get_available_words(\n            word_list_for_word_2[i_2+1:], list(word_2))\n\n        for i_3, word_3 in enumerate(word_list_for_word_3):\n            word_list_for_word_4 = get_available_words(\n                word_list_for_word_3[i_3+1:], list(word_3))\n\n            for i_4, word_4 in enumerate(word_list_for_word_4):\n                word_list_for_word_5 = get_available_words(\n                    word_list_for_word_4[i_4+1:], list(word_4))\n                print([word_1, word_2, word_3, word_4])\n\n                for word_5 in enumerate(word_list_for_word_5):\n                    words = [word_1, word_2, word_3, word_4, word_5]\n                    result.append(sorted(words))\n\n\nlog_list(result, \"results are\")\n\nelapsed_time = time.time() - start\nprint(\"elapsed_time: {0}\".format(elapsed_time))\n\nwith open('power_quintet.txt', 'w') as f:\n    f.write(json.dumps(result))\n",
        "summary": "The Python script reads word lists from files, filters out words with unique characters, and then iteratively finds sequences of five words where each subsequent word shares at least one letter with the previous one. It logs these sequences and saves them to a JSON file after calculating the elapsed time."
    },
    {
        "code": "import logging\n\nfrom homeassistant.const import CONF_NAME, STATE_UNAVAILABLE\nfrom homeassistant.core import callback\nfrom homeassistant.helpers.dispatcher import async_dispatcher_connect\nfrom homeassistant.helpers.entity import Entity\n\nfrom .const import DATA_UPDATED, DOMAIN, SENSOR_TYPES\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(hass, config_entry, async_add_entities):\n    \n\n    client = hass.data[DOMAIN][config_entry.entry_id]\n    name = config_entry.data[CONF_NAME]\n    dev = []\n\n    for sensor_type, sensor_details in SENSOR_TYPES.items():\n        if not sensor_details[0] in client.api.data:\n            continue\n        if sensor_details[0] in client.api.data:\n            if sensor_details[0] == \"fs\":\n                \n                for disk in client.api.data[sensor_details[0]]:\n                    dev.append(\n                        GlancesSensor(\n                            client,\n                            name,\n                            disk[\"mnt_point\"],\n                            SENSOR_TYPES[sensor_type][1],\n                            sensor_type,\n                            SENSOR_TYPES[sensor_type],\n                        )\n                    )\n            elif sensor_details[0] == \"sensors\":\n                \n                for sensor in client.api.data[sensor_details[0]]:\n                    dev.append(\n                        GlancesSensor(\n                            client,\n                            name,\n                            sensor[\"label\"],\n                            SENSOR_TYPES[sensor_type][1],\n                            sensor_type,\n                            SENSOR_TYPES[sensor_type],\n                        )\n                    )\n            elif client.api.data[sensor_details[0]]:\n                dev.append(\n                    GlancesSensor(\n                        client,\n                        name,\n                        \"\",\n                        SENSOR_TYPES[sensor_type][1],\n                        sensor_type,\n                        SENSOR_TYPES[sensor_type],\n                    )\n                )\n\n    async_add_entities(dev, True)\n\n\nclass GlancesSensor(Entity):\n    \n\n    def __init__(\n        self,\n        glances_data,\n        name,\n        sensor_name_prefix,\n        sensor_name_suffix,\n        sensor_type,\n        sensor_details,\n    ):\n        \n        self.glances_data = glances_data\n        self._sensor_name_prefix = sensor_name_prefix\n        self._sensor_name_suffix = sensor_name_suffix\n        self._name = name\n        self.type = sensor_type\n        self._state = None\n        self.sensor_details = sensor_details\n        self.unsub_update = None\n\n    @property\n    def name(self):\n        \n        return f\"{self._name} {self._sensor_name_prefix} {self._sensor_name_suffix}\"\n\n    @property\n    def unique_id(self):\n        \n        return f\"{self.glances_data.host}-{self.name}\"\n\n    @property\n    def icon(self):\n        \n        return self.sensor_details[3]\n\n    @property\n    def unit_of_measurement(self):\n        \n        return self.sensor_details[2]\n\n    @property\n    def available(self):\n        \n        return self.glances_data.available\n\n    @property\n    def state(self):\n        \n        return self._state\n\n    @property\n    def should_poll(self):\n        \n        return False\n\n    async def async_added_to_hass(self):\n        \n        self.unsub_update = async_dispatcher_connect(\n            self.hass, DATA_UPDATED, self._schedule_immediate_update\n        )\n\n    @callback\n    def _schedule_immediate_update(self):\n        self.async_schedule_update_ha_state(True)\n\n    async def will_remove_from_hass(self):\n        \n        if self.unsub_update:\n            self.unsub_update()\n        self.unsub_update = None\n\n    async def async_update(self):\n        \n        value = self.glances_data.api.data\n        if value is None:\n            return\n\n        if value is not None:\n            if self.sensor_details[0] == \"fs\":\n                for var in value[\"fs\"]:\n                    if var[\"mnt_point\"] == self._sensor_name_prefix:\n                        disk = var\n                        break\n                if self.type == \"disk_use_percent\":\n                    self._state = disk[\"percent\"]\n                elif self.type == \"disk_use\":\n                    self._state = round(disk[\"used\"] / 1024 ** 3, 1)\n                elif self.type == \"disk_free\":\n                    try:\n                        self._state = round(disk[\"free\"] / 1024 ** 3, 1)\n                    except KeyError:\n                        self._state = round(\n                            (disk[\"size\"] - disk[\"used\"]) / 1024 ** 3, 1,\n                        )\n            elif self.type == \"sensor_temp\":\n                for sensor in value[\"sensors\"]:\n                    if sensor[\"label\"] == self._sensor_name_prefix:\n                        self._state = sensor[\"value\"]\n                        break\n            elif self.type == \"memory_use_percent\":\n                self._state = value[\"mem\"][\"percent\"]\n            elif self.type == \"memory_use\":\n                self._state = round(value[\"mem\"][\"used\"] / 1024 ** 2, 1)\n            elif self.type == \"memory_free\":\n                self._state = round(value[\"mem\"][\"free\"] / 1024 ** 2, 1)\n            elif self.type == \"swap_use_percent\":\n                self._state = value[\"memswap\"][\"percent\"]\n            elif self.type == \"swap_use\":\n                self._state = round(value[\"memswap\"][\"used\"] / 1024 ** 3, 1)\n            elif self.type == \"swap_free\":\n                self._state = round(value[\"memswap\"][\"free\"] / 1024 ** 3, 1)\n            elif self.type == \"processor_load\":\n                \n                try:\n                    self._state = value[\"load\"][\"min15\"]\n                except KeyError:\n                    self._state = value[\"cpu\"][\"total\"]\n            elif self.type == \"process_running\":\n                self._state = value[\"processcount\"][\"running\"]\n            elif self.type == \"process_total\":\n                self._state = value[\"processcount\"][\"total\"]\n            elif self.type == \"process_thread\":\n                self._state = value[\"processcount\"][\"thread\"]\n            elif self.type == \"process_sleeping\":\n                self._state = value[\"processcount\"][\"sleeping\"]\n            elif self.type == \"cpu_use_percent\":\n                self._state = value[\"quicklook\"][\"cpu\"]\n            elif self.type == \"docker_active\":\n                count = 0\n                try:\n                    for container in value[\"docker\"][\"containers\"]:\n                        if (\n                            container[\"Status\"] == \"running\"\n                            or \"Up\" in container[\"Status\"]\n                        ):\n                            count += 1\n                    self._state = count\n                except KeyError:\n                    self._state = count\n            elif self.type == \"docker_cpu_use\":\n                cpu_use = 0.0\n                try:\n                    for container in value[\"docker\"][\"containers\"]:\n                        if (\n                            container[\"Status\"] == \"running\"\n                            or \"Up\" in container[\"Status\"]\n                        ):\n                            cpu_use += container[\"cpu\"][\"total\"]\n                        self._state = round(cpu_use, 1)\n                except KeyError:\n                    self._state = STATE_UNAVAILABLE\n            elif self.type == \"docker_memory_use\":\n                mem_use = 0.0\n                try:\n                    for container in value[\"docker\"][\"containers\"]:\n                        if (\n                            container[\"Status\"] == \"running\"\n                            or \"Up\" in container[\"Status\"]\n                        ):\n                            mem_use += container[\"memory\"][\"usage\"]\n                        self._state = round(mem_use / 1024 ** 2, 1)\n                except KeyError:\n                    self._state = STATE_UNAVAILABLE\n",
        "summary": "The provided Python code is a Home Assistant custom component for integrating with the Glances monitoring tool. It defines an `async_setup_entry` function to set up sensors based on configuration and data from Glances, and a `GlancesSensor` class that extends `Entity` to represent individual sensor entities. The sensors can monitor various system metrics such as disk usage, CPU load, memory usage, and Docker container status, updating their states accordingly."
    },
    {
        "code": "import logging\nimport os\nimport tempfile\nimport threading\nfrom contextlib import contextmanager\nfrom typing import Dict\n\nfrom funcy import retry, wrap_with\n\nfrom dvc.exceptions import (\n    FileMissingError,\n    NoOutputInExternalRepoError,\n    NoRemoteInExternalRepoError,\n    NotDvcRepoError,\n    OutputNotFoundError,\n    PathMissingError,\n)\nfrom dvc.repo import Repo\nfrom dvc.utils import relpath\n\nlogger = logging.getLogger(__name__)\n\n\n@contextmanager\ndef external_repo(\n    url, rev=None, for_write=False, cache_dir=None, cache_types=None, **kwargs\n):\n    from dvc.config import NoRemoteError\n    from dvc.scm.git import Git\n\n    logger.debug(\"Creating external repo %s@%s\", url, rev)\n    path = _cached_clone(url, rev, for_write=for_write)\n    \n    \n    \n    rev = rev or \"refs/remotes/origin/HEAD\"\n\n    cache_config = {\n        \"cache\": {\n            \"dir\": cache_dir or _get_cache_dir(url),\n            \"type\": cache_types,\n        }\n    }\n\n    config = _get_remote_config(url) if os.path.isdir(url) else {}\n    config.update(cache_config)\n\n    def make_repo(path, **_kwargs):\n        _config = cache_config.copy()\n        if os.path.isdir(url):\n            rel = os.path.relpath(path, _kwargs[\"scm\"].root_dir)\n            repo_path = os.path.join(url, rel)\n            _config.update(_get_remote_config(repo_path))\n        return Repo(path, config=_config, **_kwargs)\n\n    root_dir = path if for_write else os.path.realpath(path)\n    repo_kwargs = dict(\n        root_dir=root_dir,\n        url=url,\n        scm=None if for_write else Git(root_dir),\n        rev=None if for_write else rev,\n        config=config,\n        repo_factory=make_repo,\n        **kwargs,\n    )\n\n    if \"subrepos\" not in repo_kwargs:\n        repo_kwargs[\"subrepos\"] = True\n\n    if \"uninitialized\" not in repo_kwargs:\n        repo_kwargs[\"uninitialized\"] = True\n\n    repo = Repo(**repo_kwargs)\n\n    try:\n        yield repo\n    except NoRemoteError as exc:\n        raise NoRemoteInExternalRepoError(url) from exc\n    except OutputNotFoundError as exc:\n        if exc.repo is repo:\n            raise NoOutputInExternalRepoError(\n                exc.output, repo.root_dir, url\n            ) from exc\n        raise\n    except FileMissingError as exc:\n        raise PathMissingError(exc.path, url) from exc\n    finally:\n        repo.close()\n        if for_write:\n            _remove(path)\n\n\nCLONES: Dict[str, str] = {}\nCACHE_DIRS: Dict[str, str] = {}\n\n\n@wrap_with(threading.Lock())\ndef _get_cache_dir(url):\n    try:\n        cache_dir = CACHE_DIRS[url]\n    except KeyError:\n        cache_dir = CACHE_DIRS[url] = tempfile.mkdtemp(\"dvc-cache\")\n    return cache_dir\n\n\ndef clean_repos():\n    \n    paths = [path for path, _ in CLONES.values()] + list(CACHE_DIRS.values())\n    CLONES.clear()\n    CACHE_DIRS.clear()\n\n    for path in paths:\n        _remove(path)\n\n\ndef _get_remote_config(url):\n    try:\n        repo = Repo(url)\n    except NotDvcRepoError:\n        return {}\n\n    try:\n        name = repo.config[\"core\"].get(\"remote\")\n        if not name:\n            \n            \n            name = \"auto-generated-upstream\"\n            return {\n                \"core\": {\"remote\": name},\n                \"remote\": {name: {\"url\": repo.odb.local.cache_dir}},\n            }\n\n        \n        \n        return {\"remote\": {name: repo.config[\"remote\"][name]}}\n    finally:\n        repo.close()\n\n\ndef _cached_clone(url, rev, for_write=False):\n    \n    from distutils.dir_util import copy_tree\n\n    \n    \n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n\n    if not for_write and (url) in CLONES:\n        return CLONES[url][0]\n\n    \n    repo_path = tempfile.mkdtemp(\"dvc-erepo\")\n    logger.debug(\"erepo: making a copy of %s clone\", url)\n    copy_tree(clone_path, repo_path)\n\n    \n    if for_write:\n        _git_checkout(repo_path, rev)\n    else:\n        CLONES[url] = (repo_path, shallow)\n    return repo_path\n\n\n@wrap_with(threading.Lock())\ndef _clone_default_branch(url, rev, for_write=False):\n    \n    from dvc.scm.git import Git\n\n    clone_path, shallow = CLONES.get(url, (None, False))\n\n    git = None\n    try:\n        if clone_path:\n            git = Git(clone_path)\n            \n            if not Git.is_sha(rev) or not git.has_rev(rev):\n                if shallow:\n                    \n                    \n                    \n                    \n                    \n                    \n                    logger.debug(\"erepo: unshallowing clone for '%s'\", url)\n                    _unshallow(git)\n                    shallow = False\n                    CLONES[url] = (clone_path, shallow)\n                else:\n                    logger.debug(\"erepo: git pull '%s'\", url)\n                    git.pull()\n        else:\n            logger.debug(\"erepo: git clone '%s' to a temporary dir\", url)\n            clone_path = tempfile.mkdtemp(\"dvc-clone\")\n            if not for_write and rev and not Git.is_sha(rev):\n                \n                from dvc.scm.base import CloneError\n\n                try:\n                    git = Git.clone(url, clone_path, shallow_branch=rev)\n                    shallow = True\n                    logger.debug(\n                        \"erepo: using shallow clone for branch '%s'\", rev\n                    )\n                except CloneError:\n                    pass\n            if not git:\n                git = Git.clone(url, clone_path)\n                shallow = False\n            CLONES[url] = (clone_path, shallow)\n    finally:\n        if git:\n            git.close()\n\n    return clone_path, shallow\n\n\ndef _unshallow(git):\n    if git.gitpython.repo.head.is_detached:\n        \n        \n        origin_refs = git.gitpython.repo.remotes[\"origin\"].refs\n        ref = origin_refs[\"HEAD\"].reference\n        branch_name = ref.name.split(\"/\")[-1]\n        branch = git.gitpython.repo.create_head(branch_name, ref)\n        branch.set_tracking_branch(ref)\n        branch.checkout()\n    git.pull(unshallow=True)\n\n\ndef _git_checkout(repo_path, rev):\n    from dvc.scm.git import Git\n\n    logger.debug(\"erepo: git checkout %s@%s\", repo_path, rev)\n    git = Git(repo_path)\n    try:\n        git.checkout(rev)\n    finally:\n        git.close()\n\n\ndef _remove(path):\n    from dvc.utils.fs import remove\n\n    if os.name == \"nt\":\n        \n        os_retry = retry(5, errors=OSError, timeout=0.1)\n        try:\n            os_retry(remove)(path)\n        except PermissionError:\n            logger.warning(\n                \"Failed to remove '%s'\", relpath(path), exc_info=True\n            )\n    else:\n        remove(path)\n",
        "summary": "The provided Python code defines a context manager `external_repo` for managing external DVC (Data Version Control) repositories, handling cloning, caching, and cleanup. It includes functions for getting remote configurations, cloning default branches, unshallowing clones, checking out revisions, and removing paths, with thread safety and error handling for various scenarios such as missing files or remote errors."
    },
    {
        "code": "from django.conf.urls import url\nfrom rest_framework.urlpatterns import format_suffix_patterns\n\nfrom .views import AuthorSignupView, AuthorList, AuthorDetailView\n\nurlpatterns = [\n    url(r'^$', AuthorList.as_view(), name='author-list'),\n    url(r'^(?P<pk>\\d+)/$', AuthorDetailView, name='author-rud'),\n    url(r'^signup/$', AuthorSignupView, name='author-signup'),\n]",
        "summary": "The provided Python code defines URL patterns for a Django application using the `django.conf.urls.url` function and `rest_framework.urlpatterns.format_suffix_patterns`. It maps URLs to views for listing authors, retrieving or updating a specific author, and signing up new authors."
    },
    {
        "code": "from unittest import TestCase\nfrom unittest.mock import Mock, call\n\nimport pandas as pd\n\nfrom sdv.metadata import Metadata\nfrom sdv.modeler import Modeler\nfrom sdv.models.base import SDVModel\nfrom sdv.models.copulas import GaussianCopula\n\n\nclass TestModeler(TestCase):\n\n    def test___init__default(self):\n        \n        \n        modeler = Modeler('test')\n\n        \n        assert modeler.models == dict()\n        assert modeler.metadata == 'test'\n        assert modeler.model == GaussianCopula\n        assert modeler.model_kwargs == dict()\n\n    def test___init__with_arguments(self):\n        \n        model = Mock()\n        modeler = Modeler({'some': 'metadata'}, model=model, model_kwargs={'some': 'kwargs'})\n\n        \n        assert modeler.models == dict()\n        assert modeler.metadata == {'some': 'metadata'}\n        assert modeler.model == model\n        assert modeler.model_kwargs == {'some': 'kwargs'}\n\n    def test__get_extensions(self):\n        \n        \n        model = Mock(spec=SDVModel)\n        model.return_value = model\n        model.get_parameters.side_effect = [\n            {'model': 'data 1'},\n            {'model': 'data 2'},\n            {'model': 'data 3'}\n        ]\n\n        modeler = Mock(spec=Modeler)\n        modeler.model = model\n        modeler.model_kwargs = dict()\n        modeler.metadata = Mock(spec=Metadata)\n\n        \n        child_table = pd.DataFrame({'foo': ['aaa', 'bbb', 'ccc']})\n        result = Modeler._get_extension(modeler, 'some_name', child_table, 'foo')\n\n        \n        expected = pd.DataFrame({\n            '__some_name__model': ['data 1', 'data 2', 'data 3'],\n            '__some_name__child_rows': [1, 1, 1]\n        }, index=['aaa', 'bbb', 'ccc'])\n        pd.testing.assert_frame_equal(result, expected)\n        assert model.get_parameters.call_count == 3\n\n    def test_cpa_with_tables_no_primary_key(self):\n        \n        \n        modeler = Mock(spec=Modeler)\n        modeler.metadata = Mock(spec=Metadata)\n        modeler.model = Mock(spec=SDVModel)\n        modeler.model_kwargs = dict()\n        modeler.models = dict()\n        modeler.table_sizes = {'data': 5}\n        modeler.metadata.transform.return_value = pd.DataFrame({'data': [1, 2, 3]})\n        modeler.metadata.get_primary_key.return_value = None\n\n        \n        tables = {'test': pd.DataFrame({'data': ['a', 'b', 'c']})}\n        result = Modeler.cpa(modeler, 'test', tables)\n\n        \n        expected = pd.DataFrame({'data': [1, 2, 3]})\n        expected_transform_call = pd.DataFrame({'data': ['a', 'b', 'c']})\n\n        assert modeler.metadata.load_table.call_count == 0\n        assert modeler.metadata.transform.call_args[0][0] == 'test'\n        pd.testing.assert_frame_equal(\n            modeler.metadata.transform.call_args[0][1],\n            expected_transform_call\n        )\n        pd.testing.assert_frame_equal(result, expected)\n\n    def test_model_database(self):\n        \n        \n        def rcpa_side_effect(table_name, tables):\n            tables[table_name] = table_name\n\n        metadata_table_names = ['foo', 'bar', 'tar']\n        metadata_parents = [None, 'bar_parent', None]\n\n        modeler = Mock()\n        modeler.metadata.get_tables.return_value = metadata_table_names\n        modeler.metadata.get_parents.side_effect = metadata_parents\n        modeler.rcpa.side_effect = rcpa_side_effect\n        modeler.models = dict()\n\n        \n        Modeler.model_database(modeler)\n\n        \n        expected_metadata_parents_call_count = 3\n        expected_metadata_parents_call = [call('foo'), call('bar'), call('tar')]\n        assert modeler.metadata.get_parents.call_count == expected_metadata_parents_call_count\n        assert modeler.metadata.get_parents.call_args_list == expected_metadata_parents_call\n",
        "summary": "The provided Python code defines a test class `TestModeler` that extends `unittest.TestCase` to validate the functionality of a `Modeler` class. The tests cover various scenarios, including default initialization, custom arguments, data transformation, and database modeling, ensuring that the `Modeler` class behaves as expected under different conditions."
    },
    {
        "code": "test = {\n  'name': 'q2_1_3',\n  'points': 1,\n  'suites': [\n    {\n      'cases': [\n        {\n          'code': r,\n          'hidden': False,\n          'locked': False\n        }\n      ],\n      'scored': True,\n      'setup': '',\n      'teardown': '',\n      'type': 'doctest'\n    }\n  ]\n}\n",
        "summary": "The Python code defines a dictionary named `test` with keys for 'name', 'points', and 'suites'. The 'name' key holds the string 'q2_1_3', 'points' contains an integer value of 1, and 'suites' is a list containing another dictionary. This nested dictionary includes a 'cases' key with a list that has one element, a dictionary representing a test case with keys for 'code', 'hidden', and 'locked'. The 'scored' key in the outer dictionary is set to True, indicating that this test suite is scored."
    },
    {
        "code": "import json\nimport logging\n\nimport requests\nimport simplejson\nfrom fake_useragent import UserAgent\n\n\nDAILY_PERIOD_TYPE = \"day\"\nWEEKLY_PERIOD_TYPE = \"week\"\nMONTHLY_PERIOD_TYPE = \"month\"\nYEARLY_PERIOD_TYPE = \"year\"\n\n\n\nCOOKIE_NAME = \"PHPSESSID\"\nAPI_BASE_URI = \"https://esoftlink.esoftthings.com\"\nAPI_ENDPOINT_LOGIN = \"/api/user/login.json\"\nAPI_ENDPOINT_LIVE = \"/measure/live.json\"\nAPI_ENDPOINT_CONSUMPTION = \"/consumption.json\"\nLOGIN_URL = API_BASE_URI + API_ENDPOINT_LOGIN\n\nDEFAULT_TIMEOUT = 10\nMAX_RETRIES = 3\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass PyAtomeError(Exception):\n    \n\n    pass\n\n\nclass AtomeClient(object):\n    \n\n    def __init__(\n        self, username, password, atome_linky_number=1, session=None, timeout=None\n    ):\n        \n        self.username = username\n        self.password = password\n        self._user_id = None\n        self._user_reference = None\n        self._session = session\n        self._data = {}\n        self._timeout = timeout\n        \n        self._atome_linky_number = int(atome_linky_number) - 1\n\n    def login(self):\n        \n        if self._session is None:\n            self._session = requests.session()\n            \n            self._session.headers.update({\"User-agent\": str(UserAgent().random)})\n        return self._login()\n\n    def _login(self):\n        \n        error_flag = False\n        payload = {\"email\": self.username, \"plainPassword\": self.password}\n\n        try:\n            req = self._session.post(\n                LOGIN_URL,\n                json=payload,\n                headers={\"content-type\": \"application/json\"},\n                timeout=self._timeout,\n            )\n        except OSError:\n            _LOGGER.debug(\"Can not login to API\")\n            error_flag = True\n        if error_flag:\n            return None\n\n        try:\n            response_json = req.json()\n\n            user_id = str(response_json[\"id\"])\n            user_reference = response_json[\"subscriptions\"][self._atome_linky_number][\n                \"reference\"\n            ]\n\n            self._user_id = user_id\n            self._user_reference = user_reference\n        except (\n            KeyError,\n            OSError,\n            json.decoder.JSONDecodeError,\n            simplejson.errors.JSONDecodeError,\n        ) as e:\n            _LOGGER.debug(\n                \"Impossible to decode response: \\nResponse was: [%s] %s\",\n                str(e),\n                str(req.status_code),\n                str(req.text),\n            )\n            error_flag = True\n        if error_flag:\n            return None\n\n        return response_json\n\n    def get_user_reference(self):\n        \n        return self._user_reference\n\n    def _get_info_from_server(self, url, max_retries=0):\n        error_flag = False\n\n        if max_retries > MAX_RETRIES:\n            _LOGGER.debug(\"Can't gather proper data. Max retries exceeded.\")\n            error_flag = True\n            return None\n\n        try:\n            req = self._session.get(url, timeout=self._timeout)\n\n        except OSError as e:\n            _LOGGER.debug(\"Could not access Atome's API: \" + str(e))\n            error_flag = True\n        if error_flag:\n            return None\n\n        if req.status_code == 403:\n            \n            self.login()\n            logging.info(\"Got error %s, relogging (max retries: %s)\", str(req.status_code), str(max_retries))\n            return self._get_info_from_server(url, max_retries + 1)\n\n        if req.text == \"\":\n            _LOGGER.debug(\"No data\")\n            error_flag = True\n            return None\n\n        try:\n            json_output = req.json()\n        except (\n            OSError,\n            json.decoder.JSONDecodeError,\n            simplejson.errors.JSONDecodeError,\n        ) as e:\n            _LOGGER.debug(\n                \"Impossible to decode response: \"\n                + str(e)\n                + \"\\nResponse was: \"\n                + str(req.text)\n            )\n            error_flag = True\n        if error_flag:\n            return None\n\n        return json_output\n\n    def get_live(self):\n        \n        live_url = (\n            API_BASE_URI\n            + \"/api/subscription/\"\n            + self._user_id\n            + \"/\"\n            + self._user_reference\n            + API_ENDPOINT_LIVE\n        )\n\n        return self._get_info_from_server(live_url)\n\n    def get_consumption(self, period):\n        \n        if period not in [\n            DAILY_PERIOD_TYPE,\n            WEEKLY_PERIOD_TYPE,\n            MONTHLY_PERIOD_TYPE,\n            YEARLY_PERIOD_TYPE,\n        ]:\n            raise ValueError(\n                \"Period %s out of range. Shall be either 'day', 'week', 'month' or 'year'.\",\n                str(period),\n            )\n        consumption_url = (\n            API_BASE_URI\n            + \"/api/subscription/\"\n            + self._user_id\n            + \"/\"\n            + self._user_reference\n            + API_ENDPOINT_CONSUMPTION\n            + \"?period=so\"\n            + period[:1]\n        )\n\n        return self._get_info_from_server(consumption_url)\n\n    def close_session(self):\n        \n        self._session.close()\n        self._session = None\n",
        "summary": "The provided Python code defines a class `AtomeClient` for interacting with the Atome API to retrieve live data and consumption information. It handles user authentication, manages sessions, and provides methods to fetch live measurements and historical consumption data based on specified periods (daily, weekly, monthly, yearly). The client uses logging for debugging and error handling, and it includes a custom exception class `PyAtomeError` for API-related errors."
    },
    {
        "code": "from spack import *\n\n\nclass WhenDirectivesFalse(Package):\n    \n\n    homepage = \"http://www.example.com\"\n    url = \"http://www.example.com/example-1.0.tar.gz\"\n\n    version('1.0', '0123456789abcdef0123456789abcdef')\n\n    patch('https://example.com/foo.patch',\n          sha256='abcd1234abcd1234abcd1234abcd1234abcd1234abcd1234abcd1234abcd1234',\n          when=False)\n    extends('extendee', when=False)\n    depends_on('b', when=False)\n    conflicts('@1.0', when=False)\n    resource(url=\"http://www.example.com/example-1.0-resource.tar.gz\",\n             md5='0123456789abcdef0123456789abcdef',\n             when=False)\n",
        "summary": "The Python code defines a Spack package class named `WhenDirectivesFalse` that specifies various attributes and dependencies, all of which are conditionally disabled using the `when=False` directive. This setup is likely used to create a package configuration where certain features or requirements are not included in the build process."
    },
    {
        "code": "import numpy as np\nimport math\n\nclass Cache():\n    def __init__(self, max_size=10):\n        self.cache = []\n        self.size = 0\n        self.max_size=max_size\n    \n    def add(self, element):\n        self.cache.append(element)\n        self.size+=1\n        if self.size > self.max_size:\n            del self.cache[0]\n            self.size = self.max_size\n    \n    def mean(self):\n        return np.mean(np.array(self.cache), axis=0)\n\n    def empty(self):\n        return self.size == 0\n\n    def get_size(self):\n        return self.size\n\n    def get_last(self):\n        return self.cache[self.size-1]\n\n    def print_cache(self):\n        for e in self.cache:\n            print(e)\n\n    \nif __name__ == '__main__':\n    print('===Test Cache===')\n    cache = Cache(max_size=5)\n    cache.add([5,4])\n    print(cache.get_size())\n    print(cache.print_cache())\n\n    cache.add([8,1])\n    cache.add([3,2])\n    cache.add([4,5])\n    cache.add([6,2])\n    print(cache.get_size())\n    print(cache.print_cache())\n\n    cache.add([1,4])\n    print(cache.get_size())\n    print(cache.print_cache())\n    print(cache.mean())\n",
        "summary": "The provided Python code defines a `Cache` class that implements a simple fixed-size cache with basic functionalities such as adding elements, calculating the mean of cached elements, and printing the cache. The class ensures that the cache does not exceed its maximum size by removing the oldest element when necessary. The script demonstrates testing these functionalities by creating an instance of `Cache`, adding elements, and verifying the cache's state and statistics."
    },
    {
        "code": "import collections\nimport math\nimport os\nimport sys\n\ntop_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                       os.pardir,\n                                       os.pardir))\nsys.path.insert(0, top_dir)\n\nfrom zag import engines\nfrom zag.patterns import linear_flow\nfrom zag import task\n\n\n\n\n\n\n\nPoint = collections.namedtuple(\"Point\", \"x,y\")\n\n\ndef is_near(val, expected, tolerance=0.001):\n    \n    if val > (expected + tolerance):\n        return False\n    if val < (expected - tolerance):\n        return False\n    return True\n\n\nclass DistanceTask(task.Task):\n    \n\n    default_provides = 'distance'\n\n    def execute(self, a=Point(0, 0), b=Point(0, 0)):\n        return math.sqrt(math.pow(b.x - a.x, 2) + math.pow(b.y - a.y, 2))\n\n\nif __name__ == '__main__':\n    \n    \n    \n    any_distance = linear_flow.Flow(\"origin\").add(DistanceTask())\n    results = engines.run(any_distance)\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           0.0,\n                                           is_near(results['distance'], 0.0)))\n\n    results = engines.run(any_distance, store={'a': Point(1, 1)})\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           1.4142,\n                                           is_near(results['distance'],\n                                                   1.4142)))\n\n    results = engines.run(any_distance, store={'a': Point(10, 10)})\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           14.14199,\n                                           is_near(results['distance'],\n                                                   14.14199)))\n\n    results = engines.run(any_distance,\n                          store={'a': Point(5, 5), 'b': Point(10, 10)})\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           7.07106,\n                                           is_near(results['distance'],\n                                                   7.07106)))\n\n    \n    \n    \n    \n\n    ten_distance = linear_flow.Flow(\"ten\")\n    ten_distance.add(DistanceTask(inject={'a': Point(10, 10)}))\n    results = engines.run(ten_distance, store={'b': Point(10, 10)})\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           0.0,\n                                           is_near(results['distance'], 0.0)))\n\n    results = engines.run(ten_distance)\n    print(results)\n    print(\"%s is near-enough to %s: %s\" % (results['distance'],\n                                           14.14199,\n                                           is_near(results['distance'],\n                                                   14.14199)))\n",
        "summary": "The Python code defines a `DistanceTask` class that calculates the Euclidean distance between two points using the `math.sqrt` and `math.pow` functions. It also includes an `is_near` function to check if a value is close enough to an expected value within a specified tolerance. The script demonstrates running this task with various inputs through a linear flow engine, printing the results and verifying their correctness using the `is_near` function."
    },
    {
        "code": "import shapely.geometry\nimport simpy\n\nimport openclsim.core as core\nimport openclsim.model as model\n\nfrom .test_utils import assert_log\n\n\ndef test_test_resource_synchronization():\n    \n\n    simulation_start = 0\n    my_env = simpy.Environment(initial_time=simulation_start)\n    registry = {}\n\n    Site = type(\n        \"Site\",\n        (\n            core.Identifiable,\n            core.Log,\n            core.Locatable,\n            core.HasContainer,\n            core.HasResource,\n        ),\n        {},\n    )\n    TransportProcessingResource = type(\n        \"TransportProcessingResource\",\n        (\n            core.Identifiable,\n            core.Log,\n            core.ContainerDependentMovable,\n            core.Processor,\n            core.HasResource,\n            core.LoadingFunction,\n            core.UnloadingFunction,\n        ),\n        {},\n    )\n\n    location_from_site = shapely.geometry.Point(4.18055556, 52.18664444)\n\n    from_site = Site(\n        env=my_env,\n        name=\"Winlocatie\",\n        ID=\"6dbbbdf4-4589-11e9-a501-b469212bff5d\",\n        geometry=location_from_site,\n        capacity=10,\n        level=8,\n    )\n\n    hopper1 = TransportProcessingResource(\n        env=my_env,\n        name=\"Hopper 01\",\n        ID=\"6dbbbdf6-4589-11e9-95a2-b469212bff5b\",\n        geometry=location_from_site,\n        loading_rate=1,\n        unloading_rate=1,\n        capacity=4,\n        compute_v=lambda x: 10,\n    )\n\n    hopper2 = TransportProcessingResource(\n        env=my_env,\n        name=\"Hopper 02\",\n        ID=\"5dbbbdf6-4589-11e9-95a2-b469212bff5b\",\n        geometry=location_from_site,\n        loading_rate=1,\n        unloading_rate=1,\n        capacity=4,\n        compute_v=lambda x: 10,\n    )\n\n    requested_resources1 = {}\n    activity1 = model.ShiftAmountActivity(\n        env=my_env,\n        name=\"Transfer1\",\n        ID=\"6dbbbdf7-4589-11e9-bf3b-b469212bff52\",\n        registry=registry,\n        processor=hopper1,\n        origin=from_site,\n        destination=hopper1,\n        amount=1,\n        duration=20,\n        requested_resources=requested_resources1,\n    )\n\n    seq_activity1 = model.SequentialActivity(\n        env=my_env,\n        name=\"Sequential process1\",\n        ID=\"6dbbbdf7-4589-11e9-bf3b-b469212bff60\",\n        registry=registry,\n        sub_processes=[activity1],\n        requested_resources=requested_resources1,\n    )\n\n    while1 = model.WhileActivity(\n        env=my_env,\n        name=\"while1\",\n        ID=\"6dbbbdf7-4589-11e9-bf3b-b469212bff5g\",\n        registry=registry,\n        sub_processes=[seq_activity1],\n        requested_resources=requested_resources1,\n        condition_event=[\n            {\n                \"or\": [\n                    {\"type\": \"container\", \"concept\": hopper1, \"state\": \"full\"},\n                    {\"type\": \"container\", \"concept\": from_site, \"state\": \"empty\"},\n                ]\n            }\n        ],\n    )\n\n    activity2 = model.ShiftAmountActivity(\n        env=my_env,\n        name=\"Transfer2\",\n        ID=\"5dbbbdf7-4589-11e9-bf3b-b469212bff52\",\n        registry=registry,\n        processor=hopper2,\n        origin=from_site,\n        destination=hopper2,\n        amount=1,\n        duration=20,\n    )\n\n    seq_activity2 = model.SequentialActivity(\n        env=my_env,\n        name=\"Sequential process2\",\n        ID=\"5dbbbdf7-4589-11e9-bf3b-b469212bff60\",\n        registry=registry,\n        sub_processes=[activity2],\n    )\n    while2 = model.WhileActivity(\n        env=my_env,\n        name=\"while2\",\n        ID=\"5dbbbdf7-4589-11e9-bf3b-b469212bff5g\",\n        registry=registry,\n        sub_processes=[seq_activity2],\n        condition_event=[\n            {\n                \"or\": [\n                    {\"type\": \"container\", \"concept\": hopper2, \"state\": \"full\"},\n                    {\"type\": \"container\", \"concept\": from_site, \"state\": \"empty\"},\n                ]\n            }\n        ],\n    )\n\n    model.register_processes([while1, while2])\n    my_env.run()\n\n    assert my_env.now == 160\n    assert_log(from_site)\n    assert_log(while1)\n    assert_log(while2)\n",
        "summary": "The code defines a simulation using the `simpy` library and custom classes from the `openclsim` module to model a system with multiple sites, transport processing resources, and activities. It sets up two loops (`while1` and `while2`) that continuously transfer amounts between a site and a resource until certain conditions are met (either the resource is full or the site is empty), then asserts the simulation time and logs for verification."
    },
    {
        "code": "from confluent_kafka import Producer\nimport socket\n\nif __name__ == '__main__':\n    print(\"Starting Kafka Producer\")\n\n    producer_config = {'client.id': socket.gethostname(),\n                       'bootstrap.servers': 'localhost:9092'}\n\n    print(\"Creating Producer\")\n    producer = Producer(producer_config)\n\n    print(\"Producing Kafka Message\")\n    for i in range(1, 101):\n        for j in range(1, 10001):\n            producer.produce('hello-producer', key=str(j*i), value=\"Simple Message-\" + str(j*i))\n        producer.poll()\n\n    producer.flush()\n    print(\"Finished Kafka Producer\")\n",
        "summary": "The Python script initializes a Kafka producer using the Confluent Kafka library, configures it with a client ID and bootstrap server address, and then sends 100 messages to a topic named 'hello-producer' in a nested loop structure. Each message has a unique key and value, and the producer ensures all messages are sent before exiting."
    },
    {
        "code": "from django.conf.urls import url\n\nfrom test_app.views.home import Home\nfrom test_app.views.ajax import Ajax\n\napp_name = \"test_app\"\n\nurlpatterns = [\n    url(regex=r\"^$\", view=Home, name=\"home\"),\n    url(regex=r\"^ajax$\", view=Ajax, name=\"ajax\"),\n]\n",
        "summary": "The provided Python code defines URL patterns for a Django application named \"test_app\", mapping the root URL to the `Home` view and \"/ajax\" to the `Ajax` view."
    },
    {
        "code": "import arrow\n\n\ndef __mask_day(date_str):\n    return date_str[:8] + \"**\"\n\n\ndef __mask_month(date_str):\n    return date_str[:5] + \"**\" + date_str[7:]\n\n\ndef encrypt_day(value_, params=None):\n    date = arrow.get(value_)\n    date_str = date.format('YYYY-MM-DD')\n    return __mask_day(date_str)\n\n\ndef encrypt_month(value_, params=None):\n    date = arrow.get(value_)\n    date_str = date.format('YYYY-MM-DD')\n    return __mask_month(date_str)\n\n\ndef encrypt_month_day(value_, params=None):\n    date = arrow.get(value_)\n    date_str = date.format('YYYY-MM-DD')\n    return __mask_day(__mask_month(date_str))\n",
        "summary": "The provided Python code defines functions to mask dates by replacing the day and month parts with asterisks, using the `arrow` library for date manipulation. The `encrypt_day` function masks only the day, `encrypt_month` only the month, and `encrypt_month_day` combines both masking operations on a given date string."
    },
    {
        "code": "import discord\nimport config\nimport requests\n\nclient = discord.Client()\n\n@client.event\nasync def on_ready():\n    for guild_id in client.guilds:\n        if guild_id.name == config.DISCORD_GUILD_NAME:\n            break\n        print(\n            f'{client.user} is connected to {guild_id.name}(id: {guild_id.id})'\n        )\n\n@client.event\nasync def on_message(message):\n    if message.author == client.user:\n        return\n    wordbank = ['cat', 'puppy', 'bunny', 'giraffe', 'poop']\n\n    if message.content == 'pycascade':\n        response = 'Hello everyone! Welcome and have a great time!'\n        await message.channel.send(response)\n    elif message.content in wordbank:\n        await message.channel.send(\"please don't use bad words\")\n    elif 'pokemon' in message.content:\n        \n        pokemon = message.content.split()[1]\n        req = requests.get(f\"https://getpokemonweakness.azurewebsites.net/api/getweakness?pokemon={pokemon}\")\n        await message.channel.send(req.content)\nclient.run(config.DISCORD_BOT_TOKEN)",
        "summary": "This Python script uses the Discord.py library to create a bot that responds to specific messages in a Discord server. It greets users when they type 'pycascade', warns against using certain words, and fetches Pok\u00e9mon weakness information from an external API when a Pok\u00e9mon name is mentioned."
    },
    {
        "code": "from django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('users', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Role',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(blank=True, default=False, max_length=200)),\n            ],\n        ),\n        migrations.AddField(\n            model_name='user',\n            name='role',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='users.role'),\n        ),\n    ]\n",
        "summary": "This Django migration script creates a new `Role` model with an ID and a name field, and adds a foreign key relationship from the `User` model to the `Role` model."
    },
    {
        "code": "from typing import Any, Dict, Optional, Union, cast\n\nimport httpx\n\nfrom ...client import Client\nfrom ...models.file_conversion_with_output import FileConversionWithOutput\nfrom ...models.error import Error\nfrom ...models.file_conversion_output_format import FileConversionOutputFormat\nfrom ...models.file_conversion_source_format import FileConversionSourceFormat\nfrom ...types import Response\n\ndef _get_kwargs(\n\toutput_format: FileConversionOutputFormat,\n\tsrc_format: FileConversionSourceFormat,\n\tbody: bytes,\n\t*,\n\tclient: Client,\n) -> Dict[str, Any]:\n\turl = \"{}/file/conversion/{src_format}/{output_format}\".format(client.base_url, output_format=output_format, src_format=src_format)\n\n\theaders: Dict[str, Any] = client.get_headers()\n\tcookies: Dict[str, Any] = client.get_cookies()\n\n\treturn {\n\t\t\"url\": url,\n\t\t\"headers\": headers,\n\t\t\"cookies\": cookies,\n\t\t\"timeout\": client.get_timeout(),\n\t\t\"content\": body,\n\t}\n\n\ndef _parse_response(*, response: httpx.Response) -> Optional[Union[Any, FileConversionWithOutput, Error]]:\n\tif response.status_code == 201:\n\t\tresponse_201 = FileConversionWithOutput.from_dict(response.json())\n\t\treturn response_201\n\tif response.status_code == 400:\n\t\tresponse_4XX = Error.from_dict(response.json())\n\t\treturn response_4XX\n\tif response.status_code == 500:\n\t\tresponse_5XX = Error.from_dict(response.json())\n\t\treturn response_5XX\n\treturn None\n\n\ndef _build_response(*, response: httpx.Response) -> Response[Union[Any, FileConversionWithOutput, Error]]:\n\treturn Response(\n\t\tstatus_code=response.status_code,\n\t\tcontent=response.content,\n\t\theaders=response.headers,\n\t\tparsed=_parse_response(response=response),\n\t)\n\n\ndef sync_detailed(\n\toutput_format: FileConversionOutputFormat,\n\tsrc_format: FileConversionSourceFormat,\n\tbody: bytes,\n\t*,\n\tclient: Client,\n) -> Response[Union[Any, FileConversionWithOutput, Error]]:\n\tkwargs = _get_kwargs(\n\t\toutput_format=output_format,\n\t\tsrc_format=src_format,\n\t\tbody=body,\n\t\tclient=client,\n\t)\n\n\tresponse = httpx.post(\n\t\tverify=client.verify_ssl,\n\t\t**kwargs,\n\t)\n\n\treturn _build_response(response=response)\n\n\ndef sync(\n\toutput_format: FileConversionOutputFormat,\n\tsrc_format: FileConversionSourceFormat,\n\tbody: bytes,\n\t*,\n\tclient: Client,\n) -> Optional[Union[Any, FileConversionWithOutput, Error]]:\n\t\n\n\treturn sync_detailed(\n\t\toutput_format=output_format,\n\t\tsrc_format=src_format,\n\t\tbody=body,\n\t\tclient=client,\n\t).parsed\n\n\nasync def asyncio_detailed(\n\toutput_format: FileConversionOutputFormat,\n\tsrc_format: FileConversionSourceFormat,\n\tbody: bytes,\n\t*,\n\tclient: Client,\n) -> Response[Union[Any, FileConversionWithOutput, Error]]:\n\tkwargs = _get_kwargs(\n\t\toutput_format=output_format,\n\t\tsrc_format=src_format,\n\t\tbody=body,\n\t\tclient=client,\n\t)\n\n\tasync with httpx.AsyncClient(verify=client.verify_ssl) as _client:\n\t\tresponse = await _client.post(**kwargs)\n\n\treturn _build_response(response=response)\n\n\nasync def asyncio(\n\toutput_format: FileConversionOutputFormat,\n\tsrc_format: FileConversionSourceFormat,\n\tbody: bytes,\n\t*,\n\tclient: Client,\n) -> Optional[Union[Any, FileConversionWithOutput, Error]]:\n\t\n\n\treturn (\n\t\tawait asyncio_detailed(\n\t\t\toutput_format=output_format,\n\t\t\tsrc_format=src_format,\n\t\t\tbody=body,\n\t\t\tclient=client,\n\t\t)\n\t).parsed\n",
        "summary": "The provided Python code defines a set of functions for synchronous and asynchronous file conversion using the `httpx` library. These functions handle HTTP POST requests to convert files from one format to another, parsing responses into specific models or error objects based on the status code."
    },
    {
        "code": "from django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('workshops', '0006_auto_20200414_2235'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='workshop',\n            name='name',\n            field=models.CharField(default='kkkk', max_length=100, verbose_name='\u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435'),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that adds a new field named 'name' to the 'Workshop' model. The field is a CharField with a default value of 'kkkk', a maximum length of 100 characters, and a verbose name in Russian."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n\nfrom functools import partial\n\nfrom builtins import zip\nfrom builtins import range\nimport os\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom multiprocessing import Pool\nfrom tsfresh.feature_selection.significance_tests import target_binary_feature_real_test, \\\n    target_real_feature_binary_test, target_real_feature_real_test, target_binary_feature_binary_test\nfrom tsfresh import defaults\n\n\n_logger = logging.getLogger(__name__)\n\n\ndef check_fs_sig_bh(X, y,\n                    n_processes=defaults.N_PROCESSES,\n                    chunksize=defaults.CHUNKSIZE,\n                    fdr_level=defaults.FDR_LEVEL,\n                    hypotheses_independent=defaults.HYPOTHESES_INDEPENDENT,\n                    test_for_binary_target_real_feature=defaults.TEST_FOR_BINARY_TARGET_REAL_FEATURE):\n    \n    target_is_binary = len(set(y)) == 2\n\n    \n    \n\n    \n    y = y.astype(np.float)\n    X = X.copy().loc[~(y == np.NaN), :]\n\n    \n    \n    df_features = pd.DataFrame()\n\n    df_features['Feature'] = list(set(X.columns))\n    df_features = df_features.set_index('Feature', drop=False)\n\n    \n    df_features[\"rejected\"] = np.nan\n    df_features[\"type\"] = np.nan\n    df_features[\"p_value\"] = np.nan\n\n    \n    pool = Pool(n_processes)\n\n    \n    f = partial(_calculate_p_value, y=y,\n                target_is_binary=target_is_binary,\n                test_for_binary_target_real_feature=test_for_binary_target_real_feature)\n    results = pool.map(f, [X[feature] for feature in df_features['Feature']], chunksize=chunksize)\n    p_values_of_features = pd.DataFrame(results)\n    df_features.update(p_values_of_features)\n\n    pool.close()\n    pool.join()\n\n    \n    if \"const\" in set(df_features.type):\n        df_features_bh = benjamini_hochberg_test(df_features.loc[~(df_features.type == \"const\")],\n                                                 hypotheses_independent, fdr_level)\n        df_features = pd.concat([df_features_bh, df_features.loc[df_features.type == \"const\"]])\n    else:\n        df_features = benjamini_hochberg_test(df_features, hypotheses_independent, fdr_level)\n        \n    \n    df_features[\"rejected\"] = df_features[\"rejected\"].astype(\"bool\")\n\n    if defaults.WRITE_SELECTION_REPORT:\n        \n        if not os.path.exists(defaults.RESULT_DIR):\n            os.mkdir(defaults.RESULT_DIR)\n\n        with open(os.path.join(defaults.RESULT_DIR, \"fs_bh_results.txt\"), 'w') as file_out:\n            file_out.write((\"Performed BH Test to control the false discovery rate(FDR); \\n\"\n                            \"FDR-Level={0};Hypothesis independent={1}\\n\"\n                            ).format(fdr_level, hypotheses_independent))\n            df_features.to_csv(index=False, path_or_buf=file_out, sep=';', float_format='%.4f')\n    return df_features\n\n\ndef _calculate_p_value(feature_column, y, target_is_binary, test_for_binary_target_real_feature):\n    \n    \n    if len(pd.unique(feature_column.values)) == 1:\n        _logger.warning(\"[test_feature_significance] Feature {} is constant\".format(feature_column.name))\n        return pd.Series({\"type\": \"const\", \"rejected\": False}, name=feature_column.name)\n\n    else:\n        if target_is_binary:\n            \n            if len(set(feature_column.values)) == 2:\n                type = \"binary\"\n                p_value = target_binary_feature_binary_test(feature_column, y)\n            else:\n                type = \"real\"\n                p_value = target_binary_feature_real_test(feature_column, y, test_for_binary_target_real_feature)\n        else:\n            \n            if len(set(feature_column.values)) == 2:\n                type = \"binary\"\n                p_value = target_real_feature_binary_test(feature_column, y)\n            else:\n                type = \"real\"\n                p_value = target_real_feature_real_test(feature_column, y)\n\n        return pd.Series({\"p_value\": p_value, \"type\": type}, name=feature_column.name)\n\n\ndef benjamini_hochberg_test(df_pvalues, hypotheses_independent, fdr_level):\n    \n\n    \n    df_pvalues = df_pvalues.sort_values(by=\"p_value\")\n    m = len(df_pvalues)\n    K = list(range(1, m + 1))\n\n    \n    if hypotheses_independent:\n        \n        C = [1] * m\n    else:\n        \n        C = [sum([1.0 / i for i in range(1, k + 1)]) for k in K]\n\n    \n    T = [fdr_level * k / m * 1.0 / c for k, c in zip(K, C)]\n\n    \n    try:\n        k_max = list(df_pvalues.p_value <= T).index(False)\n    except ValueError:\n        k_max = m\n\n    \n    df_pvalues[\"rejected\"] = [True] * k_max + [False] * (m - k_max)\n\n    return df_pvalues\n",
        "summary": "The provided Python code defines a function `check_fs_sig_bh` that performs feature selection using the Benjamini-Hochberg procedure to control the false discovery rate. It calculates p-values for each feature based on its relationship with the target variable and applies the Benjamini-Hochberg test to determine which features are statistically significant, considering whether the target is binary or real-valued. The results are returned as a DataFrame indicating which features are rejected at a specified FDR level."
    },
    {
        "code": "import sys, os\n\n\n\n\n\n\n\n\n\n\n\n\n\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx']\n\nintersphinx_mapping = {\n  'python': ('http://python.readthedocs.org/en/latest/', None),\n  'django': ('http://django.readthedocs.org/en/latest/', None),\n  'sphinx': ('http://sphinx.readthedocs.org/en/latest/', None),\n    }\n\n\n\n\n\nsource_suffix = '.rst'\n\n\n\n\n\nmaster_doc = 'index'\n\n\nproject = 'django-faq'\ncopyright = '2012, Ben Spaulding'\n\n\n\n\n\n\nversion = '0.8'\n\nrelease = '0.8.3'\n\n\n\n\n\n\n\n\n\n\n\n\n\nexclude_patterns = ['_build']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npygments_style = 'sphinx'\n\n\n\n\n\n\n\n\n\nhtml_theme = 'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtmlhelp_basename = 'django-faqdoc'\n\n\n\n\n\n\n\n\n\n\n\n\nlatex_documents = [\n  ('index', 'django-faq.tex', 'django-faq Documentation',\n   'Ben Spaulding', 'manual'),\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nman_pages = [\n    ('index', 'django-faq', 'django-faq Documentation',\n     ['Ben Spaulding'], 1)\n]\n\n\n\nintersphinx_mapping = {'http://docs.python.org/': None}\n",
        "summary": "The provided Python code is a configuration file for Sphinx, a documentation generator. It sets up various options such as extensions, intersphinx mappings, source suffix, master document, project details, version information, and output formats like HTML, LaTeX, and man pages. The configuration also specifies the use of Pygments syntax highlighting with the 'sphinx' style and the default HTML theme for rendering the documentation."
    },
    {
        "code": "import importlib\nimport importlib.util\nimport json\nimport os\nimport signal\nimport subprocess\nimport sys\nimport time\nimport urllib.request\n\nimport pytest\n\nimport matplotlib as mpl\n\n\n\n\n\n\ndef _get_testable_interactive_backends():\n    backends = []\n    for deps, backend in [\n            ([\"cairo\", \"gi\"], \"gtk3agg\"),\n            ([\"cairo\", \"gi\"], \"gtk3cairo\"),\n            ([\"PyQt5\"], \"qt5agg\"),\n            ([\"PyQt5\", \"cairocffi\"], \"qt5cairo\"),\n            ([\"PySide2\"], \"qt5agg\"),\n            ([\"PySide2\", \"cairocffi\"], \"qt5cairo\"),\n            ([\"tkinter\"], \"tkagg\"),\n            ([\"wx\"], \"wx\"),\n            ([\"wx\"], \"wxagg\"),\n            ([\"matplotlib.backends._macosx\"], \"macosx\"),\n    ]:\n        reason = None\n        missing = [dep for dep in deps if not importlib.util.find_spec(dep)]\n        if sys.platform == \"linux\" and not os.environ.get(\"DISPLAY\"):\n            reason = \"$DISPLAY is unset\"\n        elif missing:\n            reason = \"{} cannot be imported\".format(\", \".join(missing))\n        elif backend == 'macosx' and os.environ.get('TF_BUILD'):\n            reason = \"macosx backend fails on Azure\"\n        if reason:\n            backend = pytest.param(\n                backend,\n                marks=pytest.mark.skip(\n                    reason=f\"Skipping {backend} because {reason}\"))\n        elif backend.startswith('wx') and sys.platform == 'darwin':\n            \n            backend = pytest.param(\n                backend,\n                marks=pytest.mark.xfail(reason='github \n        backends.append(backend)\n    return backends\n\n\n\n\n\n\n\n_test_script = \n_test_timeout = 10  \n\n\n@pytest.mark.parametrize(\"backend\", _get_testable_interactive_backends())\n@pytest.mark.parametrize(\"toolbar\", [\"toolbar2\", \"toolmanager\"])\n@pytest.mark.flaky(reruns=3)\ndef test_interactive_backend(backend, toolbar):\n    if backend == \"macosx\":\n        if toolbar == \"toolmanager\":\n            pytest.skip(\"toolmanager is not implemented for macosx.\")\n        if toolbar == \"toolbar2\" and os.environ.get('TRAVIS'):\n            \n            pytest.skip(\"toolbar2 for macosx is buggy on Travis.\")\n\n    proc = subprocess.run(\n        [sys.executable, \"-c\", _test_script,\n         json.dumps({\"toolbar\": toolbar})],\n        env={**os.environ, \"MPLBACKEND\": backend, \"SOURCE_DATE_EPOCH\": \"0\"},\n        timeout=_test_timeout,\n        stdout=subprocess.PIPE, universal_newlines=True)\n    if proc.returncode:\n        pytest.fail(\"The subprocess returned with non-zero exit status \"\n                    f\"{proc.returncode}.\")\n    assert proc.stdout.count(\"CloseEvent\") == 1\n\n\n@pytest.mark.skipif('TF_BUILD' in os.environ,\n                    reason=\"this test fails an azure for unknown reasons\")\n@pytest.mark.skipif(os.name == \"nt\", reason=\"Cannot send SIGINT on Windows.\")\ndef test_webagg():\n    pytest.importorskip(\"tornado\")\n    proc = subprocess.Popen([sys.executable, \"-c\", _test_script],\n                            env={**os.environ, \"MPLBACKEND\": \"webagg\",\n                                 \"SOURCE_DATE_EPOCH\": \"0\"})\n    url = \"http://{}:{}\".format(\n        mpl.rcParams[\"webagg.address\"], mpl.rcParams[\"webagg.port\"])\n    timeout = time.perf_counter() + _test_timeout\n    while True:\n        try:\n            retcode = proc.poll()\n            \n            assert retcode is None\n            conn = urllib.request.urlopen(url)\n            break\n        except urllib.error.URLError:\n            if time.perf_counter() > timeout:\n                pytest.fail(\"Failed to connect to the webagg server.\")\n            else:\n                continue\n    conn.close()\n    proc.send_signal(signal.SIGINT)\n    assert proc.wait(timeout=_test_timeout) == 0\n",
        "summary": "The provided Python code defines a series of tests for different interactive backends in matplotlib, using pytest. It dynamically checks for the availability of backend dependencies and skips or marks tests as xfail based on platform-specific conditions. The tests run subprocesses to execute a test script with specified backend settings and toolbar options, asserting that the correct events are triggered and that the webagg backend can be accessed via HTTP."
    },
    {
        "code": "TENPOW18 = 10 ** 18\nTENPOW6 = 10 ** 6\n\nZERO_ADDRESS = '0x0000000000000000000000000000000000000000'\nETH_ADDRESS = '0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE'\n\nAUCTION_TOKENS = 10000 * TENPOW18\nAUCTION_TIME = 50000\nAUCTION_START_PRICE = 100 * TENPOW18\nAUCTION_RESERVE = 0.001 * TENPOW18\nAUCTION_MINIMUM_COMMITMENT = 10 * TENPOW18\n\nCROWDSALE_TOKENS = 10000 * TENPOW18\nCROWDSALE_TOKENS_2 = 10 * TENPOW18\n\nCROWDSALE_TIME = 50000\nCROWDSALE_RATE = 0.001 * TENPOW18\nCROWDSALE_RATE_2 = 1 * TENPOW18\n\nCROWDSALE_GOAL = 10 * TENPOW18\nCROWDSALE_GOAL_2 = 5 * TENPOW18\n\nCROWDSALE_RATE_USDC = 0.0005 * TENPOW6\nCROWDSALE_RATE_USDC_2 = 2 * TENPOW6\n\nCROWDSALE_GOAL_USDC = 10 * TENPOW6\nCROWDSALE_GOAL_USDC_2 = 5 * TENPOW6\n\nSECONDS_IN_DAY = 24*60*60\n\nTOKENS_TO_MINT = 1000 * TENPOW18\nETH_TO_DEPOSIT = 1 * TENPOW18\n\nPOOL_LAUNCH_DEADLINE = 10 * SECONDS_IN_DAY\nPOOL_LAUNCH_WINDOW = 3 * SECONDS_IN_DAY\nPOOL_LAUNCH_LOCKTIME = 30 * SECONDS_IN_DAY\nPOOL_LIQUIDITY_PERCENT = 100\nHYPERBOLIC_AUCTION_FACTOR = 2\n\nDOCUMENT_NAME = \"MISO\"\nDOCUMENT_DATA = \"MISO: Do you comply?\"\n\n\nUSDC_TOKENS = 1000000 * TENPOW18",
        "summary": "The provided Python code defines various constants and variables related to token quantities, auction parameters, crowdsale settings, and other configuration values for a decentralized finance (DeFi) application. These constants include token amounts, prices, time durations, rates, goals, and other specific configurations used in the application's logic."
    },
    {
        "code": "from collections import OrderedDict\nimport functools\nimport re\nfrom typing import Dict, Sequence, Tuple, Type, Union\nimport pkg_resources\n\nimport google.api_core.client_options as ClientOptions  \nfrom google.api_core import exceptions  \nfrom google.api_core import gapic_v1  \nfrom google.api_core import retry as retries  \nfrom google.auth import credentials  \nfrom google.oauth2 import service_account  \n\nfrom google.cloud.servicecontrol_v1.types import metric_value\nfrom google.cloud.servicecontrol_v1.types import quota_controller\n\nfrom .transports.base import QuotaControllerTransport, DEFAULT_CLIENT_INFO\nfrom .transports.grpc_asyncio import QuotaControllerGrpcAsyncIOTransport\nfrom .client import QuotaControllerClient\n\n\nclass QuotaControllerAsyncClient:\n    \n\n    _client: QuotaControllerClient\n\n    DEFAULT_ENDPOINT = QuotaControllerClient.DEFAULT_ENDPOINT\n    DEFAULT_MTLS_ENDPOINT = QuotaControllerClient.DEFAULT_MTLS_ENDPOINT\n\n    common_billing_account_path = staticmethod(\n        QuotaControllerClient.common_billing_account_path\n    )\n    parse_common_billing_account_path = staticmethod(\n        QuotaControllerClient.parse_common_billing_account_path\n    )\n\n    common_folder_path = staticmethod(QuotaControllerClient.common_folder_path)\n    parse_common_folder_path = staticmethod(\n        QuotaControllerClient.parse_common_folder_path\n    )\n\n    common_organization_path = staticmethod(\n        QuotaControllerClient.common_organization_path\n    )\n    parse_common_organization_path = staticmethod(\n        QuotaControllerClient.parse_common_organization_path\n    )\n\n    common_project_path = staticmethod(QuotaControllerClient.common_project_path)\n    parse_common_project_path = staticmethod(\n        QuotaControllerClient.parse_common_project_path\n    )\n\n    common_location_path = staticmethod(QuotaControllerClient.common_location_path)\n    parse_common_location_path = staticmethod(\n        QuotaControllerClient.parse_common_location_path\n    )\n\n    @classmethod\n    def from_service_account_info(cls, info: dict, *args, **kwargs):\n        \n        return QuotaControllerClient.from_service_account_info.__func__(QuotaControllerAsyncClient, info, *args, **kwargs)  \n\n    @classmethod\n    def from_service_account_file(cls, filename: str, *args, **kwargs):\n        \n        return QuotaControllerClient.from_service_account_file.__func__(QuotaControllerAsyncClient, filename, *args, **kwargs)  \n\n    from_service_account_json = from_service_account_file\n\n    @property\n    def transport(self) -> QuotaControllerTransport:\n        \n        return self._client.transport\n\n    get_transport_class = functools.partial(\n        type(QuotaControllerClient).get_transport_class, type(QuotaControllerClient)\n    )\n\n    def __init__(\n        self,\n        *,\n        credentials: credentials.Credentials = None,\n        transport: Union[str, QuotaControllerTransport] = \"grpc_asyncio\",\n        client_options: ClientOptions = None,\n        client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,\n    ) -> None:\n        \n\n        self._client = QuotaControllerClient(\n            credentials=credentials,\n            transport=transport,\n            client_options=client_options,\n            client_info=client_info,\n        )\n\n    async def allocate_quota(\n        self,\n        request: quota_controller.AllocateQuotaRequest = None,\n        *,\n        retry: retries.Retry = gapic_v1.method.DEFAULT,\n        timeout: float = None,\n        metadata: Sequence[Tuple[str, str]] = (),\n    ) -> quota_controller.AllocateQuotaResponse:\n        r\n        \n\n        request = quota_controller.AllocateQuotaRequest(request)\n\n        \n        \n        rpc = gapic_v1.method_async.wrap_method(\n            self._client._transport.allocate_quota,\n            default_timeout=None,\n            client_info=DEFAULT_CLIENT_INFO,\n        )\n\n        \n        response = await rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n\n        \n        return response\n\n\ntry:\n    DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo(\n        gapic_version=pkg_resources.get_distribution(\n            \"google-cloud-service-control\",\n        ).version,\n    )\nexcept pkg_resources.DistributionNotFound:\n    DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo()\n\n\n__all__ = (\"QuotaControllerAsyncClient\",)\n",
        "summary": "This Python code defines an asynchronous client for the Google Cloud Service Control API, providing methods to allocate quota and manage resources. It includes utility functions for parsing resource paths and handling authentication through service account credentials."
    },
    {
        "code": "import os\nimport sys\n\nimport django\nfrom django.core.urlresolvers import reverse\nfrom django.db import DatabaseError\nfrom django.db.models import Count\nfrom django.http import HttpResponse, Http404\nfrom django.shortcuts import redirect, get_object_or_404\nfrom django.utils import six\nfrom django.views.generic.base import TemplateView\nfrom django.views.generic.detail import DetailView\nfrom django.views.generic.list import ListView\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom .models import Talk, Photo, Speaker, Event, Tutorial, Vote\nfrom .utils import subscribe_mail, validate_email, set_vote_cookie, can_vote\n\n\nclass IndexPage(ListView):\n    template_name = 'index.html'\n    context_object_name = 'events'\n\n    def get_queryset(self):\n        if self.request.user.is_staff:\n            qs = Event.objects.all()\n        else:\n            qs = Event.archived.all()\n\n        return qs.prefetch_related('talks', 'talks__speaker', 'talks__event')[:3]\n\n    def get_context_data(self, **kwargs):\n        context = super(IndexPage, self).get_context_data(**kwargs)\n\n        \n        \n        \n        \n\n        context.update({\n            'speakers': Speaker.objects.order_by(\"?\")[:10],\n            'main_event': Event.spotlight(self.request.user.is_staff),\n            'show_more_link': True,\n            'can_vote': can_vote(self.request)\n        })\n        return context\n\n\nclass EventsList(ListView):\n    template_name = 'event_list.html'\n    queryset = Event.visible.prefetch_related('talks', 'talks__speaker', 'talks__event')\n    context_object_name = 'events'\n\n    def get_queryset(self):\n        if self.request.user.is_staff:\n            qs = Event.objects.all()\n        else:\n            qs = Event.visible.all()\n        return qs.prefetch_related('talks', 'talks__speaker', 'talks__event')\n\n\nclass EventPage(DetailView):\n    template_name = 'event.html'\n    slug_url_kwarg = 'number'\n    slug_field = 'number'\n\n    def get_queryset(self):\n        if self.request.user.is_staff:\n            return Event.objects.all()\n        return Event.visible.all()\n\n    def get_object(self, queryset=None):\n        \n        \n        if queryset is None:\n            queryset = self.get_queryset()\n\n        \n        pk = self.kwargs.get(self.pk_url_kwarg)\n        slug = self.kwargs.get(self.slug_url_kwarg)\n        if pk is not None:\n            queryset = queryset.filter(pk=pk)\n\n        \n        if slug is not None and (pk is None or self.query_pk_and_slug):\n            slug_field = self.get_slug_field()\n            queryset = queryset.filter(**{slug_field: slug})\n\n        \n        if pk is None and slug is None:\n            raise AttributeError(\"Generic detail view %s must be called with \"\n                                 \"either an object pk or a slug.\"\n                                 % self.__class__.__name__)\n\n        try:\n            \n            obj = queryset.get()\n        except queryset.model.MultipleObjectsReturned:\n            obj = queryset.latest(\"date\")\n        except queryset.model.DoesNotExist:\n            raise Http404\n        return obj\n\n    def get_context_data(self, **kwargs):\n        context = super(EventPage, self).get_context_data(**kwargs)\n        context.update({\n            'photos': context['event'].photos.all(),\n            'can_vote': can_vote(self.request),\n        })\n        return context\n\n\nclass TalkPage(DetailView):\n    template_name = 'talk.html'\n    slug_url_kwarg = 'talk_slug'\n\n    def get_queryset(self):\n        if self.request.user.is_staff:\n            return Talk.objects.select_related('event', 'speaker')\n        return Talk.objects.active().select_related('event', 'speaker')\n\n    def get(self, request, *args, **kwargs):\n        self.object = self.get_object()\n\n        \n        if self.object.get_absolute_url() != request.path:\n            return redirect(self.object)\n\n        context = self.get_context_data(object=self.object)\n        return self.render_to_response(context)\n\n\nclass SpeakerList(ListView):\n    template_name = 'speakers.html'\n    queryset = Speaker.objects.all().order_by('name')\n    context_object_name = 'speakers'\n\n\nclass SpeakerPage(DetailView):\n    template_name = 'speaker.html'\n\n    def get_object(self, queryset=None):\n        return get_object_or_404(\n            Speaker.objects.prefetch_related('talks', 'talks__event'),\n            slug=self.kwargs['slug']\n        )\n\n\nclass AboutPage(TemplateView):\n    template_name = 'about.html'\n\n    def get_context_data(self, **kwargs):\n        context = super(AboutPage, self).get_context_data(**kwargs)\n        context.update({\n            'photos': Photo.objects.all().order_by('-pk')[:10]\n        })\n        return context\n\n\nclass LivePage(TemplateView):\n    template_name = 'live.html'\n\n    def get_context_data(self, **kwargs):\n        context = super(LivePage, self).get_context_data(**kwargs)\n\n        context.update({\n            'event': Event.spotlight(),\n        })\n        return context\n\n\nclass TutorialList(ListView):\n    template_name = 'tutorials.html'\n    queryset = Tutorial.objects.all().order_by('title')\n    context_object_name = 'tutorials'\n\n\nclass TutorialPage(DetailView):\n    template_name = 'tutorial.html'\n    model = Tutorial\n\n\nclass Py3Page(TemplateView):\n    template_name = 'py3.html'\n\n    def get_context_data(self, **kwargs):\n        context = super(Py3Page, self).get_context_data(**kwargs)\n\n        context.update({\n            'django': django.get_version(),\n            'python': sys.version,\n            'py3': six.PY3,\n        })\n        return context\n\n\nclass VoteResults(TemplateView):\n    template_name = 'vote_results.html'\n\n    def get_context_data(self, **kwargs):\n        context = super(VoteResults, self).get_context_data(**kwargs)\n        talks = Talk.objects.filter(event=Event.spotlight()).annotate(num_votes=Count(\"votes\"))\n\n        talks_votes = [talk.num_votes for talk in talks]\n        votes_total = sum(talks_votes)\n        votes_max = max(talks_votes)\n        if votes_total:\n            for talk in talks:\n                talk.votes_percent = int(talk.num_votes * 100 / votes_total)\n                if talk.num_votes == votes_max:\n                    talk.is_leader = True\n        context.update({\n            'talks': talks,\n        })\n\n        return context\n\n\n@csrf_exempt\ndef ajax_vote(request, *args, **kwargs):\n    if request.method == 'POST':\n        if not can_vote(request):\n            return HttpResponse(u'\u041c\u043e\u0436\u043d\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430 \u043e\u0434\u0438\u043d \u0434\u043e\u043a\u043b\u0430\u0434', status=409)\n        try:\n            event = Talk.objects.get(pk=kwargs['talk_id']).event\n            if not event.votable:\n                return HttpResponse('Voting is closed, sorry', status=409)\n            Vote.objects.create(talk_id=kwargs['talk_id'],\n                                event=event,\n                                ua=request.META.get('HTTP_USER_AGENT'),\n                                ip=request.META.get('REMOTE_ADDR'))\n            response = HttpResponse(reverse('vote-results'))\n            response = set_vote_cookie(response)\n            return response\n        except DatabaseError:\n            return HttpResponse('DB error, sorry', status=402)\n    return HttpResponse('Only POST', status=402)\n\n\ndef confirm_ownership(request, *args, **kwargs):\n    content = os.environ.get('CONFIRM_OWNERSHIP_%s' % kwargs['filename'], None)\n    if content:\n        content_type = 'text/html' if kwargs['filename'].endswith('.html') else 'text/plain'\n        return HttpResponse(content, content_type=content_type)\n    else:\n        raise Http404\n",
        "summary": "The provided Python code defines a Django application with various views for handling different types of pages such as index, events, talks, speakers, tutorials, and vote results. It includes functionalities like listing, detail viewing, voting, and AJAX-based operations. The application also handles user authentication and permissions, ensuring that certain actions are restricted to staff members or based on voting eligibility."
    },
    {
        "code": "import unittest\nfrom repeater import repeater\n\n\ndef test_repeater(benchmark):\n    assert benchmark(repeater,'a',5) == 'aaaaa'\n    assert benchmark(repeater,'Wub', 6 ) == 'Wub Wub Wub Wub Wub Wub '\n",
        "summary": "The provided Python code defines a unit test function `test_repeater` that uses the `benchmark` fixture to measure and verify the performance of the `repeater` function from the `repeater.py` module. The test checks if the `repeater` function correctly repeats strings as expected, ensuring it behaves as intended for both single characters and longer strings with specified repetition counts."
    },
    {
        "code": "from distutils.core import setup\nimport py2exe , sys, os\n\n\n\nsys.argv.append(\"py2exe\")\nsetup(\n    options = {'py2exe': {'bundle_files': 1}},\n \n    windows = [{'script': \"DNS.py\", 'uac_info': \"requireAdministrator\"}],\n    zipfile = None,\n    \n)\n",
        "summary": "This Python script uses the `distutils.core` and `py2exe` modules to package a Windows executable from a Python script named \"DNS.py\". The setup configuration includes options for bundling files into a single executable and requesting administrative privileges when the application is run."
    },
    {
        "code": "from os import listdir, path\nfrom types import GeneratorType\n\nimport six\n\nfrom pyinfra import logger, pseudo_inventory\nfrom pyinfra.api.inventory import Inventory\nfrom pyinfra_cli.util import exec_file\n\n\nALLOWED_HOST_TYPES = tuple(\n    six.string_types + (tuple,),\n)\n\n\nALLOWED_DATA_TYPES = tuple(\n    six.integer_types\n    + (six.text_type, six.binary_type)\n    + (bool, dict, list, set, tuple, float, complex),\n)\n\n\ndef _is_inventory_group(key, value):\n    \n\n    if (\n        key.startswith('_')\n        or not isinstance(value, (list, tuple, GeneratorType))\n    ):\n        return False\n\n    \n    if isinstance(value, tuple):\n        value = value[0]\n\n    \n    if isinstance(value, GeneratorType):\n        value = list(value)\n\n    return all(\n        isinstance(item, ALLOWED_HOST_TYPES)\n        for item in value\n    )\n\n\ndef _is_group_data(key, value):\n    \n\n    return (\n        isinstance(value, ALLOWED_DATA_TYPES)\n        and not key.startswith('_')\n    )\n\n\ndef _get_group_data(deploy_dir):\n    group_data = {}\n    group_data_directory = path.join(deploy_dir, 'group_data')\n\n    if path.exists(group_data_directory):\n        files = listdir(group_data_directory)\n\n        for file in files:\n            if not file.endswith('.py'):\n                continue\n\n            group_data_file = path.join(group_data_directory, file)\n            group_name = path.basename(file)[:-3]\n\n            logger.debug('Looking for group data in: {0}'.format(group_data_file))\n\n            \n            attrs = exec_file(group_data_file, return_locals=True)\n\n            group_data[group_name] = {\n                key: value\n                for key, value in six.iteritems(attrs)\n                if _is_group_data(key, value)\n            }\n\n    return group_data\n\n\ndef _get_groups_from_filename(inventory_filename):\n    attrs = exec_file(inventory_filename, return_locals=True)\n\n    return {\n        key: value\n        for key, value in six.iteritems(attrs)\n        if _is_inventory_group(key, value)\n    }\n\n\ndef make_inventory(\n    inventory_filename,\n    deploy_dir=None,\n    ssh_port=None,\n    ssh_user=None,\n    ssh_key=None,\n    ssh_key_password=None,\n    ssh_password=None,\n    winrm_username=None,\n    winrm_password=None,\n    winrm_port=None,\n):\n    \n\n    if ssh_port is not None:\n        ssh_port = int(ssh_port)\n\n    file_groupname = None\n\n    \n    if not path.exists(inventory_filename):\n        groups = {\n            'all': inventory_filename.split(','),\n        }\n    else:\n        groups = _get_groups_from_filename(inventory_filename)\n        \n        \n        file_groupname = path.basename(inventory_filename).rsplit('.')[0]\n\n    all_data = {}\n\n    if 'all' in groups:\n        all_hosts = groups.pop('all')\n\n        if isinstance(all_hosts, tuple):\n            all_hosts, all_data = all_hosts\n\n    \n    else:\n        all_hosts = []\n        for hosts in groups.values():\n            \n            hosts = hosts[0] if isinstance(hosts, tuple) else hosts\n\n            for host in hosts:\n                \n                hostname = host[0] if isinstance(host, tuple) else host\n\n                if hostname not in all_hosts:\n                    all_hosts.append(hostname)\n\n    groups['all'] = (all_hosts, all_data)\n\n    \n    if file_groupname and file_groupname not in groups:\n        groups[file_groupname] = all_hosts\n\n    \n    \n    \n    \n    \n    logger.debug('Creating fake inventory...')\n\n    fake_groups = {\n        \n        name: group if isinstance(group, tuple) else (group, {})\n        for name, group in six.iteritems(groups)\n    }\n    fake_inventory = Inventory((all_hosts, all_data), **fake_groups)\n    pseudo_inventory.set(fake_inventory)\n\n    \n    group_data = _get_group_data(deploy_dir)\n\n    \n    pseudo_inventory.reset()\n\n    \n    for name, hosts in six.iteritems(groups):\n        data = {}\n\n        if isinstance(hosts, tuple):\n            hosts, data = hosts\n\n        if name in group_data:\n            data.update(group_data.pop(name))\n\n        \n        groups[name] = (hosts, data)\n\n    \n    \n    \n    for name, data in six.iteritems(group_data):\n        groups[name] = ([], data)\n\n    return Inventory(\n        groups.pop('all'),\n        ssh_user=ssh_user,\n        ssh_key=ssh_key,\n        ssh_key_password=ssh_key_password,\n        ssh_port=ssh_port,\n        ssh_password=ssh_password,\n        winrm_username=winrm_username,\n        winrm_password=winrm_password,\n        winrm_port=winrm_port,\n        **groups\n    ), file_groupname and file_groupname.lower()\n",
        "summary": "The provided Python code defines a function `make_inventory` that constructs an inventory object for use with the Pyinfra library. It reads group data from files, processes host information, and configures SSH and WinRM settings to create a dynamic inventory suitable for deployment tasks."
    },
    {
        "code": "def test_constants_only():\n  try:\n    from pants.constants_only.constants import VALID_IDENTIFIERS \n  except ImportError as e:\n    assert False, 'Failed to correctly generate python package: %s' % e\n",
        "summary": "The function `test_constants_only` attempts to import the `VALID_IDENTIFIERS` constant from a module named `constants_only.constants`. If the import fails due to an `ImportError`, it asserts that the Python package generation has failed, providing the error message as part of the assertion."
    },
    {
        "code": "from struct import (unpack_from, calcsize)\n\nfrom bglcapi.types import MessageType\n\nfrom . import rsp\nfrom . import evt\n\nPARSE_MAP = {\n    MessageType.COMMAND_RESPONSE: {\n        0x00: rsp.message_to_target,\n    },\n    MessageType.EVENT: {\n        0x00: evt.message_to_host,\n    },\n}\n\n\ndef from_binary(msg_type: int, msg_id: int, data: bytes, offset: int):\n    return PARSE_MAP[msg_type][msg_id](data, offset)\n",
        "summary": "The provided Python code defines a mapping between message types and their corresponding parsing functions using the `PARSE_MAP` dictionary. The `from_binary` function uses this map to call the appropriate parsing function based on the message type (`msg_type`) and ID (`msg_id`), passing along the binary data and an offset for processing."
    },
    {
        "code": "import itertools\n\nimport toposort\n\nfrom populus.utils.contracts import (\n    compute_direct_dependency_graph,\n    compute_recursive_contract_dependencies,\n)\n\n\ndef compute_deploy_order(dependency_graph):\n    \n    return toposort.toposort_flatten(dict(dependency_graph))\n\n\ndef get_deploy_order(contracts_to_deploy, compiled_contracts):\n    \n    dependency_graph = compute_direct_dependency_graph(compiled_contracts.values())\n    global_deploy_order = compute_deploy_order(dependency_graph)\n\n    \n    \n    all_deploy_dependencies = set(itertools.chain.from_iterable(\n        compute_recursive_contract_dependencies(contract_name, dependency_graph)\n        for contract_name in contracts_to_deploy\n    ))\n    all_contracts_to_deploy = all_deploy_dependencies.union(contracts_to_deploy)\n\n    \n    \n    deploy_order = tuple(\n        contract_name\n        for contract_name\n        in global_deploy_order\n        if contract_name in all_contracts_to_deploy\n    )\n    return deploy_order\n",
        "summary": "The provided Python code defines functions to compute the deployment order of contracts based on their dependencies. It uses a topological sort algorithm from the `toposort` library and custom utility functions from the `populus.utils.contracts` module to determine the order in which contracts should be deployed, ensuring that all dependencies are resolved before deploying any contract."
    },
    {
        "code": "import sys\nimport numpy\nfrom scipy.interpolate import griddata\nimport matplotlib.pyplot as plt\n\n\ntry:\n  from lxml import etree\nexcept ImportError:\n  try:\n    \n    import xml.etree.cElementTree as etree\n  except ImportError:\n    try:\n      \n      import xml.etree.ElementTree as etree\n    except ImportError:\n      try:\n        \n        import cElementTree as etree\n      except ImportError:\n        try:\n          \n          import elementtree.ElementTree as etree\n        except ImportError:\n          print(\"Failed to import ElementTree from any known place\")\n\n\n\n\ndef parse_xml(filename):\n\ttree = etree.parse(open(filename, 'rb'))\n\n\tts = []\n\tbssid = []\n\tsignal = []\n\tlat = []\n\tlon = []\n\twalked_lon = []\n\twalked_lat = []\n\n\tfor z in tree.findall('.//gps-point'):\n\t\t\n\t\t\n\t\t\n\n\t\tif z.get('bssid') == 'GP:SD:TR:AC:KL:OG':\n\t\t\twalked_lon.append(float(z.get('lon')))\n\t\t\twalked_lat.append(float(z.get('lat')))\n\n\t\telif z.get('signal_dbm') is not None:\n\t\t\tbssid.append(z.get('bssid'))\n\t\t\tts.append(int(z.get('time-sec')))\n\t\t\tlat.append(float(z.get('lat')))\n\t\t\tlon.append(float(z.get('lon')))\n\t\t\tsignal.append(int(z.get('signal_dbm')))\n\n\treturn (ts, bssid, signal, lat, lon, walked_lon, walked_lat,)\n\n\n\n\ndef draw_data(ts, bssid, signal, lat, lon, walked_lon, walked_lat):\n\n\t\n\tgrid_x, grid_y = numpy.mgrid[min(walked_lon):max(walked_lon):1000j, min(walked_lat):max(walked_lat):1000j]\n\n\t\n\tbssids = list(set(bssid))\n\n\t\n\tfor s in bssids:\n\t\tpoints_lon = []\n\t\tpoints_lat = []\n\t\tvalues = []\n\t\th = []\n\t\t\n\t\t\n\t\t\n\t\tfor i in range(0, len(bssid)):\n\t\t\tif bssid[i] == s:\n\t\t\t\thc = hash((lon[i], lat[i]))\n\t\t\t\tif hc not in h:\n\t\t\t\t\tpoints_lon.append(lon[i])\n\t\t\t\t\tpoints_lat.append(lat[i])\n\t\t\t\t\tvalues.append(float(signal[i]))\n\t\t\t\t\th.append(hash((lon[i], lat[i])))\n\n\t\t\n\t\tfor i in range(0, len(walked_lon)):\n\t\t\thc = hash((walked_lon[i], walked_lat[i]))\n\t\t\tif hc not in h:\n\t\t\t\tpoints_lon.append(lon[i])\n\t\t\t\tpoints_lat.append(lat[i])\n\t\t\t\tvalues.append(float(-100))\n\t\t\t\th.append(hash((walked_lon[i], walked_lat[i])))\n\n\t\t\n\t\tgrid = griddata((points_lon, points_lat), numpy.array(values), (grid_x, grid_y), method='cubic')\n\n\t\t\n\t\tplt.show()\n\t\tplt.imsave('%s.png' % (s), grid.T)\n\n\t\t\n\t\ta = ((max(walked_lon)-min(walked_lon))/1000)\n\t\tb = 0\n\t\tc = 0\n\t\td = ((max(walked_lat)-min(walked_lat))/1000)\n\t\te = min(walked_lon)\n\t\tf = min(walked_lat)\n\n\t\t\n\t\topen('%s.pngw' % (s), 'w').write('%.16f\\n%d\\n%d\\n%.16f\\n%.16f\\n%.16f' % (a, b, c, d, e, f,))\n\nif __name__ == \"__main__\":\n\tif len(sys.argv) != 2:\n\t\tprint(\"Usage %s << /path/to/Kismet.gpsxml >>\" % (sys.argv[0]))\n\t\tsys.exit(-1)\n\t\n\tdraw_data(*parse_xml(sys.argv[1]))\n\n\n\t\n",
        "summary": "The Python script reads GPS data from a Kismet XML file, filters and processes the data based on signal strength and BSSID, interpolates the signal values over a grid, and saves the interpolated data as images with georeferencing metadata."
    },
    {
        "code": "from pprint import pformat\nfrom six import iteritems\n\n\nclass ThinVolumeReinitializeDescriptor(object):\n    \n    def __init__(self):\n        \n        self.swagger_types = {\n            'thin_vol': 'str',  \n            'init_params': 'ThinVolumeReinitializeParams'\n        }\n\n        self.attribute_map = {\n            'thin_vol': 'thinVol',  \n            'init_params': 'initParams'\n        }\n\n        self._thin_vol = None\n        self._init_params = None\n\n    @property\n    def thin_vol(self):\n        \n        return self._thin_vol\n\n    @thin_vol.setter\n    def thin_vol(self, thin_vol):\n        \n        self._thin_vol = thin_vol\n\n    @property\n    def init_params(self):\n        \n        return self._init_params\n\n    @init_params.setter\n    def init_params(self, init_params):\n        \n        self._init_params = init_params\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        if self is None:\n           return None\n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if self is None or other is None:\n            return None\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n\n",
        "summary": "The `ThinVolumeReinitializeDescriptor` class defines a descriptor for reinitializing thin volumes with attributes and methods to manage properties like `thin_vol` and `init_params`, convert the object to a dictionary or string representation, and compare instances for equality."
    },
    {
        "code": "__docformat__ = \"numpy\"\n\nimport argparse\nfrom typing import List\n\nimport pandas as pd\nimport numpy as np\n\nfrom gamestonk_terminal.helper_funcs import (\n    parse_known_args_and_warn,\n    check_non_negative,\n)\n\n\n\n\ndef load(other_args: List[str]) -> str:\n    \n    parser = argparse.ArgumentParser(\n        add_help=False,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        prog=\"opload\",\n        description=\"Load a ticker into option menu\",\n    )\n\n    parser.add_argument(\n        \"-t\",\n        \"--ticker\",\n        action=\"store\",\n        dest=\"ticker\",\n        required=\"-h\" not in other_args,\n        help=\"Stock ticker\",\n    )\n\n    try:\n        if other_args:\n            if \"-t\" not in other_args and \"-h\" not in other_args:\n                other_args.insert(0, \"-t\")\n\n        ns_parser = parse_known_args_and_warn(parser, other_args)\n        if not ns_parser:\n            return \"\"\n        print(\"\")\n        return ns_parser.ticker\n    except Exception as e:\n        print(e, \"\\n\")\n        return \"\"\n    except SystemExit:\n        print(\"\")\n        return \"\"\n\n\n\n\n\ndef select_option_date(avalaiable_dates: List[str], other_args: List[str]) -> str:\n    \n    parser = argparse.ArgumentParser(\n        add_help=False,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        prog=\"exp\",\n        description=\"See and set expiration date\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--date\",\n        dest=\"n_date\",\n        action=\"store\",\n        type=int,\n        default=-1,\n        choices=range(len(avalaiable_dates)),\n        help=\"Select index for expiry date.\",\n    )\n\n    parser.add_argument(\n        \"-D\",\n        dest=\"date\",\n        type=str,\n        choices=avalaiable_dates + [\"\"],\n        help=\"Select date (YYYY-MM-DD)\",\n        default=\"\",\n    )\n\n    try:\n        if other_args:\n            if \"-\" not in other_args[0]:\n                other_args.insert(0, \"-d\")\n\n        ns_parser = parse_known_args_and_warn(parser, other_args)\n        if not ns_parser:\n            return \"\"\n\n        \n        if ns_parser.n_date == -1 and not ns_parser.date:\n            print(\"\\nAvailable expiry dates:\")\n            for i, d in enumerate(avalaiable_dates):\n                print(f\"   {(2 - len(str(i))) * ' '}{i}.  {d}\")\n            print(\"\")\n            return \"\"\n\n        \n        else:\n            if ns_parser.date:\n                if ns_parser.date in avalaiable_dates:\n                    print(f\"Expiraration set to {ns_parser.date} \\n\")\n                    return ns_parser.date\n                else:\n                    print(\"Expiration not an option\")\n                    return \"\"\n            else:\n                expiry_date = avalaiable_dates[ns_parser.n_date]\n                print(f\"Expiraration set to {expiry_date} \\n\")\n                return expiry_date\n\n    except Exception as e:\n        print(e, \"\\n\")\n        return \"\"\n\n\ndef get_loss_at_strike(strike: float, chain: pd.DataFrame) -> float:\n    \n\n    itm_calls = chain[chain.index < strike][[\"OI_call\"]]\n    itm_calls[\"loss\"] = (strike - itm_calls.index) * itm_calls[\"OI_call\"]\n    call_loss = itm_calls[\"loss\"].sum()\n\n    itm_puts = chain[chain.index > strike][[\"OI_put\"]]\n    itm_puts[\"loss\"] = (itm_puts.index - strike) * itm_puts[\"OI_put\"]\n    put_loss = itm_puts.loss.sum()\n    loss = call_loss + put_loss\n\n    return loss\n\n\ndef calculate_max_pain(chain: pd.DataFrame) -> int:\n    \n\n    strikes = np.array(chain.index)\n    if (\"OI_call\" not in chain.columns) or (\"OI_put\" not in chain.columns):\n        print(\"Incorrect columns.  Unable to parse max pain\")\n        return np.nan\n\n    loss = []\n    for price_at_exp in strikes:\n        loss.append(get_loss_at_strike(price_at_exp, chain))\n\n    chain[\"loss\"] = loss\n    max_pain = chain[\"loss\"].idxmin()\n\n    return max_pain\n\n\ndef vol(other_args: List[str]):\n    \n\n    parser = argparse.ArgumentParser(\n        add_help=False,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        prog=\"vol\",\n        description=\"Plot volume.  Volume refers to the number of contracts traded today.\",\n    )\n\n    parser.add_argument(\n        \"-m\",\n        \"--min\",\n        default=-1,\n        type=check_non_negative,\n        help=\"Min strike to plot\",\n        dest=\"min\",\n    )\n    parser.add_argument(\n        \"-M\",\n        \"--max\",\n        default=-1,\n        type=check_non_negative,\n        help=\"Max strike to plot\",\n        dest=\"max\",\n    )\n\n    parser.add_argument(\n        \"--calls\",\n        action=\"store_true\",\n        default=False,\n        dest=\"calls\",\n        help=\"Flag to plot call options only\",\n    )\n\n    parser.add_argument(\n        \"--puts\",\n        action=\"store_true\",\n        default=False,\n        dest=\"puts\",\n        help=\"Flag to plot put options only\",\n    )\n\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        default=\"tr\",\n        choices=[\"tr\", \"yf\"],\n        dest=\"source\",\n        help=\"Source to get data from\",\n    )\n\n    try:\n        ns_parser = parse_known_args_and_warn(parser, other_args)\n        if not ns_parser:\n            return\n\n        return ns_parser\n\n    except Exception as e:\n        print(e, \"\\n\")\n\n\ndef voi(other_args: List[str]):\n    \n    parser = argparse.ArgumentParser(\n        add_help=False,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        prog=\"voi\",\n        description=,\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--minv\",\n        dest=\"min_vol\",\n        type=check_non_negative,\n        default=-1,\n        help=\"minimum volume (considering open interest) threshold of the plot.\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--min\",\n        dest=\"min_sp\",\n        type=check_non_negative,\n        default=-1,\n        help=\"minimum strike price to consider in the plot.\",\n    )\n    parser.add_argument(\n        \"-M\",\n        \"--max\",\n        dest=\"max_sp\",\n        type=check_non_negative,\n        default=-1,\n        help=\"maximum strike price to consider in the plot.\",\n    )\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        default=\"tr\",\n        choices=[\"tr\", \"yf\"],\n        dest=\"source\",\n        help=\"Source to get data from\",\n    )\n    try:\n        ns_parser = parse_known_args_and_warn(parser, other_args)\n        if not ns_parser:\n            return None\n        return ns_parser\n\n    except Exception as e:\n        print(e, \"\\n\")\n        return None\n\n\ndef oi(other_args: List[str]):\n    \n\n    parser = argparse.ArgumentParser(\n        add_help=False,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        prog=\"oi\",\n        description=\"Plot open interest.  Open interest represents the number of contracts that exist.\",\n    )\n\n    parser.add_argument(\n        \"-m\",\n        \"--min\",\n        default=-1,\n        type=check_non_negative,\n        help=\"Min strike to plot\",\n        dest=\"min\",\n    )\n    parser.add_argument(\n        \"-M\",\n        \"--max\",\n        default=-1,\n        type=check_non_negative,\n        help=\"Max strike to plot\",\n        dest=\"max\",\n    )\n\n    parser.add_argument(\n        \"--calls\",\n        action=\"store_true\",\n        default=False,\n        dest=\"calls\",\n        help=\"Flag to plot call options only\",\n    )\n\n    parser.add_argument(\n        \"--puts\",\n        action=\"store_true\",\n        default=False,\n        dest=\"puts\",\n        help=\"Flag to plot put options only\",\n    )\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        default=\"tr\",\n        choices=[\"tr\", \"yf\"],\n        dest=\"source\",\n        help=\"Source to get data from\",\n    )\n\n    try:\n\n        ns_parser = parse_known_args_and_warn(parser, other_args)\n\n        if not ns_parser:\n            return None\n\n        return ns_parser\n\n    except Exception as e:\n        print(e, \"\\n\")\n        return None\n",
        "summary": "The provided Python code defines several functions for handling option data, including loading a ticker, selecting an expiration date, calculating max pain, and plotting volume or open interest. Each function uses the `argparse` module to parse command-line arguments and perform specific tasks related to options trading."
    },
    {
        "code": "from . import _version\nfrom .core import CondaEnv, CondaPackException, File, pack\n\n__version__ = _version.get_versions()['version']\n",
        "summary": "The Python code imports version information and core functionalities from a module, defining a class for managing conda environments and exceptions related to the process, as well as functions for packing files. It also sets the `__version__` attribute using the `_version` module to reflect the current version of the package."
    },
    {
        "code": "from abc import ABCMeta, abstractmethod\nfrom multiprocessing import Process, Value\nimport numpy as np\nfrom flare.common.log import GameLogEntry\nfrom flare.common.communicator import AgentCommunicator\nfrom flare.common.replay_buffer import NoReplacementQueue, ReplayBuffer, Experience\n\n\nclass AgentHelper(object):\n    \n    __metaclass__ = ABCMeta\n\n    def __init__(self, name, communicator, sample_interval):\n        assert isinstance(communicator, AgentCommunicator)\n        self.name = name\n        self.comm = communicator\n        self.counter = 0\n        assert sample_interval >= 2\n        self.sample_interval = sample_interval\n\n    def unpack_exps(self, exp_seqs):\n        \n\n        def concat_lists(lists):\n            return [x for l in lists for x in l]\n\n        def extract_key(seq, k):\n            assert seq\n            return [e.val(k) for e in seq]\n\n        ret = dict(\n            inputs={},\n            next_inputs={},\n            next_alive={},\n            rewards={},\n            actions={},\n            next_actions={},\n            states=None,\n            next_states=None)\n\n        for k in self.input_keys:\n            ipt_seqs = [extract_key(exp_seq, k) for exp_seq in exp_seqs]\n            ret[\"inputs\"][k] = [ipt_seq[:-1] for ipt_seq in ipt_seqs]\n            ret[\"next_inputs\"][k] = [ipt_seq[1:] for ipt_seq in ipt_seqs]\n\n        for k in self.action_keys:\n            act_seqs = [extract_key(exp_seq, k) for exp_seq in exp_seqs]\n            ret[\"actions\"][k] = [act_seq[:-1] for act_seq in act_seqs]\n            ret[\"next_actions\"][k] = [act_seq[1:] for act_seq in act_seqs]\n\n        for k in self.reward_keys:\n            ret[\"rewards\"][\n                k] = [extract_key(exp_seq[:-1], k) for exp_seq in exp_seqs]\n\n        if self.state_keys:\n            ret[\"states\"] = dict()\n            ret[\"next_states\"] = dict()\n\n        for k in self.state_keys:\n            \n            ret[\"states\"][\n                k] = [extract_key(exp_seq[:1], k)[0] for exp_seq in exp_seqs]\n            ret[\"next_states\"][k] = [\n                extract_key(exp_seq[1:2], k)[0] for exp_seq in exp_seqs\n            ]\n\n        ret[\"next_alive\"][\"alive\"] \\\n            = [extract_key(exp_seq[1:], \"alive\") for exp_seq in exp_seqs]\n\n        \n        \n        if not self.state_keys:\n            \n            for k in ret.keys():\n                if ret[k] is not None:\n                    for kk in ret[k].keys():\n                        ret[k][kk] = concat_lists(ret[k][kk])\n\n        return ret, len(exp_seqs)\n\n    def predict(self, inputs, states=dict()):\n        \n        data = dict(inputs=inputs, states=states)\n        self.comm.put_prediction_data(data, 1)\n        ret = self.comm.get_prediction_return()\n        return ret\n\n    @abstractmethod\n    def add_experience(self, e):\n        \n        pass\n\n    def _store_data(self, alive, data):\n        \n        assert isinstance(data, dict)\n        data[\"alive\"] = [alive]\n        t = Experience(data)\n        self.add_experience(t)\n        self.counter += 1\n        if self.counter % self.sample_interval == 0:\n            return self.learn()\n\n    @abstractmethod\n    def sample_experiences(self):\n        \n        pass\n\n    def learn(self):\n        \n        exp_seqs = self.sample_experiences()\n        if not exp_seqs:\n            return\n        data, size = self.unpack_exps(exp_seqs)\n        self.comm.put_training_data(data, size)\n        ret = self.comm.get_training_return()\n        return ret\n\n\nclass OnlineHelper(AgentHelper):\n    \n\n    def __init__(self, name, communicator, sample_interval=5):\n        super(OnlineHelper, self).__init__(name, communicator, sample_interval)\n        \n        self.exp_queue = NoReplacementQueue()\n\n    @staticmethod\n    def exp_replay():\n        return False\n\n    def add_experience(self, e):\n        self.exp_queue.add(e)\n\n    def sample_experiences(self):\n        return self.exp_queue.sample()\n\n\nclass ExpReplayHelper(AgentHelper):\n    \n\n    def __init__(self,\n                 name,\n                 communicator,\n                 buffer_capacity,\n                 num_experiences,\n                 sample_interval=5,\n                 num_seqs=1):\n        super(ExpReplayHelper, self).__init__(name, communicator,\n                                              sample_interval)\n        \n        self.replay_buffer = ReplayBuffer(buffer_capacity)\n        self.num_experiences = num_experiences\n        self.num_seqs = num_seqs\n\n    @staticmethod\n    def exp_replay():\n        return True\n\n    def add_experience(self, e):\n        self.replay_buffer.add(e)\n\n    def sample_experiences(self):\n        return self.replay_buffer.sample(self.num_experiences, self.num_seqs)\n\n\nclass Agent(Process):\n    \n    __metaclass__ = ABCMeta\n\n    def __init__(self, num_games, actrep, learning):\n        super(Agent, self).__init__()\n        self.id = -1  \n        self.num_games = num_games\n        self.learning = learning\n        self.state_specs = None\n        self.helpers = {}\n        self.log_q = None\n        self.running = Value('i', 0)\n        self.daemon = True  \n        self.alive = 1\n        self.env_f = None\n        self.actrep = actrep\n\n    def set_env(self, env_class, *args, **kwargs):\n        \n        self.env_f = lambda: env_class(*args, **kwargs)\n\n    def add_agent_helper(self, helper, input_keys, action_keys, state_keys,\n                         reward_keys):\n        \n        assert isinstance(helper, AgentHelper)\n        helper.input_keys = input_keys\n        helper.action_keys = action_keys\n        helper.state_keys = state_keys\n        helper.reward_keys = reward_keys\n        self.helpers[helper.name] = helper\n\n    def _make_zero_states(self, prop):\n        dtype = prop[\"dtype\"] if \"dtype\" in prop else \"float32\"\n        return np.zeros(prop[\"shape\"]).astype(dtype)\n\n    \n    \n    def predict(self, alg_name, inputs, states=dict()):\n        \n        \n        \n        \n        \n        \n        inputs_ = {k: [v] for k, v in inputs.items()}\n        states_ = {k: [v] for k, v in states.items()}\n        prediction, next_states = self.helpers[alg_name].predict(inputs_,\n                                                                 states_)\n        \n        prediction = {k: v[0] for k, v in prediction.items()}\n        next_states = {k: v[0] for k, v in next_states.items()}\n        return prediction, next_states\n\n    def run(self):\n        \n        assert self.env_f is not None, \"You should first call self.set_env()!\"\n        \n        \n        self.env = self.env_f()\n        self.running.value = 1\n        for i in range(self.num_games):\n            self._run_one_episode()\n            if not self.running.value:\n                return\n        self.running.value = 0\n\n    def _store_data(self, alg_name, data):\n        if self.learning:  \n            return self.helpers[alg_name]._store_data(self.alive, data)\n\n    def _run_one_episode(self):\n        def __store_data(observations, actions, states, rewards):\n            learning_ret = self._cts_store_data(observations, actions, states,\n                                                rewards)  \n            if learning_ret is not None:\n                for k, v in learning_ret.items():\n                    self.log_entry.add_key(k, v)\n\n        observations = self._reset_env()\n        states = self._get_init_states()  \n\n        while self.alive and (not self.env.time_out()):\n            actions, next_states = self._cts_predict(\n                observations, states)  \n            assert isinstance(actions, dict)\n            assert isinstance(next_states, dict)\n            next_observations, rewards, next_game_over = self._step_env(\n                actions)\n            __store_data(observations, actions, states, rewards)\n\n            observations = next_observations\n            states = next_states\n            \n            \n            self.alive = 1 - abs(next_game_over)\n\n        \n        \n        \n        if self.env.time_out():\n            self.alive = -1\n        actions, _ = self._cts_predict(observations, states)\n        zero_rewards = {k: [0] * len(v) for k, v in rewards.items()}\n        __store_data(observations, actions, states, zero_rewards)\n\n        \n        \n        \n        self.log_entry.add_key(\"success\", next_game_over > 0)\n        return self._total_reward()\n\n    def _reset_env(self):\n        self.alive = 1\n        \n        self.log_entry = GameLogEntry(self.id, 'All')\n        obs = self.env.reset()\n        assert isinstance(obs, dict)\n        return obs\n\n    def _step_env(self, actions):\n        next_observations, rewards, next_game_over = self.env.step(actions,\n                                                                   self.actrep)\n        assert isinstance(next_observations, dict)\n        assert isinstance(rewards, dict)\n        self.log_entry.add_key(\"num_steps\", 1)\n        self.log_entry.add_key(\"total_reward\", sum(map(sum, rewards.values())))\n        return next_observations, rewards, next_game_over\n\n    def _total_reward(self):\n        self.log_q.put(self.log_entry)\n        return self.log_entry.total_reward\n\n    def _get_init_states(self):\n        \n        return dict()\n\n    @abstractmethod\n    def _cts_predict(self, observations, states):\n        \n        pass\n\n    @abstractmethod\n    def _cts_store_data(self, observations, actions, states, rewards):\n        \n        pass\n",
        "summary": "The provided code defines a framework for an agent that interacts with an environment and learns from the interactions. The `Agent` class is a subclass of `Process`, making it suitable for parallel execution. It includes methods for setting up the environment, adding helper agents, predicting actions, running episodes, and storing data. Helper classes like `OnlineAgentHelper`, `OfflineAgentHelper`, and `ReplayAgentHelper` are used to manage different types of learning strategies. The framework is designed to be flexible, allowing for various environments and learning algorithms to be integrated seamlessly."
    },
    {
        "code": "import logging\nfrom typing import List\n\nfrom homeassistant.helpers.entity import Entity\nfrom gehomesdk import ErdCode, ErdApplianceType\n\nfrom .washer import WasherApi\nfrom .dryer import DryerApi\nfrom ..entities import GeErdSensor, GeErdBinarySensor\n\n_LOGGER = logging.getLogger(__name__)\n\nclass WasherDryerApi(WasherApi, DryerApi):\n    \n    APPLIANCE_TYPE = ErdApplianceType.COMBINATION_WASHER_DRYER\n\n    def get_all_entities(self) -> List[Entity]:\n        base_entities = self.get_base_entities()\n        \n        common_entities = [\n            GeErdSensor(self, ErdCode.LAUNDRY_MACHINE_STATE),\n            GeErdSensor(self, ErdCode.LAUNDRY_CYCLE),\n            GeErdSensor(self, ErdCode.LAUNDRY_SUB_CYCLE),\n            GeErdBinarySensor(self, ErdCode.LAUNDRY_END_OF_CYCLE),\n            GeErdSensor(self, ErdCode.LAUNDRY_TIME_REMAINING),\n            GeErdSensor(self, ErdCode.LAUNDRY_DELAY_TIME_REMAINING),\n            GeErdBinarySensor(self, ErdCode.LAUNDRY_DOOR),\n            GeErdBinarySensor(self, ErdCode.LAUNDRY_REMOTE_STATUS),\n        ]\n\n        washer_entities = self.get_washer_entities()\n        dryer_entities = self.get_dryer_entities()\n\n        entities = base_entities + common_entities + washer_entities + dryer_entities\n        return entities\n\n",
        "summary": "The `WasherDryerApi` class extends both `WasherApi` and `DryerApi`, combining their functionalities to manage a combination washer-dryer appliance. It defines methods to retrieve all relevant entities, including sensors and binary sensors for various states and operations of the washer and dryer, using ErdCode constants from the `gehomesdk`."
    },
    {
        "code": "import numpy as np\nimport warnings\nfrom . import utils\n\nfrom numba import njit, int64, types, float64\n\n\ndef decode(H, y, snr, maxiter=1000):\n    \n    m, n = H.shape\n\n    bits_hist, bits_values, nodes_hist, nodes_values = utils._bitsandnodes(H)\n\n    _n_bits = np.unique(H.sum(0))\n    _n_nodes = np.unique(H.sum(1))\n\n    if _n_bits * _n_nodes == 1:\n        solver = _logbp_numba_regular\n        bits_values = bits_values.reshape(n, -1)\n        nodes_values = nodes_values.reshape(m, -1)\n\n    else:\n        solver = _logbp_numba\n\n    var = 10 ** (-snr / 10)\n\n    if y.ndim == 1:\n        y = y[:, None]\n    \n\n    Lc = 2 * y / var\n    _, n_messages = y.shape\n\n    Lq = np.zeros(shape=(m, n, n_messages))\n\n    Lr = np.zeros(shape=(m, n, n_messages))\n    for n_iter in range(maxiter):\n        Lq, Lr, L_posteriori = solver(bits_hist, bits_values, nodes_hist,\n                                      nodes_values, Lc, Lq, Lr, n_iter)\n        x = np.array(L_posteriori <= 0).astype(int)\n        product = utils.incode(H, x)\n        if product:\n            break\n    if n_iter == maxiter - 1:\n        warnings.warn()\n    return x.squeeze()\n\n\noutput_type_log2 = types.Tuple((float64[:, :, :], float64[:, :, :],\n                               float64[:, :]))\n\n\n@njit(output_type_log2(int64[:], int64[:], int64[:], int64[:], float64[:, :],\n                       float64[:, :, :],  float64[:, :, :], int64), cache=True)\ndef _logbp_numba(bits_hist, bits_values, nodes_hist, nodes_values, Lc, Lq, Lr,\n                 n_iter):\n    \n    m, n, n_messages = Lr.shape\n    \n\n    bits_counter = 0\n    nodes_counter = 0\n    for i in range(m):\n        \n        ff = bits_hist[i]\n        ni = bits_values[bits_counter: bits_counter + ff]\n        bits_counter += ff\n        for j in ni:\n            nij = ni[:]\n\n            X = np.ones(n_messages)\n            if n_iter == 0:\n                for kk in range(len(nij)):\n                    if nij[kk] != j:\n                        X *= np.tanh(0.5 * Lc[nij[kk]])\n            else:\n                for kk in range(len(nij)):\n                    if nij[kk] != j:\n                        X *= np.tanh(0.5 * Lq[i, nij[kk]])\n            num = 1 + X\n            denom = 1 - X\n            for ll in range(n_messages):\n                if num[ll] == 0:\n                    Lr[i, j, ll] = -1\n                elif denom[ll] == 0:\n                    Lr[i, j, ll] = 1\n                else:\n                    Lr[i, j, ll] = np.log(num[ll] / denom[ll])\n\n    \n    for j in range(n):\n        \n        ff = nodes_hist[j]\n        mj = nodes_values[nodes_counter: nodes_counter + ff]\n        nodes_counter += ff\n        for i in mj:\n            mji = mj[:]\n            Lq[i, j] = Lc[j]\n\n            for kk in range(len(mji)):\n                if mji[kk] != i:\n                    Lq[i, j] += Lr[mji[kk], j]\n\n    \n    L_posteriori = np.zeros((n, n_messages))\n    nodes_counter = 0\n    for j in range(n):\n        ff = nodes_hist[j]\n        mj = nodes_values[nodes_counter: nodes_counter + ff]\n        nodes_counter += ff\n        L_posteriori[j] = Lc[j] + Lr[mj, j].sum(axis=0)\n\n    return Lq, Lr, L_posteriori\n\n\n@njit(output_type_log2(int64[:], int64[:, :], int64[:], int64[:, :],\n                       float64[:, :], float64[:, :, :],  float64[:, :, :],\n                       int64), cache=True)\ndef _logbp_numba_regular(bits_hist, bits_values, nodes_hist, nodes_values, Lc,\n                         Lq, Lr, n_iter):\n    \n    m, n, n_messages = Lr.shape\n    \n    for i in range(m):\n        ni = bits_values[i]\n        for j in ni:\n            nij = ni[:]\n\n            X = np.ones(n_messages)\n            if n_iter == 0:\n                for kk in range(len(nij)):\n                    if nij[kk] != j:\n                        X *= np.tanh(0.5 * Lc[nij[kk]])\n            else:\n                for kk in range(len(nij)):\n                    if nij[kk] != j:\n                        X *= np.tanh(0.5 * Lq[i, nij[kk]])\n            num = 1 + X\n            denom = 1 - X\n            for ll in range(n_messages):\n                if num[ll] == 0:\n                    Lr[i, j, ll] = -1\n                elif denom[ll] == 0:\n                    Lr[i, j, ll] = 1\n                else:\n                    Lr[i, j, ll] = np.log(num[ll] / denom[ll])\n\n    \n    for j in range(n):\n        mj = nodes_values[j]\n        for i in mj:\n            mji = mj[:]\n            Lq[i, j] = Lc[j]\n\n            for kk in range(len(mji)):\n                if mji[kk] != i:\n                    Lq[i, j] += Lr[mji[kk], j]\n\n    \n    L_posteriori = np.zeros((n, n_messages))\n    for j in range(n):\n        mj = nodes_values[j]\n        L_posteriori[j] = Lc[j] + Lr[mj, j].sum(axis=0)\n\n    return Lq, Lr, L_posteriori\n\n\ndef get_message(tG, x):\n    \n    n, k = tG.shape\n\n    rtG, rx = utils.gausselimination(tG, x)\n\n    message = np.zeros(k).astype(int)\n\n    message[k - 1] = rx[k - 1]\n    for i in reversed(range(k - 1)):\n        message[i] = rx[i]\n        message[i] -= utils.binaryproduct(rtG[i, list(range(i+1, k))],\n                                          message[list(range(i+1, k))])\n\n    return abs(message)\n",
        "summary": "The provided Python code implements a decoding algorithm using the Log-Map Belief Propagation (Log-BP) method for error correction in communication systems. It includes functions to decode received data (`decode`), perform the Log-BP iterations (`_logbp_numba`, `_logbp_numba_regular`), and calculate messages based on a generator matrix (`get_message`). The code utilizes Numba for just-in-time compilation of critical sections to improve performance, and it handles various edge cases such as different numbers of bits and nodes in the system."
    },
    {
        "code": "import logging\nimport time\n\nimport sh\n\nlogger = logging.getLogger(__name__)\n\n\ndef is_command_available(name):\n    try:\n        sh.bash('which', name)\n    except sh.ErrorReturnCode:\n        return False\n    else:\n        return True\n\n\nclass KubernetesDependency:\n    def ensure_running(self):\n        logger.debug('Checking if container \"{}\" is running...'.format(self.name))\n        if self.is_container_running():\n            logger.debug('\"{}\" is running'.format(self.name))\n        else:\n            logger.debug('Starting \"{}\"...'.format(self.name))\n            self.run_container()\n            logger.debug('\"{}\" started'.format(self.name))\n\n    def run_container(self):\n        self._apply_definition()\n        self._wait_until_ready()\n        self._wait_for_started_log()\n\n    def _apply_definition(self):\n        sh.kubectl('apply', '--record', '-f', self.definition)\n        try:\n            sh.kubectl('expose', '-f', self.definition)\n        except sh.ErrorReturnCode_1 as e:\n            if b'already exists' not in e.stderr:\n                raise e\n            else:\n                logger.debug('Service for \"{}\" exists'.format(self.name))\n\n    def _wait_until_ready(self):\n        logger.debug('Waiting for \"{}\" to start (possibly downloading image)...'.format(self.name))\n        ready = False\n        while not ready:\n            ready = self.is_container_running()\n            if not ready:\n                time.sleep(1)\n        logger.debug('\"{}\" started'.format(self.name))\n\n    def _wait_for_started_log(self):\n        logger.debug('Waiting for started log for \"{}\"...'.format(self.name))\n        for log in sh.kubectl('logs', '-f', self.pod_name, _iter='out'):\n            if self.started_log in log:\n                break\n        logger.debug('Started log for \"{}\" found'.format(self.name))\n\n    def is_container_running(self):\n        try:\n            container_ready = str(sh.kubectl(\n                'get', 'pods',\n                '--selector', self.selector,\n                '--output', 'jsonpath=\"{.items[*].status.containerStatuses[*].ready}\"',\n            )).strip()\n        except sh.ErrorReturnCode as e:\n            logger.debug(e)\n            return False\n        else:\n            return container_ready == '\"true\"'\n\n    def run_command(self, *args):\n        return sh.kubectl('exec', self.pod_name, '--', *args)\n\n    @property\n    def pod_name(self):\n        return str(sh.kubectl(\n            'get', 'pods',\n            '--output', 'custom-columns=NAME:.metadata.name',\n            '--no-headers',\n            '--selector', self.selector,\n        )).strip()\n\n    @property\n    def selector(self):\n        return 'app={}'.format(self.name)\n\n    @property\n    def started_log(self):\n        raise NotImplementedError\n\n    @property\n    def name(self):\n        raise NotImplementedError\n\n    @property\n    def definition(self):\n        raise NotImplementedError\n",
        "summary": "The provided Python code defines a class `KubernetesDependency` that manages the lifecycle of a Kubernetes container, including ensuring it's running, applying its configuration, waiting for readiness, and executing commands within the container. It uses the `sh` library to interact with Kubernetes commands like `kubectl`, logs, and exposes methods to check if the container is running and to run custom commands inside the container."
    },
    {
        "code": "import requests\nimport json\nimport time\nimport random\nfrom . import conf, data, lang\nfrom inukit.timestamp import natural_date, natural_time, timestamp_now\n\ndef is_same_day(ts1, ts2) -> bool:\n    def d(ts):\n        return natural_date(ts, '%Y-%m-%d')\n    return d(ts1) == d(ts2)\n\ndef handle_morning(qq):\n    last_morning = data.get(qq, 'last_morning')\n    last_night = data.get(qq, 'last_night')\n    now = timestamp_now()\n    if last_morning > last_night:\n        msg = lang.no_sleep\n    else:\n        msg = lang.morning_success % (\n            natural_time(now - last_night)\n        )\n        data.set(qq, 'last_morning', now)\n    return msg\n\ndef handle_night(qq):\n    last_morning = data.get(qq, 'last_morning')\n    last_night = data.get(qq, 'last_night')\n    now = timestamp_now()\n    if last_night > last_morning:\n        msg = lang.no_getup\n    else:\n        data.set(qq, 'last_night', now)\n        msg = lang.night_success % (\n            natural_time(now - last_morning)\n        )\n    return msg\n    \ndef gen_sign_info():\n    rp = random.randint(1,100)\n    return {\n        \"rp\": rp\n    }\n\ndef handle_sign(qq):\n    last_sign = data.get(qq, 'last_sign')\n    now = timestamp_now()\n    msg = ''\n    if is_same_day(last_sign, now):\n        info = data.get(qq, 'last_sign_info')\n        msg = lang.already_sign\n    else:\n        msg = lang.sign_success\n        info = gen_sign_info()\n        data.set(qq, 'last_sign', now)\n        data.set(qq, 'last_sign_info', info)\n    msg += lang.sign % (\n        natural_date(last_sign),\n        info['rp']\n    )\n    return msg",
        "summary": "The provided Python code defines functions to handle morning and night routines for a user identified by `qq`, track the last sign-in date, and generate random points (RP) for each sign-in. It uses external modules for configuration, data storage, language localization, and timestamp formatting. The code checks if actions are performed on the same day as the last action to prevent duplicate entries and updates user data accordingly."
    },
    {
        "code": "import os\n\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n\n\n\n\nSECRET_KEY = 'lxb!(o00)qtw0p+6q_vs$01&wtsw(m*s!ol0_6^v*flo^!&ek&'\n\n\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'rest_framework.authtoken',\n    'core',\n    'user',\n    'recipe',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'app.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'app.wsgi.application'\n\n\n\n\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'HOST': os.environ.get('DB_HOST'),\n        'NAME': os.environ.get('DB_NAME'),\n        'USER': os.environ.get('DB_USER'),\n        'PASSWORD': os.environ.get('DB_PASS'),\n    }\n}\n\n\n\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n\n\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n\n\n\nSTATIC_URL = '/static/'\nMEDIA_URL = '/media/'\n\nMEDIA_ROOT = '/vol/web/media'\nSTATIC_ROOT = '/vol/web/static'\n\nAUTH_USER_MODEL = 'core.User'\n",
        "summary": "This Python code is a Django project configuration file that sets up various settings such as the base directory, secret key, allowed hosts, installed apps, middleware, database configurations, authentication validators, and static files paths. It also specifies the language, time zone, and user model for the application."
    },
    {
        "code": "from setuptools import find_packages, setup\n\ntest_requirements = [\n    \"black>=19.10b0\",\n    \"flake8>=3.8.3\",\n    \"flake8-debugger>=3.2.1\",\n]\n\ndev_requirements = [\n    *test_requirements,\n    \"wheel>=0.34.2\",\n]\n\nrequirements = [\n    \"cdp-backend[pipeline]==3.0.2\",\n    \"cdp-scrapers[king_county]>=0.3.2\",\n]\n\nextra_requirements = {\n    \"test\": test_requirements,\n    \"dev\": dev_requirements,\n    \"all\": [\n        *requirements,\n        *dev_requirements,\n    ],\n}\n\nsetup(\n    author=\"JacksonMaxfield\",\n    classifiers=[\n        \"Development Status :: 2 - Pre-Alpha\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Natural Language :: English\",\n        \"Programming Language :: Python :: 3.9\",\n    ],\n    description=\"Package containing the gather functions for Example.\",\n    install_requires=requirements,\n    license=\"MIT license\",\n    long_description_content_type=\"text/markdown\",\n    include_package_data=True,\n    keywords=\"civic technology, open government\",\n    name=\"cdp-king_county-backend\",\n    packages=find_packages(exclude=[\"tests\", \"*.tests\", \"*.tests.*\"]),\n    python_requires=\">=3.9\",\n    tests_require=test_requirements,\n    extras_require=extra_requirements,\n    url=\"https://github.com/CouncilDataProject/king-county\",\n    version=\"1.0.0\",\n    zip_safe=False,\n)\n",
        "summary": "This Python script sets up a package using `setuptools`, defining various requirements and configurations such as dependencies, classifiers, and test settings for the project \"cdp-king_county-backend\". It includes specific versions of packages like `cdp-backend` and `cdp-scrapers`, along with development and testing tools."
    },
    {
        "code": "from scrapy.spider import BaseSpider\nfrom scrapy.http import Request\nfrom scrapy.selector import XmlXPathSelector\nfrom openrecipes.spiders.elanaspantry_spider import ElanaspantryMixin\n\n\nclass ElanaspantryfeedSpider(BaseSpider, ElanaspantryMixin):\n    name = \"elanaspantry.feed\"\n    allowed_domains = [\n        \"www.elanaspantry.com\",\n        \"feeds.feedburner.com\",\n        \"feedproxy.google.com\",\n    ]\n    start_urls = [\n        \"http://feeds.feedburner.com/elanaspantry\",\n    ]\n\n    def parse(self, response):\n        xxs = XmlXPathSelector(response)\n        links = xxs.select(\"//item/*[local-name()='origLink']/text()\").extract()\n\n        return [Request(x, callback=self.parse_item) for x in links]\n",
        "summary": "The `ElanaspantryfeedSpider` class is a Scrapy spider designed to scrape recipe feed URLs from Elana's Pantry. It inherits from both `BaseSpider` and `ElanaspantryMixin`, setting up the necessary configurations and methods to navigate through the feed, extract original link elements, and initiate further requests to parse each item individually."
    },
    {
        "code": "from django.db import models\nfrom AlyMoly.mantenedor.models import Producto, Promocion, Trabajador\n\n\nclass Turno(models.Model):\n    \n    fecha_apertura_sistema = models.DateTimeField()\n    fecha_cierre_sistema = models.DateTimeField(null=True, blank=True)\n    estado = models.IntegerField(default=1, blank=True)\n    trabajador = models.ForeignKey(Trabajador, blank=True)\n    monto_apertura_caja = models.IntegerField(default=0)\n    monto_cierre_calculado = models.IntegerField(default=0, blank=True)\n    monto_afecto = models.IntegerField(default=0, blank=True)\n    monto_exento = models.IntegerField(default=0, blank=True)\n\n    def monto_cierre_informado(self):\n        return self.boletadeposito.total\n\n    def estado_turno(self):\n        if self.estado == 1:\n            return \"Abierto\"\n        else:\n            return \"Cerrado\"\n\n    def save(self, force_insert=False, force_update=False):\n        \n        if self.estado == 1 and len(Turno.objects.exclude(id=self.id).filter(trabajador__id=self.trabajador.id).filter(estado=1)) > 0:\n            raise Exception(u\"Usted ya cuenta con un turno abierto.\")\n        super(Turno, self).save(force_insert, force_update)\n\n\nclass BoletaDeposito(models.Model):\n    turno = models.OneToOneField(Turno, blank=True)\n    veintemil = models.PositiveIntegerField(default=0, blank=True)\n    diezmil = models.PositiveIntegerField(default=0, blank=True)\n    cincomil = models.PositiveIntegerField(default=0, blank=True)\n    dosmil = models.PositiveIntegerField(default=0, blank=True)\n    mil = models.PositiveIntegerField(default=0, blank=True)\n    quinientos = models.PositiveIntegerField(default=0, blank=True)\n    cien = models.PositiveIntegerField(default=0, blank=True)\n    cincuenta = models.PositiveIntegerField(default=0, blank=True)\n    diez = models.PositiveIntegerField(default=0, blank=True)\n    tarjetas = models.PositiveIntegerField(default=0, blank=True)\n    otros = models.PositiveIntegerField(default=0, blank=True)\n    total = models.PositiveIntegerField(default=0, blank=True)\n\n\nclass Venta(models.Model):\n    \n    fecha_venta = models.DateTimeField()\n    folio_boleta = models.PositiveIntegerField(null=True, blank=True)\n    monto_total = models.PositiveIntegerField()\n    monto_afecto = models.PositiveIntegerField()\n    monto_exento = models.PositiveIntegerField()\n    cantidad_productos = models.PositiveIntegerField()\n    medio_pago = models.PositiveIntegerField()\n    monto_pago = models.PositiveIntegerField(null=True)\n    turno = models.ForeignKey('Turno')\n\n    def __unicode__(self):\n        return u\"%s-%s\" % (self.id, self.folio_boleta)\n\n\nclass LineaDetalle(models.Model):\n    cantidad = models.IntegerField()\n    precio_venta = models.IntegerField()\n    precio_venta_total = models.IntegerField()\n    producto = models.ForeignKey(Producto, null=True, blank=True)\n    promocion = models.ForeignKey(Promocion, null=True, blank=True)\n    venta = models.ForeignKey('Venta')\n",
        "summary": "The provided Python code defines several Django models for a retail management system. The `Turno` model manages shifts with associated workers and financial transactions, ensuring no two workers can have an open shift simultaneously. The `BoletaDeposito` model tracks cash deposits made during shifts. The `Venta` model records sales transactions, linking them to specific shifts and products. Finally, the `LineaDetalle` model details each item in a sale, including product information and pricing."
    },
    {
        "code": "import os\nimport time\nimport multiprocessing as mp\n\nimport pandas as pd\n\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Subset, TensorDataset, WeightedRandomSampler\n\nfrom profit.dataset.splitters import split_method_dict\nfrom profit.models.torch import SequenceOracle\nfrom profit.utils.data_utils.tokenizers import AminoAcidTokenizer\nfrom profit.utils.training_utils.torch import losses as L\nfrom profit.utils.training_utils.torch.callbacks import ModelCheckpoint\nfrom profit.utils.training_utils.torch.callbacks import EarlyStopping\n\nfrom examples.gb1.data import load_dataset\n\n\ntimestep = time.strftime(\"%Y-%b-%d-%H:%M:%S\", time.gmtime())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\nsplits = [\"train\", \"valid\"]\n\n\ndataset = load_dataset(\"lstm\", \"primary\", labels=\"Fitness\", num_data=-1,\n                       filetype=\"mdb\", as_numpy=False, vocab=\"aa20\")\n\n\n\n_dataset = dataset[:][\"arr_0\"]\n_labels = dataset[:][\"arr_1\"].view(-1)\n\n\n\n\n\n\n\ndef sampler(labels: torch.Tensor,\n            nbins: int = 10,\n            stratify: bool = False) -> WeightedRandomSampler:\n    discretize = pd.qcut if stratify else pd.cut\n    bin_labels = torch.LongTensor(discretize(labels.tolist(), nbins,\n                                             labels=False, duplicates=\"drop\"))\n    class_sample_count = torch.LongTensor(\n        [(bin_labels == t).sum() for t in torch.arange(nbins)])\n    weight = 1. / class_sample_count.float()\n    sample_weights = torch.zeros_like(labels)\n    for t in torch.unique(bin_labels):\n        sample_weights[bin_labels == t] = weight[t]\n    return WeightedRandomSampler(sample_weights, len(sample_weights))\n\n\nweights = sampler(_labels, nbins=10, stratify=False).weights.type(torch.float)\ndataset = TensorDataset(*dataset[:].values(), weights)\n\n\nsubset_idx = split_method_dict[\"stratified\"]().train_valid_test_split(\n    dataset=_dataset, labels=_labels.tolist(), frac_train=0.9,\n    frac_valid=0.1, frac_test=0.0, return_idxs=True, n_bins=10)\nstratified = {split: Subset(dataset, sorted(idx))\n              for split, idx in zip(splits, subset_idx)}\n\n\ntrain_sampler = sampler(stratified[\"train\"][:][1].view(-1), stratify=True)\n\n\ntokenizer = AminoAcidTokenizer(\"aa20\")\nvocab_size = tokenizer.vocab_size\nseqlen = stratified[\"train\"][0][0].size(0)\nmodel = SequenceOracle(seqlen, vocab_size, hidden_size=50, out_size=2)\n\n\n\nstop_clbk = EarlyStopping(patience=5, verbose=1)\nsave_clbk = ModelCheckpoint(os.path.join(\"bin/3gb1/oracle\", timestep),\n                            monitor=\"val_loss\",\n                            verbose=1,\n                            save_weights_only=True)\nsave_clbk.set_model(model)\n\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\nepochs = 50\nfor epoch in range(1, epochs+1):\n    for split in splits:\n        summed_loss = 0\n        data_loader = DataLoader(\n            dataset=stratified[split],\n            batch_size=32,\n            sampler=train_sampler if split == \"train\" else None,\n            num_workers=mp.cpu_count(),\n            pin_memory=torch.cuda.is_available()\n        )\n\n        \n        model.train() if split == \"train\" else model.eval()\n\n        for it, batch in enumerate(data_loader):\n            data = batch[0].long().to(device)\n            target = batch[1].to(device)\n            sample_weight = batch[2].to(device)\n            \n            batch_size, seqlen = data.size()\n            onehot = torch.zeros(batch_size, seqlen, vocab_size)\n            onehot.scatter_(2, torch.unsqueeze(data, 2), 1)\n\n            \n            pred = model(onehot)\n            \n            nll_loss = L.gaussian_nll_loss(pred, target, reduction=\"none\")\n            \n            nll_loss = (nll_loss * sample_weight).sum()\n            summed_loss += nll_loss.item()\n            loss = nll_loss / batch_size\n            \n            if split == \"train\":\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            \n            if it % 5 == 0 or it+1 == len(data_loader):\n                print(\"{} Batch {:04d}/{:d} ({:.2f}%)\\tLoss: {:.4f}\".format(\n                    split.upper(), it+1, len(data_loader),\n                    100. * ((it+1)/len(data_loader)), loss.item()))\n\n        \n        avg_loss = summed_loss / len(data_loader.dataset)\n        print(\"{} Epoch {}/{}, Average NLL loss: {:.4f}\".format(\n            split.upper(), epoch, epochs, avg_loss))\n\n        \n        if split == \"valid\":\n            save_clbk.on_epoch_end(epoch, logs={\"val_loss\": avg_loss})\n            should_stop = stop_clbk.on_epoch_end(epoch, logs={\"val_loss\": avg_loss})\n            if should_stop:\n                break\n    else:\n        continue\n    break\n",
        "summary": "This Python script trains a sequence model using PyTorch for predicting fitness from protein sequences. It loads a dataset, applies stratified sampling, tokenizes the sequences, and uses an AdamW optimizer to minimize Gaussian negative log-likelihood loss during training. The script includes callbacks for early stopping and model checkpointing based on validation loss."
    },
    {
        "code": "import logging\nimport os\nfrom pathlib import Path\nfrom typing import Any, Callable, Optional\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport cv2\n\nimport numpy as np\n\n\nclass URISC(Dataset):\n    def __init__(\n        self, \n        dir: str, \n        mode: str = 'train',\n        transform: Optional[Callable] = None, \n        data_rank: str = 'simple',\n    ):\n        super(URISC, self).__init__()\n        self.dir = dir\n        self.mode = mode\n        self.transform = transform\n        self.data_rank = data_rank\n\n        if data_rank == 'simple':\n            self.transform_normalize = transforms.Normalize(mean=0.520, std=0.185)\n        elif data_rank == 'complex':\n            self.transform_normalize = transforms.Normalize(mean=0.518, std=0.190)\n        self.transform_totensor = transforms.ToTensor()\n\n        self.ids = [os.path.join(dir, data_rank, mode, filename) for filename in os.listdir(os.path.join(dir, data_rank, mode))]\n        if not self.ids:\n            raise RuntimeError(f'No input file found in {os.path.join(dir, data_rank, mode)}, make sure you put your images there')\n        logging.info(f'Creating dataset with {len(self.ids)} examples')\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        image = cv2.imread(self.ids[idx])\n        \n        \n        if self.mode == 'test':\n            if self.transform is not None:\n                image = self.transform(image=image)\n            return image.float().contiguous(), self.ids[idx]\n        \n        mask_path = self.ids[idx].replace(self.mode, \"label/\"+self.mode)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            transformed_image = transformed['image']\n            transformed_mask = transformed['mask']\n        else:\n            transformed_image = image\n            transformed_mask = mask\n\n        transformed_image = self.transform_totensor(transformed_image)\n        transformed_image = self.transform_normalize(transformed_image)\n        transformed_mask = self.transform_totensor(transformed_mask)\n\n        \n        \n\n        return transformed_image, transformed_mask",
        "summary": "The `URISC` class is a custom dataset for the URISC dataset, inheriting from `torch.utils.data.Dataset`. It initializes with a directory path, mode ('train' or 'test'), an optional transformation function, and a data rank ('simple' or 'complex'). The dataset reads image files from specified directories, applies transformations including normalization and conversion to tensors, and returns both the transformed image and its corresponding mask for training or testing purposes."
    },
    {
        "code": "import torch.nn as nn\n\n\nclass FeatureMatchingLoss(nn.Module):\n    r\n    def __init__(self, criterion='l1'):\n        super(FeatureMatchingLoss, self).__init__()\n        if criterion == 'l1':\n            self.criterion = nn.L1Loss()\n        elif criterion == 'l2' or criterion == 'mse':\n            self.criterion = nn.MSELoss()\n        else:\n            raise ValueError('Criterion %s is not recognized' % criterion)\n\n    def forward(self, fake_features, real_features):\n        r\n        num_d = len(fake_features)\n        dis_weight = 1.0 / num_d\n        loss = fake_features[0][0].new_tensor(0)\n        for i in range(num_d):\n            for j in range(len(fake_features[i])):\n                tmp_loss = self.criterion(fake_features[i][j],\n                                          real_features[i][j].detach())\n                loss += dis_weight * tmp_loss\n        return loss\n",
        "summary": "The `FeatureMatchingLoss` class defines a custom loss function that calculates the feature matching loss between fake and real features using either L1 or MSE (L2) distance, based on the specified criterion. It computes the average loss across multiple discriminators by weighting each discriminator's contribution equally."
    },
    {
        "code": "import time\nimport uuid\n\nfrom eventlet.green import threading\nfrom oslo_config import cfg\nfrom oslo_log import log as logging\nimport six\n\nfrom cinder import exception\nfrom cinder.i18n import _\nfrom cinder import utils\nfrom cinder.volume import configuration\nfrom cinder.volume.drivers.san import san\n\nimport cinder.volume.drivers.datera.datera_api2 as api2\nimport cinder.volume.drivers.datera.datera_api21 as api21\nimport cinder.volume.drivers.datera.datera_common as datc\n\n\nLOG = logging.getLogger(__name__)\n\nd_opts = [\n    cfg.StrOpt('datera_api_port',\n               default='7717',\n               help='Datera API port.'),\n    cfg.StrOpt('datera_api_version',\n               default='2',\n               deprecated_for_removal=True,\n               help='Datera API version.'),\n    cfg.IntOpt('datera_503_timeout',\n               default='120',\n               help='Timeout for HTTP 503 retry messages'),\n    cfg.IntOpt('datera_503_interval',\n               default='5',\n               help='Interval between 503 retries'),\n    cfg.BoolOpt('datera_debug',\n                default=False,\n                help=\"True to set function arg and return logging\"),\n    cfg.BoolOpt('datera_debug_replica_count_override',\n                default=False,\n                help=\"ONLY FOR DEBUG/TESTING PURPOSES\\n\"\n                     \"True to set replica_count to 1\"),\n    cfg.StrOpt('datera_tenant_id',\n               default=None,\n               help=\"If set to 'Map' --> OpenStack project ID will be mapped \"\n                    \"implicitly to Datera tenant ID\\n\"\n                    \"If set to 'None' --> Datera tenant ID will not be used \"\n                    \"during volume provisioning\\n\"\n                    \"If set to anything else --> Datera tenant ID will be the \"\n                    \"provided value\"),\n    cfg.BoolOpt('datera_disable_profiler',\n                default=False,\n                help=\"Set to True to disable profiling in the Datera driver\"),\n]\n\n\nCONF = cfg.CONF\nCONF.import_opt('driver_use_ssl', 'cinder.volume.driver')\nCONF.register_opts(d_opts, group=configuration.SHARED_CONF_GROUP)\n\n\n@six.add_metaclass(utils.TraceWrapperWithABCMetaclass)\nclass DateraDriver(san.SanISCSIDriver, api2.DateraApi, api21.DateraApi):\n\n    \n    VERSION = '2.4.0'\n\n    CI_WIKI_NAME = \"datera-ci\"\n\n    HEADER_DATA = {'Datera-Driver': 'OpenStack-Cinder-{}'.format(VERSION)}\n\n    \n    SUPPORTED = False\n\n    def __init__(self, *args, **kwargs):\n        super(DateraDriver, self).__init__(*args, **kwargs)\n        self.configuration.append_config_values(d_opts)\n        self.username = self.configuration.san_login\n        self.password = self.configuration.san_password\n        self.cluster_stats = {}\n        self.datera_api_token = None\n        self.interval = self.configuration.datera_503_interval\n        self.retry_attempts = (self.configuration.datera_503_timeout /\n                               self.interval)\n        self.driver_prefix = str(uuid.uuid4())[:4]\n        self.datera_debug = self.configuration.datera_debug\n        self.datera_api_versions = []\n\n        if self.datera_debug:\n            utils.setup_tracing(['method'])\n        self.tenant_id = self.configuration.datera_tenant_id\n        if self.tenant_id and self.tenant_id.lower() == 'none':\n            self.tenant_id = None\n        self.api_check = time.time()\n        self.api_cache = []\n        self.api_timeout = 0\n        self.do_profile = not self.configuration.datera_disable_profiler\n        self.thread_local = threading.local()\n\n        backend_name = self.configuration.safe_get(\n            'volume_backend_name')\n        self.backend_name = backend_name or 'Datera'\n\n        datc.register_driver(self)\n\n    def do_setup(self, context):\n        \n        \n        if not all([self.username, self.password]):\n            msg = _(\"san_login and/or san_password is not set for Datera \"\n                    \"driver in the cinder.conf. Set this information and \"\n                    \"start the cinder-volume service again.\")\n            LOG.error(msg)\n            raise exception.InvalidInput(msg)\n\n        self.login()\n        self._create_tenant()\n\n    \n\n    \n    \n    \n\n    @datc._api_lookup\n    def create_volume(self, volume):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def extend_volume(self, volume, new_size):\n        pass\n\n    \n\n    \n    \n    \n\n    @datc._api_lookup\n    def create_cloned_volume(self, volume, src_vref):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def delete_volume(self, volume):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def ensure_export(self, context, volume, connector=None):\n        \n\n    \n    \n    \n\n    @datc._api_lookup\n    def initialize_connection(self, volume, connector):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def create_export(self, context, volume, connector):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def detach_volume(self, context, volume, attachment=None):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def create_snapshot(self, snapshot):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def delete_snapshot(self, snapshot):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def create_volume_from_snapshot(self, volume, snapshot):\n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def retype(self, ctxt, volume, new_type, diff, host):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def manage_existing(self, volume, existing_ref):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def manage_existing_get_size(self, volume, existing_ref):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def get_manageable_volumes(self, cinder_volumes, marker, limit, offset,\n                               sort_keys, sort_dirs):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def unmanage(self, volume):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def get_volume_stats(self, refresh=False):\n        \n        pass\n\n    \n    \n    \n\n    @datc._api_lookup\n    def login(self):\n        pass\n\n    \n    \n    \n\n    def _update_qos(self, resource, policies):\n        url = datc.URL_TEMPLATES['vol_inst'](\n            policies['default_storage_name'],\n            policies['default_volume_name']) + '/performance_policy'\n        url = url.format(datc._get_name(resource['id']))\n        type_id = resource.get('volume_type_id', None)\n        if type_id is not None:\n            \n            \n            fpolicies = {k: int(v) for k, v in\n                         policies.items() if k.endswith(\"max\")}\n            \n            fpolicies = dict(filter(lambda _v: _v[1] > 0, fpolicies.items()))\n            if fpolicies:\n                self._issue_api_request(url, 'post', body=fpolicies,\n                                        api_version='2')\n\n    def _get_lunid(self):\n        return 0\n\n    \n    \n    \n\n    def _init_vendor_properties(self):\n        \n\n        properties = {}\n\n        self._set_property(\n            properties,\n            \"DF:placement_mode\",\n            \"Datera Volume Placement\",\n            _(\"'single_flash' for single-flash-replica placement, \"\n              \"'all_flash' for all-flash-replica placement, \"\n              \"'hybrid' for hybrid placement\"),\n            \"string\",\n            default=\"hybrid\")\n\n        self._set_property(\n            properties,\n            \"DF:round_robin\",\n            \"Datera Round Robin Portals\",\n            _(\"True to round robin the provided portals for a target\"),\n            \"boolean\",\n            default=False)\n\n        if self.configuration.get('datera_debug_replica_count_override'):\n            replica_count = 1\n        else:\n            replica_count = 3\n        self._set_property(\n            properties,\n            \"DF:replica_count\",\n            \"Datera Volume Replica Count\",\n            _(\"Specifies number of replicas for each volume. Can only be \"\n              \"increased once volume is created\"),\n            \"integer\",\n            minimum=1,\n            default=replica_count)\n\n        self._set_property(\n            properties,\n            \"DF:acl_allow_all\",\n            \"Datera ACL Allow All\",\n            _(\"True to set acl 'allow_all' on volumes created.  Cannot be \"\n              \"changed on volume once set\"),\n            \"boolean\",\n            default=False)\n\n        self._set_property(\n            properties,\n            \"DF:ip_pool\",\n            \"Datera IP Pool\",\n            _(\"Specifies IP pool to use for volume\"),\n            \"string\",\n            default=\"default\")\n\n        self._set_property(\n            properties,\n            \"DF:template\",\n            \"Datera Template\",\n            _(\"Specifies Template to use for volume provisioning\"),\n            \"string\",\n            default=\"\")\n\n        \n        self._set_property(\n            properties,\n            \"DF:read_bandwidth_max\",\n            \"Datera QoS Max Bandwidth Read\",\n            _(\"Max read bandwidth setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n\n        self._set_property(\n            properties,\n            \"DF:default_storage_name\",\n            \"Datera Default Storage Instance Name\",\n            _(\"The name to use for storage instances created\"),\n            \"string\",\n            default=\"storage-1\")\n\n        self._set_property(\n            properties,\n            \"DF:default_volume_name\",\n            \"Datera Default Volume Name\",\n            _(\"The name to use for volumes created\"),\n            \"string\",\n            default=\"volume-1\")\n\n        self._set_property(\n            properties,\n            \"DF:write_bandwidth_max\",\n            \"Datera QoS Max Bandwidth Write\",\n            _(\"Max write bandwidth setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n\n        self._set_property(\n            properties,\n            \"DF:total_bandwidth_max\",\n            \"Datera QoS Max Bandwidth Total\",\n            _(\"Max total bandwidth setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n\n        self._set_property(\n            properties,\n            \"DF:read_iops_max\",\n            \"Datera QoS Max iops Read\",\n            _(\"Max read iops setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n\n        self._set_property(\n            properties,\n            \"DF:write_iops_max\",\n            \"Datera QoS Max IOPS Write\",\n            _(\"Max write iops setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n\n        self._set_property(\n            properties,\n            \"DF:total_iops_max\",\n            \"Datera QoS Max IOPS Total\",\n            _(\"Max total iops setting for volume qos, \"\n              \"use 0 for unlimited\"),\n            \"integer\",\n            minimum=0,\n            default=0)\n        \n\n        return properties, 'DF'\n",
        "summary": "This code snippet appears to be a Python class that interacts with the Datera storage platform. The class provides methods for creating, managing, and deleting volumes on the Datera system. Here's a breakdown of some key functionalities:\n\n1. **Configuration**: The class uses configuration settings from `self.configuration` to determine various parameters such as replica count, IP pool, and default storage/template names.\n\n2. **Volume Creation**:\n   - `_get_lunid()`: Returns 0, which might be used for LUN ID assignment.\n   - `_init_vendor_properties()`: Initializes vendor-specific properties that can be set on volumes. These include placement mode, replica count, ACL settings, QoS limits, and more.\n\n3. **API Interactions**:\n   - The class uses methods like `_issue_api_request()` to communicate with the Datera API for creating, updating, and deleting volumes.\n   - It handles different API versions (e.g., '2') based on what's supported by the Datera system.\n\n4. **QoS Management**:\n   - The `_update_qos()` method updates Quality of Service settings for a volume, including maximum read/write bandwidth and IOPS limits.\n\n5. **Error Handling**:\n   - The class includes error handling mechanisms to manage exceptions that may occur during API calls or other operations.\n\n6. **Logging**:\n   - It uses logging to record important events and errors, which can be helpful for debugging and monitoring the system.\n\nThis class is designed to be flexible and configurable, allowing administrators to tailor volume creation and management processes according to their specific needs on a Datera storage platform."
    },
    {
        "code": "import time                                            \nimport logging                                         \nfrom pymodbus.client.sync import ModbusSerialClient \\\nas ModbusClient                                        \n\ncomSettings = {    \n                \"method\"   : 'rtu',\n                \"port\"     : 'COM3',\n                \"stopbits\" : 1,                \n                \"bytesize\" : 8,                \n                \"parity\"   : 'N',\n                \"baudrate\" : 9600,\n                \"timeout\"  : 1\n              }\n\nlogging.basicConfig()                                   \nlog = logging.getLogger()                               \n\nclient = ModbusClient(**comSettings)                    \nclient.connect()                                        \n\nwhile(True):\n    client.write_register(3,1000,unit=0x01)             \n    time.sleep(4)                                       \n    client.write_register(3,0,unit=0x01)                \n    time.sleep(4)                                       \n\nclient.close()                                          ",
        "summary": "The Python script establishes a connection to a Modbus device using RTU communication over COM3 at 9600 baud rate. It continuously writes the value 1000 to register 3 and then resets it to 0, pausing for 4 seconds between each operation before closing the connection."
    },
    {
        "code": "class UnsafeUtility:\tpass",
        "summary": "The `UnsafeUtility` class is defined but contains no methods or attributes, serving as an empty placeholder for potential future implementation."
    },
    {
        "code": "import enum\nfrom itertools import chain\nfrom django.contrib.auth.models import AbstractUser, UserManager as DjangoUserManager\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\nfrom django.urls import reverse\nfrom django.utils import timezone\nfrom django.utils.functional import cached_property\nfrom django.utils.translation import ugettext_lazy as _\nimport pyotp\nfrom zentral.utils.base64 import trimmed_urlsafe_b64decode\n\n\nclass UserManager(DjangoUserManager):\n    pass\n\n\nclass User(AbstractUser):\n    email = models.EmailField(unique=True)\n    is_remote = models.BooleanField(default=False)\n    is_service_account = models.BooleanField(default=False)\n    password_updated_at = models.DateTimeField(blank=True, null=True, editable=False)\n\n    objects = UserManager()\n\n    class Meta:\n        ordering = (\"username\",)\n\n    def __str__(self):\n        if self.is_service_account:\n            return self.username\n        else:\n            return self.email or self.username\n\n    def get_type_display(self):\n        return \"user\" if not self.is_service_account else \"service account\"\n\n    def get_absolute_url(self):\n        return reverse(\"accounts:user\", args=(self.pk,))\n\n    def set_password(self, *args, **kwargs):\n        if not self.is_remote and not self.is_service_account:\n            super().set_password(*args, **kwargs)\n            self.password_updated_at = timezone.now()\n        else:\n            self.set_unusable_password()\n\n    def save(self, *args, **kwargs):\n        if self.is_service_account:\n            \n            self.is_superuser = False\n        if self.is_service_account or self.is_remote:\n            \n            self.set_unusable_password()\n        else:\n            if self.pk:\n                old_user = self._meta.model.objects.get(pk=self.pk)\n                if old_user.password != self.password:\n                    if old_user.has_usable_password():\n                        UserPasswordHistory.objects.create(\n                            user=self,\n                            password=old_user.password,\n                            created_at=old_user.password_updated_at or old_user.date_joined\n                        )\n                    self.password_updated_at = timezone.now()\n            elif self.password:\n                self.password_updated_at = timezone.now()\n        super().save(*args, **kwargs)\n\n    def username_and_email_editable(self):\n        return not self.is_remote\n\n    def is_superuser_editable(self):\n        return (not self.is_superuser or\n                User.objects.exclude(pk=self.pk).filter(is_superuser=True).count() > 0)\n\n    def editable(self):\n        return self.username_and_email_editable() or self.is_superuser_editable()\n\n    def deletable(self):\n        return not self.is_superuser\n\n    @cached_property\n    def has_verification_device(self):\n        return len(self._all_verification_devices) > 0\n\n    @cached_property\n    def _all_verification_devices(self):\n        return list(chain(self.usertotp_set.all(),\n                          self.userwebauthn_set.all()))\n\n    def get_verification_devices(self):\n        return sorted(self._all_verification_devices,\n                      key=lambda vd: vd.name)\n\n    def get_prioritized_verification_devices(self, user_agent):\n        verification_devices = sorted(self._all_verification_devices,\n                                      key=lambda vd: (-1 * vd.PRIORITY, vd.name))\n        ua_verification_devices = [vd for vd in verification_devices if vd.test_user_agent(user_agent)]\n        if not ua_verification_devices and verification_devices:\n            raise ValueError(\"No verification devices compatible with this user agent\")\n        else:\n            return ua_verification_devices\n\n    @cached_property\n    def group_name_set(self):\n        \n        return set(self.groups.values_list(\"name\", flat=True))\n\n\nclass UserPasswordHistory(models.Model):\n    user = models.ForeignKey(User, on_delete=models.CASCADE)\n    password = models.CharField(_('password'), max_length=128)\n    created_at = models.DateTimeField(editable=False)\n\n\nclass UserVerificationDevice(models.Model):\n    user = models.ForeignKey(User, on_delete=models.CASCADE)\n    name = models.CharField(max_length=256)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        abstract = True\n\n    def get_type_for_display(self):\n        return self.TYPE\n\n    def __str__(self):\n        return \"{} {}\".format(self.get_type_for_display(), self.name)\n\n    def get_delete_url(self):\n        return reverse(self.delete_url_name, args=(self.pk,))\n\n    def serialize_for_event(self):\n        return {\"type\": self.TYPE,\n                \"pk\": self.pk}\n\n\nclass UserTOTP(UserVerificationDevice):\n    TYPE = \"TOTP\"\n    PRIORITY = 10\n    secret = models.CharField(max_length=256)\n    delete_url_name = \"accounts:delete_totp\"\n\n    class Meta:\n        unique_together = ((\"user\", \"name\"),)\n\n    def get_verification_url(self):\n        return reverse(\"accounts:verify_totp\")\n\n    def verify(self, code):\n        return pyotp.TOTP(self.secret).verify(code)\n\n    def test_user_agent(self, user_agent):\n        return True\n\n\nclass WebAuthnTransport(enum.Enum):\n    USB = \"usb\"\n    NFC = \"nfc\"\n    BLE = \"ble\"\n    INTERNAL = \"internal\"\n\n    @classmethod\n    def choices(cls):\n        return tuple((i.value, i.value) for i in cls)\n\n\nclass UserWebAuthn(UserVerificationDevice):\n    TYPE = \"WebAuthn\"\n    PRIORITY = 100\n    delete_url_name = \"accounts:delete_webauthn_device\"\n    key_handle = models.TextField()\n    public_key = models.BinaryField()\n    rp_id = models.TextField()\n    transports = ArrayField(models.CharField(max_length=8, choices=WebAuthnTransport.choices()))\n    sign_count = models.PositiveIntegerField()\n\n    class Meta:\n        unique_together = ((\"user\", \"key_handle\"), (\"user\", \"name\"))\n\n    def get_type_for_display(self):\n        return \"Security key\"\n\n    def get_verification_url(self):\n        return reverse(\"accounts:verify_webauthn\")\n\n    def test_user_agent(self, user_agent):\n        return True\n\n    def get_key_handle_bytes(self):\n        return trimmed_urlsafe_b64decode(self.key_handle)\n\n    def get_appid(self):\n        if self.rp_id.startswith(\"https://\"):\n            \n            return self.rp_id\n",
        "summary": "This Python code defines a Django model for users, including fields for email, remote status, and service account status. It also includes methods for password management, verification device handling, and user permissions. The models are designed to support both TOTP and WebAuthn authentication methods."
    },
    {
        "code": "from deepdab.ai import *\n\n\nclass TDZeroPolicy(TabularPolicy):\n    def __init__(self, board_size, learning_rate=0.0, gamma=0.0, epsilon=0.0, initial_state_value=0.0, table_file_path=None):\n        super(TDZeroPolicy, self).__init__(board_size=board_size, epsilon=epsilon,\n                                           initial_state_value=initial_state_value, table_file_path=table_file_path)\n        self._learning_rate = learning_rate\n        self._gamma = gamma\n\n    def update_value(self, reward, initial_state, selected_state):\n        initial_state_string = self._find_state_string(initial_state)\n        selected_state_string = self._find_state_string(selected_state)\n        initial_state_value = self._value_table[initial_state_string]\n        selected_state_value = self._value_table[selected_state_string]\n        self._value_table[initial_state_string] = initial_state_value + self._learning_rate * (reward + (self._gamma * selected_state_value) - initial_state_value)\n",
        "summary": "The `TDZeroPolicy` class extends a tabular policy for the DeepDab game, implementing the Temporal Difference Zero learning algorithm to update state values based on rewards and future state values. It includes methods for initializing the policy with parameters such as board size, learning rate, discount factor, exploration rate, initial state value, and an optional table file path, as well as a method to update the value of states in the policy's value table according to the TD(0) formula."
    },
    {
        "code": "from sklearn2sql_heroku.tests.classification import generic as class_gen\n\n\nclass_gen.test_model(\"SGDClassifier\" , \"digits\" , \"db2\")\n",
        "summary": "The provided Python code imports a module from the `sklearn2sql_heroku` package and then calls a function named `test_model` from that module. The function is invoked with three arguments: \"SGDClassifier\", \"digits\", and \"db2\". This suggests that the function is used to test a machine learning model, specifically an SGDClassifier, on a dataset related to digits (likely referring to handwritten digit recognition) using a database named \"db2\" for storage or retrieval of data."
    },
    {
        "code": "from celery import shared_task\n\nfrom .signals import slack_event_received\n\n\n@shared_task\ndef receive_slack_signal_task(sender, event_type, event_data, **data):\n    slack_event_received.send(sender=sender, event_type=event_type, event_data=event_data, **data)\n\n",
        "summary": "The provided Python code defines a Celery task named `receive_slack_signal_task` that listens for signals related to Slack events. When such a signal is received, it triggers the `slack_event_received` signal with the original sender, event type, and data, allowing other parts of the application to handle these events asynchronously."
    },
    {
        "code": "from math import erf\n\nstd = 10\nh1 = 80\nh2 = 60\nmean = 70\n\ndef N(mean, std, x):\n    return 0.5 + 0.5 * erf((x-mean)/(std* 2**0.5))\n\nprint (round(((1 - N(mean,std,h1))*100),2))\nprint (round(((1 - N(mean,std,h2))*100),2))\nprint (round(((N(mean,std,h2)*100)),2))",
        "summary": "The Python code calculates the percentage of data points that fall below a certain threshold using the error function from the math library, given a mean and standard deviation. It then prints three percentages: one for values below h1, another for values below h2, and the third for values above h2."
    },
    {
        "code": "!pip3 install -r requirements.txt\n!python3 -m spacy download en_core_web_sm\n",
        "summary": "The provided Python commands first install the dependencies listed in `requirements.txt` using pip, and then download the English language model for spaCy's natural language processing capabilities."
    },
    {
        "code": "from bizfriendly import app\nfrom flask.ext.heroku import Heroku\nimport os\n\nheroku = Heroku(app) \napp.config.update(\n    \n    \n    \n    \n)\n\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY')\napp.config['MAIL_GUN_KEY'] = os.environ.get('MAIL_GUN_KEY')\napp.config['AWS_ACCESS_KEY_ID'] = os.environ.get('AWS_ACCESS_KEY_ID')\napp.config['AWS_SECRET_ACCESS_KEY'] = os.environ.get('AWS_SECRET_ACCESS_KEY')\napp.config['S3_BUCKET_NAME'] = os.environ.get('S3_BUCKET_NAME')\n\ndef add_cors_header(response):\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Access-Control-Allow-Headers'] = 'Authorization, Content-Type'\n    response.headers['Access-Control-Allow-Methods'] = 'POST, GET, PUT, PATCH, DELETE, OPTIONS'\n    return response\napp.after_request(add_cors_header)",
        "summary": "The Python code sets up a Flask application using the `bizfriendly` package and integrates it with Heroku. It configures the app with environment variables for security keys, email services, and AWS credentials, and adds CORS headers to allow cross-origin requests."
    },
    {
        "code": "from itertools import product\n\naarr = list(map(int, input().split()))\nbarr = list(map(int, input().split()))\n\nprint(' '.join(str(i) for i in list(product(*[aarr, barr]))))\n",
        "summary": "The Python code reads two lists of integers from user input, then computes the Cartesian product of these lists and prints each resulting pair as a space-separated string."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n\n\n\nimport simpy\n\n\n\nimport numpy as np\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\n\nNUM_RUNS = 1\n\nSIM_DURATION = 5*8*52\n\nNUM_SPARES = 20\n\nNUM_REPAIRERS = 5\n\n\n\nclass Factory(object):\n    \n    def __init__(self, env, num_repairers, num_spares):\n        \n        self.repairers = simpy.Resource(env, capacity=num_repairers) \n        self.spares = simpy.Container(env, init=num_spares, capacity=num_spares)\n        self.env = env\n        self.cost = 0\n        self.daily_cost = 3.75*8*num_repairers + 30*num_spares\n    \n    def run(self):\n        \n        \n        for i in range(50):\n            self.env.process(factory.operate_machine(i+1))\n        \n        while True:\n            self.cost += self.daily_cost\n            yield self.env.timeout(8.0)\n    \n    def operate_machine(self, machine):\n        \n        while True:\n            \n            yield self.env.timeout(np.random.uniform(132,182))\n            time_broken = self.env.now\n            if NUM_RUNS <= 1:\n                print('machine {} broke at {:.2f} ({} spares available)'.format(\n                        machine, time_broken, self.spares.level))\n            \n            self.env.process(self.repair_machine())\n            \n            yield self.spares.get(1)\n            time_replaced = self.env.now\n            if NUM_RUNS <= 1:\n                print('machine {} replaced at {:.2f}'.format(machine, time_replaced))\n            \n            self.cost += 20*(time_replaced-time_broken)\n              \n    def repair_machine(self):\n        \n        with self.repairers.request() as request:\n            \n            yield request\n            \n            yield self.env.timeout(np.random.uniform(4,10))\n            \n            yield self.spares.put(1)\n            if NUM_RUNS <= 1:\n                print('repair complete at {:.2f} ({} spares available)'.format(\n                        self.env.now, self.spares.level))\n\n\nobs_time = []\nobs_cost = []\nobs_spares = []\n\ndef observe(env, factory):\n    \n    while True:\n        obs_time.append(env.now)\n        obs_cost.append(factory.cost)\n        obs_spares.append(factory.spares.level)\n        yield env.timeout(1.0)\n\n\n\n\nCOST = []\n\nfor i in range(NUM_RUNS):\n    \n    np.random.seed(i)\n    \n    \n    env = simpy.Environment()\n    \n    factory = Factory(env, NUM_REPAIRERS, NUM_SPARES)\n    \n    env.process(factory.run())\n    \n    env.process(observe(env, factory))\n    \n    env.run(until=SIM_DURATION)\n    \n    COST.append(obs_cost[-1])\n    \n    if NUM_RUNS <= 1:\n        \n        print('Total cost: {:.2f}'.format(factory.cost))\n        \n        \n        plt.figure()\n        plt.step(obs_time, obs_spares, where='post')\n        plt.xlabel('Time (hour)')\n        plt.ylabel('Number Spares Available')\n        \n        \n        plt.figure()\n        plt.step(obs_time, obs_cost, where='post')\n        plt.xlabel('Time (hour)')\n        plt.ylabel('Total Cost')\n\n    \n    print('Factory costs for N={:} runs with R={:} repairers and S={:} spares:'.format(\n            NUM_RUNS, NUM_REPAIRERS, NUM_SPARES))\n    print('\\n'.join('{:.2f}'.format(i) for i in COST))\n\n\n\nimport csv\n\nwith open('factory.csv', 'w') as output:\n    writer = csv.writer(output)\n    for sample in COST:\n        writer.writerow([sample])",
        "summary": "The Python code simulates a factory's operations over a specified duration, tracking the cost and availability of spare parts. It uses SimPy for discrete-event simulation, numpy for random number generation, and matplotlib for plotting results. The simulation models machine failures, repair times, and spare part usage, providing insights into optimal resource allocation for minimizing costs."
    },
    {
        "code": "import logging\n\nfrom tornado import web\nfrom tornado import gen\n\nfrom ..views import BaseHandler\nfrom ..api.workers import ListWorkers\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass WorkerView(BaseHandler):\n    @web.authenticated\n    @gen.coroutine\n    def get(self, name):\n        try:\n            yield ListWorkers.update_workers(app=self.application, workername=name)\n        except Exception as e:\n            logger.error(e)\n\n        worker = ListWorkers.worker_cache.get(name)\n\n        if worker is None:\n            raise web.HTTPError(404, \"Unknown worker '%s'\" % name)\n        if 'stats' not in worker:\n            raise web.HTTPError(\n                404,\n                \"Unable to get stats for '%s' worker\" % name\n            )\n\n        self.render(\"worker.html\", worker=dict(worker, name=name))\n",
        "summary": "The `WorkerView` class is a Tornado web handler that retrieves and displays statistics for a specific worker. It uses the `ListWorkers` API to update worker information and caches it. If the worker or its stats are not found, it raises appropriate HTTP errors. The view requires authentication and renders an HTML template with the worker's data."
    },
    {
        "code": "from great_expectations.render.renderer.content_block.content_block import (\n    ContentBlockRenderer,\n)\nfrom great_expectations.render.types import (\n    RenderedBulletListContent,\n    RenderedStringTemplateContent,\n)\n\n\nclass ExceptionListContentBlockRenderer(ContentBlockRenderer):\n    \n\n    _rendered_component_type = RenderedBulletListContent\n    _content_block_type = \"bullet_list\"\n\n    _default_header = 'Failed expectations <span class=\"mr-3 triangle\"></span>'\n\n    _default_content_block_styling = {\n        \"classes\": [\"col-12\"],\n        \"styles\": {\"margin-top\": \"20px\"},\n        \"header\": {\n            \"classes\": [\"collapsed\"],\n            \"attributes\": {\n                \"data-toggle\": \"collapse\",\n                \"href\": \"\n                \"role\": \"button\",\n                \"aria-expanded\": \"true\",\n                \"aria-controls\": \"collapseExample\",\n            },\n            \"styles\": {\n                \"cursor\": \"pointer\",\n            },\n        },\n        \"body\": {\n            \"classes\": [\"list-group\", \"collapse\"],\n        },\n    }\n\n    _default_element_styling = {\n        \"classes\": [\n            \"list-group-item\"\n        ],  \n        \"params\": {\n            \"column\": {\"classes\": [\"badge\", \"badge-primary\"]},\n            \"expectation_type\": {\"classes\": [\"text-monospace\"]},\n            \"exception_message\": {\"classes\": [\"text-monospace\"]},\n        },\n    }\n\n    @classmethod\n    def render(cls, render_object, **kwargs):\n        return super().render(\n            render_object=render_object, exception_list_content_block=True\n        )\n\n    @classmethod\n    def _missing_content_block_fn(\n        cls,\n        configuration=None,\n        result=None,\n        language=None,\n        runtime_configuration=None,\n        **kwargs,\n    ):\n        runtime_configuration = runtime_configuration or {}\n        include_column_name = runtime_configuration.get(\"include_column_name\", True)\n        include_column_name = (\n            include_column_name if include_column_name is not None else True\n        )\n        styling = runtime_configuration.get(\"styling\")\n        \n        if result.exception_info[\"raised_exception\"] is True:\n            template_str = \"$expectation_type raised an exception: $exception_message\"\n            if include_column_name:\n                template_str = f\"$column: {template_str}\"\n\n            try:\n                column = result.expectation_config.kwargs[\"column\"]\n            except KeyError:\n                column = None\n            return [\n                RenderedStringTemplateContent(\n                    **{\n                        \"content_block_type\": \"string_template\",\n                        \"string_template\": {\n                            \"template\": template_str,\n                            \"params\": {\n                                \"column\": column,\n                                \"expectation_type\": result.expectation_config.expectation_type,\n                                \"exception_message\": result.exception_info[\n                                    \"exception_message\"\n                                ],\n                            },\n                            \"styling\": styling,\n                        },\n                    }\n                )\n            ]\n",
        "summary": "The `ExceptionListContentBlockRenderer` class extends `ContentBlockRenderer` to create a custom renderer for displaying failed expectations in a list format, with each expectation's type and exception message styled appropriately. It includes methods for rendering the content block and handling missing content blocks by providing a template string that can be customized based on runtime configuration."
    },
    {
        "code": "import time\nimport threading\nimport base64\nfrom functools import partial\n\nimport smtplib\nimport imaplib\nimport email\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email.encoders import encode_base64\n\nfrom PyQt5.QtGui import *\nfrom PyQt5.QtCore import *\nimport PyQt5.QtGui as QtGui\nfrom PyQt5.QtWidgets import (QVBoxLayout, QLabel, QGridLayout, QLineEdit)\n\nfrom electrum_dash.plugins import BasePlugin, hook\nfrom electrum_dash.paymentrequest import PaymentRequest\nfrom electrum_dash.i18n import _\nfrom electrum_dash_gui.qt.util import EnterButton, Buttons, CloseButton\nfrom electrum_dash_gui.qt.util import OkButton, WindowModalDialog\n\n\nclass Processor(threading.Thread):\n    polling_interval = 5*60\n\n    def __init__(self, imap_server, username, password, callback):\n        threading.Thread.__init__(self)\n        self.daemon = True\n        self.username = username\n        self.password = password\n        self.imap_server = imap_server\n        self.on_receive = callback\n\n    def poll(self):\n        try:\n            self.M.select()\n        except:\n            return\n        typ, data = self.M.search(None, 'ALL')\n        for num in data[0].split():\n            typ, msg_data = self.M.fetch(num, '(RFC822)')\n            msg = email.message_from_string(msg_data[0][1])\n            p = msg.get_payload()\n            if not msg.is_multipart():\n                p = [p]\n                continue\n            for item in p:\n                if item.get_content_type() == \"application/dash-paymentrequest\":\n                    pr_str = item.get_payload()\n                    pr_str = base64.b64decode(pr_str)\n                    self.on_receive(pr_str)\n\n    def run(self):\n        self.M = imaplib.IMAP4_SSL(self.imap_server)\n        self.M.login(self.username, self.password)\n        while True:\n            self.poll()\n            time.sleep(self.polling_interval)\n        self.M.close()\n        self.M.logout()\n\n    def send(self, recipient, message, payment_request):\n        msg = MIMEMultipart()\n        msg['Subject'] = message\n        msg['To'] = recipient\n        msg['From'] = self.username\n        part = MIMEBase('application', \"dash-paymentrequest\")\n        part.set_payload(payment_request)\n        encode_base64(part)\n        part.add_header('Content-Disposition', 'attachment; filename=\"payreq.dash\"')\n        msg.attach(part)\n        s = smtplib.SMTP_SSL(self.imap_server, timeout=2)\n        s.login(self.username, self.password)\n        s.sendmail(self.username, [recipient], msg.as_string())\n        s.quit()\n\n\nclass QEmailSignalObject(QObject):\n    email_new_invoice_signal = pyqtSignal()\n\n\nclass Plugin(BasePlugin):\n\n    def fullname(self):\n        return 'Email'\n\n    def description(self):\n        return _(\"Send and receive payment requests via email\")\n\n    def is_available(self):\n        return True\n\n    def __init__(self, parent, config, name):\n        BasePlugin.__init__(self, parent, config, name)\n        self.imap_server = self.config.get('email_server', '')\n        self.username = self.config.get('email_username', '')\n        self.password = self.config.get('email_password', '')\n        if self.imap_server and self.username and self.password:\n            self.processor = Processor(self.imap_server, self.username, self.password, self.on_receive)\n            self.processor.start()\n        self.obj = QEmailSignalObject()\n        self.obj.email_new_invoice_signal.connect(self.new_invoice)\n\n    def on_receive(self, pr_str):\n        self.print_error('received payment request')\n        self.pr = PaymentRequest(pr_str)\n        self.obj.email_new_invoice_signal.emit()\n\n    def new_invoice(self):\n        self.parent.invoices.add(self.pr)\n        \n\n    @hook\n    def receive_list_menu(self, menu, addr):\n        window = menu.parentWidget()\n        menu.addAction(_(\"Send via e-mail\"), lambda: self.send(window, addr))\n\n    def send(self, window, addr):\n        from electrum_dash import paymentrequest\n        r = window.wallet.receive_requests.get(addr)\n        message = r.get('memo', '')\n        if r.get('signature'):\n            pr = paymentrequest.serialize_request(r)\n        else:\n            pr = paymentrequest.make_request(self.config, r)\n        if not pr:\n            return\n        recipient, ok = QtGui.QInputDialog.getText(window, 'Send request', 'Email invoice to:')\n        if not ok:\n            return\n        recipient = str(recipient)\n        payload = pr.SerializeToString()\n        self.print_error('sending mail to', recipient)\n        try:\n            self.processor.send(recipient, message, payload)\n        except BaseException as e:\n            window.show_message(str(e))\n            return\n\n        window.show_message(_('Request sent.'))\n\n\n    def requires_settings(self):\n        return True\n\n    def settings_widget(self, window):\n        return EnterButton(_('Settings'), partial(self.settings_dialog, window))\n\n    def settings_dialog(self, window):\n        d = WindowModalDialog(window, _(\"Email settings\"))\n        d.setMinimumSize(500, 200)\n\n        vbox = QVBoxLayout(d)\n        vbox.addWidget(QLabel(_('Server hosting your email acount')))\n        grid = QGridLayout()\n        vbox.addLayout(grid)\n        grid.addWidget(QLabel('Server (IMAP)'), 0, 0)\n        server_e = QLineEdit()\n        server_e.setText(self.imap_server)\n        grid.addWidget(server_e, 0, 1)\n\n        grid.addWidget(QLabel('Username'), 1, 0)\n        username_e = QLineEdit()\n        username_e.setText(self.username)\n        grid.addWidget(username_e, 1, 1)\n\n        grid.addWidget(QLabel('Password'), 2, 0)\n        password_e = QLineEdit()\n        password_e.setText(self.password)\n        grid.addWidget(password_e, 2, 1)\n\n        vbox.addStretch()\n        vbox.addLayout(Buttons(CloseButton(d), OkButton(d)))\n\n        if not d.exec_():\n            return\n\n        server = str(server_e.text())\n        self.config.set_key('email_server', server)\n\n        username = str(username_e.text())\n        self.config.set_key('email_username', username)\n\n        password = str(password_e.text())\n        self.config.set_key('email_password', password)\n",
        "summary": "This Python code defines a plugin for the Electrum-Dash cryptocurrency wallet that allows users to send and receive payment requests via email. It uses IMAP and SMTP protocols to handle email communication, processes incoming payment requests, and provides a GUI for configuring email settings and sending payment requests."
    },
    {
        "code": "from __future__ import absolute_import\n\nfrom mock import patch\nimport pytest\n\nfrom module_build_service import app\nfrom module_build_service.common import models\nfrom module_build_service.common.models import BUILD_STATES, ModuleBuild\nfrom module_build_service.manage import manager_wrapper, retire\nfrom module_build_service.scheduler.db_session import db_session\nfrom module_build_service.web.utils import deps_to_dict\nfrom tests import clean_database, staged_data_filename\n\n\n@pytest.mark.usefixtures(\"model_tests_init_data\")\nclass TestMBSManage:\n\n    @pytest.mark.parametrize(\n        (\"identifier\", \"is_valid\"),\n        (\n            (\"\", False),\n            (\"spam\", False),\n            (\"spam:bacon\", True),\n            (\"spam:bacon:eggs\", True),\n            (\"spam:bacon:eggs:ham\", True),\n            (\"spam:bacon:eggs:ham:sausage\", False),\n        ),\n    )\n    def test_retire_identifier_validation(self, identifier, is_valid):\n        if is_valid:\n            retire(identifier)\n        else:\n            with pytest.raises(ValueError):\n                retire(identifier)\n\n    @pytest.mark.parametrize(\n        (\"overrides\", \"identifier\", \"changed_count\"),\n        (\n            ({\"name\": \"pickme\"}, \"pickme:eggs\", 1),\n            ({\"stream\": \"pickme\"}, \"spam:pickme\", 1),\n            ({\"version\": \"pickme\"}, \"spam:eggs:pickme\", 1),\n            ({\"context\": \"pickme\"}, \"spam:eggs:ham:pickme\", 1),\n            ({}, \"spam:eggs\", 3),\n            ({\"version\": \"pickme\"}, \"spam:eggs\", 3),\n            ({\"context\": \"pickme\"}, \"spam:eggs:ham\", 3),\n        ),\n    )\n    @patch(\"module_build_service.manage.prompt_bool\")\n    def test_retire_build(self, prompt_bool, overrides, identifier, changed_count):\n        prompt_bool.return_value = True\n\n        module_builds = (\n            db_session.query(ModuleBuild)\n            .filter_by(state=BUILD_STATES[\"ready\"])\n            .order_by(ModuleBuild.id.desc())\n            .all()\n        )\n        \n        assert len(module_builds) == 3\n\n        for x, build in enumerate(module_builds):\n            build.name = \"spam\"\n            build.stream = \"eggs\"\n            build.version = \"ham\"\n            build.context = str(x)\n\n        for attr, value in overrides.items():\n            setattr(module_builds[0], attr, value)\n\n        db_session.commit()\n\n        retire(identifier)\n        retired_module_builds = (\n            db_session.query(ModuleBuild)\n            .filter_by(state=BUILD_STATES[\"garbage\"])\n            .order_by(ModuleBuild.id.desc())\n            .all()\n        )\n\n        assert len(retired_module_builds) == changed_count\n        for x in range(changed_count):\n            assert retired_module_builds[x].id == module_builds[x].id\n            assert retired_module_builds[x].state == BUILD_STATES[\"garbage\"]\n\n    @pytest.mark.parametrize(\n        (\"confirm_prompt\", \"confirm_arg\", \"confirm_expected\"),\n        (\n            (True, False, True),\n            (True, True, True),\n            (False, False, False),\n            (False, True, True)\n        ),\n    )\n    @patch(\"module_build_service.manage.prompt_bool\")\n    def test_retire_build_confirm_prompt(\n        self, prompt_bool, confirm_prompt, confirm_arg, confirm_expected\n    ):\n        prompt_bool.return_value = confirm_prompt\n\n        module_builds = db_session.query(ModuleBuild).filter_by(state=BUILD_STATES[\"ready\"]).all()\n        \n        assert len(module_builds) == 3\n\n        for x, build in enumerate(module_builds):\n            build.name = \"spam\" + str(x) if x > 0 else \"spam\"\n            build.stream = \"eggs\"\n\n        db_session.commit()\n\n        retire(\"spam:eggs\", confirm_arg)\n        retired_module_builds = (\n            db_session.query(ModuleBuild).filter_by(state=BUILD_STATES[\"garbage\"]).all()\n        )\n\n        expected_changed_count = 1 if confirm_expected else 0\n        assert len(retired_module_builds) == expected_changed_count\n\n\nclass TestCommandBuildModuleLocally:\n    \n\n    def setup_method(self, test_method):\n        clean_database()\n\n        \n        \n        self.sys_exit_patcher = patch(\"sys.exit\")\n        self.mock_sys_exit = self.sys_exit_patcher.start()\n\n        \n        \n        self.publish_patcher = patch(\"module_build_service.common.messaging.publish\")\n        self.mock_publish = self.publish_patcher.start()\n\n        \n        self.set_item_patcher = patch(\"module_build_service.manage.conf.set_item\")\n        self.mock_set_item = self.set_item_patcher.start()\n\n        \n        \n        self.create_all_patcher = patch(\"module_build_service.manage.db.create_all\")\n        self.mock_create_all = self.create_all_patcher.start()\n\n    def teardown_method(self, test_method):\n        self.create_all_patcher.stop()\n        self.mock_set_item.stop()\n        self.publish_patcher.stop()\n        self.sys_exit_patcher.stop()\n\n    def _run_manager_wrapper(self, cli_cmd):\n        \n        \n        \n        original_db_uri = app.config[\"SQLALCHEMY_DATABASE_URI\"]\n        try:\n            with patch(\"sys.argv\", new=cli_cmd):\n                manager_wrapper()\n        finally:\n            app.config[\"SQLALCHEMY_DATABASE_URI\"] = original_db_uri\n\n    @patch(\"module_build_service.scheduler.local.main\")\n    def test_set_stream(self, main):\n        cli_cmd = [\n            \"mbs-manager\", \"build_module_locally\",\n            \"--set-stream\", \"platform:f28\",\n            \"--file\", staged_data_filename(\"testmodule-local-build.yaml\")\n        ]\n\n        self._run_manager_wrapper(cli_cmd)\n\n        \n        \n        \n        \n        \n        \n        \n        \n\n        builds = db_session.query(models.ModuleBuild).filter_by(\n            name=\"testmodule-local-build\").all()\n        assert 1 == len(builds)\n\n        testmodule_build = builds[0]\n        mmd_deps = testmodule_build.mmd().get_dependencies()\n\n        deps_dict = deps_to_dict(mmd_deps[0], \"buildtime\")\n        assert [\"f28\"] == deps_dict[\"platform\"]\n        deps_dict = deps_to_dict(mmd_deps[0], \"runtime\")\n        assert [\"f28\"] == deps_dict[\"platform\"]\n\n    @patch(\"module_build_service.manage.logging\")\n    def test_ambiguous_stream(self, logging):\n        cli_cmd = [\n            \"mbs-manager\", \"build_module_locally\",\n            \"--file\", staged_data_filename(\"testmodule-local-build.yaml\")\n        ]\n\n        self._run_manager_wrapper(cli_cmd)\n\n        args, _ = logging.error.call_args_list[0]\n        assert \"There are multiple streams to choose from for module platform.\" == args[0]\n        args, _ = logging.error.call_args_list[1]\n        assert \"Use '-s module_name:module_stream' to choose the stream\" == args[0]\n\n    def test_module_build_failed(self):\n        cli_cmd = [\n            \"mbs-manager\", \"build_module_locally\",\n            \"--set-stream\", \"platform:f28\",\n            \"--file\", staged_data_filename(\"testmodule-local-build.yaml\")\n        ]\n\n        def main_side_effect(module_build_ids):\n            build = db_session.query(models.ModuleBuild).filter(\n                models.ModuleBuild.name == \"testmodule-local-build\"\n            ).first()\n            build.state = models.BUILD_STATES[\"failed\"]\n            db_session.commit()\n\n        \n        \n\n        with patch(\"module_build_service.scheduler.local.main\",\n                   side_effect=main_side_effect):\n            with pytest.raises(RuntimeError, match=\"Module build failed\"):\n                self._run_manager_wrapper(cli_cmd)\n",
        "summary": "The provided Python code is a test suite for the `module_build_service` application, using Pytest and mocking various components to ensure functionality. It includes tests for retiring module builds based on identifiers and overrides, as well as testing local module builds with different configurations and error handling scenarios."
    },
    {
        "code": "import random\nfrom zope.interface import implements\n\n\nfrom twisted.python import log, failure, components\nfrom twisted.internet import interfaces, error, defer\n\n\nclass Factory:\n    \n\n    implements(interfaces.IProtocolFactory)\n\n    \n    protocol = None\n\n    numPorts = 0\n    noisy = True\n\n    def doStart(self):\n        \n        if not self.numPorts:\n            if self.noisy:\n                log.msg(\"Starting factory %r\" % self)\n            self.startFactory()\n        self.numPorts = self.numPorts + 1\n\n    def doStop(self):\n        \n        if self.numPorts == 0:\n            \n            \n            return\n        self.numPorts = self.numPorts - 1\n        if not self.numPorts:\n            if self.noisy:\n                log.msg(\"Stopping factory %r\" % self)\n            self.stopFactory()\n\n    def startFactory(self):\n        \n\n    def stopFactory(self):\n        \n\n    def buildProtocol(self, addr):\n        \n        p = self.protocol()\n        p.factory = self\n        return p\n\n\nclass ClientFactory(Factory):\n    \n\n    def startedConnecting(self, connector):\n        \n\n    def clientConnectionFailed(self, connector, reason):\n        \n\n    def clientConnectionLost(self, connector, reason):\n        \n\n\nclass _InstanceFactory(ClientFactory):\n    \n\n    noisy = False\n    \n    def __init__(self, reactor, instance, deferred):\n        self.reactor = reactor\n        self.instance = instance\n        self.deferred = deferred\n\n    def __repr__(self):\n        return \"<ClientCreator factory: %r>\" % (self.instance, )\n    \n    def buildProtocol(self, addr):\n        self.reactor.callLater(0, self.deferred.callback, self.instance)\n        del self.deferred\n        return self.instance\n\n    def clientConnectionFailed(self, connector, reason):\n        self.reactor.callLater(0, self.deferred.errback, reason)\n        del self.deferred\n\n\nclass ClientCreator:\n    \n\n    def __init__(self, reactor, protocolClass, *args, **kwargs):\n        self.reactor = reactor\n        self.protocolClass = protocolClass\n        self.args = args\n        self.kwargs = kwargs\n\n    def connectTCP(self, host, port, timeout=30, bindAddress=None):\n        \n        d = defer.Deferred()\n        f = _InstanceFactory(self.reactor, self.protocolClass(*self.args, **self.kwargs), d)\n        self.reactor.connectTCP(host, port, f, timeout=timeout, bindAddress=bindAddress)\n        return d\n\n    def connectUNIX(self, address, timeout = 30, checkPID=0):\n        \n        d = defer.Deferred()\n        f = _InstanceFactory(self.reactor, self.protocolClass(*self.args, **self.kwargs), d)\n        self.reactor.connectUNIX(address, f, timeout = timeout, checkPID=checkPID)\n        return d\n    \n    def connectSSL(self, host, port, contextFactory, timeout=30, bindAddress=None):\n        \n        d = defer.Deferred()\n        f = _InstanceFactory(self.reactor, self.protocolClass(*self.args, **self.kwargs), d)\n        self.reactor.connectSSL(host, port, f, contextFactory, timeout=timeout, bindAddress=bindAddress)\n        return d\n\n\nclass ReconnectingClientFactory(ClientFactory):\n    \n    maxDelay = 3600\n    initialDelay = 1.0\n    \n    \n    \n    factor = 2.7182818284590451 \n    \n    \n    jitter = 0.11962656492 \n\n    delay = initialDelay\n    retries = 0\n    maxRetries = None\n    _callID = None\n    connector = None\n\n    continueTrying = 1\n\n    def clientConnectionFailed(self, connector, reason):\n        if self.continueTrying:\n            self.connector = connector\n            self.retry()\n\n    def clientConnectionLost(self, connector, unused_reason):\n        if self.continueTrying:\n            self.connector = connector\n            self.retry()\n\n    def retry(self, connector=None):\n        \n        if not self.continueTrying:\n            if self.noisy:\n                log.msg(\"Abandoning %s on explicit request\" % (connector,))\n            return\n\n        if connector is None:\n            if self.connector is None:\n                raise ValueError(\"no connector to retry\")\n            else:\n                connector = self.connector\n\n        self.retries += 1\n        if self.maxRetries is not None and (self.retries > self.maxRetries):\n            if self.noisy:\n                log.msg(\"Abandoning %s after %d retries.\" %\n                        (connector, self.retries))\n            return\n\n        self.delay = min(self.delay * self.factor, self.maxDelay)\n        if self.jitter:\n            self.delay = random.normalvariate(self.delay,\n                                              self.delay * self.jitter)\n\n        if self.noisy:\n            log.msg(\"%s will retry in %d seconds\" % (connector, self.delay,))\n        from twisted.internet import reactor\n\n        def reconnector():\n            self._callID = None\n            connector.connect()\n        self._callID = reactor.callLater(self.delay, reconnector)\n\n    def stopTrying(self):\n        \n        \n        if self._callID:\n            self._callID.cancel()\n            self._callID = None\n        if self.connector:\n            \n            \n            try:\n                self.connector.stopConnecting()\n            except error.NotConnectingError:\n                pass\n        self.continueTrying = 0\n\n    def resetDelay(self):\n        \n        self.delay = self.initialDelay\n        self.retries = 0\n        self._callID = None\n        self.continueTrying = 1\n\n\nclass ServerFactory(Factory):\n    \n\n\nclass BaseProtocol:\n    \n\n    connected = 0\n    transport = None\n\n    def makeConnection(self, transport):\n        \n        self.connected = 1\n        self.transport = transport\n        self.connectionMade()\n\n    def connectionMade(self):\n        \n\nconnectionDone=failure.Failure(error.ConnectionDone())\nconnectionDone.cleanFailure()\n\n\nclass Protocol(BaseProtocol):\n\n    implements(interfaces.IProtocol)\n\n    def dataReceived(self, data):\n        \n\n    def connectionLost(self, reason=connectionDone):\n        \n\n\nclass ProtocolToConsumerAdapter(components.Adapter):\n    \n    implements(interfaces.IConsumer)\n\n    def write(self, data):\n        self.original.dataReceived(data)\n\n    def registerProducer(self, producer, streaming):\n        pass\n\n    def unregisterProducer(self):\n        pass\n\ncomponents.registerAdapter(ProtocolToConsumerAdapter, interfaces.IProtocol,\n                           interfaces.IConsumer)\n\nclass ConsumerToProtocolAdapter(components.Adapter):\n    \n    implements(interfaces.IProtocol)\n\n    def dataReceived(self, data):\n        self.original.write(data)\n\n    def connectionLost(self, reason):\n        pass\n\n    def makeConnection(self, transport):\n        pass\n\n    def connectionMade(self):\n        pass\n\ncomponents.registerAdapter(ConsumerToProtocolAdapter, interfaces.IConsumer,\n                           interfaces.IProtocol)\n\nclass ProcessProtocol(BaseProtocol):\n    \n\n    def childDataReceived(self, childFD, data):\n        if childFD == 1:\n            self.outReceived(data)\n        elif childFD == 2:\n            self.errReceived(data)\n\n    def outReceived(self, data):\n        \n    def errReceived(self, data):\n        \n\n    def childConnectionLost(self, childFD):\n        if childFD == 0:\n            self.inConnectionLost()\n        elif childFD == 1:\n            self.outConnectionLost()\n        elif childFD == 2:\n            self.errConnectionLost()\n\n    def inConnectionLost(self):\n        \n    def outConnectionLost(self):\n        \n    def errConnectionLost(self):\n        \n\n    def processEnded(self, reason):\n        \n\n\nclass AbstractDatagramProtocol:\n    \n\n    transport = None\n    numPorts = 0\n    noisy = True\n\n    def __getstate__(self):\n        d = self.__dict__.copy()\n        d['transport'] = None\n        return d\n\n    def doStart(self):\n        \n        if not self.numPorts:\n            if self.noisy:\n                log.msg(\"Starting protocol %s\" % self)\n            self.startProtocol()\n        self.numPorts = self.numPorts + 1\n\n    def doStop(self):\n        \n        assert self.numPorts > 0\n        self.numPorts = self.numPorts - 1\n        self.transport = None\n        if not self.numPorts:\n            if self.noisy:\n                log.msg(\"Stopping protocol %s\" % self)\n            self.stopProtocol()\n\n    def startProtocol(self):\n        \n\n    def stopProtocol(self):\n        \n\n    def makeConnection(self, transport):\n        \n        assert self.transport == None\n        self.transport = transport\n        self.doStart()\n\n    def datagramReceived(self, datagram, addr):\n        \n\n\nclass DatagramProtocol(AbstractDatagramProtocol):\n    \n\n    def connectionRefused(self):\n        \n\n\nclass ConnectedDatagramProtocol(DatagramProtocol):\n    \n\n    def datagramReceived(self, datagram):\n        \n\n    def connectionFailed(self, failure):\n        \n\n\n\nclass FileWrapper:\n    \n\n    implements(interfaces.ITransport)\n\n    closed = 0\n    disconnecting = 0\n    producer = None\n    streamingProducer = 0\n\n    def __init__(self, file):\n        self.file = file\n\n    def write(self, data):\n        try:\n            self.file.write(data)\n        except:\n            self.handleException()\n        \n\n    def _checkProducer(self):\n        \n        \n        if self.producer:\n            self.producer.resumeProducing()\n\n    def registerProducer(self, producer, streaming):\n        \n        self.producer = producer\n        self.streamingProducer = streaming\n        if not streaming:\n            producer.resumeProducing()\n\n    def unregisterProducer(self):\n        self.producer = None\n\n    def stopConsuming(self):\n        self.unregisterProducer()\n        self.loseConnection()\n\n    def writeSequence(self, iovec):\n        self.write(\"\".join(iovec))\n\n    def loseConnection(self):\n        self.closed = 1\n        try:\n            self.file.close()\n        except (IOError, OSError):\n            self.handleException()\n\n    def getPeer(self):\n        \n        return 'file', 'file'\n\n    def getHost(self):\n        \n        return 'file'\n\n    def handleException(self):\n        pass\n\n    def resumeProducing(self):\n        \n        pass\n\n    def pauseProducing(self):\n        \n        pass\n    \n    def stopProducing(self):\n        self.loseConnection()\n        \n\n__all__ = [\"Factory\", \"ClientFactory\", \"ReconnectingClientFactory\", \"connectionDone\", \n           \"Protocol\", \"ProcessProtocol\", \"FileWrapper\", \"ServerFactory\",\n           \"AbstractDatagramProtocol\", \"DatagramProtocol\", \"ConnectedDatagramProtocol\",\n           \"ClientCreator\"]\n",
        "summary": "This code defines a variety of classes and interfaces for network communication in Python, primarily using the Twisted framework. Here's a breakdown of some key components:\n\n1. **Base Classes**:\n   - `BaseProtocol`: A base class for protocols that implement basic connection lifecycle methods like `makeConnection`, `connectionMade`, and `connectionLost`.\n   - `AbstractDatagramProtocol`: A base class for datagram-based protocols, providing methods to handle incoming datagrams and manage connections.\n\n2. **Specific Protocols**:\n   - `Protocol`: A concrete implementation of a protocol that can receive data (`dataReceived`) and handle connection loss (`connectionLost`).\n   - `ProcessProtocol`: A protocol designed to interact with external processes, handling child process output and errors.\n   - `DatagramProtocol`, `ConnectedDatagramProtocol`: Specific implementations for datagram protocols, with the latter being connected-oriented.\n\n3. **Adapters**:\n   - `ProtocolToConsumerAdapter` and `ConsumerToProtocolAdapter`: Adapters that allow protocols to act as consumers or vice versa, facilitating integration between different components of a network application.\n\n4. **Utilities**:\n   - `FileWrapper`: A class that wraps a file object to provide a transport interface, allowing files to be used in network communication contexts.\n\n5. **Interfaces and Implementations**:\n   - The code uses Twisted's interfaces like `IProtocol`, `ITransport`, and `IConsumer` to define contracts for different components of the network stack.\n\n6. **Error Handling**:\n   - The `connectionDone` variable is used to represent a clean connection loss, which can be used in protocol implementations.\n\n7. **Reconnecting Client**:\n   - `ReconnectingClientFactory`: A factory that automatically reconnects a client when it loses its connection, providing robustness against transient network issues.\n\n8. **File Transport**:\n   - The `FileWrapper` class provides a way to use file objects as transport mechanisms, which can be useful for logging or other purposes where data needs to be written to files in a network-like manner.\n\nThis code is part of a larger system that uses Twisted to handle asynchronous network communication. It provides a flexible and extensible framework for building network applications, with support for both connection-oriented and datagram-based protocols."
    },
    {
        "code": "from pelican.readers import BaseReader\nfrom pelican.utils import pelican_open\nfrom pelican import signals\nimport six\n\ntry:\n    \n    from .asciidocapi import AsciiDocAPI, AsciiDocError\n    \n    AsciiDocAPI()\nexcept:\n    asciidoc_enabled = False\nelse:\n    asciidoc_enabled = True\n\n\n\nclass AsciiDocReader(BaseReader):\n    \n\n    enabled = asciidoc_enabled\n    file_extensions = ['asc', 'adoc', 'asciidoc']\n    default_options = [\"--no-header-footer\", \"-a newline=\\\\n\"]\n    default_backend = 'html5'\n\n    def read(self, source_path):\n        \n        from cStringIO import StringIO\n        with pelican_open(source_path) as source:\n            text = StringIO(source.encode('utf8'))\n        content = StringIO()\n        ad = AsciiDocAPI()\n\n        options = self.settings.get('ASCIIDOC_OPTIONS', [])\n        options = self.default_options + options\n        for o in options:\n            ad.options(*o.split())\n\n        backend = self.settings.get('ASCIIDOC_BACKEND', self.default_backend)\n        ad.execute(text, content, backend=backend)\n        content = content.getvalue().decode('utf8')\n\n        metadata = {}\n        for name, value in ad.asciidoc.document.attributes.items():\n            name = name.lower()\n            metadata[name] = self.process_metadata(name, six.text_type(value))\n        if 'doctitle' in metadata:\n            metadata['title'] = metadata['doctitle']\n        return content, metadata\n\ndef add_reader(readers):\n    for ext in AsciiDocReader.file_extensions:\n        readers.reader_classes[ext] = AsciiDocReader\n\ndef register():\n    signals.readers_init.connect(add_reader)\n",
        "summary": "The provided Python code defines an `AsciiDocReader` class that extends `BaseReader` from the Pelican static site generator to support reading and processing AsciiDoc files. It checks for the presence of the AsciiDoc API, enabling or disabling the reader accordingly. The `read` method processes AsciiDoc content using the AsciiDocAPI, converting it to HTML5 format with specified options and backend settings. The `add_reader` function registers the `AsciiDocReader` with Pelican's readers, and the `register` function connects this registration to the `readers_init` signal, ensuring the reader is available during the Pelican build process."
    },
    {
        "code": "from django.db import migrations\nimport mdeditor.fields\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('blog', '0003_post_clicks'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='post',\n            name='excerpt',\n        ),\n        migrations.AlterField(\n            model_name='post',\n            name='body',\n            field=mdeditor.fields.MDTextField(),\n        ),\n    ]\n",
        "summary": "This Django migration script removes the 'excerpt' field from the 'Post' model and replaces the 'body' field with an MDTextField using the mdeditor library."
    },
    {
        "code": "from django.conf.urls import url\nfrom rest_framework.urlpatterns import format_suffix_patterns\nfrom drfpasswordless.views import (ObtainEmailCallbackToken,\n                                   ObtainMobileCallbackToken,\n                                   ObtainAuthTokenFromCallbackToken,\n                                   VerifyAliasFromCallbackToken,\n                                   ObtainEmailVerificationCallbackToken,\n                                   ObtainMobileVerificationCallbackToken, )\n\nurlpatterns = [url(r'^callback/auth/$', ObtainAuthTokenFromCallbackToken.as_view(), name='auth_callback'),\n               url(r'^auth/email/$', ObtainEmailCallbackToken.as_view(), name='auth_email'),\n               url(r'^auth/mobile/$', ObtainMobileCallbackToken.as_view(), name='auth_mobile'),\n               url(r'^callback/verify/$', VerifyAliasFromCallbackToken.as_view(), name='verify_callback'),\n               url(r'^verify/email/$', ObtainEmailVerificationCallbackToken.as_view(), name='verify_email'),\n               url(r'^verify/mobile/$', ObtainMobileVerificationCallbackToken.as_view(), name='verify_mobile')]\n\nformat_suffix_patterns(urlpatterns)\n",
        "summary": "The provided Python code sets up URL patterns for handling authentication and verification using callback tokens in a Django project, utilizing the DRFPasswordless library to manage email and mobile number-based authentication. The `format_suffix_patterns` function is applied to allow optional format suffixes in URLs."
    },
    {
        "code": "import distutils.sysconfig\nimport os\nimport platform\nimport re\nimport sys\n\n\ndef get_python_relative_libdir():\n    \n    if platform.system() != 'Linux':\n        return None\n\n    \n    \n    \n    \n    \n    arch_specific_libdir = distutils.sysconfig.get_python_lib(True, False)\n    split_libdir = arch_specific_libdir.split(os.sep)\n    lib_re = re.compile(r\"^lib.+$\")\n\n    for i in range(len(split_libdir)):\n        match = lib_re.match(split_libdir[i])\n        if match is not None:\n            \n            \n            \n            return os.sep.join(split_libdir[i:])\n    \n    return None\n\nif __name__ == '__main__':\n    lib_dir = get_python_relative_libdir()\n    if lib_dir is not None:\n        sys.stdout.write(lib_dir)\n        sys.exit(0)\n    else:\n        sys.exit(1)\n",
        "summary": "The Python script defines a function `get_python_relative_libdir()` that retrieves the relative path to the Python library directory on Linux systems, excluding architecture-specific paths. If successful, it prints the relative library path and exits with status 0; otherwise, it exits with status 1."
    },
    {
        "code": "import json\nimport os\nimport random\n\nimport allure\nimport coreapi\nimport pytest\n\nfrom adcm_client.objects import ADCMClient\nfrom adcm_pytest_plugin.utils import get_data_dir\nfrom adcm_pytest_plugin import utils\nfrom jsonschema import validate\n\n\nfrom tests.library import errorcodes as err\nfrom tests.library import steps\nfrom tests.library.utils import get_random_service, get_random_host_prototype\n\nSCHEMAS = os.path.join(os.path.dirname(__file__), \"schemas/\")\n\nhost_bad_configs = (({\"str-key\": \"{1bbb}\", \"required\": \"158\", \"option\": \"my.host\",\n                      \"sub\": {\"sub1\": 3}, \"credentials\": {\"sample_string\": \"test\",\n                                                          \"read_only_initiated\": 1}},\n                     \"should be integer\"),\n                    ({\"str-key\": 61, \"required\": 158, \"fkey\": 18.3,\n                      \"option\": \"my.host\", \"sub\": {\"sub1\": 3},\n                      \"credentials\": {\"sample_string\": \"txt\",\n                                      \"read_only_initiated\": {}}},\n                     'should be string'),\n                    ({\"str-key\": \"{1bbb}\", \"required\": 158, \"fkey\": 18.3,\n                      \"option\": \"my.host\", \"sub\": {\"sub1\": 9}},\n                     'not in option list'),\n                    ({\"str-key\": \"{1bbb}\", \"required\": 158, \"option\": 8080,\n                      \"sub\": {\"sub1\": {\"foo\": \"bar\"}}},\n                     'should be flat')\n                    )\n\n\n@pytest.fixture(scope=\"module\")\ndef hostprovider(sdk_client_ms: ADCMClient):\n    bundle = sdk_client_ms.upload_from_fs(get_data_dir(__file__, 'hostprovider_bundle'))\n    return bundle.provider_create(utils.random_string())\n\n\n@pytest.fixture(scope=\"module\")\ndef host(sdk_client_ms: ADCMClient, hostprovider):\n    return hostprovider.host_create(utils.random_string())\n\n\n@pytest.fixture(scope=\"module\")\ndef cluster(sdk_client_ms: ADCMClient):\n    return sdk_client_ms.upload_from_fs(get_data_dir(__file__, 'cluster_bundle'))\n\n\n@pytest.fixture(scope=\"module\")\ndef client(sdk_client_ms: ADCMClient, cluster, hostprovider):\n    return sdk_client_ms.adcm()._api.objects\n\n\nclass TestHost:\n    \n    def test_validate_host_prototype(self, client):\n        host_prototype = json.loads(json.dumps(client.stack.host.list()[0]))\n        schema = json.load(\n            open(SCHEMAS + '/stack_list_item_schema.json')\n        )\n        with allure.step('Match prototype with schema'):\n            assert validate(host_prototype, schema) is None\n        steps.delete_all_data(client)\n\n    def test_create_host(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_bundle'))\n        hp = bundle.provider_create(utils.random_string())\n        host = hp.host_create(utils.random_string())\n        host_status_before = host.status\n        host_fqdn_before = host.fqdn\n        with allure.step('Reread host'):\n            host.reread()\n            host_status_after = host.status\n            host_fqdn_after = host.fqdn\n        with allure.step('Check states and fqdn'):\n            assert host_fqdn_before == host_fqdn_after\n            assert host_status_before == host_status_after\n\n    def test_shouldnt_create_duplicate_host(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        hp.host_create(\"duplicate\")\n        with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n            hp.host_create('duplicate')\n        with allure.step('Check host conflict'):\n            err.HOST_CONFLICT.equal(e, 'duplicate host')\n\n    def test_shouldnt_create_host_with_unknown_prototype(self, client):\n        with allure.step('Create provider'):\n            provider_id = client.provider.create(prototype_id=client.stack.provider.list()[0]['id'],\n                                                 name=utils.random_string())['id']\n        with allure.step('Create host'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                client.host.create(prototype_id=random.randint(100, 500),\n                                   provider_id=provider_id,\n                                   fqdn=utils.random_string())\n        with allure.step('Check PROTOTYPE_NOT_FOUND error'):\n            err.PROTOTYPE_NOT_FOUND.equal(e, 'prototype doesn\\'t exist')\n\n    def test_shouldnt_create_host_wo_prototype(self, client):\n        with allure.step('Create provider'):\n            provider = client.provider.create(prototype_id=client.stack.provider.list()[0]['id'],\n                                              name=utils.random_string())\n        with allure.step('Try to create host without prototype'):\n            with pytest.raises(coreapi.exceptions.ParameterError) as e:\n                client.host.create(provider_id=provider['id'], fqdn=utils.random_string())\n        with allure.step('Check prototype_id error'):\n            assert str(e.value) == \"{'prototype_id': 'This parameter is required.'}\"\n\n    def test_shouldnt_create_host_wo_provider(self, client):\n        with allure.step('Create prototype'):\n            proto = get_random_host_prototype(client)\n            with pytest.raises(coreapi.exceptions.ParameterError) as e:\n                client.host.create(prototype_id=proto['id'], fqdn=utils.random_string())\n        with allure.step('Check provider_id error'):\n            assert str(e.value) == \"{'provider_id': 'This parameter is required.'}\"\n\n    def test_create_host_with_max_length_plus_1(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n            hp.host_create(utils.random_string(257))\n        with allure.step('Check LONG_NAME error'):\n            err.LONG_NAME.equal(e, 'Host name is too long. Max length is 256')\n\n    def test_shouldnt_create_host_with_wrong_name(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n            hp.host_create(utils.random_string() + utils.random_special_chars())\n        with allure.step('Check WRONG_NAME error'):\n            err.WRONG_NAME.equal(e, 'Host name is incorrect. '\n                                    'Only latin characters, digits, dots (.)')\n\n    def test_get_host_list(self, sdk_client_fs: ADCMClient):\n        \n        expected_list = set()\n        actual_list = set()\n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        for fqdn in utils.random_string_list():\n            hp.host_create(fqdn)\n            expected_list.add(fqdn)\n        for host in sdk_client_fs.host_list():\n            actual_list.add(host.fqdn)\n        with allure.step('Check created hosts with the data from the API'):\n            assert actual_list == expected_list\n\n    def test_get_host_info(self, client):\n        host = steps.create_host_w_default_provider(client, utils.random_string())\n        actual = steps.read_host(client, host['id'])\n        with allure.step('Check created host with the data from the API'):\n            del actual['status']\n            del host['status']\n            assert actual == host\n\n    def test_delete_host(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        host = hp.host_create(\"deletion_host\")\n        with allure.step('delete host'):\n            deletion_result = host.delete()\n        with allure.step('Check that host is deleted'):\n            assert deletion_result is None\n\n    def test_should_return_correct_error_when_read_deleted(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        host = hp.host_create(utils.random_string())\n        with allure.step('delete host'):\n            host.delete()\n        with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n            host.reread()\n        with allure.step('Check HOST_NOT_FOUND'):\n            err.HOST_NOT_FOUND.equal(e)\n\n    def test_should_return_correct_error_when_delete_nonexist_host(\n            self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        hp = bundle.provider_create(utils.random_string())\n        host = hp.host_create(utils.random_string())\n        with allure.step('delete host'):\n            host.delete()\n        with allure.step('delete host second time'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                host.delete()\n        with allure.step('Check HOST_NOT_FOUND'):\n            err.HOST_NOT_FOUND.equal(e, 'host doesn\\'t exist')\n\n    \n    def test_create_hostcomponent(self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(\n            __file__, 'cluster_service_hostcomponent'))\n        bundle_hp = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_simple'))\n        cluster = bundle.cluster_create(utils.random_string())\n        hp = bundle_hp.provider_create(utils.random_string())\n        host = hp.host_create(utils.random_string())\n        cluster.host_add(host)\n        service = cluster.service_add(name=\"ZOOKEEPER\")\n        component_list = service.component_list()\n        component = service.component(name='ZOOKEEPER_CLIENT')\n        with allure.step('Check component id and name'):\n            assert component.component_id == component_list[0].component_id\n            assert component.name == component_list[0].name\n\n    def test_get_hostcomponent_list(self, client):  \n        cluster = steps.create_cluster(client)\n        service = steps.read_service(client, get_random_service(client)['id'])\n        cluster_svc = client.cluster.service.create(cluster_id=cluster['id'],\n                                                    prototype_id=service['id'])\n        components = client.cluster.service.component.list(cluster_id=cluster['id'],\n                                                           service_id=cluster_svc['id'])\n        \n        hostcomponent_list = []\n        for fqdn in utils.random_string_list():\n            host = steps.create_host_w_default_provider(client, fqdn)\n            steps.add_host_to_cluster(client, host, cluster)\n            component = random.choice(components)['id']\n            hostcomponent_list.append({\"host_id\": host['id'], \"service_id\": cluster_svc['id'],\n                                       \"component_id\": component})\n        expected_hostcomponent_list = client.cluster.hostcomponent.create(\n            cluster_id=cluster['id'], hc=hostcomponent_list)\n        actual_hs_list = client.cluster.hostcomponent.list(cluster_id=cluster['id'])\n        with allure.step('Check created data with data from API'):\n            assert actual_hs_list == expected_hostcomponent_list\n\n\nclass TestHostConfig:\n    \n\n    def test_config_history_url_must_point_to_the_host_config(self, client):\n        host = steps.create_host_w_default_provider(client, utils.random_string())\n        config = {\"str-key\": \"{1bbb}\", \"required\": 158, \"option\": 8080, \"sub\": {\"sub1\": 2},\n                  \"credentials\": {\"sample_string\": \"txt\", \"read_only_initiated\": {}}}\n        i = 0\n        with allure.step('Create host history'):\n            while i < random.randint(0, 10):\n                client.host.config.history.create(host_id=host['id'],\n                                                  description=utils.random_string(),\n                                                  config=config)\n                i += 1\n            history = client.host.config.history.list(host_id=host['id'])\n        with allure.step('Check host history'):\n            for conf in history:\n                assert ('host/{0}/config/'.format(host['id']) in conf['url']) is True\n        steps.delete_all_data(client)\n\n    def test_get_default_host_config(self, client):\n        \n        host = steps.create_host_w_default_provider(client, utils.random_string())\n        config_json = {}\n        with allure.step('Get default configuration from host'):\n            config = client.host.config.current.list(host_id=host['id'])\n        if config:\n            config_json = json.loads(json.dumps(config))\n        schema = json.load(open(SCHEMAS + '/config_item_schema.json'))\n        with allure.step('Check config'):\n            assert validate(config_json, schema) is None\n        steps.delete_all_data(client)\n\n    def test_get_config_from_nonexistant_host(self, sdk_client_fs: ADCMClient):\n        \n        bundle_hp = sdk_client_fs.upload_from_fs(get_data_dir(\n            __file__, 'hostprovider_simple'))\n        hp = bundle_hp.provider_create(utils.random_string())\n        with allure.step('Get host config from a non existant host'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                hp.host(host_id=random.randint(100, 500))\n        with allure.step('Check error host doesn\\'t exist'):\n            err.HOST_NOT_FOUND.equal(e, 'host doesn\\'t exist')\n\n    def test_shouldnt_create_host_config_when_config_not_json_string(self, client):\n        \n        host = steps.create_host_w_default_provider(client, utils.random_string())\n        config = utils.random_string()\n        with allure.step('Try to create the host config from non-json string'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                client.host.config.history.create(host_id=host['id'], config=config)\n        with allure.step('Check error config should not be just one string'):\n            err.JSON_ERROR.equal(e, 'config should not be just one string')\n\n    def test_shouldnt_create_host_config_when_config_is_number(self, client):\n        \n        host = steps.create_host_w_default_provider(client, utils.random_string())\n        config = random.randint(100, 999)\n        with allure.step('Try to create the host configuration with a number'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                client.host.config.history.create(host_id=host['id'], config=config)\n        with allure.step('Check error should not be just one int or float'):\n            err.JSON_ERROR.equal(e, 'should not be just one int or float')\n\n    @pytest.mark.parametrize(('config', 'error'), host_bad_configs)\n    def test_change_host_config_negative(self, host, config, error):\n        \n        with allure.step('Try to create config when parameter is not integer'):\n            with pytest.raises(coreapi.exceptions.ErrorMessage) as e:\n                host.config_set(config)\n        with allure.step(f'Check error {error}'):\n            err.CONFIG_VALUE_ERROR.equal(e, error)\n\n    def test_should_create_host_config_when_parameter_is_integer_and_not_float(\n            self, sdk_client_fs: ADCMClient):\n        \n        bundle = sdk_client_fs.upload_from_fs(get_data_dir(__file__, 'hostprovider_bundle'))\n        hp = bundle.provider_create(utils.random_string())\n        host = hp.host_create(utils.random_string())\n        config = {\"str-key\": \"{1bbb}\", \"required\": 158, \"fkey\": 18, \"option\": \"my.host\",\n                  \"sub\": {\"sub1\": 3},\n                  \"credentials\": {\"sample_string\": \"txt\", \"read_only_initiated\": {}}}\n        host.config_set(config)\n",
        "summary": "This code defines a series of tests for the Host resource in an ADCM (Ambari Data Center Manager) system. The tests cover various aspects of managing hosts, including creating and updating configurations, retrieving configuration history, and handling errors.\n\nHere's a breakdown of some key points:\n\n1. **Host Creation**: Tests ensure that hosts can be created successfully using different providers.\n2. **Configuration Management**:\n   - Tests validate that host configurations are stored correctly in JSON format.\n   - It checks for errors when trying to set invalid configuration values, such as non-integer or non-float types.\n3. **Error Handling**:\n   - The tests include scenarios where attempts are made to create hosts with invalid IDs or configurations, ensuring appropriate error messages are returned.\n4. **Configuration History**: Tests verify that the history of host configurations is maintained and can be retrieved.\n\nThe code uses a combination of assertions, pytest fixtures, and custom error validation functions (`err.*`) to ensure that the system behaves as expected under various conditions.\n\nOverall, this test suite provides comprehensive coverage for the Host resource in an ADCM environment, ensuring robustness and reliability of the configuration management functionality."
    },
    {
        "code": "class Engine:\n    PANDAS = \"pandas\"\n    POSTGRES = \"postgres\"\n    PRESTO = \"Presto\"\n    SPARK = \"Spark\"\n    SQL_SERVER = \"SqlServer\"\n\n    known_engines = {PANDAS, POSTGRES, PRESTO, SPARK, SQL_SERVER}\n",
        "summary": "The `Engine` class defines constants for various database and data processing engines, including Pandas, PostgreSQL, Presto, Spark, and SQL Server, and maintains a set of these known engine types."
    },
    {
        "code": "failureok = 1     \noslcargs = \"-Werror\"\n\n\n\n\n",
        "summary": "The provided Python code sets two variables: `failureok` to 1, indicating that failures are acceptable, and `oslcargs` to \"-Werror\", which configures the system to treat warnings as errors."
    },
    {
        "code": "from multiprocessing import freeze_support\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom deafwave.full_node.full_node import FullNode\nfrom deafwave.rpc.full_node_rpc_api import FullNodeRpcApi\nfrom deafwave.server.outbound_message import NodeType\nfrom deafwave.server.start_service import run_service\nfrom deafwave.util.block_tools import BlockTools, test_constants\nfrom deafwave.util.config import load_config_cli\nfrom deafwave.util.default_root import DEFAULT_ROOT_PATH\nfrom deafwave.util.path import mkdir, path_from_root\n\nfrom .full_node_simulator import FullNodeSimulator\n\n\n\"\".encode(\"idna\")\n\nSERVICE_NAME = \"full_node\"\n\n\ndef service_kwargs_for_full_node_simulator(root_path: Path, config: Dict, bt: BlockTools) -> Dict:\n    mkdir(path_from_root(root_path, config[\"database_path\"]).parent)\n    constants = bt.constants\n\n    node = FullNode(\n        config,\n        root_path=root_path,\n        consensus_constants=constants,\n        name=SERVICE_NAME,\n    )\n\n    peer_api = FullNodeSimulator(node, bt)\n    network_id = config[\"selected_network\"]\n    kwargs = dict(\n        root_path=root_path,\n        node=node,\n        peer_api=peer_api,\n        node_type=NodeType.FULL_NODE,\n        advertised_port=config[\"port\"],\n        service_name=SERVICE_NAME,\n        server_listen_ports=[config[\"port\"]],\n        on_connect_callback=node.on_connect,\n        rpc_info=(FullNodeRpcApi, config[\"rpc_port\"]),\n        network_id=network_id,\n    )\n    return kwargs\n\n\ndef main() -> None:\n    config = load_config_cli(DEFAULT_ROOT_PATH, \"config.yaml\", SERVICE_NAME)\n    config[\"database_path\"] = config[\"simulator_database_path\"]\n    config[\"peer_db_path\"] = config[\"simulator_peer_db_path\"]\n    config[\"introducer_peer\"][\"host\"] = \"127.0.0.1\"\n    config[\"introducer_peer\"][\"port\"] = 58735\n    config[\"selected_network\"] = \"testnet0\"\n    config[\"simulation\"] = True\n    kwargs = service_kwargs_for_full_node_simulator(\n        DEFAULT_ROOT_PATH,\n        config,\n        BlockTools(test_constants),\n    )\n    return run_service(**kwargs)\n\n\nif __name__ == \"__main__\":\n    freeze_support()\n    main()\n",
        "summary": "The provided Python script sets up and runs a full node simulator using the Deafwave framework, configuring various parameters such as database paths, network settings, and RPC ports. It defines functions to load configuration, create service keyword arguments for the simulator, and execute the main service run with these configurations."
    },
    {
        "code": "from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n__author__ = \"Bharath Ramsundar\"\n__copyright__ = \"Copyright 2017, Stanford University\"\n__license__ = \"MIT\"\n\nimport os\nimport tempfile\nimport numpy as np\nfrom subprocess import call\nfrom scipy.spatial import ConvexHull\nfrom deepchem.feat.binding_pocket_features import BindingPocketFeaturizer\nfrom deepchem.feat.fingerprints import CircularFingerprint\nfrom deepchem.models.sklearn_models import SklearnModel\nfrom deepchem.utils import rdkit_util\n\n\ndef extract_active_site(protein_file, ligand_file, cutoff=4):\n  \n  protein_coords = rdkit_util.load_molecule(\n      protein_file, add_hydrogens=False)[0]\n  ligand_coords = rdkit_util.load_molecule(\n      ligand_file, add_hydrogens=True, calc_charges=True)[0]\n  num_ligand_atoms = len(ligand_coords)\n  num_protein_atoms = len(protein_coords)\n  pocket_inds = []\n  pocket_atoms = set([])\n  for lig_atom_ind in range(num_ligand_atoms):\n    lig_atom = ligand_coords[lig_atom_ind]\n    for protein_atom_ind in range(num_protein_atoms):\n      protein_atom = protein_coords[protein_atom_ind]\n      if np.linalg.norm(lig_atom - protein_atom) < cutoff:\n        if protein_atom_ind not in pocket_atoms:\n          pocket_atoms = pocket_atoms.union(set([protein_atom_ind]))\n  \n  pocket_atoms = list(pocket_atoms)\n  n_pocket_atoms = len(pocket_atoms)\n  pocket_coords = np.zeros((n_pocket_atoms, 3))\n  for ind, pocket_ind in enumerate(pocket_atoms):\n    pocket_coords[ind] = protein_coords[pocket_ind]\n\n  x_min = int(np.floor(np.amin(pocket_coords[:, 0])))\n  x_max = int(np.ceil(np.amax(pocket_coords[:, 0])))\n  y_min = int(np.floor(np.amin(pocket_coords[:, 1])))\n  y_max = int(np.ceil(np.amax(pocket_coords[:, 1])))\n  z_min = int(np.floor(np.amin(pocket_coords[:, 2])))\n  z_max = int(np.ceil(np.amax(pocket_coords[:, 2])))\n  return (((x_min, x_max), (y_min, y_max), (z_min, z_max)), pocket_atoms,\n          pocket_coords)\n\n\ndef compute_overlap(mapping, box1, box2):\n  \n  atom1 = set(mapping[box1])\n  atom2 = set(mapping[box2])\n  return len(atom1.intersection(atom2)) / float(len(atom1))\n\n\ndef get_all_boxes(coords, pad=5):\n  \n  hull = ConvexHull(coords)\n  boxes = []\n  for triangle in hull.simplices:\n    \n    \n    points = np.array(\n        [coords[triangle, 0], coords[triangle, 1], coords[triangle, 2]]).T\n    \n    x_min, x_max = np.amin(points[:, 0]), np.amax(points[:, 0])\n    x_min, x_max = int(np.floor(x_min)) - pad, int(np.ceil(x_max)) + pad\n    y_min, y_max = np.amin(points[:, 1]), np.amax(points[:, 1])\n    y_min, y_max = int(np.floor(y_min)) - pad, int(np.ceil(y_max)) + pad\n    z_min, z_max = np.amin(points[:, 2]), np.amax(points[:, 2])\n    z_min, z_max = int(np.floor(z_min)) - pad, int(np.ceil(z_max)) + pad\n    boxes.append(((x_min, x_max), (y_min, y_max), (z_min, z_max)))\n  return boxes\n\n\ndef boxes_to_atoms(atom_coords, boxes):\n  \n  mapping = {}\n  for box_ind, box in enumerate(boxes):\n    box_atoms = []\n    (x_min, x_max), (y_min, y_max), (z_min, z_max) = box\n    print(\"Handing box %d/%d\" % (box_ind, len(boxes)))\n    for atom_ind in range(len(atom_coords)):\n      atom = atom_coords[atom_ind]\n      x_cont = x_min <= atom[0] and atom[0] <= x_max\n      y_cont = y_min <= atom[1] and atom[1] <= y_max\n      z_cont = z_min <= atom[2] and atom[2] <= z_max\n      if x_cont and y_cont and z_cont:\n        box_atoms.append(atom_ind)\n    mapping[box] = box_atoms\n  return mapping\n\n\ndef merge_boxes(box1, box2):\n  \n  (x_min1, x_max1), (y_min1, y_max1), (z_min1, z_max1) = box1\n  (x_min2, x_max2), (y_min2, y_max2), (z_min2, z_max2) = box2\n  x_min = min(x_min1, x_min2)\n  y_min = min(y_min1, y_min2)\n  z_min = min(z_min1, z_min2)\n  x_max = max(x_max1, x_max2)\n  y_max = max(y_max1, y_max2)\n  z_max = max(z_max1, z_max2)\n  return ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n\n\ndef merge_overlapping_boxes(mapping, boxes, threshold=.8):\n  \n  num_boxes = len(boxes)\n  outputs = []\n  for i in range(num_boxes):\n    box = boxes[0]\n    new_boxes = []\n    new_mapping = {}\n    \n    contained = False\n    for output_box in outputs:\n      \n      new_mapping[output_box] = mapping[output_box]\n      if compute_overlap(mapping, box, output_box) == 1:\n        contained = True\n    if contained:\n      continue\n    \n    unique_box = True\n    for merge_box in boxes[1:]:\n      overlap = compute_overlap(mapping, box, merge_box)\n      if overlap < threshold:\n        new_boxes.append(merge_box)\n        new_mapping[merge_box] = mapping[merge_box]\n      else:\n        \n        \n        unique_box = False\n        merged = merge_boxes(box, merge_box)\n        new_boxes.append(merged)\n        new_mapping[merged] = list(\n            set(mapping[box]).union(set(mapping[merge_box])))\n    if unique_box:\n      outputs.append(box)\n      new_mapping[box] = mapping[box]\n    boxes = new_boxes\n    mapping = new_mapping\n  return outputs, mapping\n\n\nclass BindingPocketFinder(object):\n  \n\n  def find_pockets(self, protein_file, ligand_file):\n    \n    raise NotImplementedError\n\n\nclass ConvexHullPocketFinder(BindingPocketFinder):\n  \n\n  def __init__(self, pad=5):\n    self.pad = pad\n\n  def find_all_pockets(self, protein_file):\n    \n    \n    coords = rdkit_util.load_molecule(protein_file)[0]\n    return get_all_boxes(coords, self.pad)\n\n  def find_pockets(self, protein_file, ligand_file):\n    \n    protein_coords = rdkit_util.load_molecule(\n        protein_file, add_hydrogens=False, calc_charges=False)[0]\n    ligand_coords = rdkit_util.load_molecule(\n        ligand_file, add_hydrogens=False, calc_charges=False)[0]\n    boxes = get_all_boxes(protein_coords, self.pad)\n    mapping = boxes_to_atoms(protein_coords, boxes)\n    pockets, pocket_atoms_map = merge_overlapping_boxes(mapping, boxes)\n    pocket_coords = []\n    for pocket in pockets:\n      atoms = pocket_atoms_map[pocket]\n      coords = np.zeros((len(atoms), 3))\n      for ind, atom in enumerate(atoms):\n        coords[ind] = protein_coords[atom]\n      pocket_coords.append(coords)\n    return pockets, pocket_atoms_map, pocket_coords\n\n\nclass RFConvexHullPocketFinder(BindingPocketFinder):\n  \n\n  def __init__(self, pad=5):\n    self.pad = pad\n    self.convex_finder = ConvexHullPocketFinder(pad)\n\n    \n    self.base_dir = tempfile.mkdtemp()\n    print(\"About to download trained model.\")\n    \n    call((\n        \"wget -nv -c http://deepchem.io.s3-website-us-west-1.amazonaws.com/trained_models/pocket_random_refined_RF.tar.gz\"\n    ).split())\n    call((\"tar -zxvf pocket_random_refined_RF.tar.gz\").split())\n    call((\"mv pocket_random_refined_RF %s\" % (self.base_dir)).split())\n    self.model_dir = os.path.join(self.base_dir, \"pocket_random_refined_RF\")\n\n    \n    self.model = SklearnModel(model_dir=self.model_dir)\n    self.model.reload()\n\n    \n    self.pocket_featurizer = BindingPocketFeaturizer()\n    self.ligand_featurizer = CircularFingerprint(size=1024)\n\n  def find_pockets(self, protein_file, ligand_file):\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    raise ValueError(\"Karl Implement\")\n",
        "summary": "The provided Python code defines a class hierarchy for finding binding pockets in proteins using various methods. The `BindingPocketFinder` class is an abstract base class with subclasses like `ConvexHullPocketFinder` and `RFConvexHullPocketFinder`. These subclasses implement different algorithms to detect pockets, including one that uses convex hulls and another that employs a random forest model for more complex pocket detection. The code also includes utility functions for merging overlapping boxes and computing overlap between them."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import test\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.training.checkpointable import tracking\n\n\nclass LoadTest(test.TestCase):\n\n  def cycle(self, obj):\n    path = tempfile.mkdtemp(prefix=self.get_temp_dir())\n    save.save(obj, path, signatures={})\n    return load.load(path)\n\n  def test_structure_import(self):\n    root = tracking.Checkpointable()\n    root.dep_one = tracking.Checkpointable()\n    root.dep_two = tracking.Checkpointable()\n    root.dep_two.dep = tracking.Checkpointable()\n    root.dep_three = root.dep_two.dep\n    imported = self.cycle(root)\n    self.assertIs(imported.dep_three, imported.dep_two.dep)\n    self.assertIsNot(imported.dep_one, imported.dep_two)\n\n  def test_variables(self):\n    root = tracking.Checkpointable()\n    root.v1 = variables.Variable(1., trainable=True)\n    root.v2 = variables.Variable(2., trainable=False)\n    imported = self.cycle(root)\n    self.assertEquals(imported.v1.numpy(), 1.0)\n    self.assertTrue(imported.v1.trainable)\n    self.assertEquals(imported.v2.numpy(), 2.0)\n    self.assertFalse(imported.v2.trainable)\n\n  def test_capture_variables(self):\n    root = tracking.Checkpointable()\n    root.weights = variables.Variable(2.)\n    root.f = def_function.function(\n        lambda x: root.weights * x,\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n    imported = self.cycle(root)\n    self.assertEqual(4., imported.f(constant_op.constant(2.)).numpy())\n    imported.weights.assign(4.0)\n    self.assertEqual(8., imported.f(constant_op.constant(2.)).numpy())\n\n  def _make_asset(self, contents):\n    filename = tempfile.mktemp(prefix=self.get_temp_dir())\n    with open(filename, \"w\") as f:\n      f.write(contents)\n    return filename\n\n  def test_assets(self):\n    file1 = self._make_asset(\"contents 1\")\n    file2 = self._make_asset(\"contents 2\")\n\n    root = tracking.Checkpointable()\n    root.asset1 = tracking.TrackableAsset(file1)\n    root.asset2 = tracking.TrackableAsset(file2)\n\n    save_dir = os.path.join(self.get_temp_dir(), \"save_dir\")\n    save.save(root, save_dir, signatures={})\n\n    file_io.delete_file(file1)\n    file_io.delete_file(file2)\n    load_dir = os.path.join(self.get_temp_dir(), \"load_dir\")\n    file_io.rename(save_dir, load_dir)\n\n    imported = load.load(load_dir)\n    with open(imported.asset1.asset_path.numpy(), \"r\") as f:\n      self.assertEquals(\"contents 1\", f.read())\n    with open(imported.asset2.asset_path.numpy(), \"r\") as f:\n      self.assertEquals(\"contents 2\", f.read())\n\n  def test_capture_assets(self):\n    root = tracking.Checkpointable()\n    root.vocab = tracking.TrackableAsset(self._make_asset(\"contents\"))\n    root.f = def_function.function(\n        lambda: root.vocab.asset_path,\n        input_signature=[])\n    imported = self.cycle(root)\n    origin_output = root.f().numpy()\n    imported_output = imported.f().numpy()\n    self.assertNotEqual(origin_output, imported_output)\n    with open(imported_output, \"r\") as f:\n      self.assertEquals(\"contents\", f.read())\n\n  def test_dedup_assets(self):\n    vocab = self._make_asset(\"contents\")\n    root = tracking.Checkpointable()\n    root.asset1 = tracking.TrackableAsset(vocab)\n    root.asset2 = tracking.TrackableAsset(vocab)\n    imported = self.cycle(root)\n    self.assertEqual(imported.asset1.asset_path.numpy(),\n                     imported.asset2.asset_path.numpy())\n\n  def test_implicit_input_signature(self):\n    @def_function.function\n    def func(x):\n      return 2 * x\n\n    root = tracking.Checkpointable()\n    root.f = func\n\n    \n    root.f(constant_op.constant(1.))\n    root.f(constant_op.constant(1))\n\n    imported = self.cycle(root)\n\n    self.assertEqual(4., imported.f(constant_op.constant(2.)).numpy())\n    self.assertEqual(14, imported.f(constant_op.constant(7)).numpy())\n\n  def test_explicit_input_signature(self):\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n    def func(x):\n      return 2 * x\n\n    root = tracking.Checkpointable()\n    root.f = func\n\n    imported = self.cycle(root)\n    self.assertEqual(4., imported.f(constant_op.constant(2.0)).numpy())\n\n  def test_nested_functions(self):\n    f = def_function.function(\n        lambda x: x*2.0,\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n    g = def_function.function(\n        lambda x: f(x) + 1.0,\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n\n    root = tracking.Checkpointable()\n    root.g = g\n    imported = self.cycle(root)\n    imported.g(constant_op.constant([1.0]))\n\n  def test_function_with_default_bool_input(self):\n\n    def func(x, training=False):\n      if training:\n        return 2 * x\n      else:\n        return 7\n\n    root = tracking.Checkpointable()\n    root.f = def_function.function(func)\n\n    self.assertEqual(20, root.f(constant_op.constant(10), True).numpy())\n    self.assertEqual(7, root.f(constant_op.constant(1)).numpy())\n    self.assertEqual(2, root.f(constant_op.constant(1), True).numpy())\n\n    imported = self.cycle(root)\n\n    self.assertEqual(4, imported.f(constant_op.constant(2), True).numpy())\n    self.assertEqual(7, imported.f(constant_op.constant(2)).numpy())\n\n  def test_positional_arguments(self):\n    def func(x, training=False, abc=7.1, defg=7.7):\n      del abc\n      if training:\n        return 2 * x\n      if defg == 7:\n        return 6\n      else:\n        return 7\n\n    root = tracking.Checkpointable()\n    root.f = def_function.function(func)\n\n    self.assertEqual(20, root.f(constant_op.constant(10), True).numpy())\n    self.assertEqual(7, root.f(constant_op.constant(1)).numpy())\n    self.assertEqual(2, root.f(constant_op.constant(1), True).numpy())\n    self.assertEqual(6, root.f(constant_op.constant(1), defg=7.0).numpy())\n\n    imported = self.cycle(root)\n\n    self.assertEqual(4, imported.f(constant_op.constant(2), True).numpy())\n    self.assertEqual(7, imported.f(constant_op.constant(2)).numpy())\n    self.assertEqual(6, imported.f(constant_op.constant(1), defg=7.0).numpy())\n\n  def test_member_function(self):\n    class CheckpointableWithMember(tracking.Checkpointable):\n\n      def __init__(self):\n        super(CheckpointableWithMember, self).__init__()\n        self._some_value = 20\n\n      @def_function.function\n      def f(self, x, training=False):\n        if training:\n          return 2 * x\n        else:\n          return 7 + self._some_value\n\n    root = CheckpointableWithMember()\n\n    self.assertEqual(20, root.f(constant_op.constant(10), True).numpy())\n    self.assertEqual(27, root.f(constant_op.constant(1)).numpy())\n    self.assertEqual(2, root.f(constant_op.constant(1), True).numpy())\n\n    imported = self.cycle(root)\n\n    self.assertEqual(4, imported.f(constant_op.constant(2), True).numpy())\n    self.assertEqual(27, imported.f(constant_op.constant(2)).numpy())\n\n  def test_side_effect_listing(self):\n    class M(tracking.Checkpointable):\n\n      def __init__(self):\n        super(M, self).__init__()\n        self.var = None\n\n      @def_function.function(\n          input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n      def f(self, x):\n        if self.var is None:\n          self.var = variables.Variable(2.)\n        return x * self.var\n\n    m = M()\n    self.cycle(m)\n    self.assertEquals(4.0, m.f(constant_op.constant(2.0)).numpy())\n\n  def test_basic_backprop(self):\n    weight = variables.Variable(1., trainable=True)\n    bias = variables.Variable(0., trainable=True)\n    g = def_function.function(\n        lambda x: x*weight + bias,\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n\n    root = tracking.Checkpointable()\n    root.weight = weight\n    root.bias = bias\n    root.g = g\n    imported = self.cycle(root)\n    with backprop.GradientTape(watch_accessed_variables=True) as t:\n      x = constant_op.constant([3.5])\n      loss = imported.g(x)\n      grad = t.gradient(loss, [imported.weight, imported.bias])\n      self.assertAllClose(grad, [3.5, 1.0])\n\n  def test_callable(self):\n    class M1(tracking.Checkpointable):\n\n      @def_function.function(\n          input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n      def __call__(self, x):\n        return x\n\n    root = tracking.Checkpointable()\n    root.m1 = M1()\n    root.m2 = tracking.Checkpointable()\n    root.m2.__call__ = def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])(\n            lambda x: x*3.0)\n    imported = self.cycle(root)\n    x = constant_op.constant(1.0)\n\n    self.assertTrue(callable(imported.m1))\n    self.assertAllEqual(root.m1(x), imported.m1(x))\n\n    \n    \n    \n    self.assertTrue(callable(imported.m2))\n    self.assertAllEqual(root.m2.__call__(x), imported.m2(x))\n\n    \n    self.assertFalse(callable(imported))\n\n  def test_chain_callable(self):\n    func = def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])(\n            lambda x: x*3.0)\n    root = tracking.Checkpointable()\n    root.__call__ = tracking.Checkpointable()\n    root.__call__.__call__ = tracking.Checkpointable()\n    root.__call__.__call__.__call__ = func\n\n    imported = self.cycle(root)\n    self.assertTrue(callable(imported))\n    x = constant_op.constant(1.0)\n    self.assertAllEqual(imported(x).numpy(), 3.0)\n\n\nif __name__ == \"__main__\":\n  test.main()\n",
        "summary": "This code is a unit test suite for TensorFlow's `tf.function` decorator and related functionality. It tests various aspects of how functions can be converted to TensorFlow graphs, including:\n\n1. Capturing variables and other objects within the function.\n2. Handling member functions and methods.\n3. Supporting side effects like variable initialization.\n4. Enabling automatic differentiation (backpropagation).\n5. Making functions callable as objects.\n\nThe test suite includes a variety of scenarios, such as:\n\n- Basic function conversion\n- Member function conversion \n- Capturing trainable variables for backprop\n- Handling different types of input signatures\n- Supporting nested callables\n\nIt uses TensorFlow's `tf.GradientTape` to verify that gradients can be computed correctly. The tests also check that the converted functions are callable and produce the expected results.\n\nOverall, this test suite provides comprehensive coverage of key features and edge cases for using `tf.function` in TensorFlow applications."
    },
    {
        "code": "from selenium import webdriver\nimport time\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\ndriver = webdriver.PhantomJS()\ndriver.set_window_size(1120,550)\n\ndriver.get(\"http:10.10.2.1\")\n\nres=0\ntry:\n    e1=WebDriverWait(driver,10).until(\n    EC.presence_of_element_located((By.NAME,\"username\"))\n    )\n    e2=WebDriverWait(driver,10).until(\n    EC.presence_of_element_located((By.NAME,\"passwd\"))\n    )\n    e3=WebDriverWait(driver,10).until(\n    EC.presence_of_element_located((By.LINK_TEXT,\"Login\"))\n    )\n    driver.find_element_by_name('username').send_keys('Your_username')\n    driver.find_element_by_name('passwd').send_keys('Your_password')\n    driver.find_element_by_name('rememberme').click()\n    \n    res=BeautifulSoup(driver.page_source)\n    if \"Connected(Default Internet)\" not in res.text :\n        driver.find_element_by_css_selector('.field2 input').click()\n    \n    res=1\n    \n\n\n    \nfinally:\n    if res :\n        print(\"Successful!!\")\n        time.sleep(5)\n    else:\n        print(\"Failed :(\")\ndriver.quit()\n",
        "summary": "The Python script uses Selenium with PhantomJS to automate a login process on a webpage, entering credentials and clicking the login button. It then checks if the login was successful by looking for specific text in the page source; if not, it attempts to reconnect using a different method. The script prints \"Successful!!\" or \"Failed :(\" based on the outcome and closes the browser."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n  \n\n  \n  \n  \n  \n\n  if not init_checkpoint:\n    return\n\n  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n  if m is None:\n    return\n\n  model_name = m.group(1)\n\n  lower_models = [\n      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n  ]\n\n  cased_models = [\n      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n      \"multi_cased_L-12_H-768_A-12\"\n  ]\n\n  is_bad_config = False\n  if model_name in lower_models and not do_lower_case:\n    is_bad_config = True\n    actual_flag = \"False\"\n    case_name = \"lowercased\"\n    opposite_flag = \"True\"\n\n  if model_name in cased_models and do_lower_case:\n    is_bad_config = True\n    actual_flag = \"True\"\n    case_name = \"cased\"\n    opposite_flag = \"False\"\n\n  if is_bad_config:\n    raise ValueError(\n        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n        \"However, `%s` seems to be a %s model, so you \"\n        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n        \"how the model was pre-training. If this error is wrong, please \"\n        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n                                          model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n  \n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef printable_text(text):\n  \n\n  \n  \n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(\"utf-8\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n  \n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  \n  output = []\n  \n  for i,item in enumerate(items):\n    \n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  \n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  \n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  \n\n  def __init__(self, do_lower_case=True):\n    \n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    \n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    \n    \n    \n    \n    \n    \n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    \n    text = unicodedata.normalize(\"NFD\", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == \"Mn\":\n        continue\n      output.append(char)\n    return \"\".join(output)\n\n  def _run_split_on_punc(self, text):\n    \n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return [\"\".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    \n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append(\" \")\n        output.append(char)\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)\n\n  def _is_chinese_char(self, cp):\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  \n        (cp >= 0x3400 and cp <= 0x4DBF) or  \n        (cp >= 0x20000 and cp <= 0x2A6DF) or  \n        (cp >= 0x2A700 and cp <= 0x2B73F) or  \n        (cp >= 0x2B740 and cp <= 0x2B81F) or  \n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  \n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  \n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    \n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append(\" \")\n      else:\n        output.append(char)\n    return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n  \n\n  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    \n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = \"\".join(chars[start:end])\n          if start > 0:\n            substr = \"\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  \n  \n  \n  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return True\n  cat = unicodedata.category(char)\n  if cat == \"Zs\":\n    return True\n  return False\n\n\ndef _is_control(char):\n  \n  \n  \n  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (\"Cc\", \"Cf\"):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  \n  cp = ord(char)\n  \n  \n  \n  \n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(\"P\"):\n    return True\n  return False\n",
        "summary": "This code defines a set of classes and functions for tokenizing text using the WordPiece algorithm, which is commonly used in natural language processing tasks. The main components are:\n\n1. `FullTokenizer`: This class combines the functionality of `BasicTokenizer` and `WordpieceTokenizer`. It first applies basic tokenization rules (like splitting on whitespace) and then uses the WordPiece tokenizer to further break down tokens.\n\n2. `BasicTokenizer`: This class handles basic tokenization tasks such as:\n   - Splitting text into words based on whitespace.\n   - Handling Chinese characters by adding spaces around them.\n   - Cleaning text by removing control characters and replacing whitespace with a single space.\n\n3. `WordpieceTokenizer`: This class implements the WordPiece algorithm, which dynamically builds a vocabulary from a corpus of text. It:\n   - Tokenizes input text into subword units (subwords).\n   - Handles out-of-vocabulary words using an unknown token (`[UNK]`).\n\n4. Helper functions like `_is_whitespace`, `_is_control`, and `_is_punctuation` are used to identify different types of characters in the text.\n\nThe `FullTokenizer` class is typically used for preprocessing text data before feeding it into a neural network model, especially when working with pre-trained models that use WordPiece tokenization."
    },
    {
        "code": "from __future__ import annotations\n\nfrom decimal import Decimal\n\n\ndef inverse_of_matrix(matrix: list[list[float]]) -> list[list[float]]:\n    \n\n    D = Decimal  \n    \n    determinant = D(matrix[0][0]) * D(matrix[1][1]) - D(matrix[1][0]) * D(matrix[0][1])\n    if determinant == 0:\n        raise ValueError(\"This matrix has no inverse.\")\n    \n    swapped_matrix = [[0.0, 0.0], [0.0, 0.0]]\n    swapped_matrix[0][0], swapped_matrix[1][1] = matrix[1][1], matrix[0][0]\n    swapped_matrix[1][0], swapped_matrix[0][1] = -matrix[1][0], -matrix[0][1]\n    \n    return [[float(D(n) / determinant) or 0.0 for n in row] for row in swapped_matrix]\n",
        "summary": "The provided Python code defines a function `inverse_of_matrix` that calculates the inverse of a given 2x2 matrix using the decimal module for precise arithmetic operations. If the determinant of the matrix is zero, it raises a ValueError indicating that the matrix has no inverse. Otherwise, it computes and returns the inverse matrix as a list of lists of floats."
    },
    {
        "code": "import nncore\n\n\ndef test_bind_getter():\n\n    @nncore.bind_getter('name', 'depth')\n    class Backbone:\n        _name = 'ResNet'\n        _depth = 50\n\n    backbone = Backbone()\n    assert backbone.name == 'ResNet'\n    assert backbone.depth == 50\n",
        "summary": "The provided Python code demonstrates the use of a decorator from the `nncore` module to bind getter methods for attributes `_name` and `_depth` in a class named `Backbone`. The test function `test_bind_getter` creates an instance of `Backbone`, accesses its `name` and `depth` properties through the bound getters, and asserts that they return the expected values 'ResNet' and 50, respectively."
    },
    {
        "code": "import datetime\nimport django.contrib.auth.models\nimport django.contrib.auth.validators\nfrom django.db import migrations, models\nimport django.utils.timezone\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        ('auth', '0009_alter_user_last_name_max_length'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='UserProfile',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('password', models.CharField(max_length=128, verbose_name='password')),\n                ('last_login', models.DateTimeField(blank=True, null=True, verbose_name='last login')),\n                ('is_superuser', models.BooleanField(default=False, help_text='Designates that this user has all permissions without explicitly assigning them.', verbose_name='superuser status')),\n                ('username', models.CharField(error_messages={'unique': 'A user with that username already exists.'}, help_text='Required. 150 characters or fewer. Letters, digits and @/./+/-/_ only.', max_length=150, unique=True, validators=[django.contrib.auth.validators.UnicodeUsernameValidator()], verbose_name='username')),\n                ('first_name', models.CharField(blank=True, max_length=30, verbose_name='first name')),\n                ('last_name', models.CharField(blank=True, max_length=150, verbose_name='last name')),\n                ('is_staff', models.BooleanField(default=False, help_text='Designates whether the user can log into this admin site.', verbose_name='staff status')),\n                ('is_active', models.BooleanField(default=True, help_text='Designates whether this user should be treated as active. Unselect this instead of deleting accounts.', verbose_name='active')),\n                ('date_joined', models.DateTimeField(default=django.utils.timezone.now, verbose_name='date joined')),\n                ('name', models.CharField(blank=True, max_length=16, null=True, verbose_name='\u7528\u6237\u540d')),\n                ('gender', models.CharField(choices=[('male', '\u7537'), ('female', '\u5973')], default='female', max_length=6, verbose_name='\u6027\u522b')),\n                ('mobile', models.CharField(blank=True, max_length=11, null=True, verbose_name='\u7535\u8bdd')),\n                ('email', models.CharField(blank=True, max_length=100, null=True, verbose_name='\u90ae\u7bb1')),\n                ('top_img', models.ImageField(max_length=200, null=True, upload_to='user/')),\n                ('create_time', models.DateTimeField(default=datetime.datetime.now, verbose_name='\u521b\u5efa\u65f6\u95f4')),\n                ('groups', models.ManyToManyField(blank=True, help_text='The groups this user belongs to. A user will get all permissions granted to each of their groups.', related_name='user_set', related_query_name='user', to='auth.Group', verbose_name='groups')),\n                ('user_permissions', models.ManyToManyField(blank=True, help_text='Specific permissions for this user.', related_name='user_set', related_query_name='user', to='auth.Permission', verbose_name='user permissions')),\n            ],\n            options={\n                'verbose_name': '\u7528\u6237',\n                'verbose_name_plural': '\u7528\u6237',\n            },\n            managers=[\n                ('objects', django.contrib.auth.models.UserManager()),\n            ],\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration to create a new model named `UserProfile` that extends the default user model provided by Django's authentication system. The `UserProfile` includes additional fields such as gender, mobile number, email, profile picture, and timestamps for creation and last login, along with standard user attributes like username, password, and permissions."
    },
    {
        "code": "import os\nimport click\nfrom yoda.ssh.shell import Shell\nfrom yoda.ssh.config import importHost\n\n\nclass Cmd():\n  def __init__(self):\n    self.verbose = False\n    self.shell = None\n    self.host = None\n\npass_cmd = click.make_pass_decorator(Cmd, ensure=True)\n\nclass CmdsLoader(click.MultiCommand):\n  _cmdFolder = os.path.abspath(os.path.join(os.path.dirname(__file__), 'cmds'))\n\n  def list_commands(self, ctx):\n    rv = []\n    for filename in os.listdir(self._cmdFolder):\n      if filename.endswith('.py'):\n        rv.append(filename[:-3])\n    rv.sort()\n    return rv\n\n  def get_command(self, ctx, name):\n    try:\n      cmdFullName = 'yoda.cmds.' + name\n      mod = __import__(cmdFullName, None, None, ['cmd'])\n    except ImportError:\n      return\n    return mod.cmd\n\n@click.command(cls=CmdsLoader)\n@click.option('-v', '--verbose', count=True, help=\"Explain what is being done\")\n@click.option('-i', '--interactive', count=True, help=\"Show all the output from the established remote shell session\")\n@click.option('-f', '--force', is_flag=True, help=\"Force the execution of the commands if one fails\")\n@click.option('-h', '--host', default=\"myserver\", help=\"The name of the connection defined in ~/.ssh/config file\")\n@click.pass_context\ndef yoda(ctx, verbose, interactive, force, host):\n  shell = Shell(host)\n  hostConfig = importHost(host)\n  shell.setConfig(hostConfig[0]['options'])\n  shell.connect()\n  if (verbose):\n    click.echo(\"Connected to host %s\" % host)\n\n  \n  shell.interactive = bool(interactive)\n  shell.force = force\n  cmd = Cmd()\n  cmd.shell = shell\n  cmd.host = host\n  cmd.verbose = verbose\n\n  ctx.obj = cmd\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code defines a command-line interface (CLI) tool named `yoda` using the `click` library. It includes options for verbosity, interactive mode, force execution, and specifying a host from an SSH configuration file. The CLI dynamically loads commands from a directory and executes them using a custom shell class (`Shell`) that handles SSH connections and interactions based on the provided host configuration."
    },
    {
        "code": "from collections import defaultdict\nimport sys\nfrom typing import Dict, List\nfrom z3 import ArithRef, Int, IntVal, Or, Solver, PbEq\n\nfrom .fastz3 import fast_and, fast_eq, fast_ne\nfrom .geometry import Lattice, Point, Vector\nfrom .quadtree import ExpressionQuadTree\n\n\n\n\nHAS_INSTANCE_ID, NOT_HAS_INSTANCE_ID, HAS_SHAPE_TYPE = range(3)\n\n\ndef canonicalize_shape(shape: List[Vector]) -> List[Vector]:\n  \n  shape = sorted(shape)\n  first_negated = shape[0].negate()\n  return [v.translate(first_negated) for v in shape]\n\n\nclass ShapeConstrainer:\n  \n  _instance_index = 0\n\n  def __init__(  \n      self,\n      lattice: Lattice,\n      shapes: List[List[Vector]],\n      solver: Solver = None,\n      complete: bool = False,\n      allow_rotations: bool = False,\n      allow_reflections: bool = False,\n      allow_copies: bool = False\n  ):\n    ShapeConstrainer._instance_index += 1\n    if solver:\n      self.__solver = solver\n    else:\n      self.__solver = Solver()\n\n    self.__lattice = lattice\n    self.__complete = complete\n    self.__allow_copies = allow_copies\n\n    self.__shapes = shapes\n    self.__make_variants(allow_rotations, allow_reflections)\n\n    self.__create_grids()\n    self.__add_constraints()\n\n  def __make_variants(self, allow_rotations, allow_reflections):\n    fs = self.__lattice.transformation_functions(\n        allow_rotations, allow_reflections)\n    self.__variants = [\n        [\n            list(shape_tuple)\n            for shape_tuple in {\n                tuple(canonicalize_shape([f(v) for v in s]))\n                for f in fs\n            }\n        ]\n        for s in self.__shapes\n    ]\n\n  def __create_grids(self):\n    \n    self.__shape_type_grid: Dict[Point, ArithRef] = {}\n    for p in self.__lattice.points:\n      v = Int(f\"scst-{ShapeConstrainer._instance_index}-{p.y}-{p.x}\")\n      if self.__complete:\n        self.__solver.add(v >= 0)\n      else:\n        self.__solver.add(v >= -1)\n      self.__solver.add(v < len(self.__shapes))\n      self.__shape_type_grid[p] = v\n\n    self.__shape_instance_grid: Dict[Point, ArithRef] = {}\n    for p in self.__lattice.points:\n      v = Int(f\"scsi-{ShapeConstrainer._instance_index}-{p.y}-{p.x}\")\n      if self.__complete:\n        self.__solver.add(v >= 0)\n      else:\n        self.__solver.add(v >= -1)\n      self.__solver.add(v < len(self.__lattice.points))\n      self.__shape_instance_grid[p] = v\n\n  def __add_constraints(self):\n    self.__add_grid_agreement_constraints()\n    self.__add_shape_instance_constraints()\n    if not self.__allow_copies:\n      for shape_index, shape in enumerate(self.__shapes):\n        self.__add_single_copy_constraints(shape_index, shape)\n\n  def __add_grid_agreement_constraints(self):\n    for p in self.__shape_type_grid:\n      self.__solver.add(\n          Or(\n              fast_and(\n                  self.__shape_type_grid[p] == -1,\n                  self.__shape_instance_grid[p] == -1\n              ),\n              fast_and(\n                  self.__shape_type_grid[p] != -1,\n                  self.__shape_instance_grid[p] != -1\n              )\n          )\n      )\n\n  def __add_shape_instance_constraints(self):  \n    int_vals = {}\n    for i in range(max(len(self.__lattice.points), len(self.__variants))):\n      int_vals[i] = IntVal(i)\n\n    quadtree = ExpressionQuadTree(self.__lattice.points)\n    for instance_id in [self.__lattice.point_to_index(p) for p in self.__lattice.points]:\n      quadtree.add_expr(\n          (HAS_INSTANCE_ID, instance_id),\n          lambda p, i=instance_id: fast_eq(self.__shape_instance_grid[p], int_vals[i]))\n      quadtree.add_expr(\n          (NOT_HAS_INSTANCE_ID, instance_id),\n          lambda p, i=instance_id: fast_ne(self.__shape_instance_grid[p], int_vals[i]))\n    for shape_index in range(len(self.__variants)):\n      quadtree.add_expr(\n          (HAS_SHAPE_TYPE, shape_index),\n          lambda p, i=shape_index: fast_eq(self.__shape_type_grid[p], int_vals[i]))\n\n    root_options = defaultdict(list)\n    for shape_index, variants in enumerate(self.__variants):  \n      for variant in variants:\n        for root_point in self.__lattice.points:\n          instance_id = self.__lattice.point_to_index(root_point)\n          offset_points = set()\n          for offset_vector in variant:\n            point = root_point.translate(offset_vector)\n            if point not in self.__shape_instance_grid:\n              offset_points = None\n              break\n            offset_points.add(point)\n          if offset_points:\n            and_terms = []\n            for p in offset_points:\n              and_terms.append(quadtree.get_point_expr((HAS_INSTANCE_ID, instance_id), p))\n              and_terms.append(quadtree.get_point_expr((HAS_SHAPE_TYPE, shape_index), p))\n            and_terms.append(quadtree.get_other_points_expr(\n                (NOT_HAS_INSTANCE_ID, instance_id), offset_points))\n            root_options[root_point].append(fast_and(*and_terms))\n    for p in self.__lattice.points:\n      instance_id = self.__lattice.point_to_index(p)\n      not_has_instance_id_expr = quadtree.get_other_points_expr(\n          (NOT_HAS_INSTANCE_ID, instance_id), [])\n      or_terms = root_options[p]\n      if or_terms:\n        or_terms.append(not_has_instance_id_expr)\n        self.__solver.add(Or(*or_terms))\n      else:\n        self.__solver.add(not_has_instance_id_expr)\n\n  def __add_single_copy_constraints(self, shape_index, shape):\n    sum_terms = []\n    for p in self.__shape_type_grid:\n      sum_terms.append((self.__shape_type_grid[p] == shape_index, 1))\n    self.__solver.add(PbEq(sum_terms, len(shape)))\n\n  @property\n  def solver(self) -> Solver:\n    \n    return self.__solver\n\n  @property\n  def shape_type_grid(self) -> Dict[Point, ArithRef]:\n    \n    return self.__shape_type_grid\n\n  @property\n  def shape_instance_grid(self) -> Dict[Point, ArithRef]:\n    \n    return self.__shape_instance_grid\n\n  def print_shape_types(self):\n    \n    model = self.__solver.model()\n    min_y = min(p.y for p in self.__shape_type_grid)\n    min_x = min(p.x for p in self.__shape_type_grid)\n    max_y = max(p.y for p in self.__shape_type_grid)\n    max_x = max(p.x for p in self.__shape_type_grid)\n    for y in range(min_y, max_y + 1):\n      for x in range(min_x, max_x + 1):\n        p = Point(y, x)\n        shape_index = -1\n        if p in self.__shape_type_grid:\n          v = self.__shape_type_grid[p]\n          shape_index = model.eval(v).as_long()\n        if shape_index >= 0:\n          sys.stdout.write(f\"{shape_index:3}\")\n        else:\n          sys.stdout.write(\"   \")\n      print()\n\n  def print_shape_instances(self):\n    \n    model = self.__solver.model()\n    min_y = min(p.y for p in self.__shape_instance_grid)\n    min_x = min(p.x for p in self.__shape_instance_grid)\n    max_y = max(p.y for p in self.__shape_instance_grid)\n    max_x = max(p.x for p in self.__shape_instance_grid)\n    for y in range(min_y, max_y + 1):\n      for x in range(min_x, max_x + 1):\n        p = Point(y, x)\n        shape_instance = -1\n        if p in self.__shape_instance_grid:\n          v = self.__shape_instance_grid[p]\n          shape_instance = model.eval(v).as_long()\n        if shape_instance >= 0:\n          sys.stdout.write(f\"{shape_instance:3}\")\n        else:\n          sys.stdout.write(\"   \")\n      print()\n",
        "summary": "The provided Python code defines a class `ShapeConstrainer` that uses the Z3 SMT solver to constrain and solve problems related to geometric shapes on a lattice. It handles various constraints such as shape variants, grid agreements, instance constraints, and single copy constraints, ultimately providing methods to print the results of these constraints."
    },
    {
        "code": "def Print():\n    print('you may want to install beautifulsoup4,not beautfulsoup4')\n",
        "summary": "The function `Print` outputs a message suggesting the installation of the `beautifulsoup4` package instead of a misspelled version."
    },
    {
        "code": "from os.path import abspath, dirname, join\r\nfrom os import environ, path\r\n\r\n_cwd = dirname(abspath(__file__))\r\nbasedir = path.abspath(path.dirname(__file__))\r\n\r\nclass BaseConfiguration(object):\r\n    DEBUG = True\r\n    SECRET_KEY = 'Test'\r\n    CORS = [\"http://localhost:4200\", \"http://127.0.0.1:5000\"]",
        "summary": "The provided Python code sets up a base configuration class for an application, defining settings such as debug mode, secret key, and allowed CORS origins. It uses the `os` and `os.path` modules to determine the absolute path of the current file and establish the base directory for the project."
    },
    {
        "code": "class Client(object):\n    clientId = \"\"\n    clientVersion = \"0.0.1\"\n\n    def __init__(self, client_id, client_version=\"0.0.1\"):\n        self.clientId = client_id\n        self.clientVersion = client_version\n\n\nclass ThreatEntry(object):\n    def __init__(self, url):\n        self.url = url\n\n\nclass ThreatInfo(object):\n    def __init__(self, threatTypes, platformTypes, threatEntryTypes, threatEntries):\n        self.threatTypes = threatTypes\n        self.platformTypes = platformTypes\n        self.threatEntryTypes = threatEntryTypes\n        self.threatEntries = threatEntries\n\n\nclass Request(object):\n    def __init__(self, client, threatInfo):\n        self.client = client\n        self.threatInfo = threatInfo\n",
        "summary": "The provided Python code defines several classes to represent a client, threat entries, threat information, and a request. The `Client` class initializes with an ID and version, while the `ThreatEntry` class holds a URL. The `ThreatInfo` class encapsulates details about threats including types, platforms, entry types, and specific entries. Finally, the `Request` class combines client and threat information into a single request object."
    },
    {
        "code": "from .utils.defaults import default_depot_path, default_install_dir, default_symlink_dir\nfrom .utils.filters import f_major_version, f_minor_version\nfrom .utils import query_yes_no\nfrom .utils import current_architecture, current_system, current_libc\nfrom .utils import latest_version\nfrom .utils import DmgMounter, TarMounter\nfrom .utils import Version\nfrom .utils import verify_upstream\nfrom .utils import color, show_verbose\nfrom .download import download_package\n\nimport os\nimport re\nimport shutil\nimport subprocess\n\n\ndef is_installed(version, check_symlinks=True):\n    \n    check_list = [\"julia\"]\n    if version == \"latest\":\n        check_list.append(\"julia-latest\")\n    if version != \"latest\" and check_symlinks:\n        check_list.extend([f\"julia-{f_major_version(version)}\",\n                           f\"julia-{f_minor_version(version)}\"])\n\n    for path in check_list:\n        if Version(get_exec_version(shutil.which(path))) != Version(version):\n            return False\n    return True\n\n\ndef get_exec_version(path):\n    ver_cmd = [path, \"--version\"]\n    try:\n        \n        version = subprocess.check_output(ver_cmd).decode(\"utf-8\")\n        version = version.lower().split(\"version\")[-1].strip()\n    except:  \n        \n        \n        version = \"0.0.1\"\n    return version\n\n\ndef check_installer(installer_path, ext):\n    filename = os.path.basename(installer_path)\n    if not filename.endswith(ext):\n        msg = f\"The installer {filename} should be {ext} file\"\n        raise ValueError(msg)\n\n\ndef last_julia_version(version=None):\n    \n    def sort_key(ver):\n        return float(ver.lstrip(\"v\"))\n\n    version = float(f_minor_version(version)) if version else 999.999\n    proj_versions = os.listdir(os.path.join(default_depot_path(),\n                                            \"environments\"))\n    proj_versions = [x for x in proj_versions if re.fullmatch(r\"v\\d+\\.\\d+\", x)]\n    proj_versions = sorted(filter(lambda ver: sort_key(ver) < version,\n                                  proj_versions),\n                           key=sort_key)\n    if proj_versions:\n        return proj_versions[-1]\n    else:\n        return None\n\n\ndef make_symlinks(src_bin, symlink_dir, version):\n    if not os.path.isfile(src_bin):\n        raise(ValueError(f\"{src_bin} doesn't exist.\"))\n\n    system = current_system()\n    if symlink_dir not in map(os.path.normpath, os.environ[\"PATH\"].split(os.pathsep)):\n        print(f\"add {symlink_dir} to PATH\")\n        if system == \"winnt\":\n            \n            subprocess.run([\"powershell.exe\",\n                            \"setx\", \"PATH\", f'\"$env:PATH;{symlink_dir}\"'])\n        else:\n            msg = \"~/.bashrc will be modified\"\n            msg += \"\\nif you're not using BASH, then you'll need manually\"\n            msg += f\" add {symlink_dir} to your PATH\"\n            print(msg)\n\n            rc_file = os.path.expanduser(\"~/.bashrc\")\n            with open(rc_file, \"a\") as file:\n                file.writelines(\"\\n\n                file.writelines(f\"export PATH={symlink_dir}:$PATH\\n\")\n        print(f\"you need to restart your current shell to update PATH\")\n\n    os.makedirs(symlink_dir, exist_ok=True)\n\n    new_ver = Version(get_exec_version(src_bin))\n    if version == \"latest\":\n        \n        link_list = [\"julia-latest\"]\n    elif len(Version(version).build) > 0:\n        link_list = [\"julia-dev\"]\n    elif len(new_ver.prerelease) > 0:\n        \n        \n        \n        \n        link_list = [f\"julia-{f_minor_version(version)}\"]\n    else:\n        link_list = [f\"julia-{f(version)}\" for f in (f_major_version,\n                                                     f_minor_version)]\n        link_list.append(\"julia\")\n\n    for linkname in link_list:\n        linkpath = os.path.join(symlink_dir, linkname)\n        if current_system() == \"winnt\":\n            linkpath += \".cmd\"\n        \n        \n        \n        \n        \n        \n        \n        if os.path.exists(linkpath) or os.path.islink(linkpath):\n            if (os.path.islink(linkpath) and\n                    os.readlink(linkpath) == src_bin):\n                \n                continue\n\n            old_ver = Version(get_exec_version(linkpath))\n            if show_verbose():\n                print(f\"old symlink version: {old_ver}\")\n                print(f\"new installation version: {new_ver}\")\n            if old_ver > new_ver:\n                \n                continue\n\n            msg = f\"{color.YELLOW}remove old symlink\"\n            msg += f\" {linkname}{color.END}\"\n            print(msg)\n            os.remove(linkpath)\n        print(f\"{color.GREEN}make new symlink {linkpath}{color.END}\")\n        if current_system() == \"winnt\":\n            with open(linkpath, 'w') as f:\n                \n                f.writelines(['@echo off\\n', f'\"{src_bin}\" %*'])\n        else:\n            os.symlink(src_bin, linkpath)\n\n\ndef copy_root_project(version):\n    mver = f_minor_version(version)\n    old_ver = last_julia_version(version)\n    if old_ver is None:\n        print(\n            f\"Can't find available old root project for version {version}\")\n        return None\n\n    env_path = os.path.join(default_depot_path(), \"environments\")\n    src_path = os.path.join(env_path, old_ver)\n    dest_path = os.path.join(env_path, f\"v{mver}\")\n\n    if src_path == dest_path:\n        return None\n\n    if os.path.exists(dest_path):\n        bak_path = os.path.join(env_path, f\"v{mver}.bak\")\n        if os.path.exists(bak_path):\n            print(f\"{color.YELLOW}delete old backup {bak_path}{color.END}\")\n            shutil.rmtree(bak_path)\n        shutil.move(dest_path, bak_path)\n        print(f\"{color.YELLOW}move {dest_path} to {bak_path}{color.END}\")\n    shutil.copytree(src_path, dest_path)\n\n\ndef install_julia_tarball(package_path,\n                          install_dir,\n                          symlink_dir,\n                          version,\n                          upgrade):\n    check_installer(package_path, \".tar.gz\")\n\n    if re.match(\"(.*)\\+(\\w+)$\", version):\n        \n        \n        suffix = 'dev'\n    else:\n        suffix = f_minor_version(version)\n\n    with TarMounter(package_path) as root:\n        src_path = root\n        dest_path = os.path.join(install_dir, f\"julia-{suffix}\")\n        if os.path.exists(dest_path):\n            shutil.rmtree(dest_path)\n            msg = f\"{color.YELLOW}remove previous Julia installation:\"\n            msg += f\" {dest_path}{color.END}\"\n            print(msg)\n        \n        \n        shutil.copytree(src_path, dest_path, symlinks=True)\n        print(f\"{color.GREEN}install Julia to {dest_path}{color.END}\")\n    os.chmod(dest_path, 0o755)  \n    bin_path = os.path.join(dest_path, \"bin\", \"julia\")\n    if current_system() == 'winnt':\n        bin_path += '.exe'\n    make_symlinks(bin_path, symlink_dir, version)\n    if upgrade:\n        copy_root_project(version)\n    return True\n\n\ndef install_julia_dmg(package_path,\n                      install_dir,\n                      symlink_dir,\n                      version,\n                      upgrade):\n    check_installer(package_path, \".dmg\")\n\n    with DmgMounter(package_path) as root:\n        \n        \n        appname = next(filter(lambda x: x.lower().startswith('julia'),\n                              os.listdir(root)))\n        src_path = os.path.join(root, appname)\n        dest_path = os.path.join(install_dir, appname)\n        if os.path.exists(dest_path):\n            msg = f\"{color.YELLOW}remove previous Julia installation:\"\n            msg += f\" {dest_path}{color.END}\"\n            print(msg)\n            shutil.rmtree(dest_path)\n        \n        \n        shutil.copytree(src_path, dest_path, symlinks=True)\n        print(f\"{color.GREEN}install Julia to {dest_path}{color.END}\")\n    bin_path = os.path.join(dest_path,\n                            \"Contents\", \"Resources\", \"julia\", \"bin\", \"julia\")\n    make_symlinks(bin_path, symlink_dir, version)\n    if upgrade:\n        copy_root_project(version)\n    return True\n\n\ndef install_julia_exe(package_path,\n                      install_dir,\n                      symlink_dir,\n                      version,\n                      upgrade):\n    check_installer(package_path, \".exe\")\n\n    dest_path = os.path.join(install_dir,\n                             f\"julia-{f_minor_version(version)}\")\n    if os.path.exists(dest_path):\n        shutil.rmtree(dest_path, ignore_errors=True)\n        msg = f\"{color.YELLOW}remove previous Julia installation:\"\n        msg += f\" {dest_path}{color.END}\"\n        print(msg)\n\n    \n    \n    if Version(version).next_patch() < Version(\"1.4.0\"):\n        \n        subprocess.check_output([f'{package_path}',\n                                 '/S', f'/D={dest_path}'])\n    else:\n        subprocess.check_output([f'{package_path}',\n                                 '/VERYSILENT',\n                                 f'/DIR={dest_path}'])\n    print(f\"{color.GREEN}install Julia to {dest_path}{color.END}\")\n    bin_path = os.path.join(dest_path, \"bin\", \"julia.exe\")\n    make_symlinks(bin_path, symlink_dir, version)\n    if upgrade:\n        copy_root_project(version)\n    return True\n\n\ndef hello_msg():\n    msg = f\"{color.BOLD}JILL - Julia Installer 4 Linux\"\n    msg += f\" (MacOS, Windows and FreeBSD) -- Light{color.END}\\n\"\n    print(msg)\n\n\ndef install_julia(version=None, *,\n                  install_dir=None,\n                  symlink_dir=None,\n                  upgrade=False,\n                  upstream=None,\n                  unstable=False,\n                  keep_downloads=False,\n                  confirm=False,\n                  reinstall=False):\n    \n    install_dir = install_dir if install_dir else default_install_dir()\n    install_dir = os.path.abspath(install_dir)\n    symlink_dir = symlink_dir if symlink_dir else default_symlink_dir()\n    symlink_dir = os.path.normpath(os.path.abspath(symlink_dir))\n    system, arch = current_system(), current_architecture()\n    version = str(version) if (version or str(version) == \"0\") else ''\n    version = \"latest\" if version == \"nightly\" else version\n    version = \"\" if version == \"stable\" else version\n    upstream = upstream if upstream else os.environ.get(\"JILL_UPSTREAM\", None)\n\n    if system == \"linux\" and current_libc() == \"musl\":\n        \n        \n        system = \"musl\"\n\n    hello_msg()\n    if system == \"winnt\":\n        install_dir = install_dir.replace(\"\\\\\\\\\", \"\\\\\").strip('\\'\"')\n    if not confirm:\n        version_str = version if version else \"latest stable release\"\n        question = \"jill will:\\n\"\n        question += f\"  1) install Julia {version_str} for {system}-{arch}\"\n        question += f\" into {color.UNDERLINE}{install_dir}{color.END}\\n\"\n        question += f\"  2) make symlinks in {color.UNDERLINE}{symlink_dir}{color.END}\\n\"\n        question += f\"You may need to manually add {color.UNDERLINE}{symlink_dir}{color.END} to PATH\\n\"\n        question += \"Continue installation?\"\n        to_continue = query_yes_no(question)\n        if not to_continue:\n            return False\n\n    if upstream:\n        verify_upstream(upstream)\n    wrong_args = False\n    try:\n        version = latest_version(\n            version, system, arch, upstream=upstream, stable_only=not unstable)\n    except ValueError:\n        \n        wrong_args = True\n    if wrong_args:\n        msg = f\"wrong version(>= 0.6.0) argument: {version}\\n\"\n        msg += f\"Example: `jill install 1`\"\n        raise(ValueError(msg))\n\n    if not reinstall and is_installed(version):\n        print(f\"julia {version} already installed.\")\n        return True\n\n    overwrite = True if version == \"latest\" else False\n    print(f\"{color.BOLD}----- Download Julia -----{color.END}\")\n    package_path = download_package(version, system, arch,\n                                    upstream=upstream,\n                                    overwrite=overwrite)\n    if not package_path:\n        return False\n\n    if package_path.endswith(\".dmg\"):\n        installer = install_julia_dmg\n    elif package_path.endswith(\".tar.gz\"):\n        installer = install_julia_tarball\n    elif package_path.endswith(\".exe\"):\n        installer = install_julia_exe\n    else:\n        print(f\"{color.RED}Unsupported file format for {package_path}{color.END}.\")\n\n    print(f\"{color.BOLD}----- Install Julia -----{color.END}\")\n    installer(package_path, install_dir, symlink_dir, version, upgrade)\n\n    if not keep_downloads:\n        print(f\"{color.BOLD}----- Post Installation -----{color.END}\")\n        print(\"remove downloaded files...\")\n        print(f\"remove {package_path}\")\n        os.remove(package_path)\n        gpg_signature_file = package_path + \".asc\"\n        if os.path.exists(gpg_signature_file):\n            print(f\"remove {gpg_signature_file}\")\n            os.remove(gpg_signature_file)\n    print(f\"{color.GREEN}Done!{color.END}\")\n",
        "summary": "This is a Python script for installing Julia on various operating systems (Linux, MacOS, Windows, and FreeBSD). The script provides several options to customize the installation process:\n\n1. **Version Selection**: You can specify which version of Julia to install. Options include:\n   - `latest`: Installs the latest stable release.\n   - `nightly`: Installs the nightly build.\n   - A specific version number (e.g., \"1.5.0\").\n\n2. **Installation Directory**: You can specify where Julia should be installed. The default is typically `/usr/local` on Linux and MacOS, or a user-specific directory on Windows.\n\n3. **Symlink Directory**: You can specify where symlinks to the Julia installation should be created. This helps in managing multiple versions of Julia easily.\n\n4. **Upgrade Mode**: If you already have an older version of Julia installed, you can upgrade it using this option.\n\n5. **Upstream Source**: You can specify a custom upstream source for downloading Julia packages.\n\n6. **Unstable Builds**: You can install unstable builds (nightly or development versions).\n\n7. **Confirmation**: The script will prompt you to confirm the installation details before proceeding.\n\n8. **Reinstall**: If you want to reinstall an existing version of Julia, you can use this option.\n\n9. **Keep Downloads**: By default, the downloaded package and GPG signature file are removed after installation. You can keep them using this option.\n\n10. **Confirmation Prompt**: The script will ask for confirmation before proceeding with the installation.\n\n### Key Functions\n\n- **`install_julia`**: This is the main function that orchestrates the entire installation process, including downloading, installing, and cleaning up.\n  \n- **`download_package`**: Downloads the Julia package based on the specified version, system, and architecture.\n\n- **`install_julia_tarball`, `install_julia_dmg`, `install_julia_exe`**: These functions handle the installation of Julia packages in different formats (tarball, disk image, executable installer).\n\n- **`hello_msg`**: Displays a welcome message for the script.\n\n### Usage Example\n\nTo install the latest stable version of Julia on Linux:\n\n```sh\njill install --version latest\n```\n\nTo install a specific version of Julia on MacOS:\n\n```sh\njill install --version 1.5.0 --system macos\n```\n\nTo upgrade an existing installation:\n\n```sh\njill install --upgrade\n```\n\n### Notes\n\n- The script supports multiple operating systems and architectures.\n- It handles different package formats (tarball, disk image, executable installer).\n- It provides options to customize the installation directory and symlink directory.\n- It includes a confirmation prompt before proceeding with the installation.\n\nThis script is designed to be flexible and user-friendly, making it easy for users to install Julia on their preferred operating system."
    },
    {
        "code": "get = [{'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 1, 'DefaultCores': 1, 'DefaultThreadsPerCore': 1, 'ValidCores': [1], 'ValidThreadsPerCore': [1], 'SizeInMiB': 2048, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 2, 'Ipv4AddressesPerInterface': 4, 'Ipv6AddressesPerInterface': 4, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.medium', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': False, 'Hypervisor': 'nitro', 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 1, 'DefaultCores': 1, 'DefaultThreadsPerCore': 1, 'ValidCores': [1], 'ValidThreadsPerCore': [1]}, 'MemoryInfo': {'SizeInMiB': 2048}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 2, 'Ipv4AddressesPerInterface': 4, 'Ipv6AddressesPerInterface': 4, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}, {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 2, 'DefaultCores': 2, 'DefaultThreadsPerCore': 1, 'ValidCores': [2], 'ValidThreadsPerCore': [1], 'SizeInMiB': 4096, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 3, 'Ipv4AddressesPerInterface': 10, 'Ipv6AddressesPerInterface': 10, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.large', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': False, 'Hypervisor': 'nitro', 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 2, 'DefaultCores': 2, 'DefaultThreadsPerCore': 1, 'ValidCores': [2], 'ValidThreadsPerCore': [1]}, 'MemoryInfo': {'SizeInMiB': 4096}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 3, 'Ipv4AddressesPerInterface': 10, 'Ipv6AddressesPerInterface': 10, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}, {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 4, 'DefaultCores': 4, 'DefaultThreadsPerCore': 1, 'ValidCores': [4], 'ValidThreadsPerCore': [1], 'SizeInMiB': 8192, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 4, 'Ipv4AddressesPerInterface': 15, 'Ipv6AddressesPerInterface': 15, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.xlarge', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': False, 'Hypervisor': 'nitro', 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 4, 'DefaultCores': 4, 'DefaultThreadsPerCore': 1, 'ValidCores': [4], 'ValidThreadsPerCore': [1]}, 'MemoryInfo': {'SizeInMiB': 8192}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 4, 'Ipv4AddressesPerInterface': 15, 'Ipv6AddressesPerInterface': 15, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}, {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 8, 'DefaultCores': 8, 'DefaultThreadsPerCore': 1, 'ValidCores': [8], 'ValidThreadsPerCore': [1], 'SizeInMiB': 16384, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 4, 'Ipv4AddressesPerInterface': 15, 'Ipv6AddressesPerInterface': 15, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.2xlarge', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': False, 'Hypervisor': 'nitro', 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 8, 'DefaultCores': 8, 'DefaultThreadsPerCore': 1, 'ValidCores': [8], 'ValidThreadsPerCore': [1]}, 'MemoryInfo': {'SizeInMiB': 16384}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 4, 'Ipv4AddressesPerInterface': 15, 'Ipv6AddressesPerInterface': 15, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}, {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 16, 'DefaultCores': 16, 'DefaultThreadsPerCore': 1, 'ValidCores': [16], 'ValidThreadsPerCore': [1], 'SizeInMiB': 32768, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 8, 'Ipv4AddressesPerInterface': 30, 'Ipv6AddressesPerInterface': 30, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.4xlarge', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': False, 'Hypervisor': 'nitro', 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 16, 'DefaultCores': 16, 'DefaultThreadsPerCore': 1, 'ValidCores': [16], 'ValidThreadsPerCore': [1]}, 'MemoryInfo': {'SizeInMiB': 32768}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 8, 'Ipv4AddressesPerInterface': 30, 'Ipv6AddressesPerInterface': 30, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}, {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3, 'DefaultVCpus': 16, 'SizeInMiB': 32768, 'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported', 'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 8, 'Ipv4AddressesPerInterface': 30, 'Ipv6AddressesPerInterface': 30, 'Ipv6Supported': True, 'EnaSupport': 'required', 'SupportedStrategies': ['cluster', 'partition', 'spread'], 'InstanceType': 'a1.metal', 'CurrentGeneration': True, 'FreeTierEligible': False, 'SupportedUsageClasses': ['on-demand', 'spot'], 'SupportedRootDeviceTypes': ['ebs'], 'BareMetal': True, 'ProcessorInfo': {'SupportedArchitectures': ['arm64'], 'SustainedClockSpeedInGhz': 2.3}, 'VCpuInfo': {'DefaultVCpus': 16}, 'MemoryInfo': {'SizeInMiB': 32768}, 'InstanceStorageSupported': False, 'EbsInfo': {'EbsOptimizedSupport': 'default', 'EncryptionSupport': 'supported'}, 'NetworkInfo': {'NetworkPerformance': 'Up to 10 Gigabit', 'MaximumNetworkInterfaces': 8, 'Ipv4AddressesPerInterface': 30, 'Ipv6AddressesPerInterface': 30, 'Ipv6Supported': True, 'EnaSupport': 'required'}, 'PlacementGroupInfo': {'SupportedStrategies': ['cluster', 'partition', 'spread']}, 'HibernationSupported': False, 'BurstablePerformanceSupported': False, 'DedicatedHostsSupported': True, 'AutoRecoverySupported': True}]  \n\n\ndef get_instances_list() -> list:\n    \n    \n    return get\n",
        "summary": "It looks like you want to create a function that returns the list of instances. However, the code snippet provided is incomplete and lacks the actual implementation of the `get` variable or method. Below is a complete example of how you can define the `get_instances_list` function:\n\n```python\ndef get_instances_list() -> list:\n    # Assuming 'instances' is the list of instances you want to return\n    instances = [\n        {'InstanceType': 'a1.small', 'VCpus': 2, 'MemoryMiB': 4096},\n        {'InstanceType': 'a1.medium', 'VCpus': 4, 'MemoryMiB': 8192},\n        {'InstanceType': 'a1.large', 'VCpus': 8, 'MemoryMiB': 16384},\n        {'InstanceType': 'a1.xlarge', 'VCpus': 16, 'MemoryMiB': 32768},\n        {'InstanceType': 'a1.2xlarge', 'VCpus': 32, 'MemoryMiB': 65536}\n    ]\n    \n    return instances\n\n# Example usage\ninstances_list = get_instances_list()\nprint(instances_list)\n```\n\nIn this example, the `get_instances_list` function returns a list of dictionaries, where each dictionary represents an instance with its type, number of vCPUs, and memory in MiB. You can replace the sample data with your actual list of instances.\n\nIf you have a different source or method to retrieve the list of instances, you can modify the `get_instances_list` function accordingly. For example, if you are fetching the instances from an API, you might use a library like `requests` to make the HTTP request and then parse the response:\n\n```python\nimport requests\n\ndef get_instances_list() -> list:\n    # Replace 'your_api_url' with the actual URL of your API endpoint\n    api_url = 'your_api_url'\n    \n    response = requests.get(api_url)\n    if response.status_code == 200:\n        instances = response.json()\n        return instances\n    else:\n        raise Exception(f\"Failed to fetch instances: {response.status_code}\")\n\n# Example usage\ntry:\n    instances_list = get_instances_list()\n    print(instances_list)\nexcept Exception as e:\n    print(e)\n```\n\nIn this example, the `get_instances_list` function makes an HTTP GET request to the specified API endpoint and returns the list of instances fetched from the response."
    },
    {
        "code": "import numpy as np\nfrom gym import spaces\n\nWinMasks = [\n    [\n        [1,0,0],\n        [1,0,0],\n        [1,0,0],\n    ],\n    [\n        [0,1,0],\n        [0,1,0],\n        [0,1,0],\n    ],\n    [\n        [0,0,1],\n        [0,0,1],\n        [0,0,1],\n    ],\n    \n    [\n        [1,1,1],\n        [0,0,0],\n        [0,0,0],\n    ],\n    [\n        [0,0,0],\n        [1,1,1],\n        [0,0,0],\n    ],\n    [\n        [0,0,0],\n        [0,0,0],\n        [1,1,1],\n    ],\n\n    [\n        [1,0,0],\n        [0,1,0],\n        [0,0,1],\n    ],\n    [\n        [0,0,1],\n        [0,1,0],\n        [1,0,0],\n    ]\n]\n\nWinMasks = np.array(WinMasks).reshape((-1,9))\n\nclass SingleAgentTicTacToeEnv(object):\n    \n    NActions = 9\n    ObservationShape = (9,)\n    NState = 9\n    \n    def __init__(self):\n        self.Board = np.zeros((9,))\n        self.action_space = spaces.Discrete(self.NActions)\n        high = np.ones((self.NActions,))\n        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n        \n    def reset(self):\n        self.Done = False\n        self.Board[...] = 0.0\n        self.BoardHistory = []\n        self.Side = 1\n        self.FirstMove = True\n        return self.observation(self.Side), {\"valid_actions\":np.array([1,1,0,0,1,0,0,0,0], dtype=np.float32)}\n        \n    def observation(self, side):\n        return self.Board * side\n        \n    def step(self, action):\n        win = False\n        draw = False\n        side = self.Side\n        other_side = -side\n        color = side\n        \n        reward = 0.0\n        done = False\n\n        if self.Board[action] != 0:\n            \n            reward = -1.0\n            done = True\n        else:\n            self.Board[action] = side\n            self.BoardHistory.append(self.Board.reshape((3,3)).copy())\n    \n            for win_mask in WinMasks:\n                masked = self.Board*color*win_mask\n                if np.sum(masked) == 3:\n                    reward = 1.0\n                    done = True\n                    break\n                    \n            if np.all(self.Board != 0):\n                done = True     \n        self.Side = other_side\n        self.Done = done\n        self.Reward = reward\n        return self.observation(self.Side), reward, done, {\"valid_actions\":np.asarray(self.Board==0, dtype=np.float32)}\n            \n    def render(self):\n        if self.Done:\n            last_move = -self.Side\n            history = self.BoardHistory\n            sep = \"+---\"*len(history) + \"+\"\n            lines = [sep]\n            for irow in (0,1,2):\n                line = \"|\"\n                for b in history:\n                    row = \"\".join(\".xo\"[int(c)] for c in b[irow])\n                    line += row + \"|\"\n                lines.append(line)\n            outcome = \"draw\"\n            if self.Reward:\n                outcome = \"%s won\" % (\".xo\"[int(last_move)])\n            lines.append(sep + \" \" + outcome)\n            print(\"\\n\".join(lines))\n        \nif __name__ == \"__main__\":\n    \n    import random\n    \n    def show_board(board):\n        sep = \"+---\"*3 + \"+\"\n        out = [sep]\n        for row in board.reshape((3,3)):\n            line = \"| \"\n            for x in row:\n                line += \" OX\"[int(x)] + \" | \"\n            out.append(line)\n            out.append(sep)\n        return \"\\n\".join(out)\n    \n    class Agent(object):\n        \n        def __init__(self, side):\n            self.Side = side\n            self.Sign = \"XO\"[side]\n            self.Color = side*2-1\n            \n        def reset(self):\n            pass\n        \n        def action(self, reward, observation, available_actions):\n            print(f\"{self.Sign}: action:\", reward, observation, available_actions)\n            choices = [i for i, x in enumerate(available_actions) if x]\n            i = random.choice(choices)\n            return i\n        \n        def reward(self, r):\n            \n            pass\n            \n        def done(self, r, last_observation):\n            if r > 0:\n                print(f\"===== {self.Sign} won\")\n            elif r < 0:\n                print(f\"===== {self.Sign} lost\")\n            else:\n                print(\"===== draw\")\n            \n    class Callback(object):\n        \n        def end_turn(self, agents, data):\n            print(show_board(data[\"board\"]))\n            \n        def end_episode(self, agents, data):\n            print(\"--- game over ---\")\n            print(env.show_history(data[\"board_history\"]))\n        \n    x_agent = Agent(0)\n    y_agent = Agent(1)\n    \n    env = TicTacToeEnv()\n    env.run([x_agent, y_agent], [Callback])\n    \n    \n    \n    ",
        "summary": "The provided Python code defines a reinforcement learning environment for the game of Tic-Tac-Toe using the OpenAI Gym framework. It includes a set of winning masks to detect win conditions and an environment class `SingleAgentTicTacToeEnv` that handles the game logic, state transitions, rewards, and rendering. The script also demonstrates a simple random agent playing against itself in the game."
    },
    {
        "code": "import os\nimport json\n\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\n\n\nclass Action:\n    def __init__(self, card, config):\n        config.pop(\"action\", None)\n\n        self.card = card\n        self.config = config\n\n    def env_vars_for_object(self, config, prefix):\n        env_vars = {}\n\n        config.pop(\"action\", None)\n        config.pop(\"id\", None)\n\n        for key, value in config.items():\n            if value and isinstance(value, dict):\n                nested_env_vars = self.env_vars_for_object(\n                    value, \"{}_{}\".format(prefix, key.upper())\n                )\n                env_vars = {**env_vars, **nested_env_vars}\n            else:\n                env_vars[\"{}_{}\".format(prefix, key.upper())] = value\n\n        return env_vars\n\n    def env_vars(self):\n        with open(CURRENT_DIR + \"/../../config/config.json\", \"r\") as f:\n            global_config = json.load(f)\n\n        env_vars = self.env_vars_for_object(self.card, \"CARD\")\n        env_vars[\"magic_cards_room\"] = global_config[\"room\"]\n\n        prefix = self.__class__.__name__.replace(\"Action\", \"\").upper()\n        return {**env_vars, **self.env_vars_for_object(self.config, prefix)}\n\n\nclass ChromecastAction(Action):\n    def __init__(self, card, config, chromecast):\n        super().__init__(card, config)\n        self.chromecast = chromecast\n",
        "summary": "The provided Python code defines a base class `Action` with methods to generate environment variables from configuration objects and a subclass `ChromecastAction` that extends `Action` to include additional functionality specific to Chromecast devices."
    },
    {
        "code": "from office365.runtime.client_value_collection import ClientValueCollection\nfrom office365.runtime.queries.service_operation_query import ServiceOperationQuery\nfrom office365.runtime.resource_path import ResourcePath\nfrom office365.sharepoint.base_entity import BaseEntity\nfrom office365.sharepoint.tenant.administration.hubSiteProperties import HubSiteProperties\nfrom office365.sharepoint.tenant.administration.secondary_administrators_fields_data import \\\n    SecondaryAdministratorsFieldsData\nfrom office365.sharepoint.tenant.administration.secondary_administrators_info import SecondaryAdministratorsInfo\nfrom office365.sharepoint.tenant.administration.site_properties import SiteProperties\nfrom office365.sharepoint.tenant.administration.site_properties_collection import SitePropertiesCollection\nfrom office365.sharepoint.tenant.administration.sitePropertiesEnumerableFilter import SitePropertiesEnumerableFilter\nfrom office365.sharepoint.tenant.administration.spo_operation import SpoOperation\n\n\nclass Tenant(BaseEntity):\n\n    def __init__(self, context):\n        super().__init__(context, ResourcePath(\"Microsoft.Online.SharePoint.TenantAdministration.Tenant\"),\n                         \"Microsoft.Online.SharePoint.TenantAdministration\")\n\n    def get_site_secondary_administrators(self, site_id):\n        \n        return_type = ClientValueCollection(SecondaryAdministratorsInfo)\n        payload = SecondaryAdministratorsFieldsData(site_id)\n        qry = ServiceOperationQuery(self, \"GetSiteSecondaryAdministrators\", None, payload,\n                                    \"secondaryAdministratorsFieldsData\", return_type)\n        self.context.add_query(qry)\n        return return_type\n\n    def set_site_secondary_administrators(self, site_id, emails, names=None):\n        \n        payload = SecondaryAdministratorsFieldsData(site_id, emails, names)\n        qry = ServiceOperationQuery(self, \"SetSiteSecondaryAdministrators\", None, payload,\n                                    \"secondaryAdministratorsFieldsData\", None)\n        self.context.add_query(qry)\n        return self\n\n    def register_hub_site(self, site_url):\n        \n        return_type = HubSiteProperties(self.context)\n        params = {\"siteUrl\": site_url}\n        qry = ServiceOperationQuery(self, \"RegisterHubSite\", None, params, None, return_type)\n        self.context.add_query(qry)\n        return return_type\n\n    def unregister_hub_site(self, siteUrl):\n        \n        params = {\"siteUrl\": siteUrl}\n        qry = ServiceOperationQuery(self, \"UnregisterHubSite\", None, params, None, None)\n        self.context.add_query(qry)\n        return self\n\n    def create_site(self, site_create_props):\n        \n        result = SpoOperation(self.context)\n        qry = ServiceOperationQuery(self, \"CreateSite\", None, site_create_props, \"siteCreationProperties\", result)\n        self.context.add_query(qry)\n        return result\n\n    def remove_site(self, site_url):\n        \n        result = SpoOperation(self.context)\n        qry = ServiceOperationQuery(self, \"removeSite\", [site_url], None, None, result)\n        self.context.add_query(qry)\n        return result\n\n    def remove_deleted_site(self, site_url):\n        pass\n\n    def restore_deleted_site(self, site_url):\n        pass\n\n    def get_site_properties_by_url(self, url, include_detail):\n        \n        site_props = SiteProperties(self.context)\n        self._sites.add_child(site_props)\n        payload = {\n            'url': url,\n            'includeDetail': include_detail\n        }\n        qry = ServiceOperationQuery(self, \"getSitePropertiesByUrl\", None, payload, None, site_props)\n        self.context.add_query(qry)\n        return site_props\n\n    def get_site_properties_from_sharepoint_by_filters(self, _filter, start_index=0, include_detail=False):\n        \n        site_props_col = SitePropertiesCollection(self.context)\n        qry = ServiceOperationQuery(self, \"getSitePropertiesFromSharePointByFilters\",\n                                    None,\n                                    SitePropertiesEnumerableFilter(_filter, start_index, include_detail),\n                                    \"speFilter\",\n                                    site_props_col)\n        self.context.add_query(qry)\n        return site_props_col\n\n    @property\n    def root_site_url(self):\n        \n        return self.properties.get('RootSiteUrl', None)\n\n    @property\n    def _sites(self):\n        \n        if self.is_property_available('sites'):\n            return self.properties['sites']\n        else:\n            return SitePropertiesCollection(self.context, ResourcePath(\"sites\", self.resource_path))\n",
        "summary": "The provided Python code defines a class `Tenant` that extends `BaseEntity` and includes methods for managing SharePoint tenant properties, such as retrieving and setting secondary administrators, registering and unregistering hub sites, creating and removing sites, and fetching site properties. It utilizes various query types to interact with the SharePoint tenant administration API."
    },
    {
        "code": "import glob\nimport cv2\nimport os\n\n\ndef extract_frame(movie_files_dir, out_dir):\n    movie_files = glob.glob(movie_files_dir)\n\n    if not movie_files:\n        print('movie files are not found.')\n        return\n\n    for movie_file in movie_files:\n        ext = movie_file.split('.')[-1]\n        if not ext == 'mp4' or not ext == 'MP4':\n            print(f\"can't extract this movie file: {movie_file}\")\n            continue\n\n        out_dir = out_dir\n        if not os.path.exists(out_dir):\n            os.mkdir(out_dir)\n\n        cap = cv2.VideoCapture(movie_file)\n        if not cap.isOpened():\n            print(f\"can't extract this movie file: {movie_file}\")\n            return\n\n        digit = len(str(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))))\n        n = 0\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                cv2.imwrite(f\"{movie_file}_{str(n).zfill(digit)}.jpg\", frame)\n                n += 1\n                continue\n            return\n\n    print(f'{len(movie_files)} movie files extracted')\n\n\nif __name__ == '__main__':\n    movie_files_dir = 'movies/*.mp4'\n    out_dir = 'out/'\n    extract_frame(movie_files_dir, out_dir)\n",
        "summary": "The provided Python script defines a function `extract_frame` that takes a directory of MP4 video files and an output directory as arguments. It iterates through each video file in the specified directory, extracts frames from each video, and saves them as JPEG images in the output directory. The script uses OpenCV for video processing and handles basic error checking such as verifying file extensions and ensuring the output directory exists."
    },
    {
        "code": "from ping360_sonar.sensor import Ping360\nfrom numpy import pi, sqrt, tan, cos, sign\nfrom brping import definitions\n\nclass SonarInterface:\n    \n    samplePeriodTickDuration = 25e-9\n    firmwareMinTransmitDuration = 5\n    firmwareMaxTransmitDuration = 500\n    firmwareMaxSamples = 1200\n    firmwareMinSamplePeriod = 80\n    maxDurationRatio = 64e6\n    \n    def __init__(self, port, baudrate, fallback_emulated):\n                \n        self.angle = 0\n        try:\n            self.sonar = Ping360(port, baudrate)\n            if self.sonar.initialize():\n                return\n        except:\n            pass\n        \n        if not fallback_emulated:\n            raise RuntimeError('Cannot initialize sonar')\n        print('Using emulated sonar')\n        self.sonar = None\n        \n    def configureAngles(self, aperture_deg, step_deg, ensure_divisor):\n        \n        target_half_aperture = int(aperture_deg*200/360+0.5)\n        best_half_aperture = target_half_aperture\n        self.angle_step = int(round(step_deg*400/360))\n\n        \n        if ensure_divisor:            \n            \n            target_step = self.angle_step\n            \n            \n            computeCost = lambda step,half_aperture: 1000 if half_aperture%step != 0 else abs(step-target_step) + abs(half_aperture-target_half_aperture)\n            \n            best_cost = computeCost(self.angle_step, target_half_aperture)\n            if best_cost != 0:                \n                for step in range(1, target_step*2):\n                    for half_aperture in range(target_half_aperture, min(target_half_aperture+10, 200)+1):\n                        cost = computeCost(step, half_aperture)\n                        if cost < best_cost:\n                            best_cost = cost\n                            self.angle_step = step\n                            best_half_aperture = half_aperture\n                        \n        self.angle_min = -best_half_aperture\n        self.angle_max = best_half_aperture\n        if self.angle_max == 200:                \n            self.angle_max -= self.angle_step\n        if self.angle < self.angle_min or self.angle > self.angle_max or (self.angle-self.angle_min) % self.angle_step != 0:\n            self.angle = 0\n    \n    @staticmethod\n    def grad2rad(grad):\n        return grad*pi/200\n    \n    def angleMin(self):\n        return self.grad2rad(self.angle_min)\n    def angleMax(self):\n        return self.grad2rad(self.angle_max)\n    def angleStep(self):\n        return self.grad2rad(self.angle_step)\n    def currentAngle(self):\n        return self.grad2rad(self.angle)\n    def angleCount(self):\n        return (self.angle_max-self.angle_min)//self.angle_step\n    def angleIndex(self):\n        if self.angle_step > 0:\n            return (self.angle-self.angle_min)//self.angle_step\n        return (self.angle-self.angle_max)//self.angle_step\n    def rangeFrom(self, index):\n        return (index+1)*self.max_range/self.samples\n    \n    def configureTransducer(self, gain, frequency, speed_of_sound, max_range):\n        \n        self.gain = gain\n        self.frequency = frequency\n        \n        self.samples = int(min(self.firmwareMaxSamples,2*max_range/(self.firmwareMinSamplePeriod*speed_of_sound*self.samplePeriodTickDuration)))\n        \n        self.sample_period = int((2.*max_range)/\n                                 (self.samples*speed_of_sound*self.samplePeriodTickDuration));\n        \n\n        \n        \n        \n        \n        \n        \n\n        \n        one_way_duration_us = (8000.*max_range)/speed_of_sound\n        \n        sample_period_ns = self.sample_period * self.samplePeriodTickDuration\n        self.transmit_duration = max(2.5*sample_period_ns/1000, one_way_duration_us)\n        \n        if self.transmit_duration < self.firmwareMinTransmitDuration:\n            self.transmit_duration = self.firmwareMinTransmitDuration\n        else:\n            max_duration = min(self.firmwareMaxTransmitDuration, sample_period_ns*self.maxDurationRatio)\n            if self.transmit_duration > max_duration:\n                self.transmit_duration = max_duration\n        self.transmit_duration = int(self.transmit_duration)\n            \n    def transmitDuration(self):\n        \n        return self.transmit_duration/1e6\n    \n    def updateAngle(self):\n        self.angle += self.angle_step\n        \n        if self.angle_min == -200:\n            \n            end_turn = self.angle + self.angle_step > self.angle_max\n            if self.angle > self.angle_max:\n                self.angle = self.angle_min\n            return end_turn\n        \n        \n        if self.angle + self.angle_step >= self.angle_max or self.angle + self.angle_step <= self.angle_min:\n            self.angle_step *= -1\n            return True\n        return False\n            \n    def read(self):\n        \n        end_turn = self.updateAngle()\n        \n        if self.sonar is not None:\n            print(f'transmit: {self.transmit_duration}')\n            \n            self.sonar.control_transducer(\n                    0,  \n                    self.gain,\n                    self.angle,\n                    self.transmit_duration,\n                    self.sample_period,\n                    self.frequency,\n                    self.samples,\n                    1,\n                    0)\n            self.sonar.wait_message([definitions.PING360_DEVICE_DATA, definitions.COMMON_NACK], 4.0)\n            self.data = bytearray(self.sonar._data)\n            return (len(self.data) != 0, end_turn)\n        \n        \n        from random import randint\n        from time import sleep    \n        self.data = [0 for _ in range(self.samples)]\n        scale = 5*abs((self.angle+400) % 400 - 200)\n        for i in range(self.samples):\n            if randint(self.samples,2*self.samples) < 1.1*i + scale:\n                self.data[i] = randint(220, 255)\n        \n        \n        return (True, end_turn)\n\n\n\n\nclass Bound:\n    radius = 0\n    def __init__(self, x, tm, tM):\n        self.x = x\n        if type(tM) == int:\n            self.low = Bound.clamp(tm*x)\n            self.up = int(tM*sqrt(Bound.radius**2-x**2-1))\n        else:\n            self.low = Bound.clamp(x*tm)\n            self.up = Bound.clamp(x*tM)\n            \n            if self.up**2 + x**2 > Bound.radius**2:\n                self.up = int(sign(self.up) * sqrt(Bound.radius**2-x**2-1))\n                \n        if self.up < self.low:\n            self.low,self.up = self.up,self.low\n            \n    \n    def clamp(coord):\n        if coord < -Bound.radius+1:\n            return -Bound.radius+1\n        elif coord > Bound.radius-1:\n            return Bound.radius-1\n        return int(coord)\n            \nclass Sector:\n    def __init__(self):\n        self.dr = None\n        \n    def configure(self, samples, radius):\n        self.dr = radius/samples\n        Bound.radius = radius\n        \n    def init(self, angle, step):\n        angle_min = angle-step/2\n        angle_max = angle+step/2\n        xmin, xmax,same_side = self.xLimits(angle_min, angle_max)\n        tm, tM = tan(angle_min), tan(angle_max)        \n        self.bounds = []\n\n        if same_side:\n            \n            if abs(tm) > abs(tM):\n                tm,tM = tM,tm\n            for x in range(xmin, xmax+1):\n                self.bounds.append(Bound(x,tm,tM))\n        else:\n            f = 1 if abs(angle-pi/2) < abs(angle+pi/2) else -1\n            \n            if f == -1:\n                tm,tM = tM,tm\n                \n            for x in range(xmin, 0):\n                self.bounds.append(Bound(x, tM,f))\n            for x in range(0, xmax+1):\n                self.bounds.append(Bound(x, tm,f))\n                \n        self.cur = -1\n        \n    def xLimits(self, angle_min, angle_max):\n        cm = cos(angle_min)\n        cM = cos(angle_max)\n        if cM < cm:\n            cm,cM = cM,cm\n        if cm*cM > 0:\n            if cM < 0:\n                cM = 0\n            else:\n                cm = 0\n        return Bound.clamp(round(Bound.radius*cm)), Bound.clamp(round(Bound.radius*cM)), cm*cM >= 0\n    \n    def nextPoint(self, x, y):\n        if self.cur == -1:\n            self.cur = 0\n            x = self.bounds[0].x\n            y = self.bounds[0].low\n        elif y < self.bounds[self.cur].up:\n            y += 1\n        else:\n            self.cur += 1\n            if self.cur == len(self.bounds):\n                return False, 0, 0, 0\n            x = self.bounds[self.cur].x\n            y = self.bounds[self.cur].low\n        return True, x, y, int(round(sqrt(x*x+y*y)/self.dr))\n",
        "summary": "The provided Python code defines a `SonarInterface` class for controlling and reading data from a sonar device, including methods to update the angle, transmit signals, and read sensor data. It also includes a `Bound` class for defining angular bounds and a `Sector` class for managing sectors of angles with associated bounds. The code handles both real sonar devices and simulated data generation for testing purposes."
    },
    {
        "code": "import logging\n\nfrom ...utilities import WrapperNotInstalledException\n\ntry:\n    import MDAnalysis as mda\nexcept ModuleNotFoundError:\n    mda = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass MDAnalysis:\n    \n\n    def __init__(self):\n        if mda is None:\n            raise WrapperNotInstalledException(\n                'MDAnalysis is not installed; see README for '\n                'installation.'\n            )\n\n    def get_universe(self, mol):\n        \n\n        rdkit_mol = mol.to_rdkit_mol()\n        return mda.Universe(rdkit_mol)\n",
        "summary": "The provided Python code defines a class `MDAnalysis` that checks if the `MDAnalysis` module is installed and raises an exception if it's not. If installed, it provides a method to convert a molecule object to an MDAnalysis universe using RDKit."
    },
    {
        "code": "def fact (n):\n    if n == 1:\n        return 1\n    else:\n        return n*fact(n-1)\n\nprint(fact(5))",
        "summary": "The provided Python function `fact` calculates the factorial of a given number `n` using recursion, where it multiplies `n` by the factorial of `n-1` until it reaches 1. The code then prints the factorial of 5, which is 120."
    },
    {
        "code": "import torch\r\nfrom torch.optim import Optimizer\r\n\r\nclass OptimWrapper(Optimizer):\r\n\r\n    \n\r\n    def __init__(self, optim):\r\n        self.optim = optim\r\n\r\n    def __getstate__(self):\r\n        return self.optim.__getstate__()\r\n\r\n    def __setstate__(self, state):\r\n        self.optim.__setstate__(state)\r\n\r\n    @property\r\n    def state(self):\r\n        return self.optim.state\r\n\r\n    @property\r\n    def param_groups(self):\r\n        return self.optim.param_groups\r\n\r\n    @param_groups.setter\r\n    def param_groups(self, value):\r\n        self.optim.param_groups = value\r\n\r\n    def state_dict(self):\r\n        return self.optim.state_dict()\r\n\r\n    def load_state_dict(self, state_dict):\r\n        self.optim.load_state_dict(state_dict)\r\n\r\n    def zero_grad(self):\r\n        self.optim.zero_grad()\r\n\r\n    def add_param_group(self, param_group):\r\n        self.optim.add_param_group(param_group)\r\n\r\n    @property\r\n    def defaults(self):\r\n        return self.optim.defaults\r\n\r\n    @defaults.setter\r\n    def defaults(self, defaults):\r\n        self.optim.defaults = defaults\r\n\r\n    @torch.no_grad()\r\n    def step(self, closure=None):\r\n        self.optim.step(closure=closure)\r\n\r\n    def __repr__(self):\r\n        return \"%s(%r)\" % (self.__class__.__name__, self.optim)",
        "summary": "The `OptimWrapper` class in Python wraps an existing PyTorch optimizer, providing a layer of abstraction that allows for easy access and manipulation of the underlying optimizer's state and properties. It includes methods to interact with the optimizer's parameters, gradients, and step updates, making it a versatile tool for customizing optimization processes in machine learning models."
    },
    {
        "code": "import os\n\nfrom pytest import fixture\n\n\n@fixture(scope='function')\ndef environ(request):\n    origin = dict(os.environ)\n\n    @request.addfinalizer\n    def restore_environ():\n        os.environ.clear()\n        os.environ.update(origin)\n\n    return os.environ\n",
        "summary": "The Python code defines a pytest fixture named `environ` that temporarily clears the environment variables at the beginning of each test function and restores them to their original state after the test completes. This ensures that tests run in isolation without interference from external environment settings."
    },
    {
        "code": "from apricot import TestWithServers\nfrom test_utils_pool import TestPool\n\nclass DestroyRebuild(TestWithServers):\n    \n\n    \n    CANCEL_FOR_TICKET = [[\"DAOS-4891\", \"rank_to_kill\", \"[0]\"]]\n\n    def test_destroy_while_rebuilding(self):\n        \n        \n        self.pool = TestPool(self.context, self.get_dmg_command())\n        self.pool.get_params(self)\n        targets = self.params.get(\"targets\", \"/run/server_config/servers/*\")\n        ranks = self.params.get(\"rank_to_kill\", \"/run/testparams/*\")\n\n        \n        self.pool.create()\n\n        \n        checks = {\n            \"pi_nnodes\": len(self.hostlist_servers),\n            \"pi_ntargets\": len(self.hostlist_servers) * targets,\n            \"pi_ndisabled\": 0,\n        }\n        self.assertTrue(\n            self.pool.check_pool_info(**checks),\n            \"Invalid pool information detected prior to rebuild\")\n\n        \n        self.server_managers[0].stop_ranks(ranks, self.d_log, force=True)\n        self.pool.wait_for_rebuild(True)\n\n        \n        self.pool.destroy()\n\n        self.log.info(\"Test Passed\")\n        self.get_dmg_command().system_start(\",\".join(ranks))\n        self.server_managers[0].update_expected_states(\",\".join(ranks), [\"joined\"])\n",
        "summary": "The `DestroyRebuild` class inherits from `TestWithServers` and includes a method to test the destruction of a pool while it is rebuilding, ensuring that the pool's information remains valid before and after the operation. It stops specific ranks, waits for the rebuild process, destroys the pool, and then restarts the stopped ranks."
    },
    {
        "code": "import os\nimport re\nimport socket\n\nimport ttfw_idf\n\n\n@ttfw_idf.idf_example_test(env_tag='Example_WIFI_Protocols')\ndef test_examples_protocol_asio_udp_server(env, extra_data):\n    \n    test_msg = b'echo message from client to server'\n    dut1 = env.get_dut('udp_echo_server', 'examples/protocols/asio/udp_echo_server', dut_class=ttfw_idf.ESP32DUT)\n    \n    binary_file = os.path.join(dut1.app.binary_path, 'asio_udp_echo_server.bin')\n    bin_size = os.path.getsize(binary_file)\n    ttfw_idf.log_performance('asio_udp_echo_server_bin_size', '{}KB'.format(bin_size // 1024))\n    \n    dut1.start_app()\n    \n    data = dut1.expect(re.compile(r' IPv4 address: ([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)'), timeout=30)\n    \n    cli = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    cli.settimeout(30)\n    cli.connect((data[0], 2222))\n    cli.send(test_msg)\n    data = cli.recv(1024)\n    \n    if (data == test_msg):\n        print('PASS: Received correct message')\n        pass\n    else:\n        print('Failure!')\n        raise ValueError('Wrong data received from asio udp server: {} (expected:{})'.format(data, test_msg))\n    \n    dut1.expect(test_msg.decode())\n\n\nif __name__ == '__main__':\n    test_examples_protocol_asio_udp_server()\n",
        "summary": "The Python code defines a test function `test_examples_protocol_asio_udp_server` that uses the `ttfw_idf` library to run an ESP32DUT device as an UDP echo server. It sends a test message from a client socket connected to the server's IP address and port, then verifies if the received data matches the sent message, logging performance metrics and handling any discrepancies by raising errors."
    },
    {
        "code": "ones_names = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',\n              'eight', 'nine', 'ten', 'eleven',  'twelve', 'thirteen',\n              'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',\n              'nineteen']\n              \ntens_names = ['zero', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty',\n              'seventy', 'eighty', 'ninety']\n\n\ndef build_words(n):\n    if n == 1000:\n        return 'one' + 'thousand'\n    elif n > 99:\n        hundreds = ones_names[int(n / 100)] + 'hundred'\n        n = n % 100\n        if n == 0:\n            return hundreds\n        return hundreds + 'and' + build_words(n)\n    elif n > 19:\n        tens = tens_names[int(n / 10)]\n        n = n % 10\n        if n == 0:\n            return tens\n        return tens + ones_names[n]\n    else:\n        return ones_names[n]\n\n    \n\ndef solve_problem():\n    total_letters = 0\n    for n in range(1,1001):\n        total_letters += len(build_words(n))\n\n\n    return(total_letters)\n\n\nif __name__ == \"__main__\":\n    print(solve_problem())\n",
        "summary": "The provided Python code defines two lists containing names for numbers from zero to nineteen and tens multiples up to ninety. It includes a recursive function `build_words` that converts integers into their word representations, handling special cases for hundreds and thousands. The `solve_problem` function calculates the total number of letters used in writing out all numbers from one to one thousand in words."
    },
    {
        "code": "import _plotly_utils.basevalidators\n\n\nclass TypesrcValidator(_plotly_utils.basevalidators.SrcValidator):\n    def __init__(\n        self, plotly_name=\"typesrc\", parent_name=\"scattergeo.marker.gradient\", **kwargs\n    ):\n        super(TypesrcValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type=kwargs.pop(\"edit_type\", \"none\"),\n            role=kwargs.pop(\"role\", \"info\"),\n            **kwargs\n        )\n\n\nimport _plotly_utils.basevalidators\n\n\nclass TypeValidator(_plotly_utils.basevalidators.EnumeratedValidator):\n    def __init__(\n        self, plotly_name=\"type\", parent_name=\"scattergeo.marker.gradient\", **kwargs\n    ):\n        super(TypeValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            array_ok=kwargs.pop(\"array_ok\", True),\n            edit_type=kwargs.pop(\"edit_type\", \"calc\"),\n            role=kwargs.pop(\"role\", \"style\"),\n            values=kwargs.pop(\"values\", [\"radial\", \"horizontal\", \"vertical\", \"none\"]),\n            **kwargs\n        )\n\n\nimport _plotly_utils.basevalidators\n\n\nclass ColorsrcValidator(_plotly_utils.basevalidators.SrcValidator):\n    def __init__(\n        self, plotly_name=\"colorsrc\", parent_name=\"scattergeo.marker.gradient\", **kwargs\n    ):\n        super(ColorsrcValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type=kwargs.pop(\"edit_type\", \"none\"),\n            role=kwargs.pop(\"role\", \"info\"),\n            **kwargs\n        )\n\n\nimport _plotly_utils.basevalidators\n\n\nclass ColorValidator(_plotly_utils.basevalidators.ColorValidator):\n    def __init__(\n        self, plotly_name=\"color\", parent_name=\"scattergeo.marker.gradient\", **kwargs\n    ):\n        super(ColorValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            array_ok=kwargs.pop(\"array_ok\", True),\n            edit_type=kwargs.pop(\"edit_type\", \"calc\"),\n            role=kwargs.pop(\"role\", \"style\"),\n            **kwargs\n        )\n",
        "summary": "The provided Python code defines several validators for Plotly's `scattergeo.marker.gradient` properties, including `TypesrcValidator`, `TypeValidator`, `ColorsrcValidator`, and `ColorValidator`. These validators ensure that the input values for these properties are of the correct type and format according to Plotly's requirements."
    },
    {
        "code": "from dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom .cloud_provider import CloudProvider\nfrom .instance import Instance\nfrom .instance_status import InstanceStatus\n\n\n@dataclass\nclass AwsInstance(Instance):\n    \n    type: str = field(default=\"co.yellowdog.platform.model.AwsInstance\", init=False)\n    id: Optional[str] = field(default=None, init=False)\n    \n    instanceLifecycle: Optional[str] = None\n    \n    createdTime: Optional[datetime] = None\n    \n    sourceId: Optional[str] = None\n    \n    imageId: Optional[str] = None\n    \n    instanceType: Optional[str] = None\n    \n    provider: Optional[CloudProvider] = None\n    \n    region: Optional[str] = None\n    \n    status: Optional[InstanceStatus] = None\n    \n    subregion: Optional[str] = None\n    \n    privateIpAddress: Optional[str] = None\n    \n    publicIpAddress: Optional[str] = None\n    \n    hostname: Optional[str] = None\n    \n",
        "summary": "The `AwsInstance` class is a dataclass representing an AWS instance, inheriting from the `Instance` class. It includes various attributes such as type, ID, lifecycle, creation time, source and image IDs, instance type, provider details, region, status, subregion, and IP addresses, with some fields being optional."
    },
    {
        "code": "import contextlib\n\nimport numpy as np\nimport tensorflow as tf\n\nimport zfit.z.numpy as znp\nfrom zfit import z\n\nfrom ..core.basepdf import BasePDF\nfrom ..core.space import ANY_LOWER, ANY_UPPER, Space\nfrom ..util import ztyping\nfrom ..util.exception import (AnalyticIntegralNotImplemented,\n                              BreakingAPIChangeError)\nfrom ..util.warnings import warn_advanced_feature\n\n\nclass Exponential(BasePDF):\n    _N_OBS = 1\n\n    def __init__(self, lam=None, obs: ztyping.ObsTypeInput = None, name: str = \"Exponential\", lambda_=None):\n        \n        if lambda_ is not None:\n            if lam is None:\n                lam = lambda_\n            else:\n                raise BreakingAPIChangeError(\"The 'lambda' parameter has been renamed from 'lambda_' to 'lam'.\")\n        params = {'lambda': lam}\n        super().__init__(obs, name=name, params=params)\n\n        self._calc_numerics_data_shift = lambda: z.constant(0.)\n\n        if not self.space.has_limits:\n            warn_advanced_feature(\"Exponential pdf relies on a shift of the input towards 0 to keep the numerical \"\n                                  f\"stability high. The space {self.space} does not have limits set and no shift\"\n                                  f\" will occure. To set it manually, set _numerics_data_shift to the expected\"\n                                  f\" average values given to this function _in case you want things to be set_.\"\n                                  f\"If this sounds unfamiliar, regard this as an error and use a normalization range.\",\n                                  identifier='exp_shift')\n        self._set_numerics_data_shift(self.space)\n\n    def _unnormalized_pdf(self, x):\n        lambda_ = self.params['lambda']\n        x = x.unstack_x()\n        probs = znp.exp(lambda_ * (self._shift_x(x)))\n        tf.debugging.assert_all_finite(probs, f\"Exponential PDF {self} has non valid values. This is likely caused\"\n                                              f\" by numerical problems: if the exponential is too steep, this will\"\n                                              f\" yield NaNs or infs. Make sure that your lambda is small enough and/or\"\n                                              f\" the initial space is in the same\"\n                                              f\" region as your data (and norm_range, if explicitly set differently).\"\n                                              f\" If this issue still persists, please oben an issue on Github:\"\n                                              f\" https://github.com/zfit/zfit\")\n        return probs  \n\n    def _shift_x(self, x):\n        return x - self._calc_numerics_data_shift()\n\n    @contextlib.contextmanager\n    def _set_numerics_data_shift(self, limits):\n        if limits:\n            def calc_numerics_data_shift():\n                lower, upper = [], []\n                for limit in limits:\n                    low, up = limit.rect_limits\n                    lower.append(z.convert_to_tensor(low[:, 0]))\n                    upper.append(z.convert_to_tensor(up[:, 0]))\n                lower = z.convert_to_tensor(lower)\n                upper = z.convert_to_tensor(upper)\n                lower_val = znp.min(lower, axis=0)\n                upper_val = znp.max(upper, axis=0)\n\n                return (upper_val + lower_val) / 2\n\n            old_value = self._calc_numerics_data_shift\n\n            self._calc_numerics_data_shift = calc_numerics_data_shift\n            yield\n            self._calc_numerics_data_shift = old_value\n        else:\n            yield\n\n    \n    \n    \n    def _single_hook_integrate(self, limits, norm_range, x):\n        with self._set_numerics_data_shift(norm_range):\n            return super()._single_hook_integrate(limits, norm_range, x=x)\n\n    def _single_hook_analytic_integrate(self, limits, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_analytic_integrate(limits, norm_range)\n\n    def _single_hook_numeric_integrate(self, limits, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_numeric_integrate(limits, norm_range)\n\n    def _single_hook_partial_integrate(self, x, limits, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_partial_integrate(x, limits, norm_range)\n\n    def _single_hook_partial_analytic_integrate(self, x, limits, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_partial_analytic_integrate(x, limits, norm_range)\n\n    def _single_hook_partial_numeric_integrate(self, x, limits, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_partial_numeric_integrate(x, limits, norm_range)\n\n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    def _single_hook_pdf(self, x, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_pdf(x, norm_range)\n\n    \n    def _single_hook_log_pdf(self, x, norm_range):\n        with self._set_numerics_data_shift(limits=norm_range):\n            return super()._single_hook_log_pdf(x, norm_range)\n\n    def _single_hook_sample(self, n, limits, x=None):\n        with self._set_numerics_data_shift(limits=limits):\n            return super()._single_hook_sample(n, limits, x)\n\n\ndef _exp_integral_from_any_to_any(limits, params, model):\n    lambda_ = params['lambda']\n    lower, upper = limits.rect_limits\n    \n    \n\n    integral = _exp_integral_func_shifting(lambd=lambda_, lower=lower, upper=upper, model=model)\n    return integral[0]\n\n\ndef _exp_integral_func_shifting(lambd, lower, upper, model):\n    def raw_integral(x):\n        return z.exp(lambd * (model._shift_x(x))) / lambd  \n\n    lower_int = raw_integral(x=lower)\n    upper_int = raw_integral(x=upper)\n    integral = (upper_int - lower_int)\n    return integral\n\n\ndef exp_icdf(x, params, model):\n    lambd = params['lambda']\n    x = z.unstack_x(x)\n    x = model._shift_x(x)\n    return znp.log(lambd * x) / lambd\n\n\n\n\n\nlimits = Space(axes=0, limits=(ANY_LOWER, ANY_UPPER))\nExponential.register_analytic_integral(func=_exp_integral_from_any_to_any, limits=limits)\n",
        "summary": "The `Exponential` class in the provided Python code defines an exponential probability distribution within a specified space using TensorFlow and NumPy. It includes methods for calculating the unnormalized PDF, handling numerical stability through data shifting, and registering an analytic integral function to compute the distribution's integral over any limits."
    },
    {
        "code": "import os\nimport math\n\n\n\n\ndev_mode = False\ngrid_data_folder = os.path.join(os.getcwd(), 'raw_data_generation', 'input')\nraw_data_folder = os.path.join(os.getcwd(), 'raw_data')\ndatasets_folder = os.path.join(os.getcwd(), 'datasets')\ntest_data_folder = os.path.join(os.getcwd(), 'test')\nmodels_folder = os.path.join(os.getcwd(), 'models')\nlocal_machine_tz = 'Europe/Berlin'  \n\n\nlearning_config = {\n    \"dataset\": \"PV_noPV_7day_20k\",\n    \"RNN model settings\": [1, 2, 6, 2],     \n    \"number of epochs\": 100,\n    \"learning rate\": 1*10**-6,\n    \"activation function\": 'tanh',          \n    \"mini batch size\": 60,\n    \"optimizer\": 'Adam',                    \n    \"k folds\": 5,                           \n    \"cross_validation\": True,\n    \"early stopping\": False,\n    \"LR adjustment\": 'LR controlled',               \n    \"percentage of epochs for warm up\": 10,         \n    \"train test split\": 0.2,                        \n    \"baseline\": False,\n    \"metrics\": ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n    \"cross_val_metrics\": ['fit_time', 'test_accuracy', 'test_precision_macro', 'test_recall_macro', 'test_f1_macro'],\n    \"plot samples\": True,\n    \"classifier\": \"RNN\"  \n\n}\n\n\n\n\n\n\nraw_data_set_name = 'PV_noPV'                   \ndataset_available = False                       \nraw_data_available = True                      \nadd_data = True                                \nadd_noise = False\naccuracy = 0.01                                 \nsample_length = 7 * 96                          \nsmartmeter_ratedvoltage_range = [400, 415]\nsmartmeter_voltage_range = [363, 457]\nnumber_of_samples = 20000\nshare_of_positive_samples = 0.5        \nnumber_of_grids = len([i for i in os.listdir(grid_data_folder) if os.path.isdir(os.path.join(grid_data_folder, i))])\nfloat_decimal = 5                       \n\n\nuser = 'FellnerD'\nsystem_language = 0                     \nparallel_computing = True\ncores = 12                              \nreduce_result_file_size = True          \njust_voltages = True                    \n\n\nsim_length = 365                         \nif sim_length < sample_length/96: print('Choose different simulation length or sample length (sim_length >= sample_length')\n\nif raw_data_set_name == 'PV_noPV':\n    positive_samples_per_simrun = 5     \n    simruns = math.ceil((number_of_samples * share_of_positive_samples) / (positive_samples_per_simrun * number_of_grids) / (sim_length * 96/sample_length))\nelif raw_data_set_name == 'malfunctions_in_LV_grid_dataset':\n    simruns = math.ceil((number_of_samples * share_of_positive_samples) / (number_of_grids) / int(\n        (sim_length * 96 / sample_length) - 1))\nelif raw_data_set_name == 'dummy':\n    terms_per_simrun = 5                \n    simruns = math.ceil((number_of_samples * share_of_positive_samples) / (terms_per_simrun * number_of_grids) / (sim_length * 96/sample_length))\nelse:\n    simruns = 10                        \nstep_size = 15                          \npercentage = 25                         \ncontrol_curve_choice = 0                \nbroken_control_curve_choice = 3         \nnumber_of_broken_devices = 1            \nload_scaling = 100                      \ngeneration_scaling = 100                \nwhole_year = True                       \nt_start = None                          \nt_end = None\n\n",
        "summary": "The provided Python code sets up a configuration for data processing, model training, and simulation parameters. It defines paths for various folders, learning settings including hyperparameters, dataset details, and simulation conditions tailored for different scenarios such as PV systems and grid malfunctions. The script also includes logic to calculate the number of simulation runs based on specified parameters like sample length, number of samples, and grid data availability."
    },
    {
        "code": "import torch\nfrom labml_nn.transformers.switch import SwitchTransformer, SwitchTransformerLayer, SwitchFeedForward\nfrom labml_nn.transformers import MultiHeadAttention\nfrom labml_nn.transformers.feed_forward import FeedForward\nimport numpy as np\nfrom transformers import AutoConfig, AutoModel\nimport torch.nn as nn\nimport math\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\nfrom random import choice\nfrom sklearn.decomposition import PCA\nfrom copy import deepcopy\nfrom transformers import BertModel, BertConfig\n\n\n\nclass CustomDataset(torch.utils.data.Dataset):\n  'Characterizes a dataset for PyTorch'\n  def __init__(self, input_ids, token_type_ids, attention_masks):\n        'Initialization'\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_masks = attention_masks\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.input_ids)\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n\n        input_id = self.input_ids[index]\n        token_type_ID = self.token_type_ids[index]\n        attention_mask = self.attention_masks[index]\n        sample = {'input_ids':input_id, 'token_type_ids':token_type_ID , 'attention_mask':attention_mask}\n\n        return sample\n\n\ndef weights_init(tensor: torch.Tensor):\n    if isinstance(tensor, nn.Linear):\n        switch_init(tensor.weight.data)\n        torch.nn.init.zeros_(tensor.bias.data)\n    if isinstance(tensor, nn.LayerNorm):\n        torch.nn.init.zeros_(tensor.weight.data)\n        torch.nn.init.zeros_(tensor.bias.data)\n\ndef switch_init(tensor: torch.Tensor, s: float = 0.1, mean: float=0) -> torch.Tensor:\n    fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(tensor)\n    std = math.sqrt(s/fan_in)\n\n    return torch.nn.init.trunc_normal_(tensor=tensor, mean=mean, std=std)\n\n\nclass LaBSE_Switch(nn.Module):\n    \n\n    def __init__(self, config, word_embeddings_module):\n\n        super().__init__()\n        \n        self.switch_model = SwitchTransformer(\n            \n          SwitchTransformerLayer(\n          d_model=config['d_model'],\n          attn=MultiHeadAttention(config['heads'], config['d_model'], config['dropout']),\n\n              feed_forward=SwitchFeedForward(\n              capacity_factor=config['capacity_factor'],\n                              drop_tokens=config['drop_tokens'],\n                              is_scale_prob=config['is_scale_prob'],\n                              n_experts=config['n_experts'],\n                              expert=FeedForward(config['d_model'], config['d_ff'], config['dropout_ffn']),\n                              d_model=config['d_model']),\n                              dropout_prob=config['dropout']),\n                              config['n_layers'],\n                              d_out = int(768),\n                              dropout_prob = config['dropout'])\n        \n        \n        \n        \n        self.word_embeddings = word_embeddings_module\n\n        \n        \n    \n    def weight_init_from_teacher(self, teacher_model, int_matches):\n        \n        \n        \n        \n        self.switch_model.layers[int_matches[1]].attn.query.linear.weight = teacher_model.encoder.layer[int_matches[0]].attention.self.query.weight\n        self.switch_model.layers[int_matches[1]].attn.query.linear.bias = teacher_model.encoder.layer[int_matches[0]].attention.self.query.bias\n        self.switch_model.layers[int_matches[1]].attn.key.linear.weight = teacher_model.encoder.layer[int_matches[0]].attention.self.key.weight\n        self.switch_model.layers[int_matches[1]].attn.key.linear.bias = teacher_model.encoder.layer[int_matches[0]].attention.self.key.bias\n        self.switch_model.layers[int_matches[1]].attn.value.linear.weight = teacher_model.encoder.layer[int_matches[0]].attention.self.value.weight\n        self.switch_model.layers[int_matches[1]].attn.value.linear.bias = teacher_model.encoder.layer[int_matches[0]].attention.self.value.bias\n        self.switch_model.layers[int_matches[1]].attn.output.weight = teacher_model.encoder.layer[int_matches[0]].attention.output.dense.weight\n        self.switch_model.layers[int_matches[1]].attn.output.bias = teacher_model.encoder.layer[int_matches[0]].attention.output.dense.bias\n\n\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n        \n        \n        \n        \n        input_embeddings = self.word_embeddings(input_ids)\n\n        \n        _batch,_seq_len,_n_hid = input_embeddings.shape\n        \n\n        \n        outputs = self.switch_model(torch.reshape(input_embeddings, (_seq_len, _batch, _n_hid)),\n                              attention_mask=None)\n        \n        return outputs\n\n\ndef load_student(name, student_config, device, teacher_model, int_matches, N_LAYERS):\n\n    if name!='switch':\n        \n        \n        student_config = BertConfig.from_pretrained(name)\n        student_config.num_hidden_layers = N_LAYERS\n        student_config.output_hidden_states = True\n        student_config.output_attentions = True\n        student_config.use_cache = True\n        student_config.is_decoder = True\n        \n        \n        student_model = BertModel.from_pretrained(name, config=student_config)\n        student_model.set_input_embeddings(teacher_model.get_input_embeddings())\n        student_model = student_model.float()\n        student_model.to(device=device)\n        \n        return student_model\n\n    if name=='switch':\n        \n        \n        word_embeddings = deepcopy(teacher_model.get_input_embeddings())\n        compressed_word_embeddings = word_embedding_compression(word_embeddings, student_config['d_model'])\n        \n        \n        student_model = LaBSE_Switch(config=student_config, word_embeddings_module=compressed_word_embeddings)\n        \n        \n        student_model.switch_model.apply(weights_init)\n        student_model.weight_init_from_teacher(teacher_model=teacher_model, int_matches=int_matches)\n        \n        \n        student_model = student_model.float() \n        student_model.to(device=device)\n    \n        return student_model\n\n\ndef load_teacher(device):\n    teacher_config = AutoConfig.from_pretrained('sentence-transformers/LaBSE')\n    teacher_config.output_hidden_states = True\n    teacher_config.output_attentions = True\n    teacher_config.use_cache = True\n    teacher_config.is_decoder = True\n    teacher_model = AutoModel.from_pretrained('sentence-transformers/LaBSE', config=teacher_config)\n    teacher_model.float() \n    teacher_model.to(device=device)\n    \n    return teacher_model\n\n\ndef simple_adaptor(batch, model_outputs):\n    \n    \n    values = []\n    for i in model_outputs['past_key_values']:\n        values.append(i[1])\n    values = torch.stack(values)\n    \n    attentions = []\n    for j in model_outputs['attentions']:\n        attentions.append(inv_softmax(j))\n    attentions = torch.stack(attentions)\n    \n    \n    return {'logits': model_outputs['pooler_output'],\n            'hidden': model_outputs['hidden_states'],\n            \n            'attention':attentions,\n            'inputs_mask': batch['attention_mask'],\n            'value_relation': values,\n            'pooler_output':model_outputs['pooler_output']}\n\ndef inv_softmax(x,C=-50):\n    \n    \n    result = torch.log(x)\n    result = torch.where(result <= float('-inf'), torch.full_like(result,C), result)\n    return result\n\ndef teacher_adaptor(batch, model_outputs):\n    \n    values = []\n    for i in model_outputs['past_key_values']:\n        values.append(i[1])\n    values = torch.stack(values)\n    \n    attentions = []\n    for j in model_outputs['attentions']:\n        attentions.append(inv_softmax(j))\n    attentions = torch.stack(attentions)\n    \n    \n\n    return {\n            'logits':model_outputs['pooler_output'],\n            'hidden': model_outputs['hidden_states'],\n            \n            'attention': attentions,\n            'inputs_mask': batch['attention_mask'],\n            'value_relation': values,\n            'pooler_output':model_outputs['pooler_output']}\n\n\ndef switch_student_adaptor(batch, model_outputs):\n    \n    \n\n    \n    layers, len, len, batch_size, heads = model_outputs['attention'].shape\n    attention = model_outputs['attention'].reshape(layers, batch_size, heads, len, len)\n\n    \n    len, batch_size, d_model = model_outputs['logits'].shape\n    logits = model_outputs['logits'].reshape(batch_size, len, d_model)\n    \n\n    \n    layers, len, batch_size, heads, embedding_per_head = model_outputs['values'].shape\n    values = model_outputs['values'].reshape(layers, batch_size, heads, len, embedding_per_head)\n\n    return {\n            'logits':model_outputs['pooler_output'],\n            'counts': model_outputs['counts'],\n            'attention': attention,\n            'inputs_mask': batch['attention_mask'],\n            'route_prob': model_outputs['route_prob'],\n            'n_dropped': model_outputs['n_dropped'],\n            'value_relation': values}\n\n\ndef predict(model, teacher_model, eval_dataset, step, device, STUDENT, BATCH_SIZE, eval_metric='cosine_similarity', feedback=True):\n    \n    model.eval()\n    student_logits = []\n    teacher_logits =[]\n    batch_counts = []\n    batch_n_dropped = []\n    batch_route_prob = []\n    \n    dataloader = DataLoader(eval_dataset,batch_size=BATCH_SIZE)\n    print('Running callback function on {} dev set samples...'.format(len(eval_dataset)))\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        with torch.no_grad():\n            model_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits_S = model_outputs['pooler_output']\n            logits_T = teacher_model(input_ids=input_ids, attention_mask=attention_mask)['pooler_output']\n            cpu_logits_S = logits_S.detach().cpu()\n            cpu_logits_T = logits_T.detach().cpu()\n            \n            if STUDENT=='switch' and feedback==True:\n                counts = model_outputs['counts'].detach().cpu()\n                n_dropped = model_outputs['n_dropped']\n                route_prob = model_outputs['route_prob'].detach().cpu()\n\n        for i in range(len(cpu_logits_S)):\n            student_logits.append(cpu_logits_S[i].numpy())\n            teacher_logits.append(cpu_logits_T[i].numpy())\n            \n        if STUDENT=='switch' and feedback==True:\n            for i in range(len(counts)):\n                batch_counts.append(counts[i].numpy())\n                batch_n_dropped.append(n_dropped[i])\n                batch_route_prob.append(route_prob[i].numpy())\n                \n    model.train()\n    student_logits = np.array(student_logits)\n    teacher_logits = np.array(teacher_logits)\n\n    if eval_metric=='cosine_similarity':\n        \n        similarities = np.diag(cosine_similarity(student_logits, teacher_logits))\n        print (\"Average cosine similarity for these samples: \", np.mean(similarities))\n    \n    if eval_metric=='mse':\n        mse_error = mean_squared_error(student_logits, teacher_logits)\n        print (\"Average mean squared error for these samples: \", mse_error)\n        \n    if STUDENT=='switch' and feedback==True:\n        switch_counts = np.array(batch_counts)\n        switch_n_dropped = np.array(batch_n_dropped)\n        switch_route_prob = np.array(batch_route_prob)\n        print('SWITCH BEHAVIOUR:')\n        print('Counts Shape: \\n', switch_counts.shape)\n        print('Counts: \\n', switch_counts)\n        print('N_dropped: \\n', switch_n_dropped)\n        print('Route Prob: \\n', switch_route_prob)\n\n    return torch.Tensor([np.mean(similarities)])\n\n\ndef generate_random_params(params):\n    \n    chosen_params = {}\n    for param in params:\n        chosen_params[param] = choice(params[param])\n    return chosen_params\n\ndef word_embedding_compression(word_embedding_module, d_model):\n    \n    \n    word_embedding_matrix = word_embedding_module.weight\n    assert word_embedding_matrix.shape[1]>=d_model, 'The desired word embedding dimensionality is greater than the teacher word embeddings. That is not compression! Make d_model smaller.'\n    \n    if word_embedding_matrix.shape[1]==d_model:\n        return word_embedding_module\n    \n    pca = PCA(n_components = d_model)\n    compressed_word_embedding_matrix = pca.fit_transform(word_embedding_matrix.detach().cpu().numpy())\n    compressed_word_embedding_matrix = torch.from_numpy(compressed_word_embedding_matrix)\n    word_embedding_module.weight = torch.nn.parameter.Parameter(compressed_word_embedding_matrix)\n    return word_embedding_module",
        "summary": "This code defines a series of functions and classes for training, evaluating, and adapting a neural network model, particularly focusing on a \"switch\" architecture that dynamically routes inputs to different sub-networks. Here's a breakdown of the key components:\n\n1. **Model Architecture**:\n   - The `LaMA` class represents the main model structure.\n   - It includes an encoder (`encoder`) and a decoder (`decoder`).\n   - The `SwitchDecoder` is a specialized decoder that uses dynamic routing to decide which sub-networks to use.\n\n2. **Dynamic Routing Mechanism**:\n   - The `SwitchDecoder` contains methods for dynamic routing, including `route_probabilities`, `select_subnetworks`, and `forward`.\n   - These methods handle the decision-making process based on input features and previous routing decisions.\n\n3. **Evaluation Metrics**:\n   - Functions like `predict` are used to evaluate the model's performance.\n   - It compares student logits (from the switch model) with teacher logits (from a reference model).\n   - Supported metrics include cosine similarity and mean squared error.\n\n4. **Parameter Compression**:\n   - The `word_embedding_compression` function reduces the dimensionality of word embeddings using PCA, which is useful for compressing models while maintaining performance.\n\n5. **Utility Functions**:\n   - `generate_random_params` allows generating random parameter settings from a given set of options.\n   - `switch_student_adaptor` and other adaptors are used to format model outputs in a way that can be easily evaluated.\n\n6. **Training Loop**:\n   - The code does not include an explicit training loop, but it sets up the environment for one by defining how to evaluate the model on a dataset.\n\n### Key Points:\n\n- **Dynamic Routing**: The switch architecture allows for dynamic routing of inputs to different sub-networks based on input features and previous routing decisions. This is crucial for tasks where different parts of the input might be more relevant to different parts of the output.\n  \n- **Evaluation**: The `predict` function provides a way to evaluate the model's performance by comparing student logits with teacher logits. It supports multiple metrics, making it flexible for different types of evaluation.\n\n- **Compression**: The `word_embedding_compression` function demonstrates how to reduce the dimensionality of word embeddings using PCA, which can be useful for compressing models while maintaining performance.\n\n### Usage:\n\nTo use this code, you would typically:\n1. Define your dataset and dataloader.\n2. Initialize your model (`LaMA`) with appropriate parameters.\n3. Train your model (not shown in the provided code).\n4. Evaluate the model using the `predict` function on a validation or test set.\n\nThis setup is particularly useful for tasks where dynamic routing can improve performance, such as machine translation or text summarization."
    },
    {
        "code": "import cv2\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('-p2', \"--path\", help=\"path to input video\")\nparser.add_argument('-p1', \"--spath\", help=\"path where the images should be stored\")\nparser.add_argument('-n', \"--num\", help=\"number from which image label naming starts\", type=int)\nargs = parser.parse_args()\nnum = args.num\ncap = cv2.VideoCapture(args.path)\ncount = 0\npath = args.spath\nprint(args.num, args.path, args.spath)\nret = True\nwhile ret:\n    ret, frame=cap.read()\n    count += 1\n    if count % 10 == 0:\n        cv2.imwrite(path+str(num)+'.jpg', frame)\n        print(path+str(num)+'.jpg')\n        num += 1\n    \n\n",
        "summary": "The Python script uses OpenCV to read frames from a video file specified by the user. It then saves every tenth frame as an image, starting with a label number provided by the user and incrementing for each subsequent image saved. The images are stored in a directory path also provided by the user."
    },
    {
        "code": "class Justifier:\n    def __init__(self, **kw):\n        super().__init__(**kw)\n        self.justs = [0] * 9\n        self.offsets = [(0, 0, 0, 1, 1, 1, 1, 1, 1),\n                        (0, -1, -2, 0, 0, 0, 1, 1, 1),\n                        (0, -1, -2, 0, -1, -2, 0, 0, 0)]\n\n    def init_justs(self, justs):\n        for i in justs:\n            i = i // 3\n            os = self.offsets[i]\n            if os:\n                self.justs = [sum(x) for x in zip(self.justs, os)]\n                self.offsets[i] = None\n\n    def calc_just(self, justs):\n        for i in justs:\n            i = self.justs[i] + (i % 3)\n            if i == 1:\n                return 'justify-content-center'\n            elif i > 1:\n                return 'justify-content-end'\n        return 'justify-content-start'\n",
        "summary": "The `Justifier` class initializes with predefined justification and offset values. It provides methods to update these values based on input justifications and calculate the final justification style as a string, which can be used for layout purposes such as centering or aligning content in a container."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport os\nimport time\n\nimport tensorflow as tf\nimport minitaur_logging_pb2\n\nNUM_MOTORS = 8\n\n\ndef _update_base_state(base_state, values):\n  base_state.x = values[0]\n  base_state.y = values[1]\n  base_state.z = values[2]\n\n\ndef preallocate_episode_proto(episode_proto, max_num_steps):\n  \n  for _ in range(max_num_steps):\n    step_log = episode_proto.state_action.add()\n    step_log.info_valid = False\n    step_log.time.seconds = 0\n    step_log.time.nanos = 0\n    for _ in range(NUM_MOTORS):\n      motor_state = step_log.motor_states.add()\n      motor_state.angle = 0\n      motor_state.velocity = 0\n      motor_state.torque = 0\n      motor_state.action = 0\n    _update_base_state(step_log.base_position, [0, 0, 0])\n    _update_base_state(step_log.base_orientation, [0, 0, 0])\n    _update_base_state(step_log.base_angular_vel, [0, 0, 0])\n\n\ndef update_episode_proto(episode_proto, minitaur, action, step):\n  \n  max_num_steps = len(episode_proto.state_action)\n  if step >= max_num_steps:\n    tf.logging.warning(\n        \"{}th step is not recorded in the logging since only {} steps were \"\n        \"pre-allocated.\".format(step, max_num_steps))\n    return\n  step_log = episode_proto.state_action[step]\n  step_log.info_valid = minitaur.IsObservationValid()\n  time_in_seconds = minitaur.GetTimeSinceReset()\n  step_log.time.seconds = int(time_in_seconds)\n  step_log.time.nanos = int((time_in_seconds - int(time_in_seconds)) * 1e9)\n\n  motor_angles = minitaur.GetMotorAngles()\n  motor_velocities = minitaur.GetMotorVelocities()\n  motor_torques = minitaur.GetMotorTorques()\n  for i in range(minitaur.num_motors):\n    step_log.motor_states[i].angle = motor_angles[i]\n    step_log.motor_states[i].velocity = motor_velocities[i]\n    step_log.motor_states[i].torque = motor_torques[i]\n    step_log.motor_states[i].action = action[i]\n\n  _update_base_state(step_log.base_position, minitaur.GetBasePosition())\n  _update_base_state(step_log.base_orientation, minitaur.GetBaseRollPitchYaw())\n  _update_base_state(step_log.base_angular_vel,\n                     minitaur.GetBaseRollPitchYawRate())\n\n\nclass MinitaurLogging(object):\n  \n\n  def __init__(self, log_path=None):\n    self._log_path = log_path\n\n  \n  def save_episode(self, episode_proto):\n    \n    if not self._log_path or not episode_proto.state_action:\n      return self._log_path\n    if not tf.gfile.Exists(self._log_path):\n      tf.gfile.MakeDirs(self._log_path)\n    ts = time.time()\n    time_stamp = datetime.datetime.fromtimestamp(ts).strftime(\n        \"%Y-%m-%d-%H:%M:%S\")\n    log_path = os.path.join(self._log_path,\n                            \"minitaur_log_{}\".format(time_stamp))\n    with tf.gfile.Open(log_path, \"w\") as f:\n      f.write(episode_proto.SerializeToString())\n    return log_path\n\n  def restore_episode(self, log_path):\n    \n    with tf.gfile.Open(log_path) as f:\n      content = f.read()\n      episode_proto = minitaur_logging_pb2.MinitaurEpisode()\n      episode_proto.ParseFromString(content)\n      return episode_proto\n",
        "summary": "The provided Python code defines a class `MinitaurLogging` for logging and restoring episodes of a Minitaur robot's state using TensorFlow. It includes methods to preallocate an episode proto with a specified number of steps, update the proto with current robot state data, and save or restore the logged episode data to and from disk."
    },
    {
        "code": "from unittest import TestCase\n\nimport six\n\nfrom popolo_data.importer import Popolo\n\n\nEXAMPLE_AREA = {\n    \"id\": \"area/tartu_linn\",\n    \"identifiers\": [\n        {\n            \"identifier\": \"Q3032626\",\n            \"scheme\": \"wikidata\"\n        }\n    ],\n    \"name\": \"Tartu linn\",\n    \"other_names\": [\n        {\n            \"lang\": \"fr\",\n            \"name\": \"Dixi\u00e8me circonscription l\u00e9gislative d'Estonie\",\n            \"note\": \"multilingual\"\n        },\n        {\n            \"lang\": \"et\",\n            \"name\": \"Valimisringkond nr 10\",\n            \"note\": \"multilingual\"\n        },\n        {\n            \"lang\": \"en\",\n            \"name\": \"Electoral District 10 (Tartu)\",\n            \"note\": \"multilingual\"\n        }\n    ],\n    \"type\": \"constituency\"\n}\n\n\nclass TestAreas(TestCase):\n\n    def test_empty_file_gives_no_areas(self):\n        popolo = Popolo({})\n        assert len(popolo.areas) == 0\n\n    def test_single_area_with_name(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        assert len(popolo.areas) == 1\n        area = popolo.areas[0]\n        assert area.name == 'Tartu linn'\n\n    def test_area_id(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        assert area.id == 'area/tartu_linn'\n\n    def test_area_type(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        assert area.type == 'constituency'\n\n    def test_area_identifiers(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        assert area.identifiers == [\n            {\n                \"identifier\": \"Q3032626\",\n                \"scheme\": \"wikidata\"\n            }\n        ]\n\n    def test_area_other_names(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        assert area.other_names == [\n            {\n                \"lang\": \"fr\",\n                \"name\": \"Dixi\u00e8me circonscription l\u00e9gislative d'Estonie\",\n                \"note\": \"multilingual\"\n            },\n            {\n                \"lang\": \"et\",\n                \"name\": \"Valimisringkond nr 10\",\n                \"note\": \"multilingual\"\n            },\n            {\n                \"lang\": \"en\",\n                \"name\": \"Electoral District 10 (Tartu)\",\n                \"note\": \"multilingual\"\n            }\n        ]\n\n    def test_area_wikidata(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        assert area.wikidata == 'Q3032626'\n\n    def test_area_repr(self):\n        popolo = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area = popolo.areas[0]\n        if six.PY2:\n            assert repr(area) == b\"<Area: Tartu linn>\"\n        else:\n            assert repr(area) == u\"<Area: Tartu linn>\"\n\n    def test_area_identity_equality_and_inequality(self):\n        popolo_a = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area_a = popolo_a.areas[0]\n        popolo_b = Popolo({\"areas\": [EXAMPLE_AREA]})\n        area_b = popolo_b.areas[0]\n        assert area_a == area_b\n        assert not (area_a != area_b)\n",
        "summary": "The provided Python code defines a test class `TestAreas` that inherits from `unittest.TestCase`. It tests the functionality of a `Popolo` class, specifically focusing on how it handles and processes area data. The tests cover various aspects such as empty files, single areas with names, identifiers, types, other names, and equality checks, ensuring that the `Popolo` class correctly imports and represents area information according to the Popolo format specifications."
    },
    {
        "code": "__author__ = 'alvertisjo'\n\nfrom django.core.serializers import json\nimport requests\nfrom requests.packages.urllib3 import Timeout\nfrom requests.packages.urllib3.exceptions import ConnectionError\n\nclass OpenProductData(object):\n\n    def getData(self):\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        pass\n    def readDataFromFile(self):\n        pass\n    def storeToGraph(self,data):\n        \n        \n        \n        url= 'http://snf-561492.vm.okeanos.grnet.gr:7474/'\n        \n        \n        \n        \n        \n        pass",
        "summary": "The provided Python code defines a class `OpenProductData` with methods to get data, read data from a file, and store data in a graph database using the Neo4j server at 'http://snf-561492.vm.okeanos.grnet.gr:7474/'. The actual implementation of these methods is left as placeholders indicated by `pass`."
    },
    {
        "code": "\ufeff\n\nimport argparse\nimport subprocess\nimport sys\nfrom aslc.Semantic import compileString\nfrom aslc.GLSLBackend import GLSLBackend\n\nBackend = GLSLBackend\n\n\nargParser = argparse.ArgumentParser(description=\"AbstractGPU Shading Language Compiler\")\nargParser.add_argument('inputFiles', metavar='input file', type=str, nargs='+')\nargParser.add_argument('-I', dest='includeDirectories', metavar='include directory', type=str, nargs='+', action='append')\nargParser.add_argument('-D', dest='macroDefinitions', metavar='macro definitions', type=str, nargs='+', action='append')\nargs = argParser.parse_args()\n\nincludeDirectories = args.includeDirectories\nmacroDefinitions = args.macroDefinitions\ninputFiles = args.inputFiles\noutputPath = '.'\n\n\ndef preprocessInput(inputFile):\n    with open(inputFile, 'r') as f:\n        return ('\n\n    args = ['cpp', '-o', '-', '-nostdinc', '-E']\n    if includeDirectories is not None:\n        for include in includeDirectories:\n            args.append('-I')\n            args.append(include[0])\n\n    if macroDefinitions is not None:\n        for macro in macroDefinitions:\n            args.append('-D')\n            args.append(macro[0])\n\n    args.append(inputFile)\n    \n    \n    proc = subprocess.Popen(args, stdout=subprocess.PIPE)\n    (stdout, stderr) = proc.communicate()\n    if proc.returncode != 0:\n        sys.exit(proc.returncode)\n    return stdout\n    \ndef processInputFile(inputFile):\n    preprocessed = preprocessInput(inputFile)\n    compiled = compileString(preprocessed)\n    return compiled\n    \n\nfor inputFile in inputFiles:\n    module = processInputFile(inputFile)\n    if module is None:\n        sys.exit(-1)\n    backend = Backend(module)\n    backend.generate(outputPath) \n    \n\n",
        "summary": "The Python script serves as a compiler for the AbstractGPU Shading Language (AGSL), utilizing the GLSLBackend to convert AGSL code into GLSL. It accepts input files, optional include directories, and macro definitions, preprocesses the input using `cpp`, compiles it with AGSL's semantic analysis, and then generates GLSL output in the specified or default directory."
    },
    {
        "code": "from django.views.generic import TemplateView\n\nclass HomePageView(TemplateView):\n    template_name = \"home.html\"\n\n\n\n",
        "summary": "The provided Python code defines a Django view class named `HomePageView` that inherits from `TemplateView`. This class is configured to render the content of the `home.html` template when accessed."
    },
    {
        "code": "from . import Simulations \nfrom . import Spacecraft \n",
        "summary": "The provided Python code imports modules named `Simulations` and `Spacecraft` from the current package, allowing access to their functionalities within the script."
    },
    {
        "code": "import os\nimport unittest\n\nfrom hhslib.security import *\n\nPASSPHRASE = '12345'\n\nSAMPLE_IN_FILE_NAME = \"resources/secret.in\"\nSAMPLE_OUT_FILE_NAME = \"resources/secret.out\"\n\nOUT_FILE = \"resources/outfile.out\"\nOUT_FILE_GPG = \"resources/outfile.out.gpg\"\n\nORIGINAL_FILE_CONTENTS = \"HomeSetup Secrets\"\nENCODED_FILE_CONTENTS = \"SG9tZVNldHVwIFNlY3JldHM=\"\n\n\nclass TestHhsLib(unittest.TestCase):\n\n    \n    def setUp(self):\n        with open(SAMPLE_IN_FILE_NAME, 'w') as f_in:\n            f_in.write(ORIGINAL_FILE_CONTENTS)\n        with open(SAMPLE_OUT_FILE_NAME, 'w') as f_in:\n            f_in.write(ENCODED_FILE_CONTENTS)\n    \n    \n    def tearDown(self):\n        if os.path.exists(OUT_FILE):\n            os.remove(OUT_FILE)\n        if os.path.exists(OUT_FILE_GPG):\n            os.remove(OUT_FILE_GPG)\n    \n    \n\n    \n    def test_should_encode_file(self):\n        with open(SAMPLE_IN_FILE_NAME, 'r') as f_in:\n            contents = str(f_in.read().strip())\n            self.assertEquals(ORIGINAL_FILE_CONTENTS, contents)\n        encode(SAMPLE_IN_FILE_NAME, OUT_FILE)\n        with open(OUT_FILE, 'r') as f_out:\n            contents = str(f_out.read().strip())\n            self.assertEquals(ENCODED_FILE_CONTENTS, contents)\n\n    \n    def test_should_decode_file(self):\n        with open(SAMPLE_OUT_FILE_NAME, 'r') as f_in:\n            contents = str(f_in.read().strip())\n            self.assertEquals(ENCODED_FILE_CONTENTS, contents)\n        decode(SAMPLE_OUT_FILE_NAME, OUT_FILE)\n        with open(OUT_FILE, 'r') as f_out:\n            contents = str(f_out.read().strip())\n            self.assertEquals(ORIGINAL_FILE_CONTENTS, contents)\n\n    \n    def test_should_encrypt_decrypt_file(self):\n        with open(SAMPLE_IN_FILE_NAME, 'r') as f_in:\n            contents = str(f_in.read().strip())\n            self.assertEquals(ORIGINAL_FILE_CONTENTS, contents)\n        encrypt(SAMPLE_IN_FILE_NAME, OUT_FILE_GPG, PASSPHRASE)\n        decrypt(OUT_FILE_GPG, OUT_FILE, PASSPHRASE)\n        with open(OUT_FILE, 'r') as f_out:\n            contents = str(f_out.read().strip())\n            self.assertEquals(ORIGINAL_FILE_CONTENTS, contents)\n\n\n\nif __name__ == '__main__':\n    suite = unittest.TestLoader().loadTestsFromTestCase(TestHhsLib)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n",
        "summary": "The provided Python code defines a test suite for a security library using the `unittest` framework. It includes tests to encode and decode files, as well as encrypt and decrypt files with a passphrase, ensuring that the original contents are preserved through these operations."
    },
    {
        "code": "import threading\nimport requests\n\n\nimport Encrypt\nimport unicodedata\n\nfrom ttk import Style, Button, Label, Entry, Progressbar, Checkbutton\nfrom Tkinter import Tk, Frame, RIGHT, BOTH, RAISED\nfrom Tkinter import TOP, X, N, LEFT\nfrom Tkinter import END, Listbox, MULTIPLE\nfrom Tkinter import Toplevel, DISABLED\n\nfrom Tkinter import StringVar, Scrollbar\nfrom multiprocessing import Queue\nfrom random import choice, randint\nfrom fbchat import log, client\nfrom fbchat.graphql import *\n\n\n\nclass GuiClient(client.Client):\n    def __init__(self, email, password, user_agent=None, max_tries=5, session_cookies=None, logging_level=logging.INFO):\n        \n\n        self.sticky, self.pool = (None, None)\n        self._session = requests.session()\n        self.req_counter = 1\n        self.seq = \"0\"\n        self.payloadDefault = {}\n        self.client = 'mercury'\n        self.default_thread_id = None\n        self.default_thread_type = None\n        self.req_url = ReqUrl()\n        self.most_recent_message = None\n        self.most_recent_messages_queue = Queue()\n\n        if not user_agent:\n            user_agent = choice(USER_AGENTS)\n\n        self._header = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n            'Referer': self.req_url.BASE,\n            'Origin': self.req_url.BASE,\n            'User-Agent': user_agent,\n            'Connection': 'keep-alive',\n        }\n\n        handler.setLevel(logging_level)\n\n        \n        if not session_cookies or not self.setSession(session_cookies) or not self.isLoggedIn():\n            self.login(email, password, max_tries)\n        else:\n            self.email = email\n            self.password = password\n\n    def onMessage(self, author_id, message_object, thread_id, thread_type, **kwargs):\n        self.markAsDelivered(author_id, thread_id)\n        self.markAsRead(author_id)\n        if (message_object is not None):\n            self.most_recent_message = message_object\n            self.most_recent_messages_queue.put(message_object)\n\n    def stopListening(self):\n        \n        print(\"Logging off...\")\n        self.listening = False\n        self.sticky, self.pool = (None, None)\n\n    def listen(self, markAlive=True):\n        \n        self.startListening()\n        self.onListening()\n\n        while self.listening and self.doOneListen(markAlive):\n            pass\n\n        self.stopListening()\n\n\nclass GUI(Frame):\n    \n\n    def __init__(self, parent, client):\n        self.queue = Queue()\n        \n        \n        self.email = \"\"\n        self.password = \"\"\n        self.name = \"\"\n        self.parent = parent\n        self.initialized = False\n        self.loadWindow = None\n        self.remember = False\n        self.client = None\n        self.msg_list = None\n        self.changingConvo = False\n        self.loginScreen()\n\n    def centerWindow(self, notself=None):\n        \n\n        if notself is not None:  \n            sw = self.parent.winfo_screenwidth()\n            sh = self.parent.winfo_screenheight()\n            x = (sw - self.w / 2) / 2\n            y = (sh - self.h / 2) / 2\n            notself.geometry('%dx%d+%d+%d' % (self.w / 1.8, self.h / 1.8, x, y))\n        else:\n            sw = self.parent.winfo_screenwidth()\n            sh = self.parent.winfo_screenheight()\n            x = (sw - self.w) / 2\n            y = (sh - self.h) / 2\n            self.parent.geometry('%dx%d+%d+%d' % (self.w, self.h, x, y))\n\n    def startWindow(self):\n        \n        Frame.__init__(self, self.parent, background=\"white\")\n        self.style = Style()\n        self.style.theme_use(\"default\")\n        self.pack(fill=BOTH, expand=1)\n        if (not self.initialized):\n            self.centerWindow()\n        else:\n            self.parent.geometry('%dx%d' % (self.w, self.h))\n        self.initialized = True\n\n    def resetWindow(self):\n        \n        if (self.initialized):\n            self.destroy()\n        if (self.loadWindow is not None):\n            self.loadWindow.destroy()\n\n        self.startWindow()\n\n    def loginScreen(self):\n        \n\n        \n        self.h = 150\n        self.w = 350\n        self.resetWindow()\n        self.parent.title(\"Welcome\")\n\n        \n        emailFrame = Frame(self)\n        emailFrame.pack(fill=X, side=TOP)\n\n        emailLabel = Label(emailFrame, text=\"Email:\", background=\"white\")\n        emailLabel.pack(side=LEFT, padx=15, pady=10)\n\n        self.emailEntry = Entry(emailFrame, width=30)\n        self.emailEntry.insert(0, self.email)\n        self.emailEntry.pack(side=LEFT, padx=35, pady=10)\n        \n\n        \n        passwordFrame = Frame(self)\n        passwordFrame.pack(fill=X, side=TOP)\n\n        passwordLabel = Label(passwordFrame, text=\"Password:\", background=\"white\")\n        passwordLabel.pack(side=LEFT, padx=15, pady=10)\n\n        self.passwordEntry = Entry(passwordFrame, show=\"*\", width=30)\n        self.passwordEntry.bind(\"<Return>\", self.start)\n        self.passwordEntry.insert(0, self.password)\n        self.passwordEntry.pack(side=LEFT, padx=35, pady=10)\n        \n\n        \n        frame = Frame(self, borderwidth=1)\n        frame.pack(fill=BOTH, expand=True)\n        self.pack(fill=BOTH, expand=True)\n\n        exitButton = Button(self, text=\"Exit\", command=self.parent.destroy)\n        exitButton.pack(side=RIGHT, padx=5, pady=5)\n        self.loginButton = Button(self, text=\"Log In\", command=self.start)\n        self.loginButton.pack(side=RIGHT)\n        \n\n    def start(self, opt=\"\"):\n        \n        thread1 = ThreadedTask(self.queue, self.login)\n        thread2 = ThreadedTask(self.queue, self.loadingScreen)\n        thread2.start()\n        thread1.start()\n\n        self.checkThread(thread1, self.chatUI)\n\n    def loadingScreen(self):\n        \n        for i in self.winfo_children():\n            if Button == type(i):\n                i.configure(state=DISABLED)\n\n        self.loadWindow = Toplevel(self.parent)\n        loadingstring = \"Logging in...\"\n        loadinglabel = Label(self.loadWindow, text=loadingstring, background=\"white\")\n        progressbar = Progressbar(self.loadWindow, orient=\"horizontal\",\n                                  length=300, mode=\"indeterminate\")\n        progressbar.pack(pady=self.h / 10)\n\n        loadinglabel.pack()\n\n        self.centerWindow(self.loadWindow)\n        self.loadWindow.title(\"Wait\")\n        progressbar.start()\n\n    def login(self):\n        \n\n        if(self.client is not None):\n            if(self.client.isLoggedIn()):\n                self.client.logout()\n        self.email = self.emailEntry.get()\n        self.password = self.passwordEntry.get()\n\n        \n        self.client = GuiClient(self.email, self.password)\n        print(self.client._fetchInfo(self.client.uid)[self.client.uid].get('first_name'))\n        self.thread3 = ThreadedTask(self.queue, self.listen)\n        self.thread3.start()\n\n    def listen(self):\n        \n        self.client.listen()\n\n    def chatUI(self):\n        \n        self.h = 350\n        self.w = 700\n        self.resetWindow()\n        self.parent.title(\"Messenger\")\n\n        \n        self.right_frame = Frame(self)\n        self.right_frame.pack(side=RIGHT, fill='y')\n        self.messages_frame = Frame(self.right_frame)\n        self.messages_frame.pack(side=TOP)\n\n        self.my_msg = StringVar()  \n        self.my_msg.set(\"\")\n\n        self.msg_scrollbar = Scrollbar(self.messages_frame)  \n\n        \n\n        self.msg_list = Listbox(self.messages_frame, height=15, width=50, yscrollcommand=self.msg_scrollbar.set)\n        self.msg_scrollbar.config(command=self.msg_list.yview)\n\n        self.msg_scrollbar.pack(side=RIGHT, fill='y', padx=5)\n        self.msg_list.pack(side=RIGHT)\n\n        self.entry_field = Entry(self.right_frame, textvariable=self.my_msg)\n        self.entry_field.bind(\"<Return>\", self.send)\n        self.send_button = Button(self.right_frame, text=\"Send\", command=self.send)\n        self.entry_field.pack(side=\"top\", fill=X, padx=5, pady=5)\n        self.send_button.pack(side=\"top\")\n\n        self.exitButton = Button(self.right_frame, text=\"Exit\", command=self.exit)\n        self.exitButton.pack(side=\"bottom\", padx=5, pady=5)\n\n        \n        self.left_frame = Frame(self)\n        self.left_frame.pack(side=LEFT, fill='y')\n\n        self.usr_scrollbar = Scrollbar(self.left_frame)\n        self.usr_list = Listbox(self.left_frame, height=15, width=50, yscrollcommand=self.usr_scrollbar.set)\n        self.usr_scrollbar.config(command=self.usr_list.yview)\n\n        self.usr_search_bar = Entry(self.left_frame, textvariable=\"\")\n        self.usr_search_button = Button(self.left_frame, text=\"Search\", command=self.search)\n\n        self.usr_search_bar.pack(side=\"top\", fill=X, pady=2, padx=1)\n        self.usr_search_button.pack(side=\"top\", fill=X, pady=2, padx=1)\n\n        self.usr_scrollbar.pack(side=RIGHT, fill='y', padx=5)\n        self.usr_list.pack(side=RIGHT, fill='y')\n\n        \n        self.search()\n\n        self.usr_list.bind('<Double-1>', self.changeConvo)\n\n    def search(self):\n        fresh_users = self.client.fetchAllUsers()\n        self.users = []\n\n        if (self.usr_search_bar.get() is not \"\"):\n            for user in fresh_users:\n                if (self.usr_search_bar.get() in user.name):\n                    self.users.append(user)\n        else:\n            self.users = fresh_users\n\n        if (self.usr_list.size() is not 0):\n            self.usr_list.delete(0, END)\n\n        for user in self.users:\n            self.usr_list.insert(END, \" \" + user.name)\n\n        \n        self.currentUser = self.users[0]  \n        self.usr_search_bar.delete(0, END)\n\n    def send(self, _=\"\"):\n        \n        plaintext = self.entry_field.get()\n        key = randint(-60, 60)\n        ciphertext = Encrypt.encrypt(plaintext, key)\n        ciphertext = \"{}Q_Q{}\".format(key, ciphertext)\n        message = Message(text=unicode(ciphertext, \"ascii\"))\n        self.client.send(message, self.currentUser.uid)\n        self.entry_field.delete(0, END)\n        self.client.most_recent_message = message\n        self.msg_list.insert(0, self.name + \": \" + plaintext)\n        self.msg_list.see(END)\n\n    def changeConvo(self, param):\n        \n        print(\"CHANGING CONVO\")\n        selectionIndex = self.usr_list.curselection()\n        self.currentUser = self.users[selectionIndex[0]]\n        self.changingConvo = True\n        self.updateConversation()\n\n    def updateConversation(self):\n        \n        if (self.changingConvo): \n            print(\"[updateConversation] we are changing conversation\")\n            messages = self.client.fetchThreadMessages(self.currentUser.uid)\n            self.msg_list.delete(0, END)\n            for message in messages:\n                text = self.decrypt_w_uc(message)\n                self.msg_list.insert(0, self.client._fetchInfo(message.author)[message.author][\n                    \"first_name\"] + \": \" + text)\n            \n            self.msg_list.see(END)\n            \n            self.changingConvo = False\n        else: \n            \n            last_message = self.msg_list.get(END)\n            if (self.client is not None and self.client.isLoggedIn() and self.client.most_recent_message is not None):\n                msg_object = self.client.most_recent_message\n                msg_author = self.client.most_recent_message.author\n                name = \"\"\n                if (msg_author is None):\n                    msg_author = self.name\n                else:\n                    name = self.client._fetchInfo(msg_author)[msg_author][\"first_name\"]\n                text = self.decrypt_w_uc(msg_object)\n                new_last_message = name + \": \" + text\n                if (last_message != new_last_message):\n                    \n                    if (name + \": \" in last_message):\n                        while (self.client.most_recent_messages_queue.empty() is not True):\n                            message = self.client.most_recent_messages_queue.get()\n                            text = self.decrypt_w_uc(message)\n                            self.msg_list.insert(END, self.client._fetchInfo(message.author)[message.author][\n                                \"first_name\"] + \": \" + text)\n                            self.msg_list.see(END)\n                    else:\n                        messages = self.client.fetchThreadMessages(self.currentUser.uid)\n                        self.msg_list.delete(0, END)\n                        for message in messages:\n                            text = self.decrypt_w_uc(message)\n                            self.msg_list.insert(0, self.client._fetchInfo(message.author)[message.author][\n                                \"first_name\"] + \": \" + text)\n                        self.msg_list.see(END)\n                        self.client.most_recent_message = messages[0]\n\n    def decrypt_w_uc(self, message):\n        \n        clean_text = \"\"\n        if \"Q_Q\" in message.text:  \n            key, ciphertext = message.text.split(\"Q_Q\")\n            clean_text = Encrypt.decrypt(ciphertext, int(key))\n        else:\n            clean_text = message.text\n        \n        clean_clean_text = \"\"\n        for character in clean_text:\n            \n            if type(character) is unicode:\n                clean_clean_text += unicodedata.normalize('NFKD', character).encode('ascii', 'replace')\n            else:\n                clean_clean_text += character\n\n        return clean_clean_text\n\n    def exit(self):\n        \n        self.client.stopListening()\n        self.parent.destroy()\n\n    def checkThread(self, thread, function):\n\n        \n        if thread.is_alive():\n            self.parent.after(1000, lambda: self.checkThread(thread, function))\n        else:\n            function()\n\n\nclass ThreadedTask(threading.Thread):\n    \n\n    def __init__(self, queue, function):\n        \n        threading.Thread.__init__(self)\n        self.queue = queue\n        self.function = function\n\n    def run(self):\n        \n        self.function()\n\n\ndef tk_loop(root, ex):\n    \n    if (ex.msg_list is not None):\n        ex.updateConversation()\n    root.after(2000, tk_loop, root, ex)\n\n\ndef initiate_tk_loop(root, ex):\n    \n    root.after(2000, tk_loop, root, ex)\n\n\ndef removeEmoji(msg):\n    \n    new_msg = \"\"\n    for ch in msg:\n        pass\n\n    return new_msg\n\nif __name__ == \"__main__\":\n\n    \n    root = Tk()\n    root.resizable(width=False, height=False)\n    ex = GUI(root, client)\n\n    \n    initiate_tk_loop(root, ex)\n    root.mainloop()",
        "summary": "The provided code is a Python script that creates a graphical user interface (GUI) for a messaging application using the Tkinter library. The script includes several classes and functions to handle various aspects of the application, such as user authentication, message encryption/decryption, and real-time message updates.\n\nHere's a breakdown of the key components:\n\n1. **GUI Class**: This class is responsible for creating the main window and handling user interactions. It initializes the GUI elements (like entry fields, buttons, and listboxes) and sets up event handlers for user actions such as sending messages and changing conversations.\n\n2. **ThreadedTask Class**: This class extends `threading.Thread` and is used to run background tasks in separate threads. The `run` method of this class calls the specified function when the thread starts.\n\n3. **tk_loop Function**: This function is a recursive loop that updates the conversation list every 2 seconds. It checks if there are new messages and updates the GUI accordingly.\n\n4. **initiate_tk_loop Function**: This function sets up the initial call to `tk_loop` using `root.after`, which schedules the function to be called after a specified delay (in this case, 2000 milliseconds).\n\n5. **removeEmoji Function**: This function is intended to remove emojis from messages but currently does not implement any logic.\n\n6. **Main Execution Block**: The script creates an instance of the `GUI` class and starts the Tkinter event loop using `root.mainloop()`.\n\n### Key Features:\n\n- **User Authentication**: The GUI prompts the user for their name and password, which are then used to authenticate with a server (not shown in the provided code).\n\n- **Message Encryption/Decryption**: Messages are encrypted before being sent and decrypted upon receipt. The encryption uses a simple key-based method.\n\n- **Real-Time Updates**: The application updates the conversation list in real-time as new messages arrive, using a background thread to handle message fetching.\n\n### Potential Improvements:\n\n1. **Error Handling**: Add error handling for network operations and user input validation.\n2. **Security**: Improve the encryption method to use more secure algorithms like AES.\n3. **User Interface**: Enhance the GUI with better styling and layout.\n4. **Testing**: Write unit tests for the message encryption/decryption functions.\n\n### Example Usage:\n\nTo run this script, you would need to have Python installed on your system. Ensure that the `Tkinter` library is available (it usually comes pre-installed with Python). You can then execute the script from the command line or an IDE.\n\n```sh\npython script_name.py\n```\n\nThis will open a window where users can enter their name and password, send messages, and view conversations in real-time."
    },
    {
        "code": "from pipdeptree import get_installed_distributions, build_dist_index, construct_tree\nfrom bs4 import BeautifulSoup\nfrom json import dump, load\nfrom urllib.request import urlretrieve\nfrom pathlib import Path\nfrom unittest import mock\nfrom pkginfo import SDist\nfrom johnnydep.cli import JohnnyDist\n\nimport requests\nimport setuptools\nimport tarfile\n\n\ndef read_packages():\n    with open(\"python_packages_list.json\", \"r\") as f:\n        package_info = load(f)\n\n    return package_info\n\n\ndef download(download_link, output_folder, package_name, version):\n    url = download_link\n    dst = Path(output_folder).joinpath(\"{}_{}.tar.gz\".format(package_name, version))\n    urlretrieve(url, dst)\n\n\ndef get_packages(package_name):\n    \n    url = \"https://pypi.org{}\".format(package_name)\n    r = requests.get(url)\n    soup = BeautifulSoup(r.content, features='html.parser')\n    tar = 0\n\n    for id, link in enumerate(soup.find_all('a', href=True)):\n        if \".tar.gz\" in link[\"href\"]:\n            download(link[\"href\"], \"downloaded_packages/tar\", package_name.split(\"/\")[-2], id)\n            tar += 1\n\n    return {\"tar\": tar}\n\n\ndef extract_info_from_setup():\n    with mock.patch.object(setuptools, 'setup') as mock_setup:\n        import data_collector.downloaded_packages.setup\n\n    args, kwargs = mock_setup.call_args\n    print(kwargs)\n\n\ndef unpack(package_name):\n    with tarfile.open(package_name, mode=\"r:gz\") as tf:\n        tf.extractall()\n\n\ndef parse(pkg_info):\n    mypackage = SDist(pkg_info)\n\n    return PackageInfo(version=mypackage.version, author=mypackage.author_email,\n                       license=mypackage.license,\n                       name=mypackage.name,\n                       maintainer=mypackage.maintainer_email, additional_details=mypackage.__dict__)\n\n\ndef get_dependencies(package):\n    url = 'https://pypi.org/pypi/{}/json'\n    json = requests.get(url.format(package)).json()\n    print(json.keys())\n    \n\n\ndef get_johnny_dep(package):\n    dist = JohnnyDist(package, index_url=None, env=None, extra_index_url=None)\n    return dist.serialise(fields=[\"name\", \"requires\", \"required_by\", \"project_name\", \"versions_available\"], format=None, recurse=True)\n\n\nif __name__ == '__main__':\n    \n    \n    \n    \n    \n    \n    print(get_dependencies(\"pandas\"))\n    \n\n",
        "summary": "The provided Python script includes functions for downloading, extracting, and parsing package information from PyPI using various libraries such as `pipdeptree`, `BeautifulSoup`, `requests`, and `setuptools`. It demonstrates how to interact with the PyPI API to retrieve package details and dependencies."
    },
    {
        "code": "import sys\n\nif len(sys.argv) != 2:\n\tsys.stderr.write('Usage: python3.X %s sy_fraction\\n' % sys.argv[0])\n\traise SystemExit(1)\n\nsy_fraction = sys.argv[1]\n\nparameters = []\nwith open('input_data/infection_parameters.txt','r') as fin:\n\tparameters = fin.readlines()\n\nfor ind,param in enumerate(parameters):\n\tif 'average fraction to get tested' in param:\n\t\tparameters[ind+1] = str(sy_fraction) + '\\n'\n\t\tbreak\n\nwith open('input_data/infection_parameters.txt','w') as fout:\n\tfout.writelines(parameters)\n",
        "summary": "The Python script checks if the correct number of command-line arguments is provided, then reads a file named 'infection_parameters.txt' to find and replace a specific line containing 'average fraction to get tested' with the value passed as an argument. It writes the modified content back to the same file."
    },
    {
        "code": "def is_rhyme(word1, word2, k):\n    \n    if (k == 0):        \n        return False\n\n    if (len(word1) < k or len(word2) < k):      \n        return False                            \n\n    rev_word1 = word1[::-1]     \n    rev_word2 = word2[::-1]     \n\n    \n    \n    return rev_word1[:k] == rev_word2[:k]\n\n\n\n\n\n\n\n\n\n",
        "summary": "The function `is_rhyme` checks if the first `k` characters of two input words are the same when both words are reversed. It returns `True` if they match and `False` otherwise, provided that `k` is not zero and both words have at least `k` characters."
    },
    {
        "code": "import sys\n\nif len(sys.argv) != 2:\n    print(\"Usage: python3 orgmybmarks.py bookmarks.html\")\n    quit()\nfile = open(sys.argv[1])\n\nfileout = open(\"out.html\", \"w\")\nhreflist = []\n\nnumm = 1\nwhile True:\n    line = file.readline()\n    if not line:\n        break\n    if line.find(\"HREF\") == -1:\n        continue\n    num = line.find(\"HREF\")\n    href = line[num + 6:]\n    num = href.find(\"\\\"\")\n    href = href[:num]\n    hreflist.append(href)\n    print(\"%d now %s\" % (numm, href))\n    numm += 1\nnumbef = len(hreflist)\nhreflist = list(set(hreflist))  \nnumaft = len(hreflist)\nfir = \nfileout.write(fir)\nfor i in range(len(hreflist)):\n    sec = \"        <DT><A HREF=\\\"%s\\\">%d</A>\\n\" % (hreflist[i], i)\n    fileout.write(sec)\nend = \nfileout.write(end)\nfile.close()\nfileout.close()\nprint(\"finished! now you have %d bookmarks, %d duplicated bookmarks deleted!\" % (numaft, numbef - numaft))\n",
        "summary": "The Python script reads a bookmarks.html file provided as a command-line argument, extracts unique HREF links from it, and writes them to a new out.html file in a structured format. It also counts the number of unique and duplicate bookmarks found during the process."
    },
    {
        "code": "import main\n\ndef test_execute():\n    count = main.results\n    assert count == 400410",
        "summary": "The provided Python code imports a module named `main` and defines a function `test_execute` that asserts the value of `results` from the `main` module to be equal to 400410."
    },
    {
        "code": "import socket\nimport time\n\nfrom xmlrpclib_to import ServerProxy\nimport httpretty\nimport pytest\n\n\nXML_RESPONSE = \n\n\ndef timeout(request, url, headers):\n        time.sleep(1)\n        return 200, headers, XML_RESPONSE\n\n\n@httpretty.activate\ndef test_timeout():\n\n    httpretty.register_uri(\n        httpretty.POST,\n        'http://example.com/RPC2',\n        content_type='text/xml',\n        body=timeout\n    )\n    proxy = ServerProxy('http://example.com', timeout=0.5)\n    with pytest.raises(socket.timeout):\n        proxy.test()\n\n\n@httpretty.activate\ndef test_timeout_https():\n    httpretty.register_uri(\n        httpretty.POST,\n        'https://example.com/RPC2',\n        content_type='text/xml',\n        body=timeout\n    )\n\n    proxy = ServerProxy('https://example.com', timeout=0.5)\n    with pytest.raises(socket.timeout):\n        proxy.test()\n\n\nif __name__ == \"__main__\":\n    test_timeout()\n    test_timeout_https()\n\n",
        "summary": "The Python code sets up tests for a server proxy using the `xmlrpclib_to` library and `httpretty` to simulate HTTP responses. It defines two test functions, `test_timeout` and `test_timeout_https`, which use `pytest.raises` to verify that a socket timeout exception is raised when attempting to connect to 'http://example.com' and 'https://example.com' with a timeout of 0.5 seconds, where the server response simulates a delay of 1 second before returning a 200 status code."
    },
    {
        "code": "from django.db.models.signals import m2m_changed\r\nfrom django.dispatch import receiver\r\nfrom .models import Image\r\n\r\n\r\n@receiver(m2m_changed, sender=Image.users_like.through)\r\ndef users_like_changed(sender, instance, **kwargs):\r\n    instance.total_likes = instance.users_like.count()\r\n    instance.save()\r\n",
        "summary": "The provided Python code defines a signal receiver that updates the `total_likes` attribute of an `Image` instance whenever there is a change in its many-to-many relationship with the `users_like` field. This ensures that the `total_likes` count reflects the current number of users who have liked the image, and it saves the updated instance to the database."
    },
    {
        "code": "{\n\t\"targets\": [\n\t\t{\n\t\t\t\"target_name\": \"binding\",\n\t\t\t\"win_delay_load_hook\": \"true\",\n\t\t\t\"conditions\": [\n\t\t\t\t[\"target_arch == 'x64' or target_arch == 'ia32'\", {\n\t\t\t\t\t\"sources\": [\n\t\t\t\t\t\t\"src/binding.cpp\",\n\t\t\t\t\t\t\"src/BLAKE2/sse/blake2b.c\",\n\t\t\t\t\t\t\"src/BLAKE2/sse/blake2bp.c\",\n\t\t\t\t\t\t\"src/BLAKE2/sse/blake2s.c\",\n\t\t\t\t\t\t\"src/BLAKE2/sse/blake2sp.c\"\n\t\t\t\t\t],\n\t\t\t\t\t\"include_dirs\": [\n\t\t\t\t\t\t\"<!(node -e \\\"require('nan')\\\")\",\n\t\t\t\t\t\t\"src/BLAKE2/sse\"\n\t\t\t\t\t]\n\t\t\t\t}],\n\t\t\t\t[\"target_arch == 'arm64'\", {\n\t\t\t\t\t\"sources\": [\n\t\t\t\t\t\t\"src/binding.cpp\",\n\t\t\t\t\t\t\"src/BLAKE2/neon/blake2b-neon.c\",\n\t\t\t\t\t\t\"src/BLAKE2/neon/blake2bp.c\",\n\t\t\t\t\t\t\"src/BLAKE2/neon/blake2s-neon.c\",\n\t\t\t\t\t\t\"src/BLAKE2/neon/blake2sp.c\"\n\t\t\t\t\t],\n\t\t\t\t\t\"include_dirs\": [\n\t\t\t\t\t\t\"<!(node -e \\\"require('nan')\\\")\",\n\t\t\t\t\t\t\"src/BLAKE2/neon\"\n\t\t\t\t\t]\n\t\t\t\t}],\n\t\t\t\t[\"target_arch != 'x64' and target_arch != 'ia32' and target_arch != 'arm64'\", {\n\t\t\t\t\t\"sources\": [\n\t\t\t\t\t\t\"src/binding.cpp\",\n\t\t\t\t\t\t\"src/BLAKE2/ref/blake2b-ref.c\",\n\t\t\t\t\t\t\"src/BLAKE2/ref/blake2bp-ref.c\",\n\t\t\t\t\t\t\"src/BLAKE2/ref/blake2s-ref.c\",\n\t\t\t\t\t\t\"src/BLAKE2/ref/blake2sp-ref.c\"\n\t\t\t\t\t],\n\t\t\t\t\t\"include_dirs\": [\n\t\t\t\t\t\t\"<!(node -e \\\"require('nan')\\\")\",\n\t\t\t\t\t\t\"src/BLAKE2/ref\"\n\t\t\t\t\t]\n\t\t\t\t}]\n\t\t\t],\n\t\t\t\"cflags_c\": [\n\t\t\t\t\"-std=c99\",\n\t\t\t\t\"-Wstrict-aliasing\",\n\t\t\t\t\"-Wextra\",\n\t\t\t\t\"-Wno-unused-function\",\n\t\t\t\t\"-Wno-unused-const-variable\"\n\t\t\t],\n\t\t\t\"cflags_cc\": [\n\t\t\t\t\"-Wstrict-aliasing\",\n\t\t\t\t\"-Wextra\",\n\t\t\t\t\"-Wno-unused-function\",\n\t\t\t\t\"-Wno-unused-const-variable\",\n\t\t\t\t\"-Wno-unused-parameter\"\n\t\t\t],\n\t\t\t'xcode_settings': {\n\t\t\t\t'OTHER_CFLAGS': [\n\t\t\t\t\t\"-Wstrict-aliasing\",\n\t\t\t\t\t\"-Wextra\",\n\t\t\t\t\t\"-Wno-unused-function\",\n\t\t\t\t\t\"-Wno-unused-const-variable\",\n\t\t\t\t\t\"-Wno-unused-parameter\"\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"msvs_settings\": {\n\t\t\t\t\"VCCLCompilerTool\": {\n\t\t\t\t\t\"AdditionalOptions\": [\"/arch:AVX\"]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t]\n}\n",
        "summary": "This Python code defines a target named \"binding\" for building a C++ module with conditional compilation based on the target architecture. It includes different source files and include directories for x64, ia32, arm64, and other architectures, along with specific compiler flags for each platform."
    },
    {
        "code": "import numpy as np\nimport pandas as pd\nimport pytest\nfrom pandas.testing import assert_frame_equal\n\n\n@pytest.fixture\ndef process_test_df():\n    \"Base DataFrame\"\n    return pd.DataFrame(\n        {\"text\": [\"a_b_c\", \"c_d_e\", np.nan, \"f_g_h\"], \"numbers\": range(1, 5)}\n    )\n\n\n@pytest.fixture\ndef test_returns_dataframe():\n    \"Base DataFrame\"\n    return pd.DataFrame(\n        {\"text\": [\"a1a2\", \"b1\", \"c1\"], \"numbers\": [1, 2, 3]},\n        index=[\"A\", \"B\", \"C\"],\n    )\n\n\ndef test_column_name_type(process_test_df):\n    \n    with pytest.raises(TypeError):\n        process_test_df.process_text([\"text\"])\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_new_column_names_type(process_test_df):\n    \n    with pytest.raises(TypeError):\n        process_test_df.process_text(\n            column_name=\"text\", new_column_names={\"nutext\": \"rar\"}\n        )\n\n\ndef test_column_name_presence(process_test_df):\n    \n    with pytest.raises(ValueError):\n        process_test_df.process_text(\n            column_name=\"Test\", string_function=\"lower\"\n        )\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_new_column_names_presence_str(test_returns_dataframe):\n    \n    with pytest.raises(ValueError):\n        test_returns_dataframe.process_text(\n            column_name=\"text\",\n            new_column_names=\"text\",\n            string_function=\"extractall\",\n            pat=r\"([ab])?(\\d)\",\n        )\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_new_column_names_presence_list(test_returns_dataframe):\n    \n    with pytest.raises(ValueError):\n        test_returns_dataframe.process_text(\n            column_name=\"text\",\n            new_column_names=[\"numbers\", \"newtext\"],\n            string_function=\"extractall\",\n            pat=r\"([ab])?(\\d)\",\n        )\n\n\n@pytest.mark.xfail(reason=\"merge_frame is deprecated.\")\ndef test_merge_frame_type(test_returns_dataframe):\n    \n    with pytest.raises(TypeError):\n        test_returns_dataframe.process_text(\n            column_name=\"text\",\n            new_column_names=[\"number\", \"newtext\"],\n            string_function=\"extractall\",\n            pat=r\"([ab])?(\\d)\",\n            merge_frame=\"True\",\n        )\n\n\n@pytest.mark.xfail(reason=\"string_function must be present.\")\ndef test_string_function_is_None(process_test_df):\n    \n    result = process_test_df.process_text(column_name=\"text\")\n    assert_frame_equal(result, process_test_df)\n\n\ndef test_str_split(process_test_df):\n    \n\n    expected = process_test_df.assign(\n        text=process_test_df[\"text\"].str.split(\"_\")\n    )\n\n    result = process_test_df.process_text(\n        column_name=\"text\", string_function=\"split\", pat=\"_\"\n    )\n\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_new_column_names(process_test_df):\n    \n    result = process_test_df.process_text(\n        column_name=\"text\",\n        new_column_names=\"new_text\",\n        string_function=\"slice\",\n        start=2,\n    )\n    expected = process_test_df.assign(\n        new_text=process_test_df[\"text\"].str.slice(start=2)\n    )\n    assert_frame_equal(result, expected)\n\n\n@pytest.fixture\ndef no_nulls_df():\n    return pd.DataFrame({\"text\": [\"a\", \"b\", \"c\", \"d\"], \"numbers\": range(1, 5)})\n\n\ndef test_str_cat(no_nulls_df):\n    \n\n    result = no_nulls_df.process_text(\n        column_name=\"text\",\n        string_function=\"cat\",\n        others=[\"A\", \"B\", \"C\", \"D\"],\n    )\n\n    expected = no_nulls_df.assign(\n        text=no_nulls_df[\"text\"].str.cat(others=[\"A\", \"B\", \"C\", \"D\"])\n    )\n\n    assert_frame_equal(result, expected)\n\n\ndef test_str_cat_result_is_a_string(no_nulls_df):\n    \n\n    result = no_nulls_df.process_text(\n        column_name=\"text\",\n        string_function=\"cat\",\n    )\n\n    expected = no_nulls_df.assign(text=no_nulls_df[\"text\"].str.cat())\n\n    assert_frame_equal(result, expected)\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_str_cat_result_is_a_string_and_new_column_names(no_nulls_df):\n    \n\n    result = no_nulls_df.process_text(\n        column_name=\"text\", string_function=\"cat\", new_column_names=\"combined\"\n    )\n\n    expected = no_nulls_df.assign(combined=no_nulls_df[\"text\"].str.cat())\n\n    assert_frame_equal(result, expected)\n\n\ndef test_str_get():\n    \n\n    df = pd.DataFrame(\n        {\"text\": [\"aA\", \"bB\", \"cC\", \"dD\"], \"numbers\": range(1, 5)}\n    )\n\n    expected = df.assign(text=df[\"text\"].str.get(1))\n\n    result = df.process_text(column_name=\"text\", string_function=\"get\", i=-1)\n\n    assert_frame_equal(result, expected)\n\n\ndef test_str_lower():\n    \n\n    df = pd.DataFrame(\n        {\n            \"codes\": range(1, 7),\n            \"names\": [\n                \"Graham Chapman\",\n                \"John Cleese\",\n                \"Terry Gilliam\",\n                \"Eric Idle\",\n                \"Terry Jones\",\n                \"Michael Palin\",\n            ],\n        }\n    )\n\n    expected = df.assign(names=df[\"names\"].str.lower())\n\n    result = df.process_text(column_name=\"names\", string_function=\"lower\")\n\n    assert_frame_equal(result, expected)\n\n\ndef test_str_wrong(process_test_df):\n    \n    with pytest.raises(KeyError):\n        process_test_df.process_text(\n            column_name=\"text\", string_function=\"invalid_function\"\n        )\n\n\ndef test_str_wrong_parameters(process_test_df):\n    \n    with pytest.raises(TypeError):\n        process_test_df.process_text(\n            column_name=\"text\", string_function=\"split\", pattern=\"_\"\n        )\n\n\n@pytest.fixture\ndef returns_frame_1():\n    return pd.DataFrame(\n        {\n            \"ticker\": [\n                \"spx 5/25/2001 p500\",\n                \"spx 5/25/2001 p600\",\n                \"spx 5/25/2001 p700\",\n            ]\n        }\n    )\n\n\n@pytest.mark.xfail(reason=\"merge_frame is deprecated.\")\ndef test_return_dataframe_merge_is_None(returns_frame_1):\n    \n\n    expected_output = returns_frame_1[\"ticker\"].str.split(\" \", expand=True)\n    result = returns_frame_1.process_text(\n        column_name=\"ticker\", string_function=\"split\", expand=True, pat=\" \"\n    )\n    assert_frame_equal(result, expected_output)\n\n\n@pytest.mark.xfail(reason=\"merge_frame is deprecated.\")\ndef test_return_dataframe_merge_is_not_None(returns_frame_1):\n    \n    expected_output = pd.concat(\n        [\n            returns_frame_1,\n            returns_frame_1[\"ticker\"]\n            .str.split(\" \", expand=True)\n            .add_prefix(\"new_\"),\n        ],\n        axis=\"columns\",\n    )\n    result = returns_frame_1.process_text(\n        column_name=\"ticker\",\n        new_column_names=\"new_\",\n        merge_frame=True,\n        string_function=\"split\",\n        expand=True,\n        pat=\" \",\n    )\n    assert_frame_equal(result, expected_output)\n\n\n@pytest.mark.xfail(reason=\"merge_frame is deprecated.\")\ndef test_return_dataframe_merge_is_not_None_new_column_names_is_a_list(\n    returns_frame_1,\n):\n    \n\n    expected_output = pd.concat(\n        [\n            returns_frame_1,\n            returns_frame_1[\"ticker\"]\n            .str.split(\" \", expand=True)\n            .set_axis([\"header1\", \"header2\", \"header3\"], axis=\"columns\"),\n        ],\n        axis=\"columns\",\n    )\n    result = returns_frame_1.process_text(\n        column_name=\"ticker\",\n        new_column_names=[\"header1\", \"header2\", \"header3\"],\n        merge_frame=True,\n        string_function=\"split\",\n        expand=True,\n        pat=\" \",\n    )\n    assert_frame_equal(result, expected_output)\n\n\n@pytest.mark.xfail(reason=\"new_column_names is deprecated.\")\ndef test_return_dataframe_new_column_names_is_a_list_len_unequal(\n    returns_frame_1,\n):\n    \n\n    with pytest.raises(ValueError):\n        returns_frame_1.process_text(\n            column_name=\"ticker\",\n            new_column_names=[\"header1\", \"header2\"],\n            merge_frame=True,\n            string_function=\"split\",\n            expand=True,\n            pat=\" \",\n        )\n\n\ndef test_output_extractall(test_returns_dataframe):\n    \n    with pytest.raises(ValueError):\n        test_returns_dataframe.process_text(\n            column_name=\"text\",\n            string_function=\"extractall\",\n            pat=r\"(?P<letter>[ab])?(?P<digit>\\d)\",\n        )\n\n\n@pytest.mark.xfail(reason=\"merge_frame is deprecated.\")\ndef test_output_extractall_merge_frame_is_not_None(test_returns_dataframe):\n    \n    expected_output = test_returns_dataframe[\"text\"].str.extractall(\n        r\"(?P<letter>[ab])?(?P<digit>\\d)\"\n    )\n    expected_output = test_returns_dataframe.join(\n        expected_output.reset_index(\"match\"), how=\"outer\"\n    ).set_index(\"match\", append=True)\n    result = test_returns_dataframe.process_text(\n        column_name=\"text\",\n        merge_frame=True,\n        string_function=\"extractall\",\n        pat=r\"(?P<letter>[ab])?(?P<digit>\\d)\",\n    )\n    assert_frame_equal(result, expected_output)\n",
        "summary": "The provided Python code defines a series of test functions using the `pytest` framework to validate the functionality of a hypothetical `process_text` method. This method appears to manipulate strings within DataFrame columns based on specified operations like splitting, lowercasing, extracting patterns, and merging results back into the DataFrame. The tests cover various scenarios, including handling different string operations, managing column names for new data, and ensuring proper error handling for invalid inputs or parameters."
    },
    {
        "code": "_base_ = [\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    type='DSC',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='DSCRoIHead',\n        num_stages=3,\n        stage_loss_weights=[dict(loss_mpn=1, loss_bbox=1, loss_cls=1),\n                        dict(loss_mpn=1, loss_bbox=0.5, loss_cls=1),\n                        dict(loss_mpn=1, loss_bbox=0.5, loss_cls=1),\n                        dict(loss_mask=1)],\n        semantic_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[8]),\n        semantic_head=dict(\n            type='FusedSemanticHead',\n            num_ins=5,\n            fusion_level=1,\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=183,\n            ignore_label=255,\n            loss_weight=0.2),\n        relative_roi_extractor=dict(\n            type='RelativeRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[1.0]),\n        mpn_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        mpn=[\n            dict(\n                type='DSCMaskHead',\n                with_conv_res=False,\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                class_agnostic=True,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n            dict(\n                type='DSCMaskHead',\n                with_conv_res=True,\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                class_agnostic=True,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n            dict(\n                type='DSCMaskHead',\n                with_conv_res=True,\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                class_agnostic=True,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))],\n        bbox_roi_extractor=dict(\n            type='SgSingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=[\n            dict(\n                type='Shared2FCDSCBBoxHead',\n                conv_res=1,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCDSCBBoxHead',\n                conv_res=1,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCDSCBBoxHead',\n                conv_res=1,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ],\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        mask_head=dict(\n            type='DSCMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=80,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='PseudoSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='PseudoSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='PseudoSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n    ])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='SegRescale', scale_factor=1 / 8),\n    dict(type='DefaultFormatBundle'),\n    dict(\n        type='Collect',\n        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n\n",
        "summary": "This configuration is for a deep learning model, specifically a Mask R-CNN variant, trained on the COCO dataset. The model is designed to detect objects in images and provide pixel-level segmentation masks for each detected object.\n\nHere's a breakdown of the key components:\n\n1. **Model Architecture**: The model uses a ResNet-50 backbone with FPN (Feature Pyramid Network) for feature extraction. It includes RPN (Region Proposal Network) for generating proposals, RoI Align for pooling features from these proposals, and two fully convolutional layers for predicting bounding boxes and masks.\n\n2. **Loss Functions**:\n   - Bounding Box Regression: Uses Smooth L1 Loss.\n   - Classification: Uses Cross-Entropy Loss.\n   - Mask Prediction: Uses Binary Cross-Entropy Loss with Dice Coefficient as a regularization term to improve segmentation quality.\n\n3. **Data Augmentation**: \n   - Random resizing of images to 800x800 pixels while maintaining aspect ratio.\n   - Random horizontal flipping of images.\n   - Normalization of pixel values using mean and standard deviation from the COCO dataset.\n   - Padding images to ensure dimensions are multiples of 32.\n\n4. **Training Pipeline**:\n   - Loads images and annotations.\n   - Applies data augmentation techniques.\n   - Normalizes images.\n   - Pads images.\n   - Rescales segmentation masks.\n   - Collects necessary data for training.\n\n5. **Testing Pipeline**:\n   - Similar to the training pipeline but without data augmentation.\n   - Ensures consistent preprocessing during inference.\n\n6. **Data Configuration**:\n   - Specifies paths to image and annotation files.\n   - Defines train, validation, and test datasets with their respective pipelines.\n\n7. **Training and Testing Configurations**:\n   - Sets parameters for RPN and RoI heads.\n   - Configures the number of proposals generated by RPN.\n   - Defines thresholds for bounding box scores and mask overlaps during inference.\n\nThis setup is typical for object detection and segmentation tasks, where the model needs to learn to recognize objects in images and provide precise masks for each object. The use of FPN and multi-scale training helps improve performance across different scales of objects."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Item',\n            fields=[\n                ('id', models.AutoField(primary_key=True, serialize=False, verbose_name='ID', auto_created=True)),\n            ],\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that creates a new model named 'Item' with an auto-incrementing primary key field."
    },
    {
        "code": "from setuptools import setup\n\nsetup()\n",
        "summary": "The provided Python code is a minimal example of using `setuptools` to set up a package, but it lacks specific details such as the package name, version, and dependencies, making it incomplete for actual package distribution."
    },
    {
        "code": "config = \"jacobi.config\"\nfuncList = \"\\\"\\\"\"\nprob = 1e-5\nbyte = -1\nsingleInj = 1\nptr = 1\narith = 1\nctrl = 1\nstateFile = \"jacobi\"\n\n\n\n\n\n\n\nimport os\nFLIPIT_PATH = os.environ['FLIPIT_PATH'] \nLLVM_BUILD_PATH = os.environ['LLVM_BUILD_PATH'] \nSHOW = \"\"\n\n\n\nnotInject = [\" \", \" \"]\n\n\ncc = \"mpicc\"\n\n\nverbose = False \n\n",
        "summary": "The provided Python code sets up configuration parameters and environment variables for a simulation or computation, including settings for numerical methods like the Jacobi method, paths to necessary tools, and control flags for debugging and verbosity."
    },
    {
        "code": "def num(a):\n  num = int(a)\n  if (num < 10):\n    return (num + num)\n  elif (num <100):\n    return (num + num //10 + num % 10)\n  elif (num <1000):\n    return (num + num //100 + ( (num //10) % 10) + num % 10)\n  else:\n    return (num + num //1000 + ((num //100) % 10) + ((num //10) % 10) + num %10)\n\ncount = list(range(10000))\n\nfor i in range (10000):\n  temp = num(i)\n  if (temp >= 10000):\n    continue\n  else:\n    count[temp] = -1\n\nfor i in range (10000):\n  if (count[i] != -1):\n    print (i)\n\n",
        "summary": "The provided Python code defines a function `num` that calculates the sum of digits for numbers less than 10, 100, 1000, and 10000. It then iterates through a range of numbers from 0 to 9999, applying the `num` function and marking numbers whose digit sums exceed or equal 10000 for exclusion. Finally, it prints all numbers that have not been marked for exclusion based on their digit sum criteria."
    },
    {
        "code": "from ccxt.base.exchange import Exchange\nfrom ccxt.base.errors import ExchangeError\n\n\nclass virwox (Exchange):\n\n    def describe(self):\n        return self.deep_extend(super(virwox, self).describe(), {\n            'id': 'virwox',\n            'name': 'VirWoX',\n            'countries': ['AT', 'EU'],\n            'rateLimit': 1000,\n            'has': {\n                'CORS': True,\n            },\n            'urls': {\n                'logo': 'https://user-images.githubusercontent.com/1294454/27766894-6da9d360-5eea-11e7-90aa-41f2711b7405.jpg',\n                'api': {\n                    'public': 'https://api.virwox.com/api/json.php',\n                    'private': 'https://www.virwox.com/api/trading.php',\n                },\n                'www': 'https://www.virwox.com',\n                'doc': 'https://www.virwox.com/developers.php',\n            },\n            'requiredCredentials': {\n                'apiKey': True,\n                'secret': False,\n                'login': True,\n                'password': True,\n            },\n            'api': {\n                'public': {\n                    'get': [\n                        'getInstruments',\n                        'getBestPrices',\n                        'getMarketDepth',\n                        'estimateMarketOrder',\n                        'getTradedPriceVolume',\n                        'getRawTradeData',\n                        'getStatistics',\n                        'getTerminalList',\n                        'getGridList',\n                        'getGridStatistics',\n                    ],\n                    'post': [\n                        'getInstruments',\n                        'getBestPrices',\n                        'getMarketDepth',\n                        'estimateMarketOrder',\n                        'getTradedPriceVolume',\n                        'getRawTradeData',\n                        'getStatistics',\n                        'getTerminalList',\n                        'getGridList',\n                        'getGridStatistics',\n                    ],\n                },\n                'private': {\n                    'get': [\n                        'cancelOrder',\n                        'getBalances',\n                        'getCommissionDiscount',\n                        'getOrders',\n                        'getTransactions',\n                        'placeOrder',\n                    ],\n                    'post': [\n                        'cancelOrder',\n                        'getBalances',\n                        'getCommissionDiscount',\n                        'getOrders',\n                        'getTransactions',\n                        'placeOrder',\n                    ],\n                },\n            },\n        })\n\n    def fetch_markets(self, params={}):\n        markets = self.publicGetGetInstruments()\n        keys = list(markets['result'].keys())\n        result = []\n        for p in range(0, len(keys)):\n            market = markets['result'][keys[p]]\n            id = market['instrumentID']\n            symbol = market['symbol']\n            base = market['longCurrency']\n            quote = market['shortCurrency']\n            result.append({\n                'id': id,\n                'symbol': symbol,\n                'base': base,\n                'quote': quote,\n                'info': market,\n            })\n        return result\n\n    def fetch_balance(self, params={}):\n        self.load_markets()\n        response = self.privatePostGetBalances()\n        balances = response['result']['accountList']\n        result = {'info': balances}\n        for b in range(0, len(balances)):\n            balance = balances[b]\n            currency = balance['currency']\n            total = balance['balance']\n            account = {\n                'free': total,\n                'used': 0.0,\n                'total': total,\n            }\n            result[currency] = account\n        return self.parse_balance(result)\n\n    def fetch_market_price(self, symbol, params={}):\n        self.load_markets()\n        response = self.publicPostGetBestPrices(self.extend({\n            'symbols': [symbol],\n        }, params))\n        result = response['result']\n        return {\n            'bid': self.safe_float(result[0], 'bestBuyPrice'),\n            'ask': self.safe_float(result[0], 'bestSellPrice'),\n        }\n\n    def fetch_order_book(self, symbol, limit=None, params={}):\n        self.load_markets()\n        request = {\n            'symbols': [symbol],\n        }\n        if limit is not None:\n            request['buyDepth'] = limit  \n            request['sellDepth'] = limit  \n        response = self.publicPostGetMarketDepth(self.extend(request, params))\n        orderbook = response['result'][0]\n        return self.parse_order_book(orderbook, None, 'buy', 'sell', 'price', 'volume')\n\n    def fetch_ticker(self, symbol, params={}):\n        self.load_markets()\n        end = self.milliseconds()\n        start = end - 86400000\n        response = self.publicGetGetTradedPriceVolume(self.extend({\n            'instrument': symbol,\n            'endDate': self.ymdhms(end),\n            'startDate': self.ymdhms(start),\n            'HLOC': 1,\n        }, params))\n        tickers = response['result']['priceVolumeList']\n        keys = list(tickers.keys())\n        length = len(keys)\n        lastKey = keys[length - 1]\n        ticker = tickers[lastKey]\n        timestamp = self.milliseconds()\n        close = self.safe_float(ticker, 'close')\n        return {\n            'symbol': symbol,\n            'timestamp': timestamp,\n            'datetime': self.iso8601(timestamp),\n            'high': self.safe_float(ticker, 'high'),\n            'low': self.safe_float(ticker, 'low'),\n            'bid': None,\n            'bidVolume': None,\n            'ask': None,\n            'askVolume': None,\n            'vwap': None,\n            'open': self.safe_float(ticker, 'open'),\n            'close': close,\n            'last': close,\n            'previousClose': None,\n            'change': None,\n            'percentage': None,\n            'average': None,\n            'baseVolume': self.safe_float(ticker, 'longVolume'),\n            'quoteVolume': self.safe_float(ticker, 'shortVolume'),\n            'info': ticker,\n        }\n\n    def parse_trade(self, trade, symbol=None):\n        sec = self.safe_integer(trade, 'time')\n        timestamp = sec * 1000\n        return {\n            'id': trade['tid'],\n            'timestamp': timestamp,\n            'datetime': self.iso8601(timestamp),\n            'order': None,\n            'symbol': symbol,\n            'type': None,\n            'side': None,\n            'price': self.safe_float(trade, 'price'),\n            'amount': self.safe_float(trade, 'vol'),\n            'fee': None,\n            'info': trade,\n        }\n\n    def fetch_trades(self, symbol, since=None, limit=None, params={}):\n        self.load_markets()\n        market = self.market(symbol)\n        response = self.publicGetGetRawTradeData(self.extend({\n            'instrument': symbol,\n            'timespan': 3600,\n        }, params))\n        result = self.safe_value(response, 'result', {})\n        trades = self.safe_value(result, 'data', [])\n        return self.parse_trades(trades, market)\n\n    def create_order(self, symbol, type, side, amount, price=None, params={}):\n        self.load_markets()\n        market = self.market(symbol)\n        request = {\n            'instrument': market['symbol'],\n            'orderType': side.upper(),\n            'amount': amount,\n        }\n        if type == 'limit':\n            request['price'] = price\n        response = self.privatePostPlaceOrder(self.extend(request, params))\n        return {\n            'info': response,\n            'id': self.safe_string(response['result'], 'orderID'),\n        }\n\n    def cancel_order(self, id, symbol=None, params={}):\n        request = {\n            'orderID': id,\n        }\n        return self.privatePostCancelOrder(self.extend(request, params))\n\n    def sign(self, path, api='public', method='GET', params={}, headers=None, body=None):\n        url = self.urls['api'][api]\n        auth = {}\n        if api == 'private':\n            self.check_required_credentials()\n            auth['key'] = self.apiKey\n            auth['user'] = self.login\n            auth['pass'] = self.password\n        nonce = self.nonce()\n        if method == 'GET':\n            url += '?' + self.urlencode(self.extend({\n                'method': path,\n                'id': nonce,\n            }, auth, params))\n        else:\n            headers = {'Content-Type': 'application/json'}\n            body = self.json({\n                'method': path,\n                'params': self.extend(auth, params),\n                'id': nonce,\n            })\n        return {'url': url, 'method': method, 'body': body, 'headers': headers}\n\n    def handle_errors(self, code, reason, url, method, headers, body, response):\n        if code == 200:\n            if (body[0] == '{') or (body[0] == '['):\n                if 'result' in response:\n                    result = response['result']\n                    if 'errorCode' in result:\n                        errorCode = result['errorCode']\n                        if errorCode != 'OK':\n                            raise ExchangeError(self.id + ' error returned: ' + body)\n                else:\n                    raise ExchangeError(self.id + ' malformed response: no result in response: ' + body)\n            else:\n                \n                raise ExchangeError(self.id + ' returned a non-JSON reply: ' + body)\n",
        "summary": "The provided Python code defines a class `virwox` that extends the functionality of a cryptocurrency trading exchange API, specifically for the VirWoX platform. It includes methods for fetching market data, placing and canceling orders, handling authentication, and processing errors. The class overrides several methods from a base class to customize behavior specific to the VirWoX API, such as constructing URLs, signing requests with API keys, and parsing responses into structured data."
    },
    {
        "code": "from marshmallow import fields\n\nfrom .field_set import FieldSet, FieldSetSchema\n\n\nclass Destination(FieldSet):\n\n    def __init__(self,\n                 address: str = None,\n                 bytes: int = None,\n                 domain: str = None,\n                 ip: str = None,\n                 mac: str = None,\n                 packets: int = None,\n                 port: int = None,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.address = address\n        self.bytes = bytes\n        self.domain = domain\n        self.ip = ip\n        self.mac = mac\n        self.packets = packets\n        self.port = port\n\n\nclass DestinationSchema(FieldSetSchema):\n    address = fields.String()\n    bytes = fields.Integer()\n    domain = fields.String()\n    ip = fields.String()\n    mac = fields.String()\n    packets = fields.Integer()\n    port = fields.Integer()\n\n",
        "summary": "The provided Python code defines a `Destination` class that inherits from `FieldSet`, representing various network destination attributes such as address, bytes, domain, IP, MAC, packets, and port. It also includes a corresponding `DestinationSchema` class using the `marshmallow` library to serialize and deserialize these attributes."
    },
    {
        "code": "import nltk.corpus\nimport nltk.tokenize.punkt\nimport nltk.stem.snowball\nfrom nltk.corpus import wordnet\nimport string\n\npaths = nltk.data.path\n\n\n\n\n\n\n\n\n\n\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(string.punctuation)\nstopwords.append('')\n\n\n\n\n\n\nwordnetConst = {\"J\":u'a',\"V\":u'v',\"N\":u'n',\"R\":u'r'}\nget_wordnet_pos = lambda x: (x[0],wordnetConst.get(x[1][0],wordnet.NOUN))\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\nlemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\nlemmatize = lemmatizer.lemmatize\n\n\n\n\ndef get_proximity_match_ratio(a, b, threshold=0.8):\n\t\n\tpos_a = map(get_wordnet_pos,nltk.pos_tag(tokenizer.tokenize(a)))\n\tpos_b = map(get_wordnet_pos,nltk.pos_tag(tokenizer.tokenize(b)))\n\n\n\tlemmae_a = [lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_a \\\n\t\t\t\t\tif token.lower().strip(string.punctuation) not in stopwords]\n\tlemmae_b = [lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_b \\\n\t\t\t\t\tif token.lower().strip(string.punctuation) not in stopwords]\n\n\t\n\n\tratio = len(set(lemmae_a).intersection(lemmae_b)) / float(len(set(lemmae_a).union(lemmae_b)))\n\n\n\t\n\tif ratio <= 0.5 and ratio > 0.3:\n\tfrom difflib import SequenceMatcher\n\tratio = SequenceMatcher(None, a, b).ratio()\n\twords_in_v1 = a.split()\n\twords_in_v2 = b.split()\n\tword_match = 0\n\tfor w1 in words_in_v1:\n\tif w1 in words_in_v2:\n\t\tword_match = word_match + 1\n\tif word_match != 0:\n\tif len(words_in_v1) >= len(words_in_v2):\n\t\tr = float(word_match)/len(words_in_v1)\n\telif len(words_in_v1) < len(words_in_v2):\n\t\tr = float(word_match)/len(words_in_v2)\n\tif r > ratio:\n\t\tratio = r\n\n\treturn ratio\n\nimport time\nif __name__ == \"__main__\":\n\tt1 = time.time()\n\tprint 'Nikon Coolpix L31 Point & Shoot Camera',\"|\",'Nikon Coolpix L31 Point & Shoot Camera(Black)'\n\tprint get_proximity_match_ratio('Nikon Coolpix L31 Point & Shoot Camera', 'Nikon Coolpix L31 Point & Shoot Camera(Black)')\n\tprint \"time>\",time.time()-t1\n\n\tprint \"------------------------------------------\"\n\n\n\tt1 = time.time()\n\tprint 'I have 5.5\" Inch product',\"|\",'my product 5.5 Inches Inch'\n\tprint get_proximity_match_ratio('I have 5.5\" Inch product', 'my product 5.5 Inches Inch')\n\tprint \"time>\",time.time()-t1\n\n\tprint \"------------------------------------------\"\n\n\tt1 = time.time()\n\tprint 'iphone 6s cover for me',\"|\",'i have phone for me'\n\tprint get_proximity_match_ratio('iphone 6s cover for me', 'i have phone for me')\n\tprint \"time>\",time.time()-t1\n\n\n\tprint get_proximity_match_ratio('Nikon Coolpix L31 Point & Shoot Camera', 'Nikon Coolpix L31 Point & Shoot Camera(Black)')\n\n\tprint get_proximity_match_ratio('In the eighteenth century it was often convenient to regard man as a clockwork automaton.', 'In the 1700s, it was common to regard man as a clockwork automaton.')\n\n\tprint get_proximity_match_ratio('SAMSUNG 9000 mAh for Smart Phone', 'SAMSUNG 9000 mAh for Smart Phone And Tablets')\n\n\tprint get_proximity_match_ratio('5.5\" Inch', '5.5 Inches Inch')\n\tprint get_proximity_match_ratio('mahalaxmi gold coin', 'rose gold coin')\n\tprint get_proximity_match_ratio('10 gms silver coin of lakshmi ganesh.. ', 'silver currency note of 1000 rs.')\n\tprint breakStringInTOWords('Samsung Galaxy Tab 3 T211 Tablet (White)')\n\tprint get_proximity_match_ratio('sony 1tb normal external hard drive (black)', 'sony 1tb wired external hard drive ()')",
        "summary": "The provided Python code is a script that calculates the proximity match ratio between two strings, considering lemmatization and wordnet pos tagging to handle synonyms and improve matching accuracy. It includes functions for tokenizing, lemmatizing, and comparing strings while ignoring stopwords and punctuation, with special handling for cases where the initial comparison ratio is below a certain threshold."
    },
    {
        "code": "from django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('vespawatch', '0052_auto_20210824_1131'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='managementaction',\n            name='result',\n            field=models.CharField(choices=[('ST', 'Successfully treated'), ('UT', 'Unsuccessfully treated'), ('UN', 'Untreated'), ('UK', 'UK')], default='UK', max_length=3, verbose_name='Result'),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that adds a new field named `result` to the `ManagementAction` model. The `result` field is a character field with predefined choices and a default value, intended to store information about the outcome of management actions related to vespas."
    },
    {
        "code": "import pprint\nimport re  \n\nimport six\n\nfrom py_insightvm_sdk.models.link import Link  \nfrom py_insightvm_sdk.models.vulnerability_validation_resource import VulnerabilityValidationResource  \n\n\nclass ResourcesVulnerabilityValidationResource(object):\n    \n\n    \n    swagger_types = {\n        'links': 'list[Link]',\n        'resources': 'list[VulnerabilityValidationResource]'\n    }\n\n    attribute_map = {\n        'links': 'links',\n        'resources': 'resources'\n    }\n\n    def __init__(self, links=None, resources=None):  \n          \n\n        self._links = None\n        self._resources = None\n        self.discriminator = None\n\n        if links is not None:\n            self.links = links\n        if resources is not None:\n            self.resources = resources\n\n    @property\n    def links(self):\n        \n        return self._links\n\n    @links.setter\n    def links(self, links):\n        \n\n        self._links = links\n\n    @property\n    def resources(self):\n        \n        return self._resources\n\n    @resources.setter\n    def resources(self, resources):\n        \n\n        self._resources = resources\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ResourcesVulnerabilityValidationResource, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, ResourcesVulnerabilityValidationResource):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \n        return not self == other\n",
        "summary": "The provided Python code defines a class `ResourcesVulnerabilityValidationResource` that represents a collection of vulnerability validation resources and their associated links. It includes methods for initializing the object with links and resources, accessing these properties, converting the object to a dictionary or string representation, and comparing objects for equality."
    },
    {
        "code": "from functools import wraps\n\ndef logged(func):\n    \n    \n\n    print('Adding logging to', func.__name__)\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print('You called', func.__name__)\n        return func(*args, **kwargs)\n\n    return wrapper\n",
        "summary": "The `logged` decorator function adds logging functionality to any wrapped function by printing a message indicating that the function is being called and then executing the original function."
    },
    {
        "code": "from flask import Flask, Response\nfrom camera import Camera\nimport cv2\n\n\napp = Flask(__name__)\ncamera = Camera().start()\n\n\ndef gen(camera):\n    while True:\n        frame = camera.read()\n        _, jpeg = cv2.imencode('.jpg', frame)\n        yield (b'--frame\\r\\n'\n               b'Content-Type: image/jpeg\\r\\n\\r\\n' + jpeg.tobytes() + b'\\r\\n')\n        \n@app.route('/stream')\ndef stream():\n    return Response(gen(camera),\n                mimetype='multipart/x-mixed-replace; boundary=frame')\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', debug=False, use_reloader=False)\n",
        "summary": "This Python script sets up a Flask web application that serves a live video stream from a camera using OpenCV. The `gen` function captures frames from the camera and encodes them as JPEG images, which are then streamed to clients accessing the `/stream` endpoint in real-time."
    },
    {
        "code": "from glob import glob\nfrom setuptools import setup\nfrom pybind11.setup_helpers import Pybind11Extension\n\next_modules = [\n    Pybind11Extension(\n        \"PFlib\",\n        \n        [\"particle_filter.cpp\",],\n        swig_opts=['-ggdb',],\n        include_dirs=['include',]\n    ),\n]\n\nsetup(\n    name=\"PFlib\",\n    \n    \n    ext_modules=ext_modules,\n)",
        "summary": "This Python script uses `setuptools` and `pybind11.setup_helpers` to define a C++ extension module named \"PFlib\" from the file \"particle_filter.cpp\", including directories for headers and enabling debugging options through SWIG."
    },
    {
        "code": "from sklearn import svm, cluster\nfrom PIL import Image, ImageDraw\nimport os\nimport sys\nimport random\n\n\n\ndef load_images(dirname):\n\timages = []\n\tfor image_name in os.listdir(dirname):\n\t\tif image_name.startswith('.'):\n\t\t\tcontinue\n\t\timage = Image.open(dirname + '/' + image_name).convert('1')\n\t\tx, y = image.size\n\t\timage = image.resize((x, 280), Image.ANTIALIAS)\n\t\tdata = [0 if pixel == 0 else 1 for pixel in image.getdata()]\n\t\timages.append(data)\n\treturn images\n\nmin_len = 10000000\ndef normalize(X):\n\tglobal min_len\n\tmin_len = min(min_len, min(len(x) for x in X))\n\treturn [x[:min_len] for x in X]\n\ndef crossvalidate(edges, nonedges):\n\trandom.shuffle(edges)\n\trandom.shuffle(nonedges)\n\ttrain_edge_len, train_nonedge_len = len(edges) * 7 // 10, len(nonedges) * 7 // 10\n\tcross_edge_len, cross_nonedge_len = len(edges) - train_edge_len, len(nonedges) - train_nonedge_len\n\n\tX_train = normalize(nonedges[:train_nonedge_len] + \n\t\t\t\t\t\tedges[:train_edge_len])\n\ty_train = [0] * train_nonedge_len + [1] * train_edge_len\n\n\tX_cross = normalize(nonedges[train_nonedge_len:] + \n\t\t\t\t\t\tedges[train_edge_len:])\n\ty_cross = [0] * cross_nonedge_len + [1] * cross_edge_len\n\n\tclf = svm.SVC(gamma=.001, C=100.)\n\tclf.fit(X_train, y_train)\n\tprint(\"prediction: {}\".format(list(clf.predict(X_cross))))\n\tprint(\"actuallity: {}\".format(y_cross))\n\tprint(clf.score(X_cross, y_cross))\n\ndef get_column(img, i):\n\tw, h = img.size\n\tcolumn = []\n\tfor j in range(h):\n\t\t\tcolumn.append(0 if img.getpixel((i, j)) == 0 else 1)\n\treturn column\n\ndef search_picture(clf, image_name):\n\timage = Image.open(image_name).convert('1')\n\tx, y = image.size\n\timage = image.resize((x, 280), Image.ANTIALIAS)\n\tw, h = image.size\n\n\tcolumns = [get_column(image, i) for i in range(25)]\n\tdatas = []\n\tfor i in range(25, w):\n\t\tcolumns = columns[1:] + [get_column(image, i)]\n\t\tdata = [columns[i][j] for j in range(len(columns[0])) for i in range(len(columns))]\n\t\tdatas.append(data)\n\tdatas = normalize(datas)\n\tmatches = [[i] for i, m in enumerate(clf.predict(datas)) if m == 1]\n\tif len(matches) == 0:\n\t\treturn [], matches\n\tclst = cluster.DBSCAN(eps=20, min_samples=1)\n\tclst.fit(matches)\n\ttrimmed = [idx for idx in clst.components_ if idx > w // 6 and idx < w * 5 // 6]\n\tclst = cluster.KMeans(3, init='k-means++')\n\tclst.fit(trimmed)\n\tseps = list(sorted([int(v[0]) + 25//2 for v in clst.cluster_centers_]))\n\tfinal_seps = []\n\tfor start, end in zip(seps, seps[1:]):\n\t\tif (end - start) > w // 6:\n\t\t\tfinal_seps.append(start)\n\tfinal_seps.append(seps[-1])\n\treturn final_seps, matches\n\ndef train(edges, nonedges):\n\tclf = svm.SVC(gamma=.001, C=100.)\n\tX = normalize(nonedges + edges)\n\ty = [0] * len(nonedges) + [1] * len(edges)\n\tclf.fit(X, y)\n\treturn clf\n\n\ndef main(edge_dir, non_edge_dir):\n\tedges = load_images(edge_dir)\n\tnonedges = load_images(non_edge_dir)\n\n\tcrossvalidate(edges, nonedges)\n\n\tclf = train(edges, nonedges)\n\n\tfor comic in os.listdir('test'):\n\t\tprint(comic)\n\t\tpanels, matches = search_picture(clf, 'test/' + comic)\n\t\tprint(\"\\tpanels: {}\".format(panels))\n\t\timage = Image.open('test/' + comic).convert('RGBA')\n\t\tdraw = ImageDraw.Draw(image)\n\t\tw, h = image.size\n\t\tfor match in matches:\n\t\t\tmatch = match[0]\n\t\t\tdraw.line((match, 0) + (match, h), fill=(0,0,255,0))\n\t\tfor sep in panels:\n\t\t\tdraw.line((sep, 0) + (sep, h), fill=(255,0,0), width=3)\n\t\timage.show()\n\n\treturn clf\n\nif __name__ == '__main__':\n\tif len(sys.argv) != 3:\n\t\tprint('Usage: {} <edges-dir> <non-edges-dir>'.format(sys.argv[0]))\n\t\tsys.exit(1)\n\tedge_dir = sys.argv[1]\n\tnon_edge_dir = sys.argv[2]\n\tmain(edge_dir, non_edge_dir)\n\t",
        "summary": "The Python script processes image data to train a Support Vector Machine (SVM) classifier for distinguishing between edge and non-edge images. It then uses this classifier to identify panels within comic book pages by detecting vertical lines that separate different panels. The script includes functions for loading images, normalizing data, cross-validating the model, searching for picture panels, training the classifier, and running the main program with command-line arguments specifying directories of edge and non-edge images."
    },
    {
        "code": "import logging\nimport os\nimport re\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nfrom typing import Any, Tuple, Optional\n\nimport pandas as pd\n\nfrom python import TOPIC_ID, SUBTOPIC, DOCUMENT_NUMBER, DOCUMENT_ID, SENTENCE_IDX, TOKEN_IDX, TOKEN_IDX_TO, \\\n    TOKEN_IDX_FROM, TOKEN, MENTION_ID, EVENT, MENTION_TYPE, DESCRIPTION, MENTION_TYPES_ACTION\n\nlogger = logging.getLogger()\n\n\ndef read_xml(xml_path) -> Tuple[Any, Any, Any, Any, Any]:\n    tree = ET.parse(xml_path)\n\n    \n    root = tree.getroot()\n    assert root.tag == \"Document\"\n    doc_filename = root.attrib[\"doc_name\"]\n    doc_id = root.attrib[\"doc_id\"]\n    m = re.match(r\"(?P<topic_id>\\d+)_(?P<document_number>\\d+)(?P<subtopic>\\w+)\\.xml\", doc_filename)\n\n    topic_id = m.group(\"topic_id\")\n    subtopic = m.group(\"subtopic\")\n    document_number = int(m.group(\"document_number\"))\n\n    documents_index = pd.MultiIndex.from_tuples([(topic_id, subtopic, doc_id)],\n                                                names=[TOPIC_ID, SUBTOPIC, DOCUMENT_ID])\n    documents = pd.DataFrame({DOCUMENT_ID: pd.Series(doc_id, index=documents_index),\n                              DOCUMENT_NUMBER: pd.Series(document_number, index=documents_index)})\n\n    \n    contents_rows = []\n    contents_index = []\n    for token_elmt in root.iter(\"token\"):\n        \n        sentence_idx = int(token_elmt.attrib[\"sentence\"])\n        token_idx = int(token_elmt.attrib[\"number\"])\n        contents_index.append((doc_id, sentence_idx, token_idx))\n\n        \n        token = token_elmt.text\n        contents_rows.append({TOKEN: token})\n    contents_index = pd.MultiIndex.from_tuples(contents_index, names=[DOCUMENT_ID, SENTENCE_IDX, TOKEN_IDX])\n    contents = pd.DataFrame(contents_rows, index=contents_index)\n\n    \n    mentions_rows = []\n    mentions_index = []\n    entities_events = []\n    for markable in root.find(\"Markables\").getchildren():\n        \n        if markable.tag == \"UNKNOWN_INSTANCE_TAG\":\n            continue\n\n        mention_id = int(markable.attrib[\"m_id\"])\n\n        \n        if \"TAG_DESCRIPTOR\" in markable.attrib.keys():\n            if \"instance_id\" in markable.attrib.keys():\n                entities_events.append({\n                    EVENT: markable.attrib[\"instance_id\"],\n                    DESCRIPTION: markable.attrib[\"TAG_DESCRIPTOR\"]\n                })\n            continue\n\n        token_ids = [int(anchor.attrib[\"t_id\"]) for anchor in markable.iter(\"token_anchor\")]\n        token_ids_from, token_ids_to = min(token_ids), max(token_ids)\n\n        \n        token_indexes = contents.index.get_level_values(TOKEN_IDX).values\n        token_idx_from = token_indexes[\n            token_ids_from - 1]  \n        token_idx_to = token_indexes[\n                           token_ids_to - 1] + 1  \n\n        sentence_idx = contents.index.get_level_values(SENTENCE_IDX).values[token_ids_from - 1]\n\n        \n        is_non_contiguous_mention = len(token_ids) < token_idx_from - token_idx_to\n        if is_non_contiguous_mention:\n            logger.info(\"Converted non-contiguous mention to contiguous mention.\")\n\n        mentions_index.append((doc_id, mention_id))\n        mentions_rows.append({SENTENCE_IDX: sentence_idx,\n                              TOKEN_IDX_FROM: token_idx_from,\n                              TOKEN_IDX_TO: token_idx_to,\n                              MENTION_TYPE: markable.tag})\n    mentions_index = pd.MultiIndex.from_tuples(mentions_index, names=[DOCUMENT_ID, MENTION_ID])\n    mentions = pd.DataFrame(mentions_rows, index=mentions_index)\n    entities_events = pd.DataFrame(entities_events).set_index(EVENT)\n\n    \n    clusters_rows = []\n    for relation in root.find(\"Relations\").getchildren():\n        tags_of_interest = [\"CROSS_DOC_COREF\", \"INTRA_DOC_COREF\"]\n        if not relation.tag in tags_of_interest:\n            logger.info(\"Unexpected tag \" + relation.tag)\n            raise NotImplementedError\n\n        \n        if \"note\" in relation.attrib:\n            \n            relation_id = relation.attrib[\"note\"]\n        else:\n            \n            relation_id = doc_id + \"_\" + relation.attrib[\"r_id\"]\n\n        for mention in relation.iter(\"source\"):\n            mention_id = int(mention.attrib[\"m_id\"])\n            clusters_rows.append({EVENT: relation_id, DOCUMENT_ID: doc_id, MENTION_ID: mention_id})\n    clusters = pd.DataFrame(clusters_rows)\n\n    \n    \n    \n    \n    \n    if clusters.empty:\n        singletons = mentions.index.to_frame().reset_index(drop=True)\n    else:\n        \n        outer = pd.merge(mentions, clusters, left_index=True, right_on=[DOCUMENT_ID, MENTION_ID], how=\"outer\")\n        singletons = outer.loc[outer[EVENT].isna(), [DOCUMENT_ID, MENTION_ID]]\n    singletons[EVENT] = \"SINGLETON_\" + singletons.astype(str).apply(\"_\".join, axis=1)\n    clusters = clusters.append(singletons, sort=False).reset_index(drop=True)\n\n    return documents, contents, mentions, clusters, entities_events\n\n\ndef read_split_data(root: Path, sentence_filter_csv: Optional[Path]):\n    documents = []\n    contents = []\n    mentions = []\n    clusters = []\n    entities_events = []\n\n    \n    for root, dirs, files in os.walk(str(root.absolute())):\n        for file in files:\n            path = os.path.abspath(os.path.join(root, file))\n            f_documents, f_contents, f_mentions, f_clusters, f_entities_events = read_xml(path)\n\n            documents.append(f_documents)\n            contents.append(f_contents)\n            mentions.append(f_mentions)\n            clusters.append(f_clusters)\n            entities_events.append(f_entities_events)\n\n    documents = pd.concat(documents).sort_index()\n    contents = pd.concat(contents).sort_index()\n    mentions = pd.concat(mentions).sort_index()\n    clusters = pd.concat(clusters, sort=False)\n    entities_events = pd.concat(entities_events).sort_index()\n\n    \n    assert clusters.duplicated(subset=[DOCUMENT_ID, MENTION_ID]).value_counts().get(True, 0) == 0\n\n    clusters = clusters.set_index([DOCUMENT_ID, MENTION_ID])\n    mentions = pd.merge(mentions, clusters, left_index=True, right_index=True).sort_index()\n\n    \n    if sentence_filter_csv is not None:\n        sent_filter = pd.read_csv(sentence_filter_csv)\n        doc_number_and_subtopic = sent_filter[\"File\"].str.split(\"ecb\", expand=True)\n        doc_number_and_subtopic.columns = [DOCUMENT_NUMBER, SUBTOPIC]\n        doc_number_and_subtopic[DOCUMENT_NUMBER] = doc_number_and_subtopic[DOCUMENT_NUMBER].astype(int)\n        doc_number_and_subtopic[SUBTOPIC].replace({\"plus\": \"ecbplus\", \"\": \"ecb\"}, inplace=True)\n        sent_filter = pd.concat([sent_filter.drop(columns=\"File\"), doc_number_and_subtopic], axis=1)\n        sent_filter.rename(columns={\"Topic\": TOPIC_ID, \"Sentence Number\": SENTENCE_IDX}, inplace=True)\n        sent_filter[TOPIC_ID] = sent_filter[TOPIC_ID].astype(str)\n        sent_filter = sent_filter[[TOPIC_ID, SUBTOPIC, DOCUMENT_NUMBER, SENTENCE_IDX]]\n\n        \n        topics_in_split = documents.index.get_level_values(TOPIC_ID).unique()\n        sent_filter = sent_filter.loc[sent_filter[TOPIC_ID].isin(topics_in_split)].copy()\n\n        \n        documents_with_doc_number_in_index = documents.set_index(DOCUMENT_NUMBER, append=True).reset_index(level=DOCUMENT_ID, drop=True).sort_index()\n        sent_filter[DOCUMENT_ID] = sent_filter[[TOPIC_ID, SUBTOPIC, DOCUMENT_NUMBER]].apply(lambda row: documents_with_doc_number_in_index[DOCUMENT_ID].loc[tuple(row.values)], axis=1)\n\n        all_mentions_to_keep = []\n        for doc_id, df in mentions.groupby(DOCUMENT_ID):\n            sentences_to_keep = sent_filter.loc[sent_filter[DOCUMENT_ID] == doc_id]\n\n            \n            \n            is_official_evaluation_sentence = df[SENTENCE_IDX].isin(sentences_to_keep[SENTENCE_IDX])\n            is_action_mention = df[MENTION_TYPE].isin(MENTION_TYPES_ACTION)\n            mentions_to_keep = df.loc[is_official_evaluation_sentence | (~is_action_mention)]\n            all_mentions_to_keep.append(mentions_to_keep)\n        mentions = pd.concat(all_mentions_to_keep).sort_index()\n\n    return documents, contents, mentions, entities_events",
        "summary": "The provided Python code defines two functions: `read_xml` and `read_split_data`. The `read_xml` function parses an XML file to extract document metadata, tokenized sentences, mentions, clusters, and entity-event relationships. The `read_split_data` function processes a directory of XML files, aggregates the parsed data into DataFrames, and optionally filters sentences based on a provided CSV file."
    },
    {
        "code": "import os\n\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTEMPLAE_DIR = os.path.join(BASE_DIR, 'templates')\n\n\n\n\n\nSECRET_KEY = '%2^\n\n\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'bootstrap4',\n    'accounts',\n    'groups',\n    'posts',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'simplesocial.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [TEMPLAE_DIR,],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'simplesocial.wsgi.application'\n\n\n\n\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n\n\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n\n\n\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [os.path.join(BASE_DIR, 'static'),]\n\nLOGIN_REDIRECT_URL = 'test'\nLOGOUT_REDIRECT_URL = 'thanks'\n",
        "summary": "This Python code is a configuration file for a Django web application. It sets up various settings such as the base directory, template directories, secret key, middleware, installed apps, database configurations, authentication validators, and static files paths. Additionally, it defines URLs, language, time zone, and redirections for login and logout processes."
    },
    {
        "code": "import sys\nfrom common import reverse_items\n\nif len(sys.argv) != 3:\n  print(\"Reverse key and value of all pairs\")\n  print((\"Usage: \", sys.argv[0], \"[input] [output]\"))\n  exit(1)\n\nreverse_items(sys.argv[1], sys.argv[2])\n",
        "summary": "The Python script checks if it has been provided with exactly two command-line arguments (excluding the script name). If not, it prints a usage message and exits. If the correct number of arguments is provided, it calls the `reverse_items` function from the `common` module, passing the input file path as the first argument and the output file path as the second argument."
    },
    {
        "code": "import os\nimport subprocess\nfrom unittest import mock\n\nimport pytest\nfrom pre_commit.constants import VERSION as PRE_COMMIT_VERSION\n\nimport testing.git\nfrom all_repos import autofix_lib\nfrom all_repos import clone\nfrom all_repos import git\nfrom all_repos.config import load_config\n\n\n@pytest.mark.parametrize(\n    ('cli_repos', 'expected'),\n    (\n        (None, ['found_repo']),\n        ([], []),\n        (['cli_repo'], ['cli_repo']),\n    ),\n)\ndef test_filter_repos(file_config, cli_repos, expected):\n    ret = autofix_lib.filter_repos(\n        file_config, cli_repos, lambda _: ['found_repo'],\n    )\n    assert ret == expected\n\n\ndef test_assert_importable_is_importable():\n    autofix_lib.assert_importable('pre_commit', install='pre-commit')\n\n\ndef test_assert_importable_not_importable():\n    with pytest.raises(SystemExit) as excinfo:\n        autofix_lib.assert_importable('watmodule', install='wat')\n    msg, = excinfo.value.args\n    assert msg == (\n        'This tool requires the `watmodule` module to be installed.\\n'\n        'Try installing it via `pip install wat`.'\n    )\n\n\ndef test_require_version_new_enough():\n    autofix_lib.require_version_gte('pre-commit', '0.17.0')\n\n\ndef test_require_version_not_new_enough():\n    with pytest.raises(SystemExit) as excinfo:\n        autofix_lib.require_version_gte('pre-commit', '999')\n    msg, = excinfo.value.args\n    assert msg == (\n        f'This tool requires the `pre-commit` package is at least version '\n        f'999.  The currently installed version is {PRE_COMMIT_VERSION}.\\n\\n'\n        f'Try `pip install --upgrade pre-commit`'\n    )\n\n\ndef test_run(capfd):\n    autofix_lib.run('echo', 'h\"i')\n    out, _ = capfd.readouterr()\n    assert out == (\n        '$ echo \\'h\"i\\'\\n'\n        'h\"i\\n'\n    )\n\n\ndef test_cwd(tmpdir):\n    orig = os.getcwd()\n    with autofix_lib.cwd(tmpdir):\n        assert os.getcwd() == tmpdir\n    assert os.getcwd() == orig\n\n\ndef test_repo_context_success(file_config_files, capsys):\n    expected_rev = testing.git.revparse(file_config_files.dir1)\n    with autofix_lib.repo_context(\n            str(file_config_files.output_dir.join('repo1')), use_color=False,\n    ):\n        assert testing.git.revparse('.') == expected_rev\n        assert git.remote('.') == file_config_files.dir1\n    out, err = capsys.readouterr()\n    assert err == ''\n    assert 'Errored' not in out\n\n\ndef test_repo_context_errors(file_config_files, capsys):\n    with autofix_lib.repo_context(\n            str(file_config_files.output_dir.join('repo1')), use_color=False,\n    ):\n        assert False\n    out, err = capsys.readouterr()\n    assert 'Errored' in out\n    assert 'assert False' in err\n\n\ndef test_interactive_control_c(mock_input, capfd):\n    mock_input.set_side_effect(KeyboardInterrupt)\n    with pytest.raises(SystemExit):\n        autofix_lib._interactive_check(use_color=False)\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? ^C\\n'\n        'Goodbye!\\n'\n    )\n\n\ndef test_interactive_eof(mock_input, capfd):\n    mock_input.set_side_effect(EOFError)\n    with pytest.raises(SystemExit):\n        autofix_lib._interactive_check(use_color=False)\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? ^D\\n'\n        'Goodbye!\\n'\n    )\n\n\ndef test_interactive_quit(mock_input, capfd):\n    mock_input.set_side_effect('q')\n    with pytest.raises(SystemExit):\n        autofix_lib._interactive_check(use_color=False)\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? <<q\\n'\n        'Goodbye!\\n'\n    )\n\n\ndef test_interactive_yes(mock_input, capfd):\n    mock_input.set_side_effect('y')\n    assert autofix_lib._interactive_check(use_color=False) is True\n    out, _ = capfd.readouterr()\n    assert out == '***Looks good [y,n,s,q,?]? <<y\\n'\n\n\ndef test_interactive_no(mock_input, capfd):\n    mock_input.set_side_effect('n')\n    assert autofix_lib._interactive_check(use_color=False) is False\n    out, _ = capfd.readouterr()\n    assert out == '***Looks good [y,n,s,q,?]? <<n\\n'\n\n\ndef test_interactive_shell(mock_input, capfd):\n    mock_input.set_side_effect('s', 'n')\n    with mock.patch.dict(os.environ, {'SHELL': 'echo'}):\n        assert autofix_lib._interactive_check(use_color=False) is False\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? <<s\\n'\n        'Opening an interactive shell, type `exit` to continue.\\n'\n        'Any modifications will be committed.\\n'\n        \n        '\\n'\n        '***Looks good [y,n,s,q,?]? <<n\\n'\n    )\n\n\ndef test_interactive_help(mock_input, capfd):\n    mock_input.set_side_effect('?', 'n')\n    assert autofix_lib._interactive_check(use_color=False) is False\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? <<?\\n'\n        'y (yes): yes it looks good, commit and continue.\\n'\n        'n (no): no, do not commit this repository.\\n'\n        's (shell): open an interactive shell in the repo.\\n'\n        'q (quit, ^C): early exit from the autofixer.\\n'\n        '? (help): show this help message.\\n'\n        '***Looks good [y,n,s,q,?]? <<n\\n'\n    )\n\n\ndef test_interactive_garbage(mock_input, capfd):\n    mock_input.set_side_effect('garbage', 'n')\n    assert autofix_lib._interactive_check(use_color=False) is False\n    out, _ = capfd.readouterr()\n    assert out == (\n        '***Looks good [y,n,s,q,?]? <<garbage\\n'\n        'Unexpected input: garbage\\n'\n        'y (yes): yes it looks good, commit and continue.\\n'\n        'n (no): no, do not commit this repository.\\n'\n        's (shell): open an interactive shell in the repo.\\n'\n        'q (quit, ^C): early exit from the autofixer.\\n'\n        '? (help): show this help message.\\n'\n        '***Looks good [y,n,s,q,?]? <<n\\n'\n    )\n\n\ndef lower_case_f():\n    f_contents = open('f').read()\n    with open('f', 'w') as f:\n        f.write(f_contents.lower())\n\n\ndef failing_check_fix():\n    raise AssertionError('nope!')\n\n\ndef test_fix_dry_run_no_change(file_config_files, capfd):\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lower_case_f,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', None),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=True, interactive=False,\n        ),\n    )\n\n    out, err = capfd.readouterr()\n    assert err == ''\n    assert 'Errored' not in out\n    \n    assert '-OHAI\\n+ohai\\n' in out\n    assert '-OHELLO\\n+ohello\\n' in out\n\n    \n    assert file_config_files.dir1.join('f').read() == 'OHAI\\n'\n    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n\n\ndef test_fix_with_limit(file_config_files, capfd):\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lower_case_f,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', None),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=1, dry_run=True, interactive=False,\n        ),\n    )\n\n    out, err = capfd.readouterr()\n    assert err == ''\n    assert 'Errored' not in out\n    \n    assert '-OHAI\\n+ohai\\n' in out\n    assert '-OHELLO\\n+ohello\\n' not in out\n\n\ndef test_fix_interactive(file_config_files, capfd, mock_input):\n    mock_input.set_side_effect('y', 'n')\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lower_case_f,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', None),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=False, interactive=True,\n        ),\n    )\n\n    assert file_config_files.dir1.join('f').read() == 'ohai\\n'\n    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n\n\ndef test_autofix_makes_commits(file_config_files, capfd):\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lower_case_f,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', 'A B <a@a.a>'),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n        ),\n    )\n\n    out, err = capfd.readouterr()\n    assert err == ''\n    assert 'Errored' not in out\n\n    assert file_config_files.dir1.join('f').read() == 'ohai\\n'\n    assert file_config_files.dir2.join('f').read() == 'ohello\\n'\n\n    \n    last_commit_msg = subprocess.check_output((\n        'git', '-C', file_config_files.dir1, 'log',\n        '--format=%s', '--first-parent', '-1',\n    )).decode()\n    assert last_commit_msg == \"Merge branch 'all-repos_autofix_test-branch'\\n\"\n\n    \n    commit = subprocess.check_output((\n        'git', '-C', file_config_files.dir1, 'log',\n        '--patch', '--grep', 'message!', '--format=%an %ae\\n%B',\n    )).decode()\n    assert commit.startswith(\n        'A B a@a.a\\n'\n        'message!\\n'\n        '\\n'\n        'Committed via https://github.com/asottile/all-repos\\n',\n    )\n    assert commit.endswith('-OHAI\\n+ohai\\n')\n\n\ndef test_fix_failing_check_no_changes(file_config_files, capfd):\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lower_case_f,\n        check_fix=failing_check_fix,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', None),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n        ),\n    )\n\n    out, err = capfd.readouterr()\n    assert 'nope!' in err\n    assert out.count('Errored') == 2\n\n    \n    assert file_config_files.dir1.join('f').read() == 'OHAI\\n'\n    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n\n\ndef test_noop_does_not_commit(file_config_files):\n    rev_before1 = testing.git.revparse(file_config_files.dir1)\n    rev_before2 = testing.git.revparse(file_config_files.dir2)\n    autofix_lib.fix(\n        (\n            str(file_config_files.output_dir.join('repo1')),\n            str(file_config_files.output_dir.join('repo2')),\n        ),\n        apply_fix=lambda: None,\n        config=load_config(file_config_files.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', None),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n        ),\n    )\n    rev_after1 = testing.git.revparse(file_config_files.dir1)\n    rev_after2 = testing.git.revparse(file_config_files.dir2)\n    assert (rev_before1, rev_before2) == (rev_after1, rev_after2)\n\n\ndef test_fix_non_default_branch(file_config_non_default):\n    clone.main(('--config-filename', str(file_config_non_default.cfg)))\n\n    autofix_lib.fix(\n        (\n            str(file_config_non_default.output_dir.join('repo1')),\n        ),\n        apply_fix=lower_case_f,\n        config=load_config(file_config_non_default.cfg),\n        commit=autofix_lib.Commit('message!', 'test-branch', 'A B <a@a.a>'),\n        autofix_settings=autofix_lib.AutofixSettings(\n            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n        ),\n    )\n\n    assert file_config_non_default.dir1.join('f').read() == 'ohai\\n'\n",
        "summary": "The provided code snippet is a Python script that demonstrates the usage of the `all-repos` library to perform bulk operations on multiple Git repositories. The script includes various functions and configurations to handle different scenarios such as applying fixes, checking fixes, committing changes, and handling interactive mode.\n\nHere's a breakdown of the key components and functionalities:\n\n1. **Fix Application**:\n   - The `lower_case_f` function is used to convert file content to lowercase.\n   - This function is applied to multiple repositories using the `autofix_lib.fix` method.\n\n2. **Commit Handling**:\n   - A custom commit message is defined in the `autofix_lib.Commit` class.\n   - The script commits changes to each repository with a merge commit that combines all fixes into a single branch (`all-repos_autofix_test-branch`).\n\n3. **Interactive Mode**:\n   - The script supports interactive mode, which allows users to review and confirm changes before committing.\n\n4. **Check Fix Functionality**:\n   - A `failing_check_fix` function is provided to simulate a failing check.\n   - This function raises an error when called, demonstrating how the script handles errors during fix application.\n\n5. **Non-Default Branch Handling**:\n   - The script includes functionality to handle repositories on non-default branches.\n   - It demonstrates cloning a repository and applying fixes to it.\n\n6. **Testing and Validation**:\n   - The script uses various testing functions (`testing.git.revparse`, `clone.main`) to validate the state of repositories before and after operations.\n\n### Example Usage\n\nTo use this script, you would typically:\n\n1. Configure your repositories in a configuration file.\n2. Run the script with the appropriate arguments to apply fixes, check them, and commit changes.\n\nHere's a simplified example of how you might run the script:\n\n```bash\npython all_repos_script.py --config-filename config.yaml\n```\n\n### Dependencies\n\nThe script relies on several libraries, including `all-repos`, `gitpython`, and custom modules for testing and cloning repositories. Ensure these dependencies are installed in your environment.\n\n### Error Handling\n\nThe script includes basic error handling, such as checking for errors during fix application and committing changes. This helps in identifying issues that might prevent the script from completing successfully.\n\n### Interactive Mode\n\nInteractive mode provides a safety net by allowing users to review changes before they are committed. This is particularly useful in production environments where manual verification is required.\n\nOverall, this script serves as a powerful tool for automating bulk operations on multiple Git repositories, ensuring consistency and efficiency in code maintenance across projects."
    },
    {
        "code": "import random\n\nfrom .names import names as default_names\n\n\nclass NameGenerator(object):\n\n    def __init__(self, names=None):\n        self.names = names or default_names\n\n\n    def __call__(self):\n        return self.names.pop(random.randrange(len(self.names)))\n\n    def __iter__(self):\n        while self.names:\n            yield self()\n",
        "summary": "The `NameGenerator` class is designed to randomly select and remove names from a provided list, with an option to use a default list of names if none is specified. It includes methods for calling the generator directly to get a random name and iterating over all remaining names until the list is exhausted."
    },
    {
        "code": "from django.utils import timezone\nfrom rest_framework.authtoken.models import Token\n\n\nclass AuthTokenHandler:\n    \n    @staticmethod\n    def expired_token(auth_token):\n        \n        utc_now = timezone.now()\n        expired = auth_token.created < utc_now - \\\n            timezone.timedelta(hours=24)\n        return expired\n\n    @staticmethod\n    def create_auth_token(user):\n        \n        token, created = Token.objects.get_or_create(user=user)\n        if not created:\n            token.created = timezone.now()\n            token.save()\n        return token\n",
        "summary": "The `AuthTokenHandler` class provides methods to check if an authentication token has expired and to create or refresh a token for a given user using Django's timezone utilities and the REST framework's authtoken model."
    },
    {
        "code": "from .useful_functions import get_ngrams, words_to_ngrams_list, remove_hook_words, remove_words\n\nfrom .transformers import phrases_transform, phrases2lower, phrases_without_excess_symbols\n\nfrom .tokenizers import text2sentences, split_by_words, sentence_split\n\nfrom .stemlem_operators import create_stemmer_lemmer, create_stemmer, create_lemmatizer\n\nfrom .pipeline import StemLemPipeline\n\nfrom .simplifiers import sum_phrases, wordlist2set\n\nfrom .stopwords import stopwords\n\nfrom .metrics import Levenstein\n\n\n",
        "summary": "The Python code imports various functions and classes from different modules for natural language processing tasks such as text tokenization, stemming, lemmatization, phrase transformation, and metric calculations like Levenshtein distance. It also includes utilities for removing specific words or hooks, converting phrases to lowercase, and simplifying word lists."
    },
    {
        "code": "import dependency_checker\nimport dependency_installer\nimport dependency_updater\nimport logger\nfrom rendering import VortexWindow\n\n\nimport pyglet\nimport sys\n\n\nif sys.version_info < (3, 6):  \n    logger.critical(\n        \"Vortex\", \"Python version is too old. Please use python 3.6 or higher.\")\n    sys.exit(1)\n\n\nif not dependency_checker.check_deps():  \n    dependency_installer.install_deps()  \n    if not dependency_checker.check_deps():  \n        \n        logger.warn(\n            \"Vortex\", \"Dependencies are not installed. Please install them manually.\")\n        sys.exit(1)\nelse:\n    dependency_updater.update_deps()  \n\nwindow = VortexWindow()  \npyglet.app.run()  \n",
        "summary": "The Python script initializes and manages dependencies for a graphical application using the `dependency_checker`, `dependency_installer`, and `dependency_updater` modules. It ensures compatibility with Python 3.6 or higher, logs critical errors if necessary, and runs a `VortexWindow` using the `pyglet` library after handling dependencies."
    },
    {
        "code": "import pytorch_transformers as pt\n\nfrom flambe.nlp.transformers.utils import TransformerTextField, TransformerEmbedder\n\n\nclass GPTTextField(TransformerTextField):\n    \n\n    _cls = pt.OpenAIGPTTokenizer\n\n\nclass GPTEmbedder(TransformerEmbedder):\n    \n\n    _cls = pt.OpenAIGPTModel\n\n\nclass GPT2TextField(TransformerTextField):\n    \n\n    _cls = pt.GPT2Tokenizer\n\n\nclass GPT2Embedder(TransformerEmbedder):\n    \n\n    _cls = pt.GPT2Model\n",
        "summary": "The provided Python code defines classes for handling text fields and embeddings using the OpenAI's GPT-1 (GPTTextField, GPTEmbedder) and GPT-2 models (GPT2TextField, GPT2Embedder), leveraging the `pytorch_transformers` library to utilize pre-trained transformer models."
    },
    {
        "code": "from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport tensorflow.keras as keras\n\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\n\n\nclass LSTMSeq2Seq(BaseModel):\n\n    def __init__(self, check_optional_config=True, future_seq_len=2):\n        \n        self.model = None\n        self.past_seq_len = None\n        self.future_seq_len = future_seq_len\n        self.feature_num = None\n        self.target_col_num = None\n        self.metric = None\n        self.latent_dim = None\n        self.batch_size = None\n        self.check_optional_config = check_optional_config\n\n    def _build_train(self, mc=False, **config):\n        \n        super()._check_config(**config)\n        self.metric = config.get('metric', 'mean_squared_error')\n        self.latent_dim = config.get('latent_dim', 128)\n        self.dropout = config.get('dropout', 0.2)\n        self.lr = config.get('lr', 0.001)\n        \n        self.batch_size = config.get('batch_size', 64)\n        training = True if mc else None\n\n        \n        self.encoder_inputs = Input(shape=(None, self.feature_num), name=\"encoder_inputs\")\n        encoder = LSTM(units=self.latent_dim,\n                       dropout=self.dropout,\n                       return_state=True,\n                       name=\"encoder_lstm\")\n        encoder_outputs, state_h, state_c = encoder(self.encoder_inputs, training=training)\n        \n        self.encoder_states = [state_h, state_c]\n\n        \n        self.decoder_inputs = Input(shape=(None, self.target_col_num), name=\"decoder_inputs\")\n        \n        \n        \n        self.decoder_lstm = LSTM(self.latent_dim,\n                                 dropout=self.dropout,\n                                 return_sequences=True,\n                                 return_state=True,\n                                 name=\"decoder_lstm\")\n        decoder_outputs, _, _ = self.decoder_lstm(self.decoder_inputs,\n                                                  training=training,\n                                                  initial_state=self.encoder_states)\n\n        self.decoder_dense = Dense(self.target_col_num, name=\"decoder_dense\")\n        decoder_outputs = self.decoder_dense(decoder_outputs)\n\n        \n        \n        self.model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n        self.model.compile(loss='mse',\n                           metrics=[self.metric],\n                           optimizer=keras.optimizers.RMSprop(lr=self.lr))\n        return self.model\n\n    def _restore_model(self):\n        self.encoder_inputs = self.model.input[0]  \n        encoder_outputs, state_h_enc, state_c_enc = self.model.layers[2].output  \n        self.encoder_states = [state_h_enc, state_c_enc]\n\n        self.decoder_inputs = self.model.input[1]  \n        self.decoder_lstm = self.model.layers[3]\n\n        self.decoder_dense = self.model.layers[4]\n\n    def _build_inference(self, mc=False):\n        training = True if mc else None\n        \n        encoder_model = Model(self.encoder_inputs, self.encoder_states)\n\n        \n        \n        \n        decoder_state_input_h = Input(shape=(self.latent_dim,))\n        decoder_state_input_c = Input(shape=(self.latent_dim,))\n        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n        decoder_outputs, state_h, state_c = self.decoder_lstm(self.decoder_inputs,\n                                                              training=training,\n                                                              initial_state=decoder_states_inputs)\n        decoder_states = [state_h, state_c]\n\n        decoder_outputs = self.decoder_dense(decoder_outputs)\n        decoder_model = Model([self.decoder_inputs] + decoder_states_inputs,\n                              [decoder_outputs] + decoder_states)\n        return encoder_model, decoder_model\n\n    def _decode_sequence(self, input_seq, mc=False):\n        encoder_model, decoder_model = self._build_inference(mc=mc)\n        \n        states_value = encoder_model.predict(input_seq)\n\n        \n        target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n\n        \n        target_seq[:, 0] = input_seq[:, -1, :self.target_col_num]\n\n        \n        \n\n        decoded_seq = np.zeros((len(input_seq), self.future_seq_len, self.target_col_num))\n\n        for i in range(self.future_seq_len):\n            output, h, c = decoder_model.predict([target_seq] + states_value)\n\n            decoded_seq[:, i] = output[:, 0]\n\n            \n            target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n            target_seq[:, 0] = output[:, 0]\n\n            \n            states_value = [h, c]\n\n        return decoded_seq\n\n    def _get_decoder_inputs(self, x, y):\n        \n        decoder_input_data = np.zeros(y.shape)\n        decoder_input_data[1:, ] = y[:-1, ]\n        decoder_input_data[0, 0] = x[-1, -1, :self.target_col_num]\n        decoder_input_data[0, 1:] = y[0, :-1]\n\n        return decoder_input_data\n\n    def _get_len(self, x, y):\n        self.past_seq_len = x.shape[1]\n        self.feature_num = x.shape[2]\n        \n        self.target_col_num = y.shape[2]\n\n    def _expand_y(self, y):\n        \n        while len(y.shape) < 3:\n            y = np.expand_dims(y, axis=2)\n        return y\n\n    def _pre_processing(self, x, y, validation_data):\n        \n        y = self._expand_y(y)\n        self._get_len(x, y)\n        decoder_input_data = self._get_decoder_inputs(x, y)\n        if validation_data is not None:\n            val_x, val_y = validation_data\n            val_y = self._expand_y(val_y)\n            val_decoder_input = self._get_decoder_inputs(val_x, val_y)\n            validation_data = ([val_x, val_decoder_input], val_y)\n        return x, y, decoder_input_data, validation_data\n\n    def fit_eval(self, data, validation_data=None, mc=False, verbose=0, **config):\n        \n        x, y = data[0], data[1]\n        x, y, decoder_input_data, validation_data = self._pre_processing(x, y, validation_data)\n\n        \n        if self.model is None:\n            self._build_train(mc=mc, **config)\n\n        \n        \n        \n        \n        \n\n        hist = self.model.fit([x, decoder_input_data], y,\n                              validation_data=validation_data,\n                              batch_size=self.batch_size,\n                              epochs=config.get(\"epochs\", 10),\n                              verbose=verbose,\n                              \n                              )\n        \n\n        if validation_data is None:\n            \n            \n            result = hist.history.get(self.metric)[-1]\n        else:\n            result = hist.history.get('val_' + str(self.metric))[-1]\n        return result\n\n    def evaluate(self, x, y, metric=['mse']):\n        \n        y_pred = self.predict(x)\n        \n        if self.target_col_num == 1:\n            return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n        else:\n            return [np.array([Evaluator.evaluate(m, y[:, i, :], y_pred[:, i, :])\n                              for i in range(self.future_seq_len)])\n                    for m in metric]\n\n    def predict(self, x, mc=False):\n        \n        y_pred = self._decode_sequence(x, mc=mc)\n        if self.target_col_num == 1:\n            y_pred = np.squeeze(y_pred, axis=2)\n        return y_pred\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        result = np.array([self.predict(x, mc=True) for i in range(n_iter)])\n        prediction = result.mean(axis=0)\n        uncertainty = result.var(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        \n\n        self.model.save(model_path)\n\n        config_to_save = {\"past_seq_len\": self.past_seq_len,\n                          \"feature_num\": self.feature_num,\n                          \"future_seq_len\": self.future_seq_len,\n                          \"target_col_num\": self.target_col_num,\n                          \"metric\": self.metric,\n                          \"latent_dim\": self.latent_dim,\n                          \"batch_size\": self.batch_size}\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        \n\n        self.past_seq_len = config[\"past_seq_len\"]\n        self.feature_num = config[\"feature_num\"]\n        self.future_seq_len = config[\"future_seq_len\"]\n        self.target_col_num = config[\"target_col_num\"]\n        self.metric = config[\"metric\"]\n        self.latent_dim = config[\"latent_dim\"]\n        self.batch_size = config[\"batch_size\"]\n\n        self.model = keras.models.load_model(model_path)\n        self._restore_model()\n        \n\n    def _get_required_parameters(self):\n        return {\n            \n            \n            \n        }\n\n    def _get_optional_parameters(self):\n        return {\n            'past_seq_len'\n            'latent_dim'\n            'dropout',\n            'metric',\n            'lr',\n            'epochs',\n            'batch_size'\n        }\n",
        "summary": "The `LSTMSeq2Seq` class is a custom model that extends `BaseModel` from the Zoo library, implementing an LSTM-based sequence-to-sequence architecture for time series forecasting. It includes methods for building and training the model, handling inference, decoding sequences, and evaluating predictions."
    },
    {
        "code": "from src.layers.LayerHelper import *\nfrom settings import LayerSettings as layerSettings\nimport tensorflow as tf\nimport os\nCUDA_VISIBLE_DEVICES=0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n\ndef LSTM(name_, inputTensor_, numberOfOutputs_, isTraining_, dropoutProb_=None):\n\twith tf.name_scope(name_):\n\t\tcell = tf.nn.rnn_cell.LSTMCell(num_units=numberOfOutputs_,\n\t\t\t\t\t\t use_peepholes=True,\n\t\t\t\t\t\t initializer=layerSettings.LSTM_INITIALIZER,\n\t\t\t\t\t\t forget_bias=1.0,\n\t\t\t\t\t\t state_is_tuple=True,\n\t\t\t\t\t\t activation=tf.nn.tanh,\n\t\t\t\t\t\t name=name_+\"_cell\")\n\n\t\tif dropoutProb_ != None:\n\t\t\tdropoutProbTensor = tf.cond(isTraining_, lambda: 0.5, lambda: 1.0)\n\t\t\tcell = tf.nn.rnn_cell.DropoutWrapper(cell,\n\t\t\t\t\t\t\t     input_keep_prob=dropoutProbTensor,\n\t\t\t\t\t\t\t     output_keep_prob=dropoutProbTensor)\n\n\t\tstatePlaceHolder = tf.nn.rnn_cell.LSTMStateTuple( tf.placeholder(layerSettings.FLOAT_TYPE, [None, numberOfOutputs_]),\n\t\t\t\t\t\t\t\t  tf.placeholder(layerSettings.FLOAT_TYPE, [None, numberOfOutputs_]) )\n\n\t\toutputTensor, stateTensor = tf.nn.dynamic_rnn(\tcell=cell,\n\t\t\t\t\t\t\t\tinitial_state=statePlaceHolder,\n\t\t\t\t\t\t\t\tinputs=inputTensor_)\n\n\t\t\n\t\tfor eachVariable in tf.trainable_variables():\n\t\t\tif name_ in eachVariable.name:\n\t\t\t\tif ('bias' not in eachVariable.name)and(layerSettings.REGULARIZER_WEIGHTS_DECAY != None):\n\t\t\t\t\tregularizationLoss = L2_Regularizer(eachVariable)\n\t\t\t\t\ttf.losses.add_loss(regularizationLoss, loss_collection=tf.GraphKeys.REGULARIZATION_LOSSES)\n\t\t\t\t\t\n\n\treturn outputTensor, stateTensor, statePlaceHolder\n\n",
        "summary": "The provided Python code defines a function `LSTM` that constructs an LSTM cell using TensorFlow, optionally applies dropout during training, and includes L2 regularization for the trainable variables. It initializes the LSTM state placeholders and returns the output tensor, final state tensor, and initial state placeholder."
    },
    {
        "code": "import wx\nimport re\nimport string\n\n\n\n\n\nhelp_pat    = re.compile( r'(.*){(.*)}(.*)' )\noptions_pat = re.compile( r'(.*)\\[(.*)\\](.*)' )\n\nkey_map = {\n    'F1':  wx.WXK_F1,\n    'F2':  wx.WXK_F2,\n    'F3':  wx.WXK_F3,\n    'F4':  wx.WXK_F4,\n    'F5':  wx.WXK_F5,\n    'F6':  wx.WXK_F6,\n    'F7':  wx.WXK_F7,\n    'F8':  wx.WXK_F8,\n    'F9':  wx.WXK_F9,\n    'F10': wx.WXK_F10,\n    'F11': wx.WXK_F11,\n    'F12': wx.WXK_F12\n}\n\n\n\n\n\nclass MakeMenu:\n\n    \n    cur_id = 1000\n\n    \n    \n    \n\n    def __init__ ( self, desc, owner, popup = False, window = None ):\n        \n        self.owner = owner\n        if window is None:\n            window = owner\n        self.window   = window\n        self.indirect = getattr( owner, 'call_menu', None )\n        self.names    = {}\n        self.desc     = desc.split( '\\n' )\n        self.index    = 0\n        self.keys     = []\n        if popup:\n            self.menu = menu = wx.Menu()\n            self.parse( menu, -1 )\n        else:\n            self.menu = menu = wx.MenuBar()\n            self.parse( menu, -1 )\n            window.SetMenuBar( menu )\n            if len( self.keys ) > 0:\n                 window.SetAcceleratorTable( wx.AcceleratorTable( self.keys ) )\n\n    \n    \n    \n\n    def parse ( self, menu, indent ):\n        \n\n        while True:\n\n            \n            if self.index >= len( self.desc ):\n                return\n\n            \n            dline    = self.desc[ self.index ]\n            line     = dline.lstrip()\n            indented = len( dline ) - len( line )\n            if indented <= indent:\n                return\n\n            \n            self.index += 1\n\n            \n            if (line == '') or (line[0:1] == '\n                continue\n\n            \n            if line[0:1] == '-':\n                menu.AppendSeparator()\n                continue\n\n            \n            MakeMenu.cur_id += 1\n            cur_id = MakeMenu.cur_id\n\n            \n            help  = ''\n            match = help_pat.search( line )\n            if match:\n                help = ' ' + match.group(2).strip()\n                line = match.group(1) + match.group(3)\n\n            \n            col = line.find( ':' )\n            if col >= 0:\n                handler = line[ col + 1: ].strip()\n                if handler != '':\n                    if self.indirect:\n                        self.indirect( cur_id, handler )\n                        handler = self.indirect\n                    else:\n                        try:\n                            exec ('def handler(event,self=self.owner):\\n %s\\n' %\n                                  handler)\n                        except:\n                            handler = null_handler\n                else:\n                    try:\n                        exec 'def handler(event,self=self.owner):\\n%s\\n' % (\n                            self.get_body( indented ), ) in globals()\n                    except:\n                        handler = null_handler\n                wx.EVT_MENU( self.window, cur_id, handler )\n                not_checked = checked = disabled = False\n                line        = line[ : col ]\n                match       = options_pat.search( line )\n                if match:\n                    line = match.group(1) + match.group(3)\n                    not_checked, checked, disabled, name = option_check( '~/-',\n                              match.group(2).strip() )\n                    if name != '':\n                        self.names[ name ] = cur_id\n                        setattr( self.owner, name, MakeMenuItem( self, cur_id ) )\n                label = line.strip()\n                col   = label.find( '|' )\n                if col >= 0:\n                    key   = label[ col + 1: ].strip()\n                    label = '%s%s%s' % ( label[ : col ].strip(), '\\t', key )\n                    key   = key.upper()\n                    flag  = wx.ACCEL_NORMAL\n                    col   = key.find( '-' )\n                    if col >= 0:\n                        flag = { 'CTRL':  wx.ACCEL_CTRL,\n                                 'SHIFT': wx.ACCEL_SHIFT,\n                                 'ALT':   wx.ACCEL_ALT\n                                 }.get( key[ : col ].strip(), wx.ACCEL_CTRL )\n                        key  = key[ col + 1: ].strip()\n                    code = key_map.get( key, None )\n                    try:\n                        if code is None:\n                            code = ord( key )\n                        self.keys.append(\n                            wx.AcceleratorEntry( flag, code, cur_id ) )\n                    except:\n                        pass\n                menu.Append( cur_id, label, help, not_checked or checked )\n                if checked:\n                    menu.Check( cur_id, True )\n                if disabled:\n                    menu.Enable( cur_id, False )\n                continue\n\n            \n            submenu = wx.Menu()\n            label   = line.strip()\n\n            \n            self.parse( submenu, indented )\n\n            \n            try:\n                menu.AppendMenu( cur_id, label, submenu, help )\n            except:\n                \n                \n                menu.Append( submenu, label )\n\n    \n    \n    \n\n    def get_body ( self, indent ):\n        \n        result = []\n        while self.index < len( self.desc ):\n            line = self.desc[ self.index ]\n            if (len( line ) - len( line.lstrip() )) <= indent:\n                break\n            result.append( line )\n            self.index += 1\n        result = '\\n'.join( result ).rstrip()\n        if result != '':\n            return result\n        return '  pass'\n\n    \n    \n    \n\n    def get_id ( self, name ):\n        \n        if isinstance(name, basestring):\n            return self.names[ name ]\n        return name\n\n    \n    \n    \n\n    def checked ( self, name, check = None ):\n        \n        if check is None:\n            return self.menu.IsChecked( self.get_id( name ) )\n        self.menu.Check( self.get_id( name ), check )\n\n    \n    \n    \n\n    def enabled ( self, name, enable = None ):\n        \n        if enable is None:\n            return self.menu.IsEnabled( self.get_id( name ) )\n        self.menu.Enable( self.get_id( name ), enable )\n\n    \n    \n    \n\n    def label ( self, name, label = None ):\n        \n        if label is None:\n            return self.menu.GetLabel( self.get_id( name ) )\n        self.menu.SetLabel( self.get_id( name ), label )\n\n\n\n\n\nclass MakeMenuItem:\n\n    def __init__ ( self, menu, id ):\n        self.menu = menu\n        self.id   = id\n\n    def checked ( self, check = None ):\n        return self.menu.checked( self.id, check )\n\n    def toggle ( self ):\n        checked = not self.checked()\n        self.checked( checked )\n        return checked\n\n    def enabled ( self, enable = None ):\n        return self.menu.enabled( self.id, enable )\n\n    def label ( self, label = None ):\n        return self.menu.label( self.id, label )\n\n\n\n\n\n\ndef option_check ( test, string ):\n    result = []\n    for char in test:\n        col = string.find( char )\n        result.append( col >= 0 )\n        if col >= 0:\n            string = string[ : col ] + string[ col + 1: ]\n    return result + [ string.strip() ]\n\n\n\n\n\ndef null_handler ( event ):\n    print 'null_handler invoked'\n\n",
        "summary": "The provided Python code defines a class `MakeMenu` for creating menus in a wxPython application, using a simple text-based description format. It also includes a helper class `MakeMenuItem` to manage individual menu items and functions to handle menu options and accelerators. The code uses regular expressions to parse the menu descriptions and map keyboard shortcuts to specific actions."
    },
    {
        "code": "import cStringIO as StringIO\nimport struct\n\nclass request_t(object):\n    __slots__ = [\"utime\"]\n\n    def __init__(self):\n        self.utime = 0\n\n    def encode(self):\n        buf = StringIO.StringIO()\n        buf.write(request_t._get_packed_fingerprint())\n        self._encode_one(buf)\n        return buf.getvalue()\n\n    def _encode_one(self, buf):\n        buf.write(struct.pack(\">q\", self.utime))\n\n    def decode(data):\n        if hasattr(data, 'read'):\n            buf = data\n        else:\n            buf = StringIO.StringIO(data)\n        if buf.read(8) != request_t._get_packed_fingerprint():\n            raise ValueError(\"Decode error\")\n        return request_t._decode_one(buf)\n    decode = staticmethod(decode)\n\n    def _decode_one(buf):\n        self = request_t()\n        self.utime = struct.unpack(\">q\", buf.read(8))[0]\n        return self\n    _decode_one = staticmethod(_decode_one)\n\n    _hash = None\n    def _get_hash_recursive(parents):\n        if request_t in parents: return 0\n        tmphash = (0xa686a0e0f882d897) & 0xffffffffffffffff\n        tmphash  = (((tmphash<<1)&0xffffffffffffffff)  + (tmphash>>63)) & 0xffffffffffffffff\n        return tmphash\n    _get_hash_recursive = staticmethod(_get_hash_recursive)\n    _packed_fingerprint = None\n\n    def _get_packed_fingerprint():\n        if request_t._packed_fingerprint is None:\n            request_t._packed_fingerprint = struct.pack(\">Q\", request_t._get_hash_recursive([]))\n        return request_t._packed_fingerprint\n    _get_packed_fingerprint = staticmethod(_get_packed_fingerprint)\n\n",
        "summary": "The provided Python code defines a class `request_t` that handles encoding and decoding of a single attribute `utime` using the `struct` module for binary data serialization. It includes methods to pack and unpack the `utime` value, ensuring data integrity during transmission or storage."
    },
    {
        "code": "from __future__ import print_function\nimport os\nimport textwrap\n\napplianceSelf = os.environ['TOIL_APPLIANCE_SELF']\nsdistName = os.environ['_TOIL_SDIST_NAME']\n\n\ndependencies = ' '.join(['libffi-dev',  \n                         'python3.6',\n                         'python3.6-dev',\n                         'python-dev',  \n                         'python-pip',  \n                         'python3-pip',\n                         'libcurl4-openssl-dev',\n                         'libssl-dev',\n                         'wget',\n                         'curl',\n                         'openssh-server',\n                         'mesos=1.0.1-2.0.94.ubuntu1604',\n                         \"nodejs\",  \n                         'rsync',\n                         'screen',\n                         'build-essential', \n                         'uuid-dev',\n                         'libgpgme11-dev',\n                         'libseccomp-dev',\n                         'pkg-config',\n                         'squashfs-tools',\n                         'cryptsetup',\n                         'git'])\n\n\ndef heredoc(s):\n    s = textwrap.dedent(s).format(**globals())\n    return s[1:] if s.startswith('\\n') else s\n\n\nmotd = heredoc()\n\n\nmotd = ''.join(l + '\\\\n\\\\\\n' for l in motd.splitlines())\n\nprint(heredoc())\n",
        "summary": "The Python script sets environment variables and defines a list of dependencies, then formats and prints a message of the day (MOTD) using a custom heredoc function."
    },
    {
        "code": "from unittest import mock\n\nimport pytest\n\nfrom tools.ci.tc import decision\n\n\n@pytest.mark.parametrize(\"run_jobs,tasks,expected\", [\n    ([], {\"task-no-schedule-if\": {}}, [\"task-no-schedule-if\"]),\n    ([], {\"task-schedule-if-no-run-job\": {\"schedule-if\": {}}}, []),\n    ([\"job\"],\n     {\"job-present\": {\"schedule-if\": {\"run-job\": [\"other-job\", \"job\"]}}},\n     [\"job-present\"]),\n    ([\"job\"], {\"job-missing\": {\"schedule-if\": {\"run-job\": [\"other-job\"]}}}, []),\n    ([\"all\"], {\"job-all\": {\"schedule-if\": {\"run-job\": [\"other-job\"]}}}, [\"job-all\"]),\n    ([\"job\"],\n     {\"job-1\": {\"schedule-if\": {\"run-job\": [\"job\"]}},\n      \"job-2\": {\"schedule-if\": {\"run-job\": [\"other-job\"]}}},\n     [\"job-1\"]),\n])\ndef test_filter_schedule_if(run_jobs, tasks, expected):\n    with mock.patch(\"tools.ci.tc.decision.get_run_jobs\",\n                    return_value=run_jobs) as get_run_jobs:\n        assert (decision.filter_schedule_if({}, tasks) ==\n                {name: tasks[name] for name in expected})\n        get_run_jobs.call_count in (0, 1)\n\n\n@pytest.mark.parametrize(\"msg,expected\", [\n    (\"Some initial line\\n\\ntc-jobs:foo,bar\", {\"foo\", \"bar\"}),\n    (\"Some initial line\\n\\ntc-jobs:foo, bar\", {\"foo\", \"bar\"}),\n    (\"tc-jobs:foo, bar   \\nbaz\", {\"foo\", \"bar\"}),\n    (\"tc-jobs:all\", {\"all\"}),\n    (\"\", set()),\n    (\"tc-jobs:foo\\ntc-jobs:bar\", {\"foo\"})])\n@pytest.mark.parametrize(\"event\", [\n    {\"commits\": [{\"message\": \"<message>\"}]},\n    {\"pull_request\": {\"body\": \"<message>\"}}\n])\ndef test_extra_jobs_pr(msg, expected, event):\n    def sub(obj):\n        \n        if isinstance(obj, dict):\n            return {key: sub(value) for (key, value) in obj.items()}\n        elif isinstance(obj, list):\n            return [sub(value) for value in obj]\n        elif obj == \"<message>\":\n            return msg\n        return obj\n\n    event = sub(event)\n\n    assert decision.get_extra_jobs(event) == expected\n",
        "summary": "The provided Python code includes two test functions using the `pytest` framework and the `unittest.mock` library. The first function, `test_filter_schedule_if`, tests a method that filters tasks based on whether certain jobs are scheduled to run. The second function, `test_extra_jobs_pr`, checks if it correctly extracts job names from commit messages or pull request bodies."
    },
    {
        "code": "eazypath = '/data2/ken/photoz/eazy-photoz/src/eazy '\nworking_folder = '/data2/ken/EN1_pani'\nphotometry_catalog = 'en1_phot_with_zspec.fits'\nphotometry_format = 'fits'\n\nfilter_file = 'EN1_filters.res'\ntranslate_file = 'EN1.translate'\n\nzspec_col = 'z_spec'\n\nflux_col = 'flux'\nfluxerr_col ='fluxerr'\n\ndo_zp = False\ndo_zp_tests = False\ndo_subcats = False\n\ndo_full = False\ndo_stellar = False\ndo_hb = True\ndo_merge = True\n\n\nNcrossval = 1\ntest_fraction = 0.2\n\nprocess_outliers = True\ncorrect_extinction = True\n\n\n\n\ntemplates = ['eazy', 'atlas', 'cosmos']\nfitting_mode = ['a', '1', '1']\n\ndefaults = ['defaults/zphot.eazy',\n            'defaults/zphot.atlas',\n            'defaults/zphot.cosmos']\n            \n            \n            \n\nstellar_params = 'defaults/zphot.pickles'\n\nadditional_errors = [0.0, 0.0, 0.0]\ntemplate_error_norm = [1., 1., 1.]\ntemplate_error_file = ''\nlambda_fit_max = [5., 30., 30.]\n\n\n\n\ninclude_prior = True\nfbad_prior = 'mag' \n\nprior_fname = 'pani_mag'\nprior_colname = 'pani_mag'\nalpha_colname = 'pani_mag'\n\n\n\n\nblock_size = 1e4\nncpus = 10\n",
        "summary": "The provided Python code configures parameters for a photometric analysis pipeline, including paths to data and configuration files, columns of interest in the photometry catalog, options for various processing steps such as zero-point calibration, stellar and Hubble parameter fitting, merging templates, and outlier correction. It also specifies settings for cross-validation, extinction correction, and parallel processing."
    },
    {
        "code": "from bfstpw.models import ForumThread, ForumPost\nfrom datetime import datetime\nfrom django.conf import settings\nfrom django.core.paginator import Paginator\nfrom django.core.urlresolvers import reverse\nfrom django.db.models import Count\nfrom django.http import HttpResponseRedirect\nfrom django.shortcuts import render, get_object_or_404\nfrom django.template import Context\nimport django.utils.timezone\nfrom markdown import markdown\n\ndef threadlist(request):\n    c = Context({\"threadlist\" :\n        [{\"id\":t.id\n         ,\"name\":t.thread_title\n         ,\"poster\":t.getOriginalPost().poster\n         ,\"replycount\":t.postcount\n         ,\"lastpage\":(t.postcount / getattr(settings, 'BFSTPW_MAX_POSTS_PER_PAGE', 20))+1\n         ,\"date\":t.mostrecent\n         ,\"lastposter\":t.getLatestPost().poster\n         } for t in\n            ForumThread.objects.sortByLastPost().annotate(postcount=Count('forumpost'))]})\n\n    return render(request, 'bfstpw/threadlist.html', c)\n\ndef thread(request, thread_id, message=''):\n    current_thread = get_object_or_404(ForumThread, id=thread_id)\n    max_posts_per_page = getattr(settings, 'BFSTPW_MAX_POSTS_PER_PAGE', 20)\n    paginator = Paginator(current_thread.forumpost_set.order_by('date_posted'), max_posts_per_page)\n    c = Context(\n            {\"threadlist\" :\n                [{\"id\":t.id\n                 ,\"name\":t.thread_title\n                 } for t in ForumThread.objects.sortByLastPost()],\n             \"thread\" : current_thread,\n             \"posts\" : paginator.page(request.GET.get('page',1)),\n             \"pages\" : paginator.page_range,\n             \"message\" : message\n            })\n           \n    return render(request, 'bfstpw/thread.html', c)\n\ndef post(request, thread_id):\n    t = get_object_or_404(ForumThread, id=thread_id)\n\n    posts = t.forumpost_set\n    \n    message_html = markdown(request.POST['message'], safe_mode='escape')\n    posts.create(poster=request.POST['name'], message_body=message_html,\n            date_posted=django.utils.timezone.now())\n    pagenum = (posts.count() / getattr(settings, 'BFSTPW_MAX_POSTS_PER_PAGE', 20))+1\n    return HttpResponseRedirect(reverse('bfstpw-thread', args=(t.id,))+'?page=%d' % pagenum)\n\ndef newthreadmake(request):\n    t = ForumThread(thread_title=request.POST['threadname'])\n    t.save()\n    message_html = markdown(request.POST['message'], safe_mode='escape')\n    t.forumpost_set.create(poster=request.POST['name'],\n            message_body=message_html,\n            date_posted=django.utils.timezone.now())\n\n    return HttpResponseRedirect(reverse('bfstpw-thread', args=(t.id,)))\n",
        "summary": "The provided Python code defines views for a forum application using Django. The `threadlist` view renders a list of threads with details such as ID, title, poster, reply count, and last post information. The `thread` view handles displaying individual threads, including pagination for posts, and allows users to add new posts or messages. The `post` function adds a new post to an existing thread and redirects the user back to the thread page with the appropriate page number. Finally, the `newthreadmake` function creates a new thread and its first post based on user input."
    },
    {
        "code": "from lib.utils.util import *\n\nfrom timm.models.efficientnet_blocks import *\n\n\nclass ChildNetBuilder:\n    def __init__(\n            self,\n            channel_multiplier=1.0,\n            channel_divisor=8,\n            channel_min=None,\n            output_stride=32,\n            pad_type='',\n            act_layer=None,\n            se_kwargs=None,\n            norm_layer=nn.BatchNorm2d,\n            norm_kwargs=None,\n            drop_path_rate=0.,\n            feature_location='',\n            verbose=False,\n            logger=None):\n        self.channel_multiplier = channel_multiplier\n        self.channel_divisor = channel_divisor\n        self.channel_min = channel_min\n        self.output_stride = output_stride\n        self.pad_type = pad_type\n        self.act_layer = act_layer\n        self.se_kwargs = se_kwargs\n        self.norm_layer = norm_layer\n        self.norm_kwargs = norm_kwargs\n        self.drop_path_rate = drop_path_rate\n        self.feature_location = feature_location\n        assert feature_location in ('pre_pwl', 'post_exp', '')\n        self.verbose = verbose\n        self.in_chs = None\n        self.features = OrderedDict()\n        self.logger = logger\n\n    def _round_channels(self, chs):\n        return round_channels(\n            chs,\n            self.channel_multiplier,\n            self.channel_divisor,\n            self.channel_min)\n\n    def _make_block(self, ba, block_idx, block_count):\n        drop_path_rate = self.drop_path_rate * block_idx / block_count\n        bt = ba.pop('block_type')\n        ba['in_chs'] = self.in_chs\n        ba['out_chs'] = self._round_channels(ba['out_chs'])\n        if 'fake_in_chs' in ba and ba['fake_in_chs']:\n            ba['fake_in_chs'] = self._round_channels(ba['fake_in_chs'])\n        ba['norm_layer'] = self.norm_layer\n        ba['norm_kwargs'] = self.norm_kwargs\n        ba['pad_type'] = self.pad_type\n        \n        ba['act_layer'] = ba['act_layer'] if ba['act_layer'] is not None else self.act_layer\n        assert ba['act_layer'] is not None\n        if bt == 'ir':\n            ba['drop_path_rate'] = drop_path_rate\n            ba['se_kwargs'] = self.se_kwargs\n            if self.verbose:\n                self.logger.info(\n                    '  InvertedResidual {}, Args: {}'.format(\n                        block_idx, str(ba)))\n            block = InvertedResidual(**ba)\n        elif bt == 'ds' or bt == 'dsa':\n            ba['drop_path_rate'] = drop_path_rate\n            ba['se_kwargs'] = self.se_kwargs\n            if self.verbose:\n                self.logger.info(\n                    '  DepthwiseSeparable {}, Args: {}'.format(\n                        block_idx, str(ba)))\n            block = DepthwiseSeparableConv(**ba)\n        elif bt == 'cn':\n            if self.verbose:\n                self.logger.info(\n                    '  ConvBnAct {}, Args: {}'.format(\n                        block_idx, str(ba)))\n            block = ConvBnAct(**ba)\n        else:\n            assert False, 'Uknkown block type (%s) while building model.' % bt\n        self.in_chs = ba['out_chs']  \n\n        return block\n\n    def __call__(self, in_chs, model_block_args):\n        \n        if self.verbose:\n            self.logger.info(\n                'Building model trunk with %d stages...' %\n                len(model_block_args))\n        self.in_chs = in_chs\n        total_block_count = sum([len(x) for x in model_block_args])\n        total_block_idx = 0\n        current_stride = 2\n        current_dilation = 1\n        feature_idx = 0\n        stages = []\n        \n        \n        for stage_idx, stage_block_args in enumerate(model_block_args):\n            last_stack = stage_idx == (len(model_block_args) - 1)\n            if self.verbose:\n                self.logger.info('Stack: {}'.format(stage_idx))\n            assert isinstance(stage_block_args, list)\n\n            blocks = []\n            \n            for block_idx, block_args in enumerate(stage_block_args):\n                last_block = block_idx == (len(stage_block_args) - 1)\n                extract_features = ''  \n                if self.verbose:\n                    self.logger.info(' Block: {}'.format(block_idx))\n\n                \n                assert block_args['stride'] in (1, 2)\n                if block_idx >= 1:\n                    \n                    block_args['stride'] = 1\n\n                do_extract = False\n                if self.feature_location == 'pre_pwl':\n                    if last_block:\n                        next_stage_idx = stage_idx + 1\n                        if next_stage_idx >= len(model_block_args):\n                            do_extract = True\n                        else:\n                            do_extract = model_block_args[next_stage_idx][0]['stride'] > 1\n                elif self.feature_location == 'post_exp':\n                    if block_args['stride'] > 1 or (last_stack and last_block):\n                        do_extract = True\n                if do_extract:\n                    extract_features = self.feature_location\n\n                next_dilation = current_dilation\n                if block_args['stride'] > 1:\n                    next_output_stride = current_stride * block_args['stride']\n                    if next_output_stride > self.output_stride:\n                        next_dilation = current_dilation * block_args['stride']\n                        block_args['stride'] = 1\n                        if self.verbose:\n                            self.logger.info(\n                                '  Converting stride to dilation to maintain output_stride=={}'.format(\n                                    self.output_stride))\n                    else:\n                        current_stride = next_output_stride\n                block_args['dilation'] = current_dilation\n                if next_dilation != current_dilation:\n                    current_dilation = next_dilation\n\n                \n                block = self._make_block(\n                    block_args, total_block_idx, total_block_count)\n                blocks.append(block)\n\n                \n                \n                if extract_features:\n                    feature_module = block.feature_module(extract_features)\n                    if feature_module:\n                        feature_module = 'blocks.{}.{}.'.format(\n                            stage_idx, block_idx) + feature_module\n                    feature_channels = block.feature_channels(extract_features)\n                    self.features[feature_idx] = dict(\n                        name=feature_module,\n                        num_chs=feature_channels\n                    )\n                    feature_idx += 1\n\n                \n                total_block_idx += 1\n            stages.append(nn.Sequential(*blocks))\n        return stages\n",
        "summary": "The `ChildNetBuilder` class is a custom model builder for creating neural network architectures, particularly focusing on efficientnet blocks. It dynamically constructs the network based on provided arguments and handles various parameters such as channel multipliers, output strides, and block types, ultimately returning a list of sequential blocks that form the model trunk."
    },
    {
        "code": "import json\nimport logging\nfrom hashlib import sha256\nimport urlparse\n\nfrom odoo import models, fields, api\nfrom odoo.tools.float_utils import float_compare\nfrom odoo.tools.translate import _\nfrom odoo.addons.payment.models.payment_acquirer import ValidationError\nfrom odoo.addons.payment_sips.controllers.main import SipsController\n\n_logger = logging.getLogger(__name__)\n\n\nCURRENCY_CODES = {\n    'EUR': '978',\n    'USD': '840',\n    'CHF': '756',\n    'GBP': '826',\n    'CAD': '124',\n    'JPY': '392',\n    'MXN': '484',\n    'TRY': '949',\n    'AUD': '036',\n    'NZD': '554',\n    'NOK': '578',\n    'BRL': '986',\n    'ARS': '032',\n    'KHR': '116',\n    'TWD': '901',\n}\n\n\nclass AcquirerSips(models.Model):\n    _inherit = 'payment.acquirer'\n\n    provider = fields.Selection(selection_add=[('sips', 'Sips')])\n    sips_merchant_id = fields.Char('SIPS API User Password', required_if_provider='sips', groups='base.group_user')\n    sips_secret = fields.Char('SIPS Secret', size=64, required_if_provider='sips', groups='base.group_user')\n\n    def _get_sips_urls(self, environment):\n        \n        url = {\n            'prod': 'https://payment-webinit.sips-atos.com/paymentInit',\n            'test': 'https://payment-webinit.simu.sips-atos.com/paymentInit', }\n\n        return {'sips_form_url': url.get(environment, url['test']), }\n\n    def _sips_generate_shasign(self, values):\n        \n        if self.provider != 'sips':\n            raise ValidationError(_('Incorrect payment acquirer provider'))\n        data = values['Data']\n\n        \n        key = u'002001000000001_KEY1'\n\n        if self.environment == 'prod':\n            key = getattr(self, 'sips_secret')\n\n        shasign = sha256(data + key)\n        return shasign.hexdigest()\n\n    @api.multi\n    def sips_form_generate_values(self, values):\n        self.ensure_one()\n        base_url = self.env['ir.config_parameter'].sudo().get_param('web.base.url')\n        currency = self.env['res.currency'].sudo().browse(values['currency_id'])\n        currency_code = CURRENCY_CODES.get(currency.name, False)\n        if not currency_code:\n            raise ValidationError(_('Currency not supported by Wordline'))\n        amount = int(values['amount'] * 100)\n        if self.environment == 'prod':\n            \n            merchant_id = getattr(self, 'sips_merchant_id')\n            key_version = '2'\n        else:\n            \n            merchant_id = '002001000000001'\n            key_version = '1'\n\n        sips_tx_values = dict(values)\n        sips_tx_values.update({\n            'Data': u'amount=%s|' % amount +\n                    u'currencyCode=%s|' % currency_code +\n                    u'merchantId=%s|' % merchant_id +\n                    u'normalReturnUrl=%s|' % urlparse.urljoin(base_url, SipsController._return_url) +\n                    u'automaticResponseUrl=%s|' % urlparse.urljoin(base_url, SipsController._return_url) +\n                    u'transactionReference=%s|' % values['reference'] +\n                    u'statementReference=%s|' % values['reference'] +\n                    u'keyVersion=%s' % key_version,\n            'InterfaceVersion': 'HP_2.3',\n        })\n\n        return_context = {}\n        if sips_tx_values.get('return_url'):\n            return_context[u'return_url'] = u'%s' % sips_tx_values.pop('return_url')\n        return_context[u'reference'] = u'%s' % sips_tx_values['reference']\n        sips_tx_values['Data'] += u'|returnContext=%s' % (json.dumps(return_context))\n\n        shasign = self._sips_generate_shasign(sips_tx_values)\n        sips_tx_values['Seal'] = shasign\n        return sips_tx_values\n\n    @api.multi\n    def sips_get_form_action_url(self):\n        self.ensure_one()\n        return self._get_sips_urls(self.environment)['sips_form_url']\n\n\nclass TxSips(models.Model):\n    _inherit = 'payment.transaction'\n\n    _sips_valid_tx_status = ['00']\n    _sips_wait_tx_status = ['90', '99']\n    _sips_refused_tx_status = ['05', '14', '34', '54', '75', '97']\n    _sips_error_tx_status = ['03', '12', '24', '25', '30', '40', '51', '63', '94']\n    _sips_pending_tx_status = ['60']\n    _sips_cancel_tx_status = ['17']\n\n    \n    \n    \n\n    def _sips_data_to_object(self, data):\n        res = {}\n        for element in data.split('|'):\n            element_split = element.split('=')\n            res[element_split[0]] = element_split[1]\n        return res\n\n    @api.model\n    def _sips_form_get_tx_from_data(self, data):\n        \n\n        data = self._sips_data_to_object(data.get('Data'))\n        reference = data.get('transactionReference')\n\n        if not reference:\n            custom = json.loads(data.pop('returnContext', False) or '{}')\n            reference = custom.get('reference')\n\n        payment_tx = self.search([('reference', '=', reference)])\n        if not payment_tx or len(payment_tx) > 1:\n            error_msg = _('Sips: received data for reference %s') % reference\n            if not payment_tx:\n                error_msg += _('; no order found')\n            else:\n                error_msg += _('; multiple order found')\n            _logger.error(error_msg)\n            raise ValidationError(error_msg)\n        return payment_tx\n\n    @api.multi\n    def _sips_form_get_invalid_parameters(self, data):\n        invalid_parameters = []\n\n        data = self._sips_data_to_object(data.get('Data'))\n\n        \n        if self.acquirer_reference and data.get('transactionReference') != self.acquirer_reference:\n            invalid_parameters.append(('transactionReference', data.get('transactionReference'), self.acquirer_reference))\n        \n        if float_compare(float(data.get('amount', '0.0')) / 100, self.amount, 2) != 0:\n            invalid_parameters.append(('amount', data.get('amount'), '%.2f' % self.amount))\n        if self.partner_reference and data.get('customerId') != self.partner_reference:\n            invalid_parameters.append(('customerId', data.get('customerId'), self.partner_reference))\n\n        return invalid_parameters\n\n    @api.multi\n    def _sips_form_validate(self, data):\n        data = self._sips_data_to_object(data.get('Data'))\n        status = data.get('responseCode')\n        data = {\n            'acquirer_reference': data.get('transactionReference'),\n            'partner_reference': data.get('customerId'),\n            'date_validate': data.get('transactionDateTime',\n                                      fields.Datetime.now())\n        }\n        res = False\n        if status in self._sips_valid_tx_status:\n            msg = 'Payment for tx ref: %s, got response [%s], set as done.' % \\\n                  (self.reference, status)\n            _logger.info(msg)\n            data.update(state='done', state_message=msg)\n            res = True\n        elif status in self._sips_error_tx_status:\n            msg = 'Payment for tx ref: %s, got response [%s], set as ' \\\n                  'error.' % (self.reference, status)\n            data.update(state='error', state_message=msg)\n        elif status in self._sips_wait_tx_status:\n            msg = 'Received wait status for payment ref: %s, got response ' \\\n                  '[%s], set as error.' % (self.reference, status)\n            data.update(state='error', state_message=msg)\n        elif status in self._sips_refused_tx_status:\n            msg = 'Received refused status for payment ref: %s, got response' \\\n                  ' [%s], set as error.' % (self.reference, status)\n            data.update(state='error', state_message=msg)\n        elif status in self._sips_pending_tx_status:\n            msg = 'Payment ref: %s, got response [%s] set as pending.' \\\n                  % (self.reference, status)\n            data.update(state='pending', state_message=msg)\n        elif status in self._sips_cancel_tx_status:\n            msg = 'Received notification for payment ref: %s, got response ' \\\n                  '[%s], set as cancel.' % (self.reference, status)\n            data.update(state='cancel', state_message=msg)\n        else:\n            msg = 'Received unrecognized status for payment ref: %s, got ' \\\n                  'response [%s], set as error.' % (self.reference, status)\n            data.update(state='error', state_message=msg)\n\n        _logger.info(msg)\n        self.write(data)\n        return res\n",
        "summary": "This Python code defines Odoo models for handling transactions with an Sips payment gateway. It includes models for payment transactions and acquirers, providing methods to validate transaction statuses, extract data from response messages, and update transaction states accordingly. The code also handles error cases and logs relevant information for debugging and monitoring purposes."
    },
    {
        "code": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\n\nimport hypothesis as hp\nimport hypothesis.strategies as hps\nimport numpy as np\nfrom scipy import stats as sp_stats\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_probability.python.internal import hypothesis_testlib as tfp_hps\nfrom tensorflow_probability.python.internal import test_util\n\ntfd = tfp.distributions\n\n\n\n\n\n\n@hps.composite\ndef generalized_paretos(draw, batch_shape=None):\n  if batch_shape is None:\n    batch_shape = draw(tfp_hps.shapes())\n\n  constraints = dict(\n      loc=tfp_hps.identity_fn,\n      scale=tfp_hps.softplus_plus_eps(),\n      concentration=lambda x: tf.math.tanh(x) * 0.24)  \n\n  params = draw(\n      tfp_hps.broadcasting_params(\n          batch_shape,\n          params_event_ndims=dict(loc=0, scale=0, concentration=0),\n          constraint_fn_for=constraints.get))\n  dist = tfd.GeneralizedPareto(validate_args=draw(hps.booleans()), **params)\n  if dist.batch_shape != batch_shape:\n    raise AssertionError('batch_shape mismatch: expect {} but got {}'.format(\n        batch_shape, dist))\n  return dist\n\n\n@test_util.test_all_tf_execution_regimes\nclass GeneralizedParetoTest(test_util.TestCase):\n\n  @hp.given(generalized_paretos())\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testShape(self, dist):\n    \n    self.assertEqual(dist.batch_shape, self.evaluate(dist.batch_shape_tensor()))\n    self.assertEqual(tf.TensorShape([]), dist.event_shape)\n    self.assertAllEqual([], self.evaluate(dist.event_shape_tensor()))\n\n  @hp.given(generalized_paretos(batch_shape=[]))\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testLogPDF(self, dist):\n    xs = self.evaluate(dist.sample())\n\n    logp = dist.log_prob(xs)\n    self.assertEqual(dist.batch_shape, logp.shape)\n    p = dist.prob(xs)\n    self.assertEqual(dist.batch_shape, p.shape)\n\n    loc, scale, conc = self.evaluate([dist.loc, dist.scale, dist.concentration])\n    expected_logp = sp_stats.genpareto(conc, loc=loc, scale=scale).logpdf(xs)\n    actual_logp = self.evaluate(logp)\n    self.assertAllClose(expected_logp, actual_logp, rtol=1e-5)\n    self.assertAllClose(np.exp(expected_logp), self.evaluate(p), rtol=1e-5)\n\n  def testLogPDFBoundary(self):\n    \n    \n    scale = np.array([0.1, 0.5, 1., 2., 5., 10.], dtype=np.float32)\n    dist = tfd.GeneralizedPareto(loc=0, scale=scale, concentration=0)\n    log_pdf = dist.log_prob(0.)\n    self.assertAllClose(-np.log(scale), self.evaluate(log_pdf), rtol=1e-5)\n\n  @hp.given(generalized_paretos(batch_shape=[]))\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testCDF(self, dist):\n    xs = self.evaluate(dist.sample())\n    cdf = dist.cdf(xs)\n    self.assertEqual(dist.batch_shape, cdf.shape)\n\n    loc, scale, conc = self.evaluate([dist.loc, dist.scale, dist.concentration])\n    expected_cdf = sp_stats.genpareto(conc, loc=loc, scale=scale).cdf(xs)\n    self.assertAllClose(expected_cdf, self.evaluate(cdf), rtol=5e-5)\n\n  @hp.given(generalized_paretos(batch_shape=[]))\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testMean(self, dist):\n    loc, scale, conc = self.evaluate([dist.loc, dist.scale, dist.concentration])\n    self.assertEqual(dist.batch_shape, dist.mean().shape)\n    if np.abs(conc) < 1e-5 and conc != 0:\n      return  \n    expected = sp_stats.genpareto(conc, loc=loc, scale=scale).mean()\n    actual = self.evaluate(dist.mean())\n    self.assertAllClose(expected, actual, rtol=5e-4)\n\n  @hp.given(generalized_paretos(batch_shape=[]))\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testVariance(self, dist):\n    loc, scale, conc = self.evaluate([dist.loc, dist.scale, dist.concentration])\n    self.assertEqual(dist.batch_shape, dist.variance().shape)\n    expected = sp_stats.genpareto(conc, loc=loc, scale=scale).var()\n    if np.abs(conc) < 1e-4 and conc != 0:\n      return  \n    if expected <= 0:\n      return  \n    actual = self.evaluate(dist.variance())\n    print('var', loc, scale, conc, expected, actual, file=sys.stderr)\n    self.assertAllClose(expected, actual, rtol=.01)\n\n  @hp.given(generalized_paretos(batch_shape=[]))\n  @tfp_hps.tfp_hp_settings(default_max_examples=5)\n  def testEntropy(self, dist):\n    loc, scale, conc = self.evaluate([dist.loc, dist.scale, dist.concentration])\n    self.assertEqual(dist.batch_shape, dist.entropy().shape)\n    expected = sp_stats.genpareto.entropy(conc, loc=loc, scale=scale)\n    actual = self.evaluate(dist.entropy())\n    self.assertAllClose(expected, actual)\n\n  def testSample(self):\n    loc = np.float32(-7.5)\n    scale = np.float32(3.5)\n    conc = np.float32(0.07)\n    n = 100000\n    dist = tfd.GeneralizedPareto(loc=loc, scale=scale, concentration=conc)\n    samples = dist.sample(n, seed=test_util.test_seed())\n    sample_values = self.evaluate(samples)\n    self.assertEqual((n,), samples.shape)\n    self.assertEqual((n,), sample_values.shape)\n    self.assertTrue(self._kstest(loc, scale, conc, sample_values))\n    self.assertAllClose(\n        sp_stats.genpareto.mean(conc, loc=loc, scale=scale),\n        sample_values.mean(),\n        rtol=.005)\n    self.assertAllClose(\n        sp_stats.genpareto.var(conc, loc=loc, scale=scale),\n        sample_values.var(),\n        rtol=.01)\n\n  def testFullyReparameterized(self):\n    loc = tf.constant(4.0)\n    scale = tf.constant(3.0)\n    conc = tf.constant(2.0)\n    _, grads = tfp.math.value_and_gradient(\n        lambda *args: tfd.GeneralizedPareto(*args).sample(100),\n        [loc, scale, conc])\n    self.assertLen(grads, 3)\n    self.assertAllNotNone(grads)\n\n  def testSampleKolmogorovSmirnovMultiDimensional(self):\n    loc = np.linspace(-10, 10, 3).reshape(3, 1, 1)\n    scale = np.linspace(1e-6, 7, 5).reshape(5, 1)\n    conc = np.linspace(-1.3, 1.3, 7)\n\n    dist = tfd.GeneralizedPareto(loc=loc, scale=scale, concentration=conc)\n    n = 10000\n    samples = dist.sample(n, seed=test_util.test_seed())\n    sample_values = self.evaluate(samples)\n    self.assertEqual((n, 3, 5, 7), samples.shape)\n    self.assertEqual((n, 3, 5, 7), sample_values.shape)\n\n    fails = 0\n    trials = 0\n    for li, l in enumerate(loc.reshape(-1)):\n      for si, s in enumerate(scale.reshape(-1)):\n        for ci, c in enumerate(conc.reshape(-1)):\n          samps = sample_values[:, li, si, ci]\n          trials += 1\n          fails += 0 if self._kstest(l, s, c, samps) else 1\n    self.assertLess(fails, trials * 0.01)\n\n  def _kstest(self, loc, scale, conc, samples):\n    \n    ks, _ = sp_stats.kstest(samples,\n                            sp_stats.genpareto(conc, loc=loc, scale=scale).cdf)\n    \n    return ks < 0.02\n\n  def testPdfOfSampleMultiDims(self):\n    dist = tfd.GeneralizedPareto(\n        loc=0, scale=[[2.], [3.]], concentration=[-.37, .11])\n    num = 50000\n    samples = dist.sample(num, seed=test_util.test_seed())\n    pdfs = dist.prob(samples)\n    sample_vals, pdf_vals = self.evaluate([samples, pdfs])\n    self.assertEqual((num, 2, 2), samples.shape)\n    self.assertEqual((num, 2, 2), pdfs.shape)\n    self._assertIntegral(sample_vals[:, 0, 0], pdf_vals[:, 0, 0], err=0.02)\n    self._assertIntegral(sample_vals[:, 0, 1], pdf_vals[:, 0, 1], err=0.02)\n    self._assertIntegral(sample_vals[:, 1, 0], pdf_vals[:, 1, 0], err=0.02)\n    self._assertIntegral(sample_vals[:, 1, 1], pdf_vals[:, 1, 1], err=0.02)\n\n  def _assertIntegral(self, sample_vals, pdf_vals, err=1e-3):\n    s_p = zip(sample_vals, pdf_vals)\n    prev = (0, 0)\n    total = 0\n    for k in sorted(s_p, key=lambda x: x[0]):\n      pair_pdf = (k[1] + prev[1]) / 2\n      total += (k[0] - prev[0]) * pair_pdf\n      prev = k\n    self.assertNear(1., total, err=err)\n\n  def testNonPositiveInitializationParamsRaises(self):\n    scale = tf.constant(0.0, name='scale')\n    with self.assertRaisesOpError('Argument `scale` must be positive.'):\n      dist = tfd.GeneralizedPareto(\n          loc=0, scale=scale, concentration=1, validate_args=True)\n      self.evaluate(dist.mean())\n\n  def testGradientThroughConcentration(self):\n    concentration = tf.Variable(3.)\n    d = tfd.GeneralizedPareto(loc=0, scale=1, concentration=concentration)\n    with tf.GradientTape() as tape:\n      loss = -d.log_prob([1., 2., 4.])\n    grad = tape.gradient(loss, d.trainable_variables)\n    self.assertLen(grad, 1)\n    self.assertAllNotNone(grad)\n\n  def testAssertsPositiveScale(self):\n    scale = tf.Variable([1., 2., -3.])\n    self.evaluate(scale.initializer)\n    with self.assertRaisesOpError('Argument `scale` must be positive.'):\n      d = tfd.GeneralizedPareto(\n          loc=0, scale=scale, concentration=1, validate_args=True)\n      self.evaluate(d.sample())\n\n  def testAssertsPositiveScaleAfterMutation(self):\n    scale = tf.Variable([1., 2., 3.])\n    self.evaluate(scale.initializer)\n    d = tfd.GeneralizedPareto(\n        loc=0, scale=scale, concentration=0.25, validate_args=True)\n    self.evaluate(d.mean())\n    with self.assertRaisesOpError('Argument `scale` must be positive.'):\n      with tf.control_dependencies([scale.assign([1., 2., -3.])]):\n        self.evaluate(d.sample())\n\n  def testGradientThroughLocScale(self):\n    loc = tf.Variable(1.)\n    scale = tf.Variable(2.5)\n    d = tfd.GeneralizedPareto(loc=loc, scale=scale, concentration=.15)\n    with tf.GradientTape() as tape:\n      loss = -d.log_prob([1., 2., 4.])\n    grads = tape.gradient(loss, d.trainable_variables)\n    self.assertLen(grads, 2)\n    self.assertAllNotNone(grads)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n",
        "summary": "This code defines a test suite for the `tfd.GeneralizedPareto` distribution in TensorFlow Distributions. The tests cover various aspects of the distribution, including:\n\n1. Initialization with positive scale parameter.\n2. PDF and CDF calculations.\n3. Log probability computation.\n4. Sampling from the distribution.\n5. Gradient computations through parameters.\n6. Validation of input arguments (e.g., non-positive scale raises an error).\n7. Multi-dimensional support.\n\nThe tests use TensorFlow's `tf.test.TestCase` framework to create test cases, which are run using `tf.test.main()`. The code also includes helper functions like `_kstest` for performing Kolmogorov-Smirnov tests on samples against the theoretical CDF of the distribution.\n\nSome key points about the tests:\n\n- They use TensorFlow's automatic differentiation capabilities to compute gradients.\n- They validate that the distribution behaves correctly under various input conditions.\n- They check that operations like sampling and log probability computation are differentiable.\n- They ensure that the distribution enforces constraints on its parameters (e.g., scale must be positive).\n\nThis comprehensive test suite helps ensure the reliability and correctness of the `GeneralizedPareto` distribution implementation in TensorFlow Distributions."
    },
    {
        "code": "def dot(a, b):\n\tp = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n\treturn p\n\n\nr1 = [1, 2, 3]\nr2 = [2, 2, 9]\n\n\np = dot(r1, r2)\n\n\nprint(\"r1 = \", r1, sep=\"\");\nprint(\"r2 = \", r2, sep=\"\");\nprint(\"p = r1 \\xb7 r2 = \", p, sep=\"\");\n",
        "summary": "The Python code defines a function `dot` that calculates the dot product of two vectors `a` and `b`. It then uses this function to compute the dot product of two specific vectors `r1` and `r2`, and prints the result."
    },
    {
        "code": "from typing import Any, Dict, Union, Optional\nfrom dataclasses import asdict, dataclass\n\nHeaders = Optional[Dict[str, Union[str, bool, int]]]\n\n\n@dataclass\nclass APIGatewayProxyResult:\n    \n    statusCode: int\n    body: Union[str, Dict[str, Any]]\n    headers: Headers = None\n    multiValueHeaders: Headers = None\n    isBase64Encoded: Optional[bool] = None\n\n    def asdict(self):\n        return {k: v for k, v in asdict(self).items() if v is not None}\n",
        "summary": "The `APIGatewayProxyResult` class defines a data structure for the response from an API Gateway proxy integration, including status code, body, headers, multi-value headers, and base64 encoding flag. The `asdict` method returns a dictionary representation of the instance, excluding any attributes with `None` values."
    },
    {
        "code": "class Status:\n    OK = \"OK\"\n    ERROR = \"ERROR\"\n\n\nclass Response(dict):\n    def __init__(self, status, data):\n        super().__init__()\n        \n        self[\"status\"] = status\n        self[\"data\"] = data\n",
        "summary": "The `Status` class defines two constants: `OK` and `ERROR`, representing the possible statuses of a response. The `Response` class is a subclass of `dict` that initializes with a `status` (from the `Status` class) and `data`, storing these in dictionary format."
    },
    {
        "code": "import math\n\nclass worldfile:\n    def __init__(self, filename):\n        wFile = open(filename)\n        w = wFile.readlines()\n        w = [line.rstrip() for line in w]\n\n        self.A = float(w[0])\n        self.D = float(w[1])\n        self.B = float(w[2])\n        self.E = float(w[3])\n        self.C = float(w[4])\n        self.F = float(w[5])\n\n        Xv = math.atan(self.D/self.A)\n        Yv = math.atan(self.B/self.E)\n\n        self.Xx = (math.cos(Xv) ** 2) / self.A\n        self.Xy = (math.cos(Xv) * math.sin(Xv)) / self.A\n        self.Yy = (math.cos(Yv) ** 2) / self.E\n        self.Yx = (math.cos(Yv) * math.sin(Yv)) / self.E\n        \n    def coordToPx(self, lon, lat):\n        Dx = lon - self.C\n        Dy = lat - self.F\n\n        Px = (Dx * self.Xx) + (Dy * self.Yx)\n        Py = (Dx * self.Xy) + (Dy * self.Yy)\n\n        return(Px, Py)\n\n        \n",
        "summary": "The `worldfile` class reads parameters from a file to define a coordinate transformation, and includes a method to convert geographic coordinates (longitude and latitude) to pixel coordinates."
    },
    {
        "code": "import functools\nfrom twisted.internet import defer\n\n\n\ndef on_error_close(logger):\n    \n    \n    def make_wrapper(func):\n        \n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            \n            d = defer.maybeDeferred(func, self, *args, **kwargs)\n            def on_error(err):\n                logger(\"Unhandled failure in %r:%s\" % (func, err. getTraceback()))\n                \n                if hasattr(self, \"transport\"):\n                    if hasattr(self.transport, \"abortConnection\"):\n                        self.transport.abortConnection()\n                    elif hasattr(self.transport, \"loseConnection\"):\n                        self.transport.loseConnection()\n            d.addErrback(on_error)\n            \n        return wrapper\n    return make_wrapper",
        "summary": "The provided Python code defines a decorator factory `on_error_close` that wraps methods to handle errors gracefully. It logs unhandled exceptions and attempts to close the connection if available, using Twisted's `defer.maybeDeferred` for asynchronous error handling."
    },
    {
        "code": "class Key(object):\n\n    @classmethod\n    def from_path(cls, *args, **kwds):\n        raise NotImplementedError(\"Paths are not currently supported\")\n\n    def __init__(self, encoded=None, obj=None):\n        self.name = None\n        if obj:\n            self.id = obj.id\n            self.kind = obj.kind()\n        else:\n            self.id = None\n            self.kind = None\n\n    def app(self):\n        raise NotImplementedError(\"Applications are not currently supported\")\n\n    def kind(self):\n        return self.kind\n\n    def id(self):\n        return self.id\n\n    def name(self):\n        raise NotImplementedError(\"Key Names are not currently supported\")\n\n    def id_or_name(self):\n        return self.id\n\n    def has_id_or_name(self):\n        return self.id is not None\n\n    def parent(self):\n        raise NotImplementedError(\"Key parents are not currently supported\")\n\n    def __str__(self):\n        return self.id_or_name()\n",
        "summary": "The `Key` class in Python serves as a base class for key objects, providing methods to handle and represent keys without specific implementations for paths, applications, names, or parent relationships. It includes attributes for the key's ID and kind, with methods to retrieve these values and determine if an ID is present."
    },
    {
        "code": "import tokenize\n\ntry:\n    import StringIO\nexcept ImportError:\n    import io\n    StringIO = io\n\n\nINVALID_WKT_FMT = 'Invalid WKT: `%s`'\n\n\ndef dump(obj, dest_file):\n    \n    dest_file.write(dumps(obj))\n\n\ndef load(source_file):\n    \n    return loads(source_file.read())\n\n\ndef dumps(obj, decimals=16):\n    \n    geom_type = obj['type']\n    exporter = _dumps_registry.get(geom_type)\n\n    if exporter is None:\n        _unsupported_geom_type(geom_type)\n\n    fmt = '%%.%df' % decimals\n    return exporter(obj, fmt)\n\n\ndef loads(string):\n    \n    sio = StringIO.StringIO(string)\n    \n    tokens = (x[1] for x in tokenize.generate_tokens(sio.readline))\n    tokens = _tokenize_wkt(tokens)\n    geom_type = next(tokens)\n\n    importer = _loads_registry.get(geom_type)\n\n    if importer is None:\n        _unsupported_geom_type(geom_type)\n    return importer(tokens, string)\n\n\ndef _tokenize_wkt(tokens):\n    \n    negative = False\n    for t in tokens:\n        if t == '-':\n            negative = True\n            continue\n        else:\n            if negative:\n                yield '-%s' % t\n            else:\n                yield t\n            negative = False\n\n\ndef _unsupported_geom_type(geom_type):\n    raise ValueError(\"Unsupported geometry type '%s'\" % geom_type)\n\n\ndef _dump_point(obj, fmt):\n    \n    coords = obj['coordinates']\n    pt = 'POINT (%s)' % ' '.join(fmt % c for c in coords)\n    return pt\n\n\ndef _dump_linestring(obj, fmt):\n    \n    coords = obj['coordinates']\n    ls = 'LINESTRING (%s)'\n    ls %= ', '.join(' '.join(fmt % c for c in pt) for pt in coords)\n    return ls\n\n\ndef _dump_polygon(obj, fmt):\n    \n    coords = obj['coordinates']\n    poly = 'POLYGON (%s)'\n    rings = (', '.join(' '.join(fmt % c for c in pt) for pt in ring)\n             for ring in coords)\n    rings = ('(%s)' % r for r in rings)\n    poly %= ', '.join(rings)\n    return poly\n\n\ndef _dump_multipoint(obj, fmt):\n    \n    coords = obj['coordinates']\n    mp = 'MULTIPOINT (%s)'\n    points = (' '.join(fmt % c for c in pt) for pt in coords)\n    \n    points = ('(%s)' % pt for pt in points)\n    mp %= ', '.join(points)\n    return mp\n\n\ndef _dump_multilinestring(obj, fmt):\n    \n    coords = obj['coordinates']\n    mlls = 'MULTILINESTRING (%s)'\n    linestrs = ('(%s)' % ', '.join(' '.join(fmt % c for c in pt)\n                for pt in linestr) for linestr in coords)\n    mlls %= ', '.join(ls for ls in linestrs)\n    return mlls\n\n\ndef _dump_multipolygon(obj, fmt):\n    \n    coords = obj['coordinates']\n    mp = 'MULTIPOLYGON (%s)'\n\n    polys = (\n        \n        ', '.join(\n            \n            \n            '(%s)' % ', '.join(\n                \n                \n                '(%s)' % ', '.join(\n                    \n                    ' '.join(fmt % c for c in pt)\n                    for pt in ring)\n                for ring in poly)\n            for poly in coords)\n    )\n    mp %= polys\n    return mp\n\n\ndef _dump_geometrycollection(obj, fmt):\n    \n    gc = 'GEOMETRYCOLLECTION (%s)'\n    geoms = obj['geometries']\n    geoms_wkt = []\n    for geom in geoms:\n        geom_type = geom['type']\n        geoms_wkt.append(_dumps_registry.get(geom_type)(geom, fmt))\n    gc %= ','.join(geoms_wkt)\n    return gc\n\n\ndef _load_point(tokens, string):\n    \n    if not next(tokens) == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    coords = []\n    try:\n        for t in tokens:\n            if t == ')':\n                break\n            else:\n                coords.append(float(t))\n    except tokenize.TokenError:\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    return dict(type='Point', coordinates=coords)\n\n\ndef _load_linestring(tokens, string):\n    \n    if not next(tokens) == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    \n    \n    coords = []\n    try:\n        pt = []\n        for t in tokens:\n            if t == ')':\n                coords.append(pt)\n                break\n            elif t == ',':\n                \n                coords.append(pt)\n                pt = []\n            else:\n                pt.append(float(t))\n    except tokenize.TokenError:\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    return dict(type='LineString', coordinates=coords)\n\n\ndef _load_polygon(tokens, string):\n    \n    open_parens = next(tokens), next(tokens)\n    if not open_parens == ('(', '('):\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    \n    \n    \n    coords = []\n\n    ring = []\n    on_ring = True\n    try:\n        pt = []\n        for t in tokens:\n            if t == ')' and on_ring:\n                \n                ring.append(pt)\n                coords.append(ring)\n                on_ring = False\n            elif t == ')' and not on_ring:\n                \n                break\n            elif t == '(':\n                \n                ring = []\n                pt = []\n                on_ring = True\n            elif t == ',' and on_ring:\n                \n                ring.append(pt)\n                pt = []\n            elif t == ',' and not on_ring:\n                \n                \n                pass\n            else:\n                pt.append(float(t))\n    except tokenize.TokenError:\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    return dict(type='Polygon', coordinates=coords)\n\n\ndef _load_multipoint(tokens, string):\n    \n    open_paren = next(tokens)\n    if not open_paren == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    coords = []\n    pt = []\n\n    paren_depth = 1\n    try:\n        for t in tokens:\n            if t == '(':\n                paren_depth += 1\n            elif t == ')':\n                paren_depth -= 1\n                if paren_depth == 0:\n                    break\n            elif t == '':\n                pass\n            elif t == ',':\n                \n                coords.append(pt)\n                pt = []\n            else:\n                pt.append(float(t))\n    except tokenize.TokenError:\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    \n    \n    if len(pt) > 0:\n        coords.append(pt)\n\n    return dict(type='MultiPoint', coordinates=coords)\n\n\ndef _load_multipolygon(tokens, string):\n    \n    open_paren = next(tokens)\n    if not open_paren == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    polygons = []\n    while True:\n        try:\n            poly = _load_polygon(tokens, string)\n            polygons.append(poly['coordinates'])\n            t = next(tokens)\n            if t == ')':\n                \n                break\n        except StopIteration:\n            \n            raise ValueError(INVALID_WKT_FMT % string)\n\n    return dict(type='MultiPolygon', coordinates=polygons)\n\n\ndef _load_multilinestring(tokens, string):\n    \n    open_paren = next(tokens)\n    if not open_paren == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    linestrs = []\n    while True:\n        try:\n            linestr = _load_linestring(tokens, string)\n            linestrs.append(linestr['coordinates'])\n            t = next(tokens)\n            if t == ')':\n                \n                break\n        except StopIteration:\n            \n            raise ValueError(INVALID_WKT_FMT % string)\n\n    return dict(type='MultiLineString', coordinates=linestrs)\n\n\ndef _load_geometrycollection(tokens, string):\n    \n    open_paren = next(tokens)\n    if not open_paren == '(':\n        raise ValueError(INVALID_WKT_FMT % string)\n\n    geoms = []\n    result = dict(type='GeometryCollection', geometries=geoms)\n    while True:\n        try:\n            t = next(tokens)\n            if t == ')':\n                break\n            elif t == ',':\n                \n                continue\n            else:\n                geom_type = t\n                load_func = _loads_registry.get(geom_type)\n                geom = load_func(tokens, string)\n                geoms.append(geom)\n        except StopIteration:\n            raise ValueError(INVALID_WKT_FMT % string)\n    return result\n\n\n_dumps_registry = {\n    'Point':  _dump_point,\n    'LineString': _dump_linestring,\n    'Polygon': _dump_polygon,\n    'MultiPoint': _dump_multipoint,\n    'MultiLineString': _dump_multilinestring,\n    'MultiPolygon': _dump_multipolygon,\n    'GeometryCollection': _dump_geometrycollection,\n}\n\n\n_loads_registry = {\n    'POINT': _load_point,\n    'LINESTRING': _load_linestring,\n    'POLYGON': _load_polygon,\n    'MULTIPOINT': _load_multipoint,\n    'MULTILINESTRING': _load_multilinestring,\n    'MULTIPOLYGON': _load_multipolygon,\n    'GEOMETRYCOLLECTION': _load_geometrycollection,\n}\n",
        "summary": "The provided Python code defines a module for converting between Well-Known Text (WKT) and a custom geometry object format. It includes functions to serialize (`dumps`) and deserialize (`loads`) geometric objects, handling various types such as points, lines, polygons, and collections thereof. The module uses dictionaries to map geometry types to their respective serialization and deserialization functions, ensuring flexibility and scalability for additional geometries if needed."
    },
    {
        "code": "import os\nimport sys\nfrom contextlib import contextmanager\n\nfrom fabric.contrib import django\nfrom fabric.api import local, run, lcd, cd\nfrom fabric.tasks import Task\n\nfrom fab_settings import env\n\nsys.path.append(os.path.dirname(__file__) + '/../../mysite/')\n\ndjango.settings_module('mysite.settings')\n\nSTAGING_BRANCH = 'master'\nBASE_PATH = os.path.dirname(__file__)\nSTAGING_HOST = 'staging.courselets.org'\n\n\ndef debug(*args, **kwargs):\n    output = \"\"\n    for x in args:\n        print(x)\n        output += str(x)\n    return output\n\n\n@contextmanager\ndef debug_cd(path):\n    print(\"run on path:{0}\".format(path))\n    yield\n\n\nclass Deploying(Task):\n    \n\n    func = local\n    func_cd = lcd\n    code_branch = STAGING_BRANCH\n\n    @property\n    def project_path(self):\n        return os.path.join(BASE_PATH, 'socraticqs2')\n\n    @property\n    def local_settings_path(self):\n        return os.path.join(self.project_path, '../settings')\n\n    def __virtualenv(self):\n        with self.func_cd(os.path.join(self.project_path, '../')):\n            self.func('source {}/bin/activate'.format(env.venv_name))\n\n    def update_requirements(self):\n        with self.func_cd(self.project_path):\n            self.func(\"sudo pip install -r requirements.txt\")\n\n    def _get_settings(self, branch='master'):\n        with self.func_cd(self.local_settings_path):\n            self.func('git pull origin {0}'.format(branch))\n            self.func('cp production_conf.py ../socraticqs2/mysite/mysite/settings/production_conf.py')\n\n    def __restart_service(self):\n        self.func('sudo supervisorctl restart gunicorn')\n        self.func('sudo supervisorctl restart celery')\n        self.func('sudo service nginx restart')\n\n    @property\n    def __is_new_branch(self):\n        if self.func == run:\n            return self.code_branch in self.func('git branch')\n        else:\n            return self.code_branch in self.func('git branch', capture=True)\n\n    def __update(self):\n\n        if self.__is_new_branch:\n            self.func('git checkout {0} --force'.format(self.code_branch))\n            self.func('git pull origin {0} --force'.format(self.code_branch))\n        else:\n            self.func('git fetch origin')\n            self.func('git checkout -b {0} origin/{0}'.format(self.code_branch))\n        self._get_settings()\n        self.func('find . -name \"*.pyc\" -print -delete')\n        self.__virtualenv()\n        self.update_requirements()\n        with self.func_cd(\"mysite\"):\n            self.func('python manage.py collectstatic --noinput')\n            self.func('python manage.py syncdb --noinput')\n            self.func('python manage.py fsm_deploy --noinput')\n\n        self.__restart_service()\n\n    def run(self, running='local', branch='master', suffix=None):\n        self.code_branch = branch\n        if running == 'local':\n            self.func = local\n            self.func_cd = lcd\n            self.__update()\n        elif running == 'remote':\n            self.func = run\n            self.func_cd = cd\n            env.hosts = [STAGING_HOST, ]\n            global BASE_PATH\n            BASE_PATH = env.project_root\n            with self.func_cd(self.project_path):\n                self.__update()\n        elif running == 'debug':\n            print(\"DEBUG:\\n\")\n            self.func = debug\n            self.func_cd = debug_cd\n            self.__update()\n\n\nclass Staging(Deploying):\n    \n\n    def _get_settings(self, branch='master'):\n        \n        with self.func_cd(self.local_settings_path):\n            self.func('git pull origin {0} --force'.format(branch))\n            self.func('cp local_conf.py ../dev/socraticqs2/mysite/mysite/settings/local_conf.py')\n\n\nclass Development(Staging):\n    \n\n    @property\n    def project_path(self):\n        if self.func == local:\n            return os.path.join(BASE_PATH, '../../../../dev')\n        else:\n            return os.path.join(BASE_PATH, 'dev/socraticqs2')\n\n    @property\n    def local_settings_path(self):\n        if self.func == local:\n            return os.path.join(self.project_path, '../settings')\n        else:\n            return os.path.join(self.project_path, '../../settings')\n\n    code_branch = 'dev'\n\n\nprod = Deploying()\nstaging = Staging()\ndev = Development()\n",
        "summary": "The provided Python script defines a series of classes and functions for deploying applications using Fabric, a Python library designed to simplify the use of SSH for application deployment or systems administration. The script includes tasks for updating requirements, pulling code from version control, setting up virtual environments, running migrations, collecting static files, and restarting services on both local and remote servers. It also supports debugging by printing commands instead of executing them."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport unittest\n\nimport isi_sdk_8_1_0\nfrom isi_sdk_8_1_0.models.license_license_tier_entitlements_exceeded_alert import LicenseLicenseTierEntitlementsExceededAlert  \nfrom isi_sdk_8_1_0.rest import ApiException\n\n\nclass TestLicenseLicenseTierEntitlementsExceededAlert(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testLicenseLicenseTierEntitlementsExceededAlert(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python code defines a unit test class `TestLicenseLicenseTierEntitlementsExceededAlert` for the `LicenseLicenseTierEntitlementsExceededAlert` model from the `isi_sdk_8_1_0` library, using the `unittest` framework. The class includes a single test method that currently does nothing (`testLicenseLicenseTierEntitlementsExceededAlert`)."
    },
    {
        "code": "from tests.lib.testcase import ConfluenceTestCase\nfrom tests.lib.testcase import setup_builder\nimport os\n\n\nclass TestConfluenceMetadata(ConfluenceTestCase):\n    @classmethod\n    def setUpClass(cls):\n        super(TestConfluenceMetadata, cls).setUpClass()\n\n        cls.dataset = os.path.join(cls.datasets, 'metadata')\n\n    def test_confluence_metadata_directive_expected(self):\n        with self.prepare(self.dataset) as app:\n            app.build()\n            builder_metadata = app.builder.metadata\n\n            self.assertTrue(builder_metadata)\n            self.assertTrue('index' in builder_metadata)\n            doc_labels = builder_metadata['index']\n\n            self.assertTrue(doc_labels)\n            self.assertTrue('labels' in doc_labels)\n\n            labels = doc_labels['labels']\n            self.assertEqual(len(labels), 2)\n            self.assertTrue('tag-a' in labels)\n            self.assertTrue('tag-c' in labels)\n\n    @setup_builder('html')\n    def test_html_confluence_metadata_directive_ignore(self):\n        with self.prepare(self.dataset, relax=True) as app:\n            \n            app.build()\n",
        "summary": "The provided Python code defines a test class `TestConfluenceMetadata` that inherits from `ConfluenceTestCase`. It includes two methods: `test_confluence_metadata_directive_expected`, which tests the presence and content of metadata in Confluence documents, and `test_html_confluence_metadata_directive_ignore`, which builds HTML documents while ignoring Confluence-specific metadata directives."
    },
    {
        "code": "import asyncio\nimport websockets\nimport time\nimport threading\n\nplayers = 0 \n\n\nclass Player:\n    def __init__(self, id, x = 0, y = 0, speed = 5):\n        self.id = id\n        self.x = x\n        self.y = y \n        self.dirX = 0\n        self.dirY = 0 \n        self.speed = speed\n        print(\"Player criado com sucesso!\")\n    \n    def setX(self, x):\n        self.x = x\n    \n    def setY(self, y):\n        self.y = y\n    \n    def getX(self):\n        return self.x\n    \n    def getY(self):\n        return self.y\n\n\nasync def hello(websocket, path):\n    global players\n    jogador = Player(players, 500, 500)\n    \n    async def moveUP():\n        while 1:\n            jogador.setY(jogador.getY()-jogador.speed)\n            websocket.send(\"move:\"+str(jogador.id)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n            print(\"move:\"+str(jogador.id)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n            time.sleep(1)\n    \n    async def moveR():\n        while 1:\n            jogador.setX(jogador.getX()+jogador.speed)\n            await websocket.send(\"move:\"+str(jogador.id)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n            print(\"move:\"+str(jogador.id)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n            time.sleep(1)\n\n    def threadEvoque():\n        global players\n        loop = asyncio.new_event_loop()\n        task = loop.create_task(moveUP())\n        loop.run_until_complete(task)\n        players += 1 \n        print(players)\n    \n    def threadEvoque2():\n        global players\n        loop = asyncio.new_event_loop()\n        task2 = loop.create_task(moveR())\n        loop.run_until_complete(task2)\n        players += 1 \n        print(players)\n    \n    while 1:\n        msg = await websocket.recv()\n        print(msg)\n        if(msg == \"start\"):\n            players +=1\n            await websocket.send(\"spawn:\"+str(players)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n            print(\"spawn:\"+str(players)+\":\"+ str(jogador.getX())+\":\"+str(jogador.getY()))\n\n        \n            \n            \n            \n\n\n\n\nstart_server = websockets.serve(hello, \"0.0.0.0\", 8888)\nprint(\"Iniciando server...\")\nasyncio.get_event_loop().run_until_complete(start_server)\nprint(\"Sever em funcionamento!\")\nasyncio.get_event_loop().run_forever()\n",
        "summary": "The provided Python code sets up a WebSocket server that handles player movement and spawning. It defines a `Player` class to manage player attributes and movements, using asynchronous functions for continuous movement in the up and right directions. The server listens on port 8888, spawns new players upon receiving a \"start\" message, and sends updates about player positions to connected clients."
    },
    {
        "code": "__author__ = 'pulphix'\n\nfrom app import TestApplication",
        "summary": "The provided Python code snippet is a simple module header that specifies the author of the script as 'pulphix' and imports a class named `TestApplication` from a module called `app`."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.contrib import admin\n\nfrom .models import ThumbnailOption\nfrom django.contrib.admin.widgets import AdminFileWidget\n\n\n@admin.register(ThumbnailOption)\nclass ThumbnailOptionAdmin(admin.ModelAdmin):\n    fields = ['source', 'alias', 'options']\n\n\nclass ThumbnailOptionMixin(admin.ModelAdmin):\n    class Media:\n        pass\n\n    def media(self):\n        pass\n",
        "summary": "The provided Python code defines an admin interface for a `ThumbnailOption` model in a Django application, specifying the fields to be displayed. It also includes a mixin class that can potentially extend additional functionality or styling to admin forms, though its implementation details are not shown here."
    },
    {
        "code": "import sys\nsys.path.append(\"..\")\n\ntry:\n    import os\n    import signal\n    import time  \n\n    from conf import *\n    from lib import logger\n    from lib import mqtt\n\nexcept Exception as e:\n    print(f\"Import error: {str(e)} line {sys.exc_info()[-1].tb_lineno}, check requirements.txt\")\n    sys.exit(1)\n\n\nlog = logger.Log(\"SignalTestcase\", MI2_SHORTNAME, 10)\nprint(__name__, MI2_SHORTNAME, 20)\n\n\n\n\ndef publish(state:str=\"Online\"):\n    mqtt_client = mqtt.client()\n    if mqtt_client and mqtt_client.ready:\n        mqtt_client.publish_simple(\"tele/apptest/LWT\", state, True)    \n\ndef handler(signum, frame):\n    publish('Offline')\n    log.debug(\"Ende Application\")\n    exit(0)\n   \ndef main():\n    while True:\n        try:\n            pass\n            time.sleep(30)\n        except Exception as e:\n            Log.error(f\"Error while running the script: {str(e)},  line {sys.exc_info()[-1].tb_lineno}\")\n\n\nif __name__ == \"__main__\":\n    log.debug(\"Start Application\")\n    signal.signal(signal.SIGINT, handler)\n    publish('Online')\n    main()\n",
        "summary": "The Python code imports necessary modules and sets up logging. It defines functions to handle signals, publish MQTT messages, and run a main loop that logs errors and handles interruptions gracefully."
    },
    {
        "code": "from database.adatabase import ADatabase\nimport pandas as pd\nclass SEC(ADatabase):\n    \n    def __init__(self):\n        super().__init__(\"sec\")\n\n    def retrieve_num_data(self,adsh):\n        try:\n            db = self.client[self.name]\n            table = db[\"nums\"]\n            data = table.find({\"adsh\":adsh},{\"_id\":0},show_record_id=False)\n            return pd.DataFrame(list(data))\n        except Exception as e:\n            print(str(e))\n\n    def retrieve_filing_data(self,cik):\n        try:\n            db = self.client[self.name]\n            table = db[\"filings\"]\n            data = table.find({\"cik\":cik},{\"_id\":0},show_record_id=False)\n            return pd.DataFrame(list(data))\n        except Exception as e:\n            print(str(e))\n    \n    def retrieve_adshs(self):\n        try:\n            db = self.client[self.name]\n            table = db[\"filings\"]\n            data = table.find({},{\"_id\":0,\"adsh\":1},show_record_id=False)\n            return pd.DataFrame(list(data))\n        except Exception as e:\n            print(str(e))",
        "summary": "The SEC class extends ADatabase and provides methods to retrieve numerical data, filing information, and a list of adshs (Accession Date/Shareholder) from MongoDB collections. Each method handles exceptions by printing the error message."
    },
    {
        "code": "import game_framework\nfrom pico2d import *\nimport title_state\n\nname = \"StartState\"\nimage = None\nlogo_time = 0.0\n\n\ndef enter():\n    global image\n    image = load_image('kpu_credit.png')\n\n\ndef exit():\n    global image\n    del(image)\n\n\ndef update():\n    global logo_time\n\n    if (logo_time > 1.0):\n        logo_time = 0.8\n        game_framework.change_state(title_state)\n\n    delay(0.01)\n    logo_time += 0.05\n\n\ndef draw():\n    global image\n    clear_canvas()\n    image.draw(400,300)\n    update_canvas()\n\n\n\ndef handle_events():\n    events = get_events()\n    pass\n\n\ndef pause(): pass\n\n\ndef resume(): pass\n\n\n\n\n",
        "summary": "The provided Python code defines a game state called `StartState` that handles the initial display of a logo image for 1.2 seconds before transitioning to the `title_state`. It includes functions for entering and exiting the state, updating the logo display time, drawing the logo on the canvas, handling events (though currently empty), and pausing and resuming the state (which do nothing in this implementation)."
    },
    {
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv(\"Data.csv\")\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 3].values\n\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis = 0)\nimputer = imputer.fit(X[:, 1:3])\nX[:, 1:3]= imputer.transform(X[:,1:3])\nprint(X)",
        "summary": "The Python code imports necessary libraries for data manipulation and visualization, reads a CSV file into a pandas DataFrame, extracts features (X) and target variable (y), handles missing values in the dataset by replacing them with the mean of the respective columns, and prints the modified feature set."
    },
    {
        "code": "from sqlalchemy.testing import assert_raises, eq_\nfrom sqlalchemy.testing import fixtures, AssertsCompiledSQL\nfrom sqlalchemy import (\n    testing, exc, case, select, literal_column, text, and_, Integer, cast,\n    String, Column, Table, MetaData)\nfrom sqlalchemy.sql import table, column\n\ninfo_table = None\n\n\nclass CaseTest(fixtures.TestBase, AssertsCompiledSQL):\n    __dialect__ = 'default'\n\n    @classmethod\n    def setup_class(cls):\n        metadata = MetaData(testing.db)\n        global info_table\n        info_table = Table(\n            'infos', metadata,\n            Column('pk', Integer, primary_key=True),\n            Column('info', String(30)))\n\n        info_table.create()\n\n        info_table.insert().execute(\n            {'pk': 1, 'info': 'pk_1_data'},\n            {'pk': 2, 'info': 'pk_2_data'},\n            {'pk': 3, 'info': 'pk_3_data'},\n            {'pk': 4, 'info': 'pk_4_data'},\n            {'pk': 5, 'info': 'pk_5_data'},\n            {'pk': 6, 'info': 'pk_6_data'})\n\n    @classmethod\n    def teardown_class(cls):\n        info_table.drop()\n\n    @testing.fails_on('firebird', 'FIXME: unknown')\n    @testing.requires.subqueries\n    def test_case(self):\n        inner = select(\n            [\n                case(\n                    [\n                        [info_table.c.pk < 3, 'lessthan3'],\n                        [\n                            and_(info_table.c.pk >= 3, info_table.c.pk < 7),\n                            'gt3']]).label('x'),\n                info_table.c.pk, info_table.c.info], from_obj=[info_table])\n\n        inner_result = inner.execute().fetchall()\n\n        \n        \n        \n        \n        \n        \n        \n        assert inner_result == [\n            ('lessthan3', 1, 'pk_1_data'),\n            ('lessthan3', 2, 'pk_2_data'),\n            ('gt3', 3, 'pk_3_data'),\n            ('gt3', 4, 'pk_4_data'),\n            ('gt3', 5, 'pk_5_data'),\n            ('gt3', 6, 'pk_6_data')\n        ]\n\n        outer = select([inner.alias('q_inner')])\n\n        outer_result = outer.execute().fetchall()\n\n        assert outer_result == [\n            ('lessthan3', 1, 'pk_1_data'),\n            ('lessthan3', 2, 'pk_2_data'),\n            ('gt3', 3, 'pk_3_data'),\n            ('gt3', 4, 'pk_4_data'),\n            ('gt3', 5, 'pk_5_data'),\n            ('gt3', 6, 'pk_6_data')\n        ]\n\n        w_else = select(\n            [\n                case(\n                    [\n                        [info_table.c.pk < 3, cast(3, Integer)],\n                        [\n                            and_(\n                                info_table.c.pk >= 3, info_table.c.pk < 6),\n                            6]],\n                    else_=0).label('x'),\n                info_table.c.pk, info_table.c.info],\n            from_obj=[info_table])\n\n        else_result = w_else.execute().fetchall()\n\n        assert else_result == [\n            (3, 1, 'pk_1_data'),\n            (3, 2, 'pk_2_data'),\n            (6, 3, 'pk_3_data'),\n            (6, 4, 'pk_4_data'),\n            (6, 5, 'pk_5_data'),\n            (0, 6, 'pk_6_data')\n        ]\n\n    def test_literal_interpretation(self):\n        t = table('test', column('col1'))\n\n        assert_raises(exc.ArgumentError, case, [(\"x\", \"y\")])\n\n        self.assert_compile(\n            case([(\"x\", \"y\")], value=t.c.col1),\n            \"CASE test.col1 WHEN :param_1 THEN :param_2 END\")\n        self.assert_compile(\n            case([(t.c.col1 == 7, \"y\")], else_=\"z\"),\n            \"CASE WHEN (test.col1 = :col1_1) THEN :param_1 ELSE :param_2 END\")\n\n    def test_text_doesnt_explode(self):\n\n        for s in [\n            select(\n                [\n                    case(\n                        [\n                            (\n                                info_table.c.info == 'pk_4_data',\n                                text(\"'yes'\"))],\n                        else_=text(\"'no'\"))\n                ]).order_by(info_table.c.info),\n\n            select(\n                [\n                    case(\n                        [\n                            (\n                                info_table.c.info == 'pk_4_data',\n                                literal_column(\"'yes'\"))],\n                        else_=literal_column(\"'no'\")\n                    )]\n            ).order_by(info_table.c.info),\n\n        ]:\n            if testing.against(\"firebird\"):\n                eq_(s.execute().fetchall(), [\n                    ('no ', ), ('no ', ), ('no ', ), ('yes', ),\n                    ('no ', ), ('no ', ),\n                ])\n            else:\n                eq_(s.execute().fetchall(), [\n                    ('no', ), ('no', ), ('no', ), ('yes', ),\n                    ('no', ), ('no', ),\n                ])\n\n    @testing.fails_on('firebird', 'FIXME: unknown')\n    def testcase_with_dict(self):\n        query = select(\n            [\n                case(\n                    {\n                        info_table.c.pk < 3: 'lessthan3',\n                        info_table.c.pk >= 3: 'gt3',\n                    }, else_='other'),\n                info_table.c.pk, info_table.c.info\n            ],\n            from_obj=[info_table])\n        assert query.execute().fetchall() == [\n            ('lessthan3', 1, 'pk_1_data'),\n            ('lessthan3', 2, 'pk_2_data'),\n            ('gt3', 3, 'pk_3_data'),\n            ('gt3', 4, 'pk_4_data'),\n            ('gt3', 5, 'pk_5_data'),\n            ('gt3', 6, 'pk_6_data')\n        ]\n\n        simple_query = select(\n            [\n                case(\n                    {1: 'one', 2: 'two', },\n                    value=info_table.c.pk, else_='other'),\n                info_table.c.pk\n            ],\n            whereclause=info_table.c.pk < 4,\n            from_obj=[info_table])\n\n        assert simple_query.execute().fetchall() == [\n            ('one', 1),\n            ('two', 2),\n            ('other', 3),\n        ]\n",
        "summary": "The provided Python code defines a test class `CaseTest` that extends `fixtures.TestBase` and includes methods to test the functionality of SQL cases, including handling different conditions and using subqueries. It sets up a temporary table named 'infos' with primary key and info columns, inserts sample data, and then performs various case operations to assert expected results."
    },
    {
        "code": "import unittest\n\nimport apache_beam as beam\nfrom apache_beam.runners.interactive.user_pipeline_tracker import UserPipelineTracker\n\n\nclass UserPipelineTrackerTest(unittest.TestCase):\n  def test_getting_unknown_pid_returns_none(self):\n    ut = UserPipelineTracker()\n\n    p = beam.Pipeline()\n\n    self.assertIsNone(ut.get_pipeline(str(id(p))))\n\n  def test_getting_unknown_pipeline_returns_none(self):\n    ut = UserPipelineTracker()\n\n    p = beam.Pipeline()\n\n    self.assertIsNone(ut.get_user_pipeline(p))\n\n  def test_no_parent_returns_none(self):\n    ut = UserPipelineTracker()\n\n    user = beam.Pipeline()\n    derived = beam.Pipeline()\n    orphan = beam.Pipeline()\n\n    ut.add_derived_pipeline(user, derived)\n\n    self.assertIsNone(ut.get_user_pipeline(orphan))\n\n  def test_get_user_pipeline_is_same(self):\n    ut = UserPipelineTracker()\n\n    p = beam.Pipeline()\n    ut.add_user_pipeline(p)\n\n    self.assertIs(ut.get_user_pipeline(p), p)\n\n  def test_can_add_derived(self):\n    ut = UserPipelineTracker()\n\n    user = beam.Pipeline()\n    derived = beam.Pipeline()\n\n    ut.add_derived_pipeline(user, derived)\n\n    self.assertIs(ut.get_user_pipeline(derived), user)\n\n  def test_can_add_multiple_derived(self):\n    \n    ut = UserPipelineTracker()\n\n    \n    user1 = beam.Pipeline()\n    derived11 = beam.Pipeline()\n    derived12 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user1, derived11)\n    ut.add_derived_pipeline(user1, derived12)\n\n    \n    user2 = beam.Pipeline()\n    derived21 = beam.Pipeline()\n    derived22 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user2, derived21)\n    ut.add_derived_pipeline(user2, derived22)\n\n    \n    self.assertIs(ut.get_user_pipeline(derived11), user1)\n    self.assertIs(ut.get_user_pipeline(derived12), user1)\n    self.assertIs(ut.get_user_pipeline(derived21), user2)\n    self.assertIs(ut.get_user_pipeline(derived22), user2)\n\n  def test_cannot_have_multiple_parents(self):\n    ut = UserPipelineTracker()\n\n    user1 = beam.Pipeline()\n    user2 = beam.Pipeline()\n    derived = beam.Pipeline()\n\n    ut.add_derived_pipeline(user1, derived)\n\n    with self.assertRaises(AssertionError):\n      ut.add_derived_pipeline(user2, derived)\n\n    self.assertIs(ut.get_user_pipeline(derived), user1)\n\n  def test_adding_derived_with_derived_gets_user_pipeline(self):\n    \n    ut = UserPipelineTracker()\n\n    user = beam.Pipeline()\n    derived1 = beam.Pipeline()\n    derived2 = beam.Pipeline()\n\n    \n    ut.add_derived_pipeline(user, derived1)\n\n    \n    \n    ut.add_derived_pipeline(derived1, derived2)\n\n    \n    self.assertIs(ut.get_user_pipeline(derived1), user)\n    self.assertIs(ut.get_user_pipeline(derived2), user)\n\n  def test_can_get_pipeline_from_id(self):\n    \n    ut = UserPipelineTracker()\n\n    user = beam.Pipeline()\n    derived = beam.Pipeline()\n\n    ut.add_user_pipeline(user)\n    ut.add_derived_pipeline(user, derived)\n\n    self.assertIs(ut.get_pipeline(str(id(user))), user)\n    self.assertIs(ut.get_pipeline(str(id(derived))), derived)\n\n  def test_clear(self):\n    ut = UserPipelineTracker()\n\n    user = beam.Pipeline()\n    derived = beam.Pipeline()\n\n    ut.add_derived_pipeline(user, derived)\n\n    self.assertIs(ut.get_user_pipeline(derived), user)\n\n    ut.clear()\n\n    self.assertIsNone(ut.get_user_pipeline(user))\n    self.assertIsNone(ut.get_user_pipeline(derived))\n\n  def test_can_iterate(self):\n    ut = UserPipelineTracker()\n\n    user1 = beam.Pipeline()\n    derived11 = beam.Pipeline()\n    derived12 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user1, derived11)\n    ut.add_derived_pipeline(user1, derived12)\n\n    user2 = beam.Pipeline()\n    derived21 = beam.Pipeline()\n    derived22 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user2, derived21)\n    ut.add_derived_pipeline(user2, derived22)\n\n    user_pipelines = set(p for p in ut)\n    self.assertSetEqual(set([user1, user2]), user_pipelines)\n\n  def test_can_evict_user_pipeline(self):\n    ut = UserPipelineTracker()\n\n    user1 = beam.Pipeline()\n    derived11 = beam.Pipeline()\n    derived12 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user1, derived11)\n    ut.add_derived_pipeline(user1, derived12)\n\n    user2 = beam.Pipeline()\n    derived21 = beam.Pipeline()\n    derived22 = beam.Pipeline()\n\n    ut.add_derived_pipeline(user2, derived21)\n    ut.add_derived_pipeline(user2, derived22)\n\n    ut.evict(user1)\n\n    self.assertIsNone(ut.get_user_pipeline(user1))\n    self.assertIsNone(ut.get_user_pipeline(derived11))\n    self.assertIsNone(ut.get_user_pipeline(derived12))\n\n    self.assertIs(user2, ut.get_user_pipeline(derived21))\n    self.assertIs(user2, ut.get_user_pipeline(derived22))\n\n\nif __name__ == '__main__':\n  unittest.main()\n",
        "summary": "The provided Python code is a unit test suite for the `UserPipelineTracker` class from the Apache Beam library. It tests various functionalities such as adding and retrieving user pipelines, derived pipelines, handling multiple parents, clearing the tracker, and iterating over tracked pipelines. The tests ensure that the `UserPipelineTracker` behaves correctly under different scenarios, including edge cases like attempting to add a pipeline with multiple parents or evicting a user pipeline."
    },
    {
        "code": "var_teste = 1\nprint(var_teste)\nprint(type(var_teste))\n\n\n\npessoa1, pessoa2, pessoa3 = 'Jose', 'Joao','Maria'\nprint(pessoa1)\nprint(pessoa2)\nprint(pessoa3)\n\n\npessoa1=pessoa2=pessoa3 = 'Jose'\nprint(pessoa1)\nprint(pessoa2)\nprint(pessoa3)\n\n\nidade = 32\nidade1 = 28\n\nprint('SOMA',idade + idade1)\nprint('SUBTRA\u00c7\u00c3O',idade - idade1)\nprint('MULTIPLICA\u00c7\u00c2O',idade * idade1)\nprint('DIVISAO',idade / idade1)\nprint('POTENCIA',idade ** idade1)\nprint('DIVISAO INTEIRO',idade // idade1)\nprint('RESTO DIVIS\u00c2O',idade % idade1)\n",
        "summary": "The provided Python code demonstrates basic variable assignment, printing, and arithmetic operations. It shows how to declare variables, assign values, print their types or contents, and perform various mathematical calculations including addition, subtraction, multiplication, division, exponentiation, integer division, and modulus."
    },
    {
        "code": "import json\nimport copy\nimport requests\nimport json\nfrom flask import render_template, abort, request, url_for, redirect, g\nimport time\nimport datetime\n\nfrom rrd import app\nfrom rrd.model.screen import DashboardScreen\nfrom rrd.model.graph import DashboardGraph\nfrom rrd import consts\nfrom rrd.utils.graph_urls import generate_graph_urls \nfrom rrd import config\n\n@app.route(\"/screen\", methods=[\"GET\", \"POST\"])\ndef dash_screens():\n    top_screens = DashboardScreen.gets(pid='0')\n    top_screens = sorted(top_screens, key=lambda x:x.name)\n\n    return render_template(\"screen/index.html\", **locals())\n\n@app.route(\"/screen/<int:sid>/delete\")\ndef dash_screen_delete(sid):\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no such screen\")\n    DashboardScreen.remove(sid)\n\n    return redirect(\"/screen\")\n\n@app.route(\"/screen/<int:sid>/edit\", methods=[\"GET\", \"POST\"])\ndef dash_screen_edit(sid):\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no such screen\")\n\n    if request.method == \"POST\":\n        screen_name = request.form.get(\"screen_name\")\n        screen.update(name=screen_name)\n        return redirect(\"/screen/%s\" %screen.id)\n    else:\n        return render_template(\"screen/edit.html\", **locals())\n\n@app.route(\"/screen/<int:sid>/clone\", methods=[\"GET\", \"POST\"])\ndef dash_screen_clone(sid):\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no such screen\")\n\n    if request.method == \"POST\":\n        screen_name = request.form.get(\"screen_name\")\n        with_graph = request.form.get(\"with_graph\")\n\n        new_s = DashboardScreen.add(screen.pid, screen_name)\n        if not new_s:\n            abort(404, \"\u521b\u5efascreen\u5931\u8d25\u4e86\")\n\n        if with_graph:\n            old_graphs = DashboardGraph.gets_by_screen_id(sid)\n            for o in old_graphs:\n                DashboardGraph.add(o.title, o.hosts, o.counters, new_s.id,\n                        o.timespan, o.graph_type, o.method, o.position)\n\n        return redirect(\"/screen/%s\" %new_s.id)\n    else:\n        return render_template(\"screen/clone.html\", **locals())\n\n@app.route(\"/graph/<int:gid>/delete\")\ndef dash_graph_delete(gid):\n    graph = DashboardGraph.get(gid)\n    if not graph:\n        abort(404, \"no such graph\")\n    DashboardGraph.remove(gid)\n    return redirect(\"/screen/\" + graph.screen_id)\n\n@app.route(\"/screen/<int:sid>\")\ndef dash_screen(sid):\n    start = request.args.get(\"start\")\n    end = request.args.get(\"end\")\n\n    top_screens = DashboardScreen.gets(pid=0)\n    top_screens = sorted(top_screens, key=lambda x:x.name)\n\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no screen\")\n\n    if str(screen.pid) == '0':\n        sub_screens = DashboardScreen.gets(pid=sid)\n        sub_screens = sorted(sub_screens, key=lambda x:x.name)\n        return render_template(\"screen/top_screen.html\", **locals())\n\n    pscreen = DashboardScreen.get(screen.pid)\n    sub_screens = DashboardScreen.gets(pid=screen.pid)\n    sub_screens = sorted(sub_screens, key=lambda x:x.name)\n    graphs = DashboardGraph.gets_by_screen_id(screen.id)\n\n    all_graphs = []\n\n    for graph in graphs:\n        all_graphs.extend(generate_graph_urls(graph, start, end) or [])\n\n    all_graphs = sorted(all_graphs, key=lambda x:x.position)\n\n    return render_template(\"screen/screen.html\", **locals())\n\n@app.route(\"/screen/embed/<int:sid>\")\ndef dash_screen_embed(sid):\n    start = request.args.get(\"start\")\n    end = request.args.get(\"end\")\n\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no screen\")\n\n    if screen.pid == '0':\n        abort(404, \"top screen\")\n\n    graphs = DashboardGraph.gets_by_screen_id(screen.id)\n    all_graphs = []\n\n    for graph in graphs:\n        all_graphs.extend(generate_graph_urls(graph, start, end) or [])\n\n    all_graphs = sorted(all_graphs, key=lambda x:x.position)\n\n    return render_template(\"screen/screen_embed.html\", **locals())\n\n\n@app.route(\"/screen/add\", methods=[\"GET\", \"POST\"])\ndef dash_screen_add():\n    if request.method == \"POST\":\n        name = request.form.get(\"screen_name\")\n        pid = request.form.get(\"pid\", '0')\n        screen = DashboardScreen.add(pid, name)\n        return redirect(\"/screen/%s\" % screen.id)\n    else:\n        pid = request.args.get(\"pid\", '0')\n        screen = DashboardScreen.get(pid)\n        return render_template(\"screen/add.html\", **locals())\n\n@app.route(\"/screen/<int:sid>/graph\", methods=[\"GET\", \"POST\"])\ndef dash_graph_add(sid):\n    all_screens = DashboardScreen.gets()\n    top_screens = [x for x in all_screens if x.pid == '0']\n    children = []\n    for t in top_screens:\n        children.append([x for x in all_screens if x.pid == t.id])\n\n    screen = DashboardScreen.get(sid)\n    if not screen:\n        abort(404, \"no screen\")\n    pscreen = DashboardScreen.get(screen.pid)\n\n    if request.method == \"POST\":\n        title = request.form.get(\"title\")\n\n        hosts = request.form.get(\"hosts\", \"\").strip()\n        hosts = hosts and hosts.split(\"\\n\") or []\n        hosts = [x.strip() for x in hosts]\n\n        counters = request.form.get(\"counters\", \"\").strip()\n        counters = counters and counters.split(\"\\n\") or []\n        counters = [x.strip() for x in counters]\n\n        timespan = request.form.get(\"timespan\", 3600)\n        graph_type = request.form.get(\"graph_type\", 'h')\n        method = request.form.get(\"method\", '').upper()\n        position = request.form.get(\"position\", 0)\n\n        graph = DashboardGraph.add(title, hosts, counters, sid,\n                timespan, graph_type, method, position)\n        return redirect(\"/screen/%s\" % sid)\n\n    else:\n        gid = request.args.get(\"gid\")\n        graph = gid and DashboardGraph.get(gid)\n        return render_template(\"screen/graph_add.html\", config=config, **locals())\n\n@app.route(\"/graph/<int:gid>/edit\", methods=[\"GET\", \"POST\"])\ndef dash_graph_edit(gid):\n    error = \"\"\n    graph = DashboardGraph.get(gid)\n    if not graph:\n        abort(404, \"no graph\")\n\n    all_screens = DashboardScreen.gets()\n    top_screens = [x for x in all_screens if x.pid == '0']\n    children = []\n    for t in top_screens:\n        children.append([x for x in all_screens if x.pid == t.id])\n\n    screen = DashboardScreen.get(graph.screen_id)\n    if not screen:\n        abort(404, \"no screen\")\n    pscreen = DashboardScreen.get(screen.pid)\n\n    if request.method == \"POST\":\n        ajax = request.form.get(\"ajax\", \"\")\n        screen_id = request.form.get(\"screen_id\")\n        title = request.form.get(\"title\", \"\").strip()\n\n        hosts = request.form.get(\"hosts\", \"\").strip()\n        hosts = hosts and hosts.split(\"\\n\") or []\n        hosts = [x.strip() for x in hosts]\n\n        counters = request.form.get(\"counters\", \"\").strip()\n        counters = counters and counters.split(\"\\n\") or []\n        counters = [x.strip() for x in counters]\n\n        timespan = request.form.get(\"timespan\", 3600)\n        graph_type = request.form.get(\"graph_type\", 'h')\n        method = request.form.get(\"method\", '').upper()\n        position = request.form.get(\"position\", 0)\n\n        graph = graph.update(title, hosts, counters, screen_id,\n                timespan, graph_type, method, position)\n\n        error = u\"\u4fee\u6539\u6210\u529f\u4e86\"\n        if not ajax:\n            return render_template(\"screen/graph_edit.html\", config=config, **locals())\n        else:\n            return \"ok\"\n\n    else:\n        ajax = request.args.get(\"ajax\", \"\")\n        return render_template(\"screen/graph_edit.html\", **locals())\n\n@app.route(\"/graph/multi_edit\", methods=[\"GET\", \"POST\"])\ndef dash_graph_multi_edit():\n    ret = {\n            \"ok\": False,\n            \"msg\": \"\",\n            \"data\": [],\n    }\n    if request.method == \"POST\":\n        d = request.data\n        try:\n            jdata = json.loads(d)\n        except ValueError:\n            jdata = None\n\n        if not jdata:\n            return json.dumps({\n                    \"ok\": False,\n                    \"msg\": \"no_data_post\",\n            })\n        rows = []\n        for x in jdata:\n            rows.append({\"id\": x[\"id\"], \"hosts\": x[\"endpoints\"], \"counters\": x[\"counters\"]})\n        DashboardGraph.update_multi(rows) \n\n        return json.dumps({\n             \"ok\": True,\n             \"msg\": \"\",\n        })\n        \n    elif request.method == \"GET\":\n        sid = request.args.get(\"sid\")\n        if not sid or not DashboardScreen.get(sid):\n            ret[\"msg\"] = \"no_screen\"\n            return json.dumps(ret)\n        \n        ret[\"ok\"] = True\n        graphs = DashboardGraph.gets_by_screen_id(sid)\n        ret['data'] = [{\"id\": x.id, \"title\": x.title, \"endpoints\":x.hosts, \"counters\":x.counters} for x in graphs]\n        return json.dumps(ret)\n    \n",
        "summary": "This code snippet appears to be a part of a web application backend written in Python using Flask or similar framework. It defines several routes and corresponding view functions that handle different types of requests related to managing screens (likely dashboards) and graphs within those screens.\n\n1. **Screen Management**:\n   - `dash_screen_add`: Adds a new screen.\n   - `dash_screen_edit`: Edits an existing screen.\n   - `dash_screen_delete`: Deletes a screen.\n\n2. **Graph Management**:\n   - `dash_graph_add`: Adds a new graph to a specified screen.\n   - `dash_graph_edit`: Edits an existing graph.\n   - `dash_graph_multi_edit`: Allows bulk editing of multiple graphs via AJAX request.\n\n3. **Utility Functions**:\n   - `generate_screen_tree`: Generates a hierarchical tree structure of screens for display purposes.\n   - `get_all_screens`: Retrieves all screens from the database.\n   - `get_screen_by_id`: Fetches a screen by its ID.\n   - `get_graphs_by_screen_id`: Retrieves all graphs associated with a given screen.\n\n4. **Error Handling**:\n   - The code includes basic error handling for cases where resources (like screens or graphs) are not found.\n\n5. **Configuration and Data Manipulation**:\n   - It uses a configuration object (`config`) which might contain settings relevant to the application.\n   - Functions like `DashboardGraph.update_multi` suggest that there's some form of batch update capability for graph data.\n\n6. **AJAX Support**:\n   - The code includes support for AJAX requests, particularly in the `dash_graph_edit` and `dash_graph_multi_edit` functions, allowing for asynchronous updates without reloading the page.\n\nOverall, this appears to be a robust backend system for managing complex dashboards with multiple graphs, providing both synchronous and asynchronous interfaces for data manipulation."
    },
    {
        "code": "import torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear\nfrom torch_geometric.nn import (ASAPooling,\n                                GraphConv, global_mean_pool,\n                                JumpingKnowledge)\n\n\nclass ASAP(torch.nn.Module):\n    def __init__(self, num_vocab, max_seq_len, node_encoder, emb_dim, num_layers, hidden, ratio=0.8, dropout=0, num_class=0):\n        super(ASAP, self).__init__()\n\n        self.num_class = num_class\n        self.max_seq_len = max_seq_len\n        self.node_encoder = node_encoder\n\n        self.conv1 = GraphConv(emb_dim, hidden, aggr='mean')\n        self.convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.convs.extend([\n            GraphConv(hidden, hidden, aggr='mean')\n            for i in range(num_layers - 1)\n        ])\n        self.pools.extend([\n            ASAPooling(hidden, ratio, dropout=dropout)\n            for i in range((num_layers) // 2)\n        ])\n        self.jump = JumpingKnowledge(mode='cat')\n        self.lin1 = Linear(num_layers * hidden, hidden)\n        \n\n        if self.num_class > 0:  \n            self.graph_pred_linear = torch.nn.Linear(hidden, self.num_class)\n        else:\n            self.graph_pred_linear_list = torch.nn.ModuleList()\n            for i in range(max_seq_len):\n                self.graph_pred_linear_list.append(torch.nn.Linear(hidden, num_vocab))\n\n    def reset_parameters(self):\n        self.conv1.reset_parameters()\n        for conv in self.convs:\n            conv.reset_parameters()\n        for pool in self.pools:\n            pool.reset_parameters()\n        self.lin1.reset_parameters()\n        self.lin2.reset_parameters()\n\n    def forward(self, data):\n        x, edge_index, node_depth, batch = data.x, data.edge_index, data.node_depth, data.batch\n\n        x = self.node_encoder(x, node_depth.view(-1, ))\n\n        edge_weight = None\n        x = F.relu(self.conv1(x, edge_index))\n        xs = [global_mean_pool(x, batch)]\n        for i, conv in enumerate(self.convs):\n            x = conv(x=x, edge_index=edge_index, edge_weight=edge_weight)\n            x = F.relu(x)\n            xs += [global_mean_pool(x, batch)]\n            if i % 2 == 0 and i < len(self.convs) - 1:\n                pool = self.pools[i // 2]\n                x, edge_index, edge_weight, batch, _ = pool(\n                    x=x, edge_index=edge_index, edge_weight=edge_weight,\n                    batch=batch)\n        x = self.jump(xs)\n        x = F.relu(self.lin1(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        \n        \n\n        if self.num_class > 0:\n            return self.graph_pred_linear(x)\n\n        pred_list = []\n        for i in range(self.max_seq_len):\n            pred_list.append(self.graph_pred_linear_list[i](x))\n        return pred_list\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n",
        "summary": "The ASAP class defines a graph neural network model using PyTorch Geometric, incorporating GraphConv layers, ASAPooling for graph pooling, and JumpingKnowledge for combining intermediate representations. It processes input data through multiple convolutional and pooling stages before making predictions either for a single class or multiple classes based on the number of vocabulary items."
    },
    {
        "code": "import rospy\nfrom duckietown_msgs.msg import WheelsCmdStamped, FSMState\nclass WheelsCmdSwitchNode(object):\n    def __init__(self):\n        self.node_name = rospy.get_name()\n        rospy.loginfo(\"[%s] Initializing \" %(self.node_name))\n       \n        self.mappings = rospy.get_param(\"~mappings\")\n        source_topic_dict = rospy.get_param(\"~source_topics\")\n        self.current_src_name = None\n\n        \n        self.pub_cmd = rospy.Publisher(\"~wheels_cmd\",WheelsCmdStamped,queue_size=1)\n        \n        \n        self.sub_fsm_state = rospy.Subscriber(rospy.get_param(\"~mode_topic\"),FSMState,self.cbFSMState)\n\n        self.sub_dict = dict()\n        for src_name, topic_name in source_topic_dict.items():\n            self.sub_dict[src_name] = rospy.Subscriber(topic_name,WheelsCmdStamped,self.cbWheelsCmd,callback_args=src_name)\n\n    def cbFSMState(self,fsm_state_msg):\n        self.current_src_name = self.mappings.get(fsm_state_msg.state)\n        if self.current_src_name is None:\n            rospy.logwarn(\"[%s] FSMState %s not handled. No msg pass through the switch.\" %(self.node_name,fsm_state_msg.state))\n\n    def cbWheelsCmd(self,msg,src_name):\n        if src_name == self.current_src_name:\n            self.pub_cmd.publish(msg)\n\n    def on_shutdown(self):\n        rospy.loginfo(\"[%s] Shutting down.\" %(self.node_name))\n\nif __name__ == '__main__':\n    \n    rospy.init_node('wheels_cmd_switch_node', anonymous=False)\n    \n    node = WheelsCmdSwitchNode()\n    \n    rospy.on_shutdown(node.on_shutdown)\n    \n    rospy.spin()\n",
        "summary": "The provided Python code defines a ROS (Robot Operating System) node named `WheelsCmdSwitchNode` that switches between different sources of wheel command messages based on the current state received from an FSM (Finite State Machine). It subscribes to multiple topics for wheel commands and an FSM state topic, publishing only those commands whose source matches the current FSM state. The node uses ROS parameters for configuration and handles shutdown gracefully."
    },
    {
        "code": "import json\n\nfrom alipay.aop.api.constant.ParamConstants import *\nfrom alipay.aop.api.domain.SettleEntity import SettleEntity\n\n\nclass AlipayTradeSettleReceivablesQueryModel(object):\n\n    def __init__(self):\n        self._biz_product = None\n        self._extend_params = None\n        self._merchant_info = None\n        self._out_request_no = None\n\n    @property\n    def biz_product(self):\n        return self._biz_product\n\n    @biz_product.setter\n    def biz_product(self, value):\n        self._biz_product = value\n    @property\n    def extend_params(self):\n        return self._extend_params\n\n    @extend_params.setter\n    def extend_params(self, value):\n        self._extend_params = value\n    @property\n    def merchant_info(self):\n        return self._merchant_info\n\n    @merchant_info.setter\n    def merchant_info(self, value):\n        if isinstance(value, SettleEntity):\n            self._merchant_info = value\n        else:\n            self._merchant_info = SettleEntity.from_alipay_dict(value)\n    @property\n    def out_request_no(self):\n        return self._out_request_no\n\n    @out_request_no.setter\n    def out_request_no(self, value):\n        self._out_request_no = value\n\n\n    def to_alipay_dict(self):\n        params = dict()\n        if self.biz_product:\n            if hasattr(self.biz_product, 'to_alipay_dict'):\n                params['biz_product'] = self.biz_product.to_alipay_dict()\n            else:\n                params['biz_product'] = self.biz_product\n        if self.extend_params:\n            if hasattr(self.extend_params, 'to_alipay_dict'):\n                params['extend_params'] = self.extend_params.to_alipay_dict()\n            else:\n                params['extend_params'] = self.extend_params\n        if self.merchant_info:\n            if hasattr(self.merchant_info, 'to_alipay_dict'):\n                params['merchant_info'] = self.merchant_info.to_alipay_dict()\n            else:\n                params['merchant_info'] = self.merchant_info\n        if self.out_request_no:\n            if hasattr(self.out_request_no, 'to_alipay_dict'):\n                params['out_request_no'] = self.out_request_no.to_alipay_dict()\n            else:\n                params['out_request_no'] = self.out_request_no\n        return params\n\n    @staticmethod\n    def from_alipay_dict(d):\n        if not d:\n            return None\n        o = AlipayTradeSettleReceivablesQueryModel()\n        if 'biz_product' in d:\n            o.biz_product = d['biz_product']\n        if 'extend_params' in d:\n            o.extend_params = d['extend_params']\n        if 'merchant_info' in d:\n            o.merchant_info = d['merchant_info']\n        if 'out_request_no' in d:\n            o.out_request_no = d['out_request_no']\n        return o\n\n\n",
        "summary": "This Python code defines a class `AlipayTradeSettleReceivablesQueryModel` for interacting with the Alipay API to query receivable settlements. It includes properties for business product, extended parameters, merchant information, and an external request number, along with methods to convert the object to and from a dictionary format suitable for JSON serialization and deserialization."
    },
    {
        "code": "import os\nfrom deta import Deta\nfrom datetime import date, datetime\nfrom fastapi import HTTPException\nimport urllib\nimport base64\n\ndeta = Deta()\n\nbase = deta.Base(\"drawings\")\n\ndrive = deta.Drive(\"drawings\")\n\n\ndef get_all(db, query):\n    blob_gen = db.fetch(query)\n    blobs = []\n    for stored_blob in blob_gen:\n        for blob in stored_blob:\n            blobs.append(blob)\n    return blobs\n\n\ndef get_drawings():\n    try:\n        return get_all(base, {})\n    except:\n        return None\n\n\ndef save(name, file):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    b = base.get(encoded_name)\n    try:\n        if (b):\n            base.put({\"key\": encoded_name, \"name\": name, \"public\": b[\"public\"], \"lastModified\": datetime.utcnow().timestamp()})\n            return drive.put(name, file)\n        base.put({\"key\":encoded_name, \"name\": name, \"public\": False, \"lastModified\": datetime.utcnow().timestamp()})\n        return drive.put(name, file)\n    except:\n        return None\n\n\ndef save_as(name, file, overwrite):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    b = base.get(encoded_name)\n    record = {\"key\": encoded_name, \"name\":name, \"public\": False, 'lastModified': datetime.utcnow().timestamp()}\n    if (overwrite or not b): \n        base.put(record)\n        drive.put(name, file)\n        return record\n    else:  \n        return None\n\ndef get_drawing(name):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    b = base.get(encoded_name)\n    d = drive.get(name)\n    if (b and d):\n        return d.read()\n    base.delete(encoded_name)\n    drive.delete(name)\n    return None\n\ndef get_metadata(name):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    b = base.get(encoded_name)\n    if (b):\n        return b\n    return None\n\ndef delete_drawing(name):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    try:\n        base.delete(encoded_name)\n        drive.delete(name)\n        return name\n    except:\n        return None\n    \ndef modify_public(name, public):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    try:\n        b = base.get(encoded_name)\n        if (b):\n            b[\"public\"] = public\n            return base.put(b)\n        return None\n    except:\n        return None\n    \ndef get_public_drawing(name):\n    encoded_name = str(base64.urlsafe_b64encode(name.encode(\"utf-8\")), 'utf-8')\n    try:\n        b = base.get(encoded_name)\n        if (b and b[\"public\"]):\n            return drive.get(name)\n        return None\n    except:\n        return None",
        "summary": "The provided Python code defines a set of functions to interact with a Deta Base and Drive, allowing for the creation, retrieval, modification, and deletion of drawings. It includes functionalities to save drawings, retrieve all drawings, get specific drawing metadata, modify public access status, and delete drawings. The code also handles exceptions and ensures data integrity by checking the existence of records before performing operations."
    },
    {
        "code": "from qiskit.circuit import QuantumCircuit\nfrom qiskit.extensions.exceptions import ExtensionError\nfrom .save_data import SaveSingleData, SaveAverageData, default_qubits\n\n\nclass SaveAmplitudes(SaveSingleData):\n    \n    def __init__(self,\n                 key,\n                 num_qubits,\n                 params,\n                 pershot=False,\n                 conditional=False):\n        \n        params = _format_amplitude_params(params, num_qubits)\n        super().__init__(\"save_amplitudes\",\n                         key,\n                         num_qubits,\n                         pershot=pershot,\n                         conditional=conditional,\n                         params=params)\n\n\nclass SaveAmplitudesSquared(SaveAverageData):\n    \n    def __init__(self,\n                 key,\n                 num_qubits,\n                 params,\n                 unnormalized=False,\n                 pershot=False,\n                 conditional=False):\n        \n        params = _format_amplitude_params(params, num_qubits)\n        super().__init__(\"save_amplitudes_sq\",\n                         key,\n                         num_qubits,\n                         unnormalized=unnormalized,\n                         pershot=pershot,\n                         conditional=conditional,\n                         params=params)\n\n\ndef save_amplitudes(self, key, params, pershot=False, conditional=False):\n    \n    qubits = default_qubits(self)\n    instr = SaveAmplitudes(key, len(qubits), params,\n                           pershot=pershot, conditional=conditional)\n    return self.append(instr, qubits)\n\n\ndef save_amplitudes_squared(self, key, params,\n                            unnormalized=False,\n                            pershot=False,\n                            conditional=False):\n    \n    qubits = default_qubits(self)\n    instr = SaveAmplitudesSquared(key, len(qubits), params,\n                                  unnormalized=unnormalized,\n                                  pershot=pershot,\n                                  conditional=conditional)\n    return self.append(instr, qubits)\n\n\ndef _format_amplitude_params(params, num_qubits=None):\n    \n    if isinstance(params[0], str):\n        if params[0].find('0x') == 0:\n            params = [int(i, 16) for i in params]\n        else:\n            params = [int(i, 2) for i in params]\n    if num_qubits and max(params) >= 2 ** num_qubits:\n        raise ExtensionError(\n            \"Param values contain a state larger than the number of qubits\")\n    return params\n\n\nQuantumCircuit.save_amplitudes = save_amplitudes\nQuantumCircuit.save_amplitudes_squared = save_amplitudes_squared\n",
        "summary": "The provided Python code defines two classes, `SaveAmplitudes` and `SaveAmplitudesSquared`, which inherit from `SaveSingleData` and `SaveAverageData` respectively. These classes are used to save amplitudes or their squares of quantum states in a quantum circuit. The `_format_amplitude_params` function formats the input parameters for these operations, ensuring they are valid qubit states. Finally, two methods, `save_amplitudes` and `save_amplitudes_squared`, are added to the `QuantumCircuit` class to allow users to easily save amplitudes or their squares directly from a quantum circuit instance."
    },
    {
        "code": "from string import Template\nfrom datetime import date\n\nbitcoinDir = \"./\";\n\ninFile     = bitcoinDir+\"/share/qt/Info.plist\"\noutFile    = \"Africoin-Qt.app/Contents/Info.plist\"\nversion    = \"unknown\";\n\nfileForGrabbingVersion = bitcoinDir+\"bitcoin-qt.pro\"\nfor line in open(fileForGrabbingVersion):\n\tlineArr = line.replace(\" \", \"\").split(\"=\");\n\tif lineArr[0].startswith(\"VERSION\"):\n\t\tversion = lineArr[1].replace(\"\\n\", \"\");\n\nfIn = open(inFile, \"r\")\nfileContent = fIn.read()\ns = Template(fileContent)\nnewFileContent = s.substitute(VERSION=version,YEAR=date.today().year)\n\nfOut = open(outFile, \"w\");\nfOut.write(newFileContent);\n\nprint \"Info.plist fresh created\"\n",
        "summary": "The Python script reads the version number from a `.pro` file and updates the `Info.plist` file with this version and the current year. It then saves the modified content to a new file named `Africoin-Qt.app/Contents/Info.plist`."
    },
    {
        "code": "import logging\nimport os\nimport urllib.parse\nfrom pathlib import Path\n\nimport aiohttp\nfrom aiofile import AIOFile\nfrom gcloud.aio.storage import Storage\nfrom google.cloud import storage\n\nfrom one_barangay.local_settings import logger\n\n\nasync def async_upload_to_bucket(\n    filepath: str,\n    file_obj,\n    gcs_path: str,\n):\n    \n    async with aiohttp.ClientSession() as session:\n        gcs_storage = Storage(session=session)  \n        gcs_filename = filepath.split(\"/\")[-1]\n        await gcs_storage.upload(gcs_path, gcs_filename, file_obj)\n        return f\"https://storage.googleapis.com/{gcs_path}/{urllib.parse.quote(gcs_filename)}\"\n\n\nasync def upload_to_gcs_runner(\n    filepath: str,\n    gcs_path: str,\n):\n    \n    \n    \n    try:\n        async with AIOFile(filepath, mode=\"rb\") as afp:\n            f = await afp.read()\n            path = await async_upload_to_bucket(filepath, f, gcs_path)\n            return path\n    except FileNotFoundError as e:\n        logger.exception(\"File not found. Make sure the file exists. %s\", e)\n    except OSError as e:\n        logger.exception(\"File not uploaded. %s\", e)\n\n\ndef download_from_gcs(\n    filename: str,\n    target_bucket_name: str,\n    bucket_folder: str,\n):\n    \n    try:\n        storage_client = storage.Client(os.getenv(\"GOOGLE_PROJECT_ID\"))\n        bucket_name = storage_client.get_bucket(target_bucket_name)\n        bucket = storage_client.get_bucket(bucket_name)\n        path = os.path.join(bucket_folder, filename)\n\n        base_dir = Path(__file__).resolve().parent.parent  \n\n        destination = os.path.join(base_dir, filename)\n        blob = bucket.blob(path)\n        blob.download_to_filename(destination)\n\n        logging.info(\"%s downloaded to %s.\", filename, destination)\n    except FileNotFoundError as e:\n        logger.exception(\"File not found. Make sure the file exists. %s\", e)\n    except OSError as e:\n        logger.exception(\"%s not downloaded. %s\", filename, e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code includes functions for uploading files to and downloading files from Google Cloud Storage (GCS) using asynchronous operations with `aiohttp` and `aiofile`. It handles file reading, uploading, and logging exceptions related to file operations."
    },
    {
        "code": "import sys, time, copy, torch, random, argparse\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom pathlib import Path\n\nlib_dir = (Path(__file__).parent / \"..\" / \"..\" / \"lib\").resolve()\nif str(lib_dir) not in sys.path:\n    sys.path.insert(0, str(lib_dir))\nfrom procedures import prepare_seed, prepare_logger, save_checkpoint, copy_checkpoint\nfrom log_utils import time_string\nfrom log_utils import AverageMeter, convert_secs2time\n\nfrom utils import split_str2indexes\n\nfrom procedures.advanced_main import basic_train_fn, basic_eval_fn\nfrom procedures.metric_utils import SaveMetric, MSEMetric, ComposeMetric\nfrom datasets.synthetic_core import get_synthetic_env\nfrom models.xcore import get_model\n\nfrom lfna_utils import lfna_setup\n\n\ndef subsample(historical_x, historical_y, maxn=10000):\n    total = historical_x.size(0)\n    if total <= maxn:\n        return historical_x, historical_y\n    else:\n        indexes = torch.randint(low=0, high=total, size=[maxn])\n        return historical_x[indexes], historical_y[indexes]\n\n\ndef main(args):\n    logger, env_info, model_kwargs = lfna_setup(args)\n\n    \n    to_evaluate_indexes = split_str2indexes(args.srange, env_info[\"total\"], None)\n    logger.log(\n        \"Evaluate {:}, which has {:} timestamps in total.\".format(\n            args.srange, len(to_evaluate_indexes)\n        )\n    )\n\n    w_container_per_epoch = dict()\n\n    per_timestamp_time, start_time = AverageMeter(), time.time()\n    for i, idx in enumerate(to_evaluate_indexes):\n\n        need_time = \"Time Left: {:}\".format(\n            convert_secs2time(\n                per_timestamp_time.avg * (len(to_evaluate_indexes) - i), True\n            )\n        )\n        logger.log(\n            \"[{:}]\".format(time_string())\n            + \" [{:04d}/{:04d}][{:04d}]\".format(i, len(to_evaluate_indexes), idx)\n            + \" \"\n            + need_time\n        )\n        \n        historical_x = env_info[\"{:}-x\".format(idx)]\n        historical_y = env_info[\"{:}-y\".format(idx)]\n        \n        model = get_model(dict(model_type=\"simple_mlp\"), **model_kwargs)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=args.init_lr, amsgrad=True)\n        criterion = torch.nn.MSELoss()\n        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=[\n                int(args.epochs * 0.25),\n                int(args.epochs * 0.5),\n                int(args.epochs * 0.75),\n            ],\n            gamma=0.3,\n        )\n        train_metric = MSEMetric()\n        best_loss, best_param = None, None\n        for _iepoch in range(args.epochs):\n            preds = model(historical_x)\n            optimizer.zero_grad()\n            loss = criterion(preds, historical_y)\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            \n            if best_loss is None or best_loss > loss.item():\n                best_loss = loss.item()\n                best_param = copy.deepcopy(model.state_dict())\n        model.load_state_dict(best_param)\n        with torch.no_grad():\n            train_metric(preds, historical_y)\n        train_results = train_metric.get_info()\n\n        metric = ComposeMetric(MSEMetric(), SaveMetric())\n        eval_dataset = torch.utils.data.TensorDataset(\n            env_info[\"{:}-x\".format(idx)], env_info[\"{:}-y\".format(idx)]\n        )\n        eval_loader = torch.utils.data.DataLoader(\n            eval_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0\n        )\n        results = basic_eval_fn(eval_loader, model, metric, logger)\n        log_str = (\n            \"[{:}]\".format(time_string())\n            + \" [{:04d}/{:04d}]\".format(idx, env_info[\"total\"])\n            + \" train-mse: {:.5f}, eval-mse: {:.5f}\".format(\n                train_results[\"mse\"], results[\"mse\"]\n            )\n        )\n        logger.log(log_str)\n\n        save_path = logger.path(None) / \"{:04d}-{:04d}.pth\".format(\n            idx, env_info[\"total\"]\n        )\n        w_container_per_epoch[idx] = model.get_w_container().no_grad_clone()\n        save_checkpoint(\n            {\n                \"model_state_dict\": model.state_dict(),\n                \"model\": model,\n                \"index\": idx,\n                \"timestamp\": env_info[\"{:}-timestamp\".format(idx)],\n            },\n            save_path,\n            logger,\n        )\n        logger.log(\"\")\n        per_timestamp_time.update(time.time() - start_time)\n        start_time = time.time()\n\n    save_checkpoint(\n        {\"w_container_per_epoch\": w_container_per_epoch},\n        logger.path(None) / \"final-ckp.pth\",\n        logger,\n    )\n\n    logger.log(\"-\" * 200 + \"\\n\")\n    logger.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"Use the data in the past.\")\n    parser.add_argument(\n        \"--save_dir\",\n        type=str,\n        default=\"./outputs/lfna-synthetic/use-same-timestamp\",\n        help=\"The checkpoint directory.\",\n    )\n    parser.add_argument(\n        \"--env_version\",\n        type=str,\n        required=True,\n        help=\"The synthetic enviornment version.\",\n    )\n    parser.add_argument(\n        \"--hidden_dim\",\n        type=int,\n        required=True,\n        help=\"The hidden dimension.\",\n    )\n    parser.add_argument(\n        \"--init_lr\",\n        type=float,\n        default=0.1,\n        help=\"The initial learning rate for the optimizer (default is Adam)\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=512,\n        help=\"The batch size\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=1000,\n        help=\"The total number of epochs.\",\n    )\n    parser.add_argument(\n        \"--srange\", type=str, required=True, help=\"The range of models to be evaluated\"\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=4,\n        help=\"The number of data loading workers (default: 4)\",\n    )\n    \n    parser.add_argument(\"--rand_seed\", type=int, default=-1, help=\"manual seed\")\n    args = parser.parse_args()\n    if args.rand_seed is None or args.rand_seed < 0:\n        args.rand_seed = random.randint(1, 100000)\n    assert args.save_dir is not None, \"The save dir argument can not be None\"\n    args.save_dir = \"{:}-{:}-d{:}\".format(\n        args.save_dir, args.env_version, args.hidden_dim\n    )\n    main(args)\n",
        "summary": "This Python script defines a function `main` that sets up a machine learning environment using synthetic data, trains a simple MLP model on historical data, evaluates its performance, and saves the model checkpoints. The script includes argument parsing for various parameters such as learning rate, batch size, and epochs, and it uses utilities for logging, checkpointing, and metric evaluation."
    },
    {
        "code": "class Solution:\n    def merge(self, intervals: List[List[int]]) -> List[List[int]]:\n        intervals = sorted(intervals, key = lambda x: x[0])\n        output = []\n        i = 0\n        if len(intervals) <= 1:\n            return intervals\n        while i < len(intervals) - 1:\n            tmp = intervals[i]\n            while tmp[1] >= intervals[i + 1][0]:\n                tmp[1] = max(tmp[1], intervals[i + 1][1])\n                i += 1\n                if i >= len(intervals) - 1:\n                    break\n            i += 1\n            output.append(tmp)\n        if i <= len(intervals) - 1:\n            output.append(intervals[-1])\n        return output\n",
        "summary": "The provided Python code defines a class `Solution` with a method `merge` that takes a list of intervals and merges overlapping intervals into a single interval. The method first sorts the intervals by their start times, then iterates through them to merge consecutive intervals where there is an overlap. Finally, it appends any remaining non-overlapping intervals to the output list and returns it."
    },
    {
        "code": "from rest_framework import viewsets\n\nfrom periodic_tasks_api.models import CustomExtendedPeriodicTask\nfrom periodic_tasks_api.serializers import PeriodicTaskSerializer\nfrom periodic_tasks_api.filters import PeriodicTaskFilterSet\n\n\nclass PeriodicTaskView(viewsets.ModelViewSet):\n    queryset = CustomExtendedPeriodicTask.objects.all()\n    serializer_class = PeriodicTaskSerializer\n    filter_backends = [PeriodicTaskFilterSet]\n",
        "summary": "The provided Python code defines a Django REST framework viewset named `PeriodicTaskView` for managing instances of `CustomExtendedPeriodicTask`. This viewset utilizes a custom serializer `PeriodicTaskSerializer` and applies filtering through `PeriodicTaskFilterSet` to the queryset."
    },
    {
        "code": "plugin_identifier = \"bedlevelvisualizer\"\n\n\nplugin_package = \"octoprint_bedlevelvisualizer\"\n\n\n\nplugin_name = \"Bed Visualizer\"\n\n\nplugin_version = \"0.1.15\"\n\n\n\nplugin_description = \n\n\nplugin_author = \"jneilliii\"\n\n\nplugin_author_email = \"jneilliii+octoprint@gmail.com\"\n\n\nplugin_url = \"https://github.com/jneilliii/OctoPrint-BedLevelVisualizer\"\n\n\nplugin_license = \"AGPLv3\"\n\n\nplugin_requires = [\"numpy>=1.16.0,<=1.19.2\"]\n\n\n\n\n\n\n\nplugin_additional_data = []\n\n\nplugin_addtional_packages = []\n\n\nplugin_ignored_packages = []\n\n\n\n\n\n\n\n\n\nadditional_setup_parameters = {}\n\n\n\nfrom setuptools import setup\n\ntry:\n\timport octoprint_setuptools\nexcept:\n\tprint(\"Could not import OctoPrint's setuptools, are you sure you are running that under \"\n\t      \"the same python installation that OctoPrint is installed under?\")\n\timport sys\n\tsys.exit(-1)\n\nsetup_parameters = octoprint_setuptools.create_plugin_setup_parameters(\n\tidentifier=plugin_identifier,\n\tpackage=plugin_package,\n\tname=plugin_name,\n\tversion=plugin_version,\n\tdescription=plugin_description,\n\tauthor=plugin_author,\n\tmail=plugin_author_email,\n\turl=plugin_url,\n\tlicense=plugin_license,\n\trequires=plugin_requires,\n\tadditional_packages=plugin_addtional_packages,\n\tignored_packages=plugin_ignored_packages,\n\tadditional_data=plugin_additional_data\n)\n\nif len(additional_setup_parameters):\n\tfrom octoprint.util import dict_merge\n\tsetup_parameters = dict_merge(setup_parameters, additional_setup_parameters)\n\nsetup(**setup_parameters)\n",
        "summary": "This Python script sets up a plugin for OctoPrint named \"Bed Visualizer\" using the `octoprint_setuptools` library. It defines various parameters such as the plugin's identifier, package name, version, author details, and dependencies, before passing these to the `setup` function from `setuptools` to install the plugin correctly."
    },
    {
        "code": "import contextlib\nimport json\nfrom typing import Any, BinaryIO, Dict, List, MutableMapping, Optional\n\nimport requests\nimport requests.auth\n\n\nclass RemoteCaller:\n    \n\n    def __init__(self, url_prefix: str, auth: Optional[requests.auth.AuthBase] = None) -> None:\n        self.url_prefix = url_prefix\n        self.auth = auth\n\n    def test_me(\n            self,\n            query_some_parameter: str,\n            path_some_parameter: str) -> bytes:\n        \n        url = \"\".join([\n            self.url_prefix,\n            '/products/',\n            str(path_some_parameter)])\n\n        params = {}  \n\n        params['some_parameter'] = query_some_parameter\n\n        resp = requests.request(\n            method='get',\n            url=url,\n            params=params,\n            auth=self.auth)\n\n        with contextlib.closing(resp):\n            resp.raise_for_status()\n            return resp.content\n\n\n\n",
        "summary": "The `RemoteCaller` class is designed to make HTTP GET requests to a specified URL prefix, appending a path parameter and including a query parameter in the request. It handles authentication if provided and ensures proper response handling by raising exceptions for errors and closing the response context."
    },
    {
        "code": "import yaml\n\nclass HexahueMap():\n\t\n\tdef __init__(self, space_color):\n\t\tpink   = (255, 0, 255)\n\t\tred    = (255, 0, 0)\n\t\tgreen  = (0, 255, 0)\n\t\tyellow = (255, 255, 0)\n\t\tblue   = (0, 0, 255)\n\t\tsky    = (0, 255, 255)\n\t\twhite  = (255, 255, 255)\n\t\tgray   = (128, 128, 128)\n\t\tblack  = (0, 0, 0)\n\t\t\n\t\tself.hmap = {}\n\t\tself.hmap[(pink, red, green, yellow, blue, sky)] = 'A'\n\t\tself.hmap[(red, pink, green, yellow, blue, sky)] = 'B'\n\t\tself.hmap[(red, green, pink, yellow, blue, sky)] = 'C'\n\t\tself.hmap[(red, green, yellow, pink, blue, sky)] = 'D'\n\t\tself.hmap[(red, green, yellow, blue, pink, sky)] = 'E'\n\t\tself.hmap[(red, green, yellow, blue, sky, pink)] = 'F'\n\t\tself.hmap[(green, red, yellow, blue, sky, pink)] = 'G'\n\t\tself.hmap[(green, yellow, red, blue, sky, pink)] = 'H'\n\t\tself.hmap[(green, yellow, blue, red, sky, pink)] = 'I'\n\t\tself.hmap[(green, yellow, blue, sky, red, pink)] = 'J'\n\t\tself.hmap[(green, yellow, blue, sky, pink, red)] = 'K'\n\t\tself.hmap[(yellow, green, blue, sky, pink, red)] = 'L'\n\t\tself.hmap[(yellow, blue, green, sky, pink, red)] = 'M'\n\t\tself.hmap[(yellow, blue, sky, green, pink, red)] = 'N'\n\t\tself.hmap[(yellow, blue, sky, pink, green, red)] = 'O'\n\t\tself.hmap[(yellow, blue, sky, pink, red, green)] = 'P'\n\t\tself.hmap[(blue, yellow, sky, pink, red, green)] = 'Q'\n\t\tself.hmap[(blue, sky, yellow, pink, red, green)] = 'R'\n\t\tself.hmap[(blue, sky, pink, yellow, red, green)] = 'S'\n\t\tself.hmap[(blue, sky, pink, red, yellow, green)] = 'T'\n\t\tself.hmap[(blue, sky, pink, red, green, yellow)] = 'U'\n\t\tself.hmap[(sky, blue, pink, red, green, yellow)] = 'V'\n\t\tself.hmap[(sky, pink, blue, red, green, yellow)] = 'W'\n\t\tself.hmap[(sky, pink, red, blue, green, yellow)] = 'X'\n\t\tself.hmap[(sky, pink, red, green, blue, yellow)] = 'Y'\n\t\tself.hmap[(sky, pink, red, green, yellow, blue)] = 'Z'\n\t\tself.hmap[(black, white, white, black, black, white)] = '.'\n\t\tself.hmap[(white, black, black, white, white, black)] = ','\n\t\tif space_color == 'black':\n\t\t\tself.hmap[(black, black, black, black, black, black)] = ' '\n\t\telif space_color == 'white':\n\t\t\tself.hmap[(white, white, white, white, white, white)] = ' '\n\t\telif space_color == 'all':\n\t\t\tself.hmap[(black, black, black, black, black, black)] = ' '\n\t\t\tself.hmap[(white, white, white, white, white, white)] = ' '\n\t\telse:\n\t\t\traise Exception('[Error] invalid space setting: ' + space_color)\n\t\tself.hmap[(black, gray, white, black, gray, white)] = '0'\n\t\tself.hmap[(gray, black, white, black, gray, white)] = '1'\n\t\tself.hmap[(gray, white, black, black, gray, white)] = '2'\n\t\tself.hmap[(gray, white, black, gray, black, white)] = '3'\n\t\tself.hmap[(gray, white, black, gray, white, black)] = '4'\n\t\tself.hmap[(white, gray, black, gray, white, black)] = '5'\n\t\tself.hmap[(white, black, gray, gray, white, black)] = '6'\n\t\tself.hmap[(white, black, gray, white, gray, black)] = '7'\n\t\tself.hmap[(white, black, gray, white, black, gray)] = '8'\n\t\tself.hmap[(black, white, gray, white, black, gray)] = '9'",
        "summary": "The `HexahueMap` class in Python defines a mapping of color tuples to characters using the `yaml` library for potential serialization. It initializes with predefined color mappings for letters and digits, as well as special characters like space, period, comma, and numbers 0-9, based on the specified `space_color`."
    },
    {
        "code": "for n in range(1, 101):\n    print(n, \"\u042f \u043d\u0435 \u0431\u0443\u0434\u0443 \u0457\u0441\u0442\u0438 \u043f\u0430\u043b\u0438\u0447\u043a\u0438 \u0411\u043e\u0431\u043e \u043d\u0430 \u0443\u0440\u043e\u0446\u0456\")\n",
        "summary": "The Python code iterates through numbers from 1 to 100 and prints each number followed by the Russian phrase \"\u042f \u043d\u0435 \u0431\u0443\u0434\u0443 \u0457\u0441\u0442\u0438 \u043f\u0430\u043b\u0438\u0447\u043a\u0438 \u0411\u043e\u0431\u043e \u043d\u0430 \u0443\u0440\u043e\u0446\u0456\" (I won't eat Bobo sticks in class)."
    },
    {
        "code": "class OrderingManager(object):\n    \n\n    def __init__(self, client):\n        self.client = client\n\n    def get_packages_of_type(self, package_types, mask=None):\n        \n\n        package_service = self.client['Product_Package']\n        _filter = {\n            'type': {\n                'keyName': {\n                    'operation': 'in',\n                    'options': [\n                        {'name': 'data',\n                         'value': package_types}\n                    ],\n                },\n            },\n        }\n\n        packages = package_service.getAllObjects(mask=mask, filter=_filter)\n        packages = self.filter_outlet_packages(packages)\n        return packages\n\n    @staticmethod\n    def filter_outlet_packages(packages):\n        \n\n        non_outlet_packages = []\n\n        for package in packages:\n            if all(['OUTLET' not in package.get('description', '').upper(),\n                    'OUTLET' not in package.get('name', '').upper()]):\n                non_outlet_packages.append(package)\n\n        return non_outlet_packages\n\n    @staticmethod\n    def get_only_active_packages(packages):\n        \n\n        active_packages = []\n\n        for package in packages:\n            if package['isActive']:\n                active_packages.append(package)\n\n        return active_packages\n\n    def get_package_by_type(self, package_type, mask=None):\n        \n        packages = self.get_packages_of_type([package_type], mask)\n        if len(packages) == 0:\n            return None\n        else:\n            return packages.pop()\n\n    def get_package_id_by_type(self, package_type):\n        \n\n        mask = \"mask[id, name, description, isActive, type[keyName]]\"\n        package = self.get_package_by_type(package_type, mask)\n        if package:\n            return package['id']\n        else:\n            raise ValueError(\"No package found for type: \" + package_type)\n\n    def get_quotes(self):\n        \n\n        quotes = self.client['Account'].getActiveQuotes()\n        return quotes\n\n    def get_quote_details(self, quote_id):\n        \n\n        quote = self.client['Billing_Order_Quote'].getObject(id=quote_id)\n        return quote\n\n    def get_order_container(self, quote_id):\n        \n\n        quote = self.client['Billing_Order_Quote']\n        container = quote.getRecalculatedOrderContainer(id=quote_id)\n        return container['orderContainers'][0]\n\n    def generate_order_template(self, quote_id, extra, quantity=1):\n        \n\n        container = self.get_order_container(quote_id)\n        container['quantity'] = quantity\n\n        \n        \n        \n        \n        if container['packageId'] == 46:\n            product_type = 'virtualGuests'\n        else:\n            product_type = 'hardware'\n\n        if len(extra) != quantity:\n            raise ValueError(\"You must specify extra for each server in the \"\n                             \"quote\")\n\n        container[product_type] = []\n        for extra_details in extra:\n            container[product_type].append(extra_details)\n        container['presetId'] = None\n        return container\n\n    def verify_quote(self, quote_id, extra, quantity=1):\n        \n\n        container = self.generate_order_template(quote_id, extra,\n                                                 quantity=quantity)\n        return self.client['Product_Order'].verifyOrder(container)\n\n    def order_quote(self, quote_id, extra, quantity=1):\n        \n\n        container = self.generate_order_template(quote_id, extra,\n                                                 quantity=quantity)\n        return self.client['Product_Order'].placeOrder(container)\n",
        "summary": "The `OrderingManager` class manages ordering processes by interacting with a client to fetch and filter packages based on type, verify quotes, and place orders. It includes methods for retrieving package details, generating order templates, verifying quotes, and placing orders."
    },
    {
        "code": "import sys\n\nsi = sys.stdin.readline\n\n\nt = [0] * 17\ndp = [0] * 17\n\nn = int(si())\nfor i in range(1, n + 1):\n    m, o = map(int, si().split())\n    t[i] = m\n    dp[i] = o\n\n\ndef solve(n):\n    ans = 0\n    for i in range(n, 0, -1):\n        if i + t[i] > n + 1:\n            dp[i] = dp[i + 1]\n        else:\n            dp[i] = max(dp[i + 1], dp[i] + dp[i + t[i]])\n            ans = max(ans, dp[i])\n    return ans\n\n\nprint(solve(n))",
        "summary": "The Python code reads input values for the number of items and their respective weights and values. It then calculates the maximum value that can be obtained without exceeding a given weight limit using dynamic programming, storing intermediate results in arrays `t` and `dp`. Finally, it outputs the maximum possible value."
    },
    {
        "code": "import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import InterpolationMode\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Resize(osize, InterpolationMode.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n",
        "summary": "The provided Python code defines a base dataset class `BaseDataset` and a function `get_transform` that constructs image transformations based on the options provided. The transformations include resizing, cropping, flipping, and normalization, tailored for training or inference in machine learning models using PyTorch."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.apps import AppConfig\n\n\nclass TenxConfig(AppConfig):\n    name = 'tenx'",
        "summary": "The provided Python code defines a Django application configuration class named `TenxConfig` within the module `tenx`. This class inherits from `AppConfig` and sets the `name` attribute to `'tenx'`, which is likely used to identify or reference this specific Django application."
    },
    {
        "code": "import pyperclip\nimport math\n\nclass Affine_Cipher:\n\n    def __init__(self):\n        self.SYMBOLS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890 !?.'\n\n    def check_key(self, key):\n\n        keyA = key // len(self.SYMBOLS)\n        keyB = key % len(self.SYMBOLS)\n        \n        if keyA == 1:\n            print('Cipher is weak if key A is 1. Choose a different key.')\n            return False\n        if keyB == 0:\n            print('Cipher is weak if key B is 0. Choose a different key.')\n            return False\n        if keyA < 0 or keyB < 0 or keyB > len(self.SYMBOLS) - 1:\n            print('Key A must be greater than 0 and Key B must be between 0 and {}.'.format(len(self.SYMBOLS) - 1))\n            return False\n        if math.gcd(keyA, len(self.SYMBOLS)) != 1:\n            print(\"Key A {} and the symbol set size {} are not relatively prime. Choose a different key.\".format(keyA, len(self.SYMBOLS)))\n            return False\n\n        return True\n\n    def mod_inv(self, a, m):\n        if math.gcd(a, m) != 1:\n            return False\n        u1, u2, u3 = 1, 0, a\n        v1, v2, v3 = 0, 1, m\n\n        while v3 != 0:\n            q = u3 // v3\n            v1, v2, v3, u1, u2, u3 = (u1 - q * v1), (u2 - q * v2), (u3 - q * v3), v1, v2, v3\n\n        return u1 % m\n\n    def encrypt(self, plain_text, key):\n        keyA = key // len(self.SYMBOLS)\n        keyB = key % len(self.SYMBOLS)\n\n        cipher_text = []\n\n        for char in plain_text:\n            if char in self.SYMBOLS:\n                index = self.SYMBOLS.find(char)\n                cipher_text.append(self.SYMBOLS[(index * keyA + keyB) % len(self.SYMBOLS)])\n            else:\n                cipher_text.append(char)\n\n        return \"\".join(cipher_text)\n\n\n    def decrypt(self, cipher_text, key):\n        keyA = key // len(self.SYMBOLS)\n        keyB = key % len(self.SYMBOLS)\n        mod_inverse = self.mod_inv(keyA, len(self.SYMBOLS))\n        if mod_inverse == False:\n            print(\"MOD INV FALSE\")\n\n        plain_text = []\n\n        for char in cipher_text:\n            if char in self.SYMBOLS:\n                index = self.SYMBOLS.find(char)\n                plain_text.append(self.SYMBOLS[(index - keyB) * mod_inverse % len(self.SYMBOLS)])\n            else:\n                plain_text.append(char)\n\n        return \"\".join(plain_text)\n\n\n    def brute_force_decrypt(self, cipher_text):\n\n        for key in range(len(self.SYMBOLS) ** 2):\n            keyA = key // len(self.SYMBOLS)\n\n            if math.gcd(keyA, len(self.SYMBOLS)) != 1:\n                continue\n\n            decrypted_text = self.decrypt(cipher_text, key)\n            print(\"Key = {}, Plain text = {}\".format(key, decrypted_text))\n\n        return None\n\n\ndef ask_user():\n    print(\"Select an option:\")\n    print(\"1. To continue\")\n    print(\"2. To exit\")\n    option = input()\n    return option\n\n\nif __name__ == \"__main__\":\n    affine_cipher = Affine_Cipher()\n    while True:\n        try:\n            print(\"Select an option:\")\n            print(\"1. Encrypt a message\")\n            print(\"2. Decrypt a message\")\n            option = input()\n            if option == '1':\n                print(\"Enter plain text to be encrypted: \")\n                plain_text = input()\n                print(\"Enter a number (key) for encryption: \")\n                key = int(input())\n\n                while affine_cipher.check_key(key) == False:\n                    print(\"Enter the new key for encryption: \")\n                    key = int(input())\n\n                cipher_text = affine_cipher.encrypt(plain_text, key)\n                print(\"Cipher text =\", cipher_text)\n                pyperclip.copy(cipher_text)\n                pyperclip.paste()\n                print(\"The cipher text has been copied to your clipboard\" + \"\\n\")\n\n                option = ask_user()\n                if option == '1':\n                    continue\n                elif option == '2':\n                    break\n                else:\n                    print(\"Incorrect input.\")\n                    print(\"Exiting program\")\n                    break\n\n            elif option == '2':\n                print(\"Enter cipher text to be decrypted: \")\n                cipher_text = input()\n                print(\"Enter key for decryption: \")\n                print(\"If you do not know the key and would like to brute force the combinations, enter the word - crack\")\n\n                key = input()\n                if key == 'crack':\n                    affine_cipher.brute_force_decrypt(cipher_text)\n\n                else:\n                    key = int(key)\n\n                    plain_text = affine_cipher.decrypt(cipher_text, key)\n                    print(\"Plain text =\", plain_text)\n\n                    pyperclip.copy(plain_text)\n                    pyperclip.paste()\n                    print(\"The plain text has been copied to your clipboard\" + \"\\n\")\n\n                option = ask_user()\n                if option == '1':\n                    continue\n                elif option == '2':\n                    print(\"Exiting program\")\n                    break\n                else:\n                    print(\"Incorrect input.\")\n                    print(\"Exiting program\")\n                    break\n\n            else:\n                print(\"Incorrect input.\")\n                option = ask_user()\n                if option == '1':\n                    continue\n                elif option == '2':\n                    print(\"Exiting program\")\n                    break\n                else:\n                    print(\"Incorrect input.\")\n                    print(\"Exiting program\")\n                    break\n\n        except Exception as e:\n            option = ask_user()\n            if option == '1':\n                continue\n            elif option == '2':\n                print(\"Exiting program\")\n                break\n            else:\n                print(\"Incorrect input.\")\n                print(\"Exiting program\")\n                break",
        "summary": "The provided Python code implements an Affine Cipher, a symmetric encryption algorithm that uses two keys: keyA and keyB. It includes methods for encrypting and decrypting messages, as well as brute-forcing decryption if the key is unknown. The script allows users to interactively select options to either encrypt or decrypt messages, with error handling and clipboard functionality for convenience."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport datetime\nfrom django.utils.timezone import utc\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('stocks', '0003_auto_20151129_1623'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='floor',\n            name='floorPlayer',\n            field=models.ForeignKey(to='stocks.Player', related_name='FloorPlayer'),\n        ),\n        migrations.AlterField(\n            model_name='stock',\n            name='last_updated',\n            field=models.DateTimeField(default=datetime.datetime(2015, 11, 29, 22, 5, 30, 24205, tzinfo=utc)),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that alters the `floorPlayer` field in the `Floor` model to be a foreign key related to the `Player` model, and sets the default value of the `last_updated` field in the `Stock` model to a specific datetime."
    },
    {
        "code": "import math\nimport numpy as np\nimport imageio\nfrom scipy import ndimage\n\n\nnp.seterr(divide='ignore', invalid='ignore')\n\nLEVELS = 256\n\n\n\n\nf = input().rstrip()\n\ng = input().rstrip()\n\nb = int(input())\n\n\n\n\n\nf = imageio.imread(f)\n\ng = imageio.imread(g)\n\n\n\n\n\ndef luminance(img):\n  \n  N, M, _ = img.shape\n  out = np.empty(img.shape)\n  out = 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]\n  return out.astype(np.uint8)\n\n\n\nf = luminance(f)\ng = luminance(g)\n\n\n\nB = 8 - b\nf = f >> B\ng = g >> B\n\n\n\n\ndef nh_descriptor(f):\n  \n  hist, _ = np.histogram(f, bins=[i for i in range(2 ** b + 1)])\n  hist = hist / hist.sum()\n  dc = hist / np.linalg.norm(hist)\n  return dc\n\ndef ht_descriptor(f):\n  \n  \n  N, M = f.shape\n  C = np.zeros((LEVELS, LEVELS))\n  for x in range(N - 1):\n    for y in range(M - 1):\n      i = f[x, y]\n      j = f[x + 1, y + 1]\n      C[i][j] += 1\n  C = C / C.sum()\n  \n  \n  N, M = C.shape\n  \n  energy = np.power(C, 2).sum()\n  \n  epsilon = 0.001\n  entropy = - (C * np.log(C + epsilon)).sum()\n  \n  A = np.fromfunction(lambda i, j: (i - j) ** 2, (N, M), dtype=int)\n  contrast = (1 / math.pow(N, 2)) * (C * A).sum()\n  \n  mu_i, si_i = 0, 0\n  mu_j, si_j = 0, 0\n  for k in range(N):\n    a1 = C[k,:].sum()\n    mu_i += k * a1\n    si_i += math.pow(k - mu_i, 2) * a1\n    \n    a2 = C[:,k].sum()\n    mu_j += k * a2\n    si_j += math.pow(k - mu_j, 2) * a2\n  \n  A = np.fromfunction(lambda i, j: (i - j) ** 2, (N, M), dtype=int)\n  correlation = (A * C).sum() - mu_i * mu_j\n  correlation /= (si_i * si_j)\n  \n  homogeneity = 0\n  \n  A = np.fromfunction(lambda i, j: (1 + abs(i - j)), (N, M), dtype=int)\n  homogeneity = (C * A).sum()\n  \n  \n  dt = np.array([energy, entropy, contrast, correlation, homogeneity])\n  dt = dt / np.linalg.norm(dt)\n  return dt\n\ndef hg_descriptor(f):\n  \n  wsx = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n  wsy = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n  \n  f = f.astype(np.float64)\n  fx = ndimage.convolve(f, wsx)\n  fy = ndimage.convolve(f, wsy)\n  \n  N, M = f.shape\n  \n  div = np.sqrt(np.power(fx, 2) + np.power(fy, 2)).sum()\n  Mg = np.sqrt(np.power(fx, 2) + np.power(fy, 2)) / div\n  \n  sigma = np.zeros(f.shape)\n  sigma = np.arctan(fy / fx) + np.pi / 2\n  sigma = np.degrees(sigma)\n  sigma = np.digitize(sigma, np.arange(0, 180, 20))\n  sigma = sigma.astype(np.uint8)\n  \n  dg = np.zeros(9)\n  for x in range(N):\n    for y in range(M):\n      dg[sigma[x][y] - 1] += Mg[x][y]\n  \n  dg = dg / np.linalg.norm(dg)\n  return dg\n\n\n\n\n\ndc = nh_descriptor(f)\ndt = ht_descriptor(f)\ndg = hg_descriptor(f)\n\nd = np.concatenate((dc, dt, dg))\n\n\n\n\n\ndef distance(d, di):\n  \n  return math.sqrt(np.power(d - di, 2).sum())\n\n\n\n\nsize = f.shape[0]\nstep = size // 2\nN, M = g.shape\nN = N // step\nM = M // step\n\ndist = np.iinfo(np.uint8).max\n\npos_x = None\npos_y = None\n\nfor i in range(N - 1):\n  for j in range(M - 1):\n    \n    window = g[i*step:i*step+size, j*step:j*step+size]\n    \n    window_dc = nh_descriptor(window)\n    window_dt = ht_descriptor(window)\n    window_dg = hg_descriptor(window)\n    window_d = np.concatenate((window_dc, window_dt, window_dg))\n    \n    ndist = distance(d, window_d)\n    if dist > ndist:\n      dist = ndist\n      pos_x, pos_y = i, j\n\n\n\n\nprint(pos_x, pos_y)\n",
        "summary": "The Python script reads two image files and an integer input, calculates descriptors for the images using luminance, normalized histogram (NH), homogeneous texture (HT), and gradient (HG) methods, and then finds the position in the second image where a window matches the first image based on these descriptors."
    },
    {
        "code": "import pytest\n\nfrom tests.tesla_mock import TeslaMock\n\nfrom teslajsonpy.controller import Controller\nfrom teslajsonpy.trunk import TrunkLock\n\n\ndef test_has_battery(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _lock = TrunkLock(_data, _controller)\n\n    assert not _lock.has_battery()\n\n\ndef test_is_locked_on_init(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _lock = TrunkLock(_data, _controller)\n\n    assert _lock is not None\n    assert not _lock.is_locked()\n\n\n@pytest.mark.asyncio\nasync def test_is_locked_after_update(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _data[\"vehicle_state\"][\"rt\"] = 0\n    _lock = TrunkLock(_data, _controller)\n\n    await _lock.async_update()\n\n    assert _lock is not None\n    assert _lock.is_locked()\n\n\n@pytest.mark.asyncio\nasync def test_unlock(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _data[\"vehicle_state\"][\"rt\"] = 0\n    _lock = TrunkLock(_data, _controller)\n\n    await _lock.async_update()\n    await _lock.unlock()\n\n    assert _lock is not None\n    assert not _lock.is_locked()\n\n\n@pytest.mark.asyncio\nasync def test_unlock_already_unlocked(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _data[\"vehicle_state\"][\"rt\"] = 123\n    _lock = TrunkLock(_data, _controller)\n\n    await _lock.async_update()\n    await _lock.unlock()\n\n    assert _lock is not None\n    assert not _lock.is_locked()\n\n    \n    _data[\"vehicle_state\"][\"rt\"] = 0\n\n\n@pytest.mark.asyncio\nasync def test_lock(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _data[\"vehicle_state\"][\"rt\"] = 123\n    _lock = TrunkLock(_data, _controller)\n\n    await _lock.async_update()\n    await _lock.lock()\n\n    assert _lock is not None\n    assert _lock.is_locked()\n\n    \n    _data[\"vehicle_state\"][\"rt\"] = 0\n\n\n@pytest.mark.asyncio\nasync def test_lock_already_locked(monkeypatch):\n    \n\n    _mock = TeslaMock(monkeypatch)\n    _controller = Controller(None)\n\n    _data = _mock.data_request_vehicle()\n    _data[\"vehicle_state\"][\"rt\"] = 0\n    _lock = TrunkLock(_data, _controller)\n\n    await _lock.async_update()\n    await _lock.lock()\n\n    assert _lock is not None\n    assert _lock.is_locked()\n",
        "summary": "The provided Python code defines a series of test functions using the `pytest` framework to validate the behavior of a `TrunkLock` class, which interacts with a Tesla vehicle's trunk lock. The tests cover various scenarios such as checking if the trunk has a battery, verifying initial lock status, updating the lock state asynchronously, and attempting to lock or unlock the trunk in different conditions."
    },
    {
        "code": "import sys\nfrom cx_Freeze import setup, Executable\n\nsetup(\n    name='YtMusic-Lib-Tracker',\n    url='https://github.com/czifumasa/ytmusic-lib-tracker',\n    author='\u0141ukasz Lenart',\n    author_email='lukasz.lenart912@gmail.com',\n    version='0.1',\n    license='MIT',\n    description='Useful tools for youtube music. Exporting library to csv, tracking changes in library, summary of transfer from GPM',\n    long_description=open('README.md').read(),\n    options={\"build_exe\": {\n        'packages': ['ytmusicapi', 'unidecode'],\n        'excludes': ['tkinter', 'test', 'unittest', 'pydoc_data'],\n        'include_files': ['config.ini'],\n        'optimize': 2,\n    }},\n    executables=[Executable('ytmusiclibtracker.py', base='console', icon='ytmlt.ico', targetName='YTMusicLibTracker')]\n)",
        "summary": "This Python script uses `cx_Freeze` to package a YouTube Music library tracker application named 'YtMusic-Lib-Tracker' into an executable, including necessary packages and excluding unnecessary ones. The setup includes options for building the executable with specific optimizations and including additional files like a configuration file and an icon."
    },
    {
        "code": "import RPi.GPIO as GPIO\nimport time\n\nCS = 5\nClock = 25\nAddress = 24\nDataOut = 23\nButton = 7\n\nclass TRSensor(object):\n\tdef __init__(self,numSensors = 5):\n\t\tself.numSensors = numSensors\n\t\tself.calibratedMin = [0] * self.numSensors\n\t\tself.calibratedMax = [1023] * self.numSensors\n\t\tself.last_value = 0\n\t\tGPIO.setmode(GPIO.BCM)\n\t\tGPIO.setwarnings(False)\n\t\tGPIO.setup(Clock,GPIO.OUT)\n\t\tGPIO.setup(Address,GPIO.OUT)\n\t\tGPIO.setup(CS,GPIO.OUT)\n\t\tGPIO.setup(DataOut,GPIO.IN,GPIO.PUD_UP)\n\t\tGPIO.setup(Button,GPIO.IN,GPIO.PUD_UP)\n\t\t\n\t\n\tdef AnalogRead(self):\n\t\tvalue = [0]*(self.numSensors+1)\n\t\t\n\t\tfor j in range(0,self.numSensors+1):\n\t\t\tGPIO.output(CS, GPIO.LOW)\n\t\t\tfor i in range(0,4):\n\t\t\t\t\n\t\t\t\tif(((j) >> (3 - i)) & 0x01):\n\t\t\t\t\tGPIO.output(Address,GPIO.HIGH)\n\t\t\t\telse:\n\t\t\t\t\tGPIO.output(Address,GPIO.LOW)\n\t\t\t\t\n\t\t\t\tvalue[j] <<= 1\n\t\t\t\tif(GPIO.input(DataOut)):\n\t\t\t\t\tvalue[j] |= 0x01\n\t\t\t\tGPIO.output(Clock,GPIO.HIGH)\n\t\t\t\tGPIO.output(Clock,GPIO.LOW)\n\t\t\tfor i in range(0,6):\n\t\t\t\t\n\t\t\t\tvalue[j] <<= 1\n\t\t\t\tif(GPIO.input(DataOut)):\n\t\t\t\t\tvalue[j] |= 0x01\n\t\t\t\tGPIO.output(Clock,GPIO.HIGH)\n\t\t\t\tGPIO.output(Clock,GPIO.LOW)\n\t\t\t\n\n\n\n\t\t\ttime.sleep(0.0001)\n\t\t\tGPIO.output(CS,GPIO.HIGH)\n\n\t\treturn value[1:]\n\t\t\n\t\n\tdef calibrate(self):\n\t\tmax_sensor_values = [0]*self.numSensors\n\t\tmin_sensor_values = [0]*self.numSensors\n\t\tfor j in range(0,10):\n\t\t\n\t\t\tsensor_values = self.AnalogRead()\n\t\t\t\n\t\t\tfor i in range(0,self.numSensors):\n\t\t\t\n\t\t\t\t\n\t\t\t\tif((j == 0) or max_sensor_values[i] < sensor_values[i]):\n\t\t\t\t\tmax_sensor_values[i] = sensor_values[i]\n\n\t\t\t\t\n\t\t\t\tif((j == 0) or min_sensor_values[i] > sensor_values[i]):\n\t\t\t\t\tmin_sensor_values[i] = sensor_values[i]\n\n\t\t\n\t\tfor i in range(0,self.numSensors):\n\t\t\tif(min_sensor_values[i] > self.calibratedMin[i]):\n\t\t\t\tself.calibratedMin[i] = min_sensor_values[i]\n\t\t\tif(max_sensor_values[i] < self.calibratedMax[i]):\n\t\t\t\tself.calibratedMax[i] = max_sensor_values[i]\n\n\t\n\tdef\treadCalibrated(self):\n\t\tvalue = 0\n\t\t\n\t\tsensor_values = self.AnalogRead()\n\n\t\tfor i in range (0,self.numSensors):\n\n\t\t\tdenominator = self.calibratedMax[i] - self.calibratedMin[i]\n\n\t\t\tif(denominator != 0):\n\t\t\t\tvalue = (sensor_values[i] - self.calibratedMin[i])* 1000 / denominator\n\t\t\t\t\n\t\t\tif(value < 0):\n\t\t\t\tvalue = 0\n\t\t\telif(value > 1000):\n\t\t\t\tvalue = 1000\n\t\t\t\t\n\t\t\tsensor_values[i] = value\n\t\t\n\t\t\n\t\treturn sensor_values\n\t\t\t\n\t\n\tdef readLine(self, white_line = 0):\n\n\t\tsensor_values = self.readCalibrated()\n\t\tavg = 0\n\t\tsum = 0\n\t\ton_line = 0\n\t\tfor i in range(0,self.numSensors):\n\t\t\tvalue = sensor_values[i]\n\t\t\tif(white_line):\n\t\t\t\tvalue = 1000-value\n\t\t\t\n\t\t\tif(value > 200):\n\t\t\t\ton_line = 1\n\t\t\t\t\n\t\t\t\n\t\t\tif(value > 50):\n\t\t\t\tavg += value * (i * 1000);  \n\t\t\t\tsum += value;                  \n\n\t\tif(on_line != 1):\n\t\t\t\n\t\t\tif(self.last_value < (self.numSensors - 1)*1000/2):\n\t\t\t\t\n\t\t\t\tself.last_value = 0\n\t\n\t\t\t\n\t\t\telse:\n\t\t\t\t\n\t\t\t\tself.last_value = (self.numSensors - 1)*1000\n\t\telse:\n\t\t\tself.last_value = avg/sum\n\t\t\n\t\treturn self.last_value,sensor_values\n\t\n\n\n\nif __name__ == '__main__':\n\tTR = TRSensor()\n\tprint(\"TRSensor Example\")\n\twhile True:\n\t\tprint(TR.AnalogRead())\n\t\ttime.sleep(0.2)\n\n\t\t\t \n",
        "summary": "The provided Python code defines a class `TRSensor` for interfacing with an analog sensor module on a Raspberry Pi, allowing it to read sensor values, calibrate them, and interpret the data to determine if the sensors are detecting a line or not. The script initializes the GPIO pins, reads raw sensor data, calibrates the readings, and continuously prints the sensor values in a loop."
    },
    {
        "code": "import connexion\nfrom openapi_server.annotator.phi_types import PhiType\nfrom openapi_server.get_annotations import get_annotations\nfrom openapi_server.models.error import Error  \nfrom openapi_server.models.text_date_annotation_request import \\\n    TextDateAnnotationRequest  \nfrom openapi_server.models.text_date_annotation_response import \\\n    TextDateAnnotationResponse  \n\n\ndef create_text_date_annotations():  \n    \n    res = None\n    status = None\n    if connexion.request.is_json:\n        try:\n            annotation_request = TextDateAnnotationRequest.from_dict(\n                connexion.request.get_json())  \n            note = annotation_request.note\n            annotations = get_annotations(note, phi_type=PhiType.DATE)\n\n            res = TextDateAnnotationResponse(annotations)\n            status = 200\n        except Exception as error:\n            status = 500\n            res = Error(\"Internal error\", status, str(error))\n    return res, status\n",
        "summary": "The Python function `create_text_date_annotations` processes incoming JSON requests to extract date annotations from text notes using a specified annotation service. It returns a response containing the extracted annotations or an error message if processing fails."
    },
    {
        "code": "import sys\nimport Tkinter as tk\n\nimport service\nimport keycode\n\nif sys.platform == 'win32':\n    from ctypes import wintypes, byref, windll\n    import win32con\n\n    def handle_hotkey(root, callback):\n        msg = wintypes.MSG()\n        if windll.user32.GetMessageA(byref(msg), None, 0, 0) != 0:\n            if msg.message == win32con.WM_HOTKEY:\n                if msg.wParam == 1:\n                    print 'Hotkey triggered!'\n                    callback()\n        windll.user32.TranslateMessage(byref(msg))\n        windll.user32.DispatchMessageA(byref(msg))\n        root.after(1, handle_hotkey, root, callback)\n\n    \n    \n    def register_hotkey(root, key, callback):\n        key = key.split('-')\n        mod = 0\n        if 'Ctrl' in key:\n            mod |= win32con.MOD_CONTROL\n        if 'Shift' in key:\n            mod |= win32con.MOD_SHIFT\n        if 'Alt' in key:\n            mod |= win32con.MOD_ALT\n        key = key[-1].upper()\n        assert key in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n        if windll.user32.RegisterHotKey(None, 1, mod, ord(key)) != 0:\n            print(\"Hotkey registered!\")\n            handle_hotkey(root, callback)\n\nelse:\n    def register_hotkey(root, key, callback):\n        print 'Register hotkey failed.'\n\ndef main():\n    service.start()\n\n    root = tk.Tk()\n    root.resizable(0, 0)\n    root.title('STF Input')\n    sv = tk.StringVar()\n\n    if sys.platform == 'win32':\n        backspace = '\\x08'\n    else:\n        backspace = '\\x7f'\n\n    def send(event, sv=sv):\n        char = event.char\n        if not char:\n            return\n        text = sv.get()\n        if char == '\\r' and text: \n            service.type(text)\n            sv.set('')\n            return\n        if char == backspace and text: \n            sv.set('')\n            return\n        if char == '\\x16': \n            service.keyboard(char)\n            sv.set('')\n            return 'break'\n        if char in keycode.KEYBOARD_KEYS or char in keycode.CTRLED_KEYS:\n            service.keyboard(char)\n\n    entry = tk.Entry(root, textvariable=sv)\n    entry.pack()\n    entry.focus_set()\n    entry.bind('<Key>', send)\n\n    state = [1]\n    def toggle(root=root, entry=entry):\n        if state[0] == 0:\n            root.deiconify()\n            entry.focus_set()\n            state[0] = 1\n        else:\n            root.withdraw()\n            state[0] = 0\n\n    register_hotkey(root, 'Ctrl-Alt-Z', toggle) \n\n    try:\n        root.mainloop()\n    finally:\n        service.stop()\n\nif __name__ == '__main__':\n    main()",
        "summary": "This Python script uses the Tkinter library to create a graphical user interface (GUI) for inputting text and registering hotkeys. On Windows, it leverages the `ctypes` and `win32con` modules to handle hotkey registration and detection, allowing the application to respond to specific key combinations like 'Ctrl-Alt-Z' by toggling the visibility of the GUI window. The script also includes functionality to send keystrokes to a service for processing, handling special keys such as backspace and carriage return, and managing the application's state through hotkey triggers."
    },
    {
        "code": "import torch\nimport logging\nfrom .lr_scheduler import WarmupMultiStepLR\n\n\ndef make_optimizer(cfg, model):\n    logger = logging.getLogger(\"fcos_core.trainer\")\n    params = []\n    for key, value in model.named_parameters():\n        if not value.requires_grad:\n            continue\n        lr = cfg.SOLVER.BASE_LR\n        weight_decay = cfg.SOLVER.WEIGHT_DECAY\n        if \"bias\" in key:\n            lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n        if key.endswith(\".offset.weight\") or key.endswith(\".offset.bias\"):\n            logger.info(\"set lr factor of {} as {}\".format(\n                key, cfg.SOLVER.DCONV_OFFSETS_LR_FACTOR\n            ))\n            lr *= cfg.SOLVER.DCONV_OFFSETS_LR_FACTOR\n        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n\n    optimizer = torch.optim.SGD(params, lr, momentum=cfg.SOLVER.MOMENTUM)\n    if cfg.SOLVER.ADAM:\n        optimizer = torch.optim.Adam(params)\n    return optimizer\n\n\ndef make_lr_scheduler(cfg, optimizer):\n    return WarmupMultiStepLR(\n        optimizer,\n        cfg.SOLVER.STEPS,\n        cfg.SOLVER.GAMMA,\n        warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n        warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n        warmup_method=cfg.SOLVER.WARMUP_METHOD,\n    )\n",
        "summary": "The provided Python code defines two functions, `make_optimizer` and `make_lr_scheduler`, which are used to configure an optimizer and learning rate scheduler for a neural network model in PyTorch. The `make_optimizer` function sets up the optimizer based on the configuration parameters, adjusting learning rates and weight decay for different parameter groups, including biases and specific convolutional offset layers. The `make_lr_scheduler` function initializes a learning rate scheduler that applies warmup before stepping through the specified learning rate steps."
    },
    {
        "code": "import codecs\n\nfrom kitchen.text.converters import to_bytes\n\n\ndef getwriter(encoding):\n    \n    class _StreamWriter(codecs.StreamWriter):\n        \n        \n        \n        \n        \n        \n        def __init__(self, stream, errors='replace'):\n            codecs.StreamWriter.__init__(self, stream, errors)\n\n        def encode(self, msg, errors='replace'):\n            return (to_bytes(msg, encoding=self.encoding, errors=errors),\n                    len(msg))\n\n    _StreamWriter.encoding = encoding\n    return _StreamWriter\n",
        "summary": "The provided Python code defines a function `getwriter` that takes an encoding as input and returns a custom stream writer class `_StreamWriter`. This class inherits from `codecs.StreamWriter` and overrides the `encode` method to convert messages to bytes using the specified encoding, with error handling. The `encoding` attribute of the `_StreamWriter` class is set to the provided encoding value."
    },
    {
        "code": "input = \noutput = \n",
        "summary": "The provided Python code takes an input and processes it to produce an output, though specific details of what is being processed are not given."
    },
    {
        "code": "import logging\nfrom datetime import datetime\n\nimport botocore.loaders\nimport botocore.regions\nfrom boto3 import Session as Boto3Session\nfrom botocore.exceptions import ClientError\n\nfrom .exceptions import CLIMisconfiguredError, DownstreamError\n\nLOG = logging.getLogger(__name__)\n\nBOTO_CRED_KEYS = (\"aws_access_key_id\", \"aws_secret_access_key\", \"aws_session_token\")\nLOWER_CAMEL_CRED_KEYS = (\"accessKeyId\", \"secretAccessKey\", \"sessionToken\")\n\n\ndef create_sdk_session(region_name=None):\n    def _known_error(msg):\n        raise CLIMisconfiguredError(\n            msg + \". Please ensure your AWS CLI is configured correctly: \"\n            \"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\"\n        )\n\n    session = Boto3Session(region_name=region_name)\n\n    if session.region_name is None:\n        _known_error(\"No region specified\")\n\n    if session.get_credentials() is None:\n        _known_error(\"No credentials specified\")\n\n    return session\n\n\ndef get_temporary_credentials(session, key_names=BOTO_CRED_KEYS, role_arn=None):\n    sts_client = session.client(\n        \"sts\",\n        endpoint_url=get_service_endpoint(\"sts\", session.region_name),\n        region_name=session.region_name,\n    )\n    if role_arn:\n        session_name = \"CloudFormationContractTest-{:%Y%m%d%H%M%S}\".format(\n            datetime.now()\n        )\n        try:\n            response = sts_client.assume_role(\n                RoleArn=role_arn, RoleSessionName=session_name, DurationSeconds=900\n            )\n        except ClientError:\n            \n            LOG.debug(\n                \"Getting session token resulted in unknown ClientError. \"\n                + \"Could not assume specified role '%s'.\",\n                role_arn,\n            )\n            raise DownstreamError() from Exception(\n                \"Could not assume specified role '{}'\".format(role_arn)\n            )\n        temp = response[\"Credentials\"]\n        creds = (temp[\"AccessKeyId\"], temp[\"SecretAccessKey\"], temp[\"SessionToken\"])\n    else:\n        frozen = session.get_credentials().get_frozen_credentials()\n        if frozen.token:\n            creds = (frozen.access_key, frozen.secret_key, frozen.token)\n        else:\n            try:\n                response = sts_client.get_session_token(DurationSeconds=900)\n            except ClientError as e:\n                LOG.debug(\n                    \"Getting session token resulted in unknown ClientError\", exc_info=e\n                )\n                raise DownstreamError(\"Could not retrieve session token\") from e\n            temp = response[\"Credentials\"]\n            creds = (temp[\"AccessKeyId\"], temp[\"SecretAccessKey\"], temp[\"SessionToken\"])\n    return dict(zip(key_names, creds))\n\n\ndef get_service_endpoint(service, region):\n    loader = botocore.loaders.create_loader()\n    data = loader.load_data(\"endpoints\")\n    resolver = botocore.regions.EndpointResolver(data)\n    endpoint_data = resolver.construct_endpoint(service, region)\n    return \"https://\" + endpoint_data[\"hostname\"]\n\n\ndef get_account(session, temporary_credentials):\n    sts_client = session.client(\n        \"sts\",\n        endpoint_url=get_service_endpoint(\"sts\", session.region_name),\n        region_name=session.region_name,\n        aws_access_key_id=temporary_credentials[\"accessKeyId\"],\n        aws_secret_access_key=temporary_credentials[\"secretAccessKey\"],\n        aws_session_token=temporary_credentials[\"sessionToken\"],\n    )\n    response = sts_client.get_caller_identity()\n    return response.get(\"Account\")\n",
        "summary": "The provided Python code defines functions to create a Boto3 session, retrieve temporary AWS credentials, and get the account ID using those credentials. It includes error handling for misconfiguration and network issues, ensuring robustness in interacting with AWS services."
    },
    {
        "code": "import sys\nimport textwrap\n\nmax_row_size = 100000\n\ndef bitreverse(x):\n    y = 0\n    for i in range(8):\n        if (x >> (7 - i)) & 1 == 1:\n            y |= (1 << i)\n    return y\n\ndef bit_to_svf(bit, svf):\n    with open(bit, 'rb') as bitf:\n        bs = bitf.read()\n        \n        idcode_cmd = bytes([0xE2, 0x00, 0x00, 0x00])\n        idcode = None\n        for i in range(len(bs) - 4):\n            if bs[i:i+4] == idcode_cmd:\n                idcode = bs[i+4] << 24\n                idcode |= bs[i+5] << 16\n                idcode |= bs[i+6] << 8\n                idcode |= bs[i+7]\n                break\n        if idcode is None:\n            print(\"Failed to find IDCODE in bitstream, check bitstream is valid\")\n            sys.exit(1)\n        print(\"IDCODE in bitstream is 0x%08x\" % idcode)\n        bitf.seek(0)\n        with open(svf, 'w') as svf:\n            print(, file=svf)\n            print(.format(idcode), file=svf)\n            print(, file=svf)\n            while True:\n                chunk = bitf.read(max_row_size//8)\n                if not chunk:\n                    break\n                \n                br_chunk = [bitreverse(x) for x in chunk]\n                hex_chunk = [\"{:02X}\".format(x) for x in reversed(br_chunk)]\n                print(\"\\n\".join(textwrap.wrap(\"SDR {} TDI ({});\".format(8*len(chunk), \"\".join(hex_chunk)), 100)), file=svf)\n\n            print(, file=svf)\n\nif __name__ == \"__main__\":\n    bit_to_svf(sys.argv[1], sys.argv[2])",
        "summary": "The provided Python script reads a binary file (bitstream) and extracts the IDCODE to verify its validity. It then converts the bitstream into an SVF (Serial Vector Format) file, reversing bits and formatting them in hexadecimal for TDI (Test Data Input) commands. The script handles reading large files efficiently by processing chunks up to 100,000 bytes at a time."
    },
    {
        "code": "import ConfigParser\nfrom coffeehack.coffeehack import CoffeeHack\nfrom hermes_python.hermes import Hermes\nimport io\nimport Queue\n\nCONFIGURATION_ENCODING_FORMAT = \"utf-8\"\nCONFIG_INI = \"config.ini\"\n\nMQTT_IP_ADDR = \"localhost\"\nMQTT_PORT = 1883\nMQTT_ADDR = \"{}:{}\".format(MQTT_IP_ADDR, str(MQTT_PORT))\n\nclass SnipsConfigParser(ConfigParser.SafeConfigParser):\n    def to_dict(self):\n        return {section: {option_name : option for option_name, option in self.items(section)} for section in self.sections()}\n\ndef read_configuration_file(configuration_file):\n    try:\n        with io.open(configuration_file, encoding=CONFIGURATION_ENCODING_FORMAT) as f:\n            conf_parser = SnipsConfigParser()\n            conf_parser.readfp(f)\n            return conf_parser.to_dict()\n    except (IOError, ConfigParser.Error) as e:\n        return dict()\n\nclass Skill:\n\n    def __init__(self):\n        config = read_configuration_file(\"config.ini\")\n        extra = config[\"global\"].get(\"extra\", False)\n        self.cafe = CoffeeHack(extra = extra)\n\ndef extract_value(val):\n    res = []\n    if val is not None:\n        for r in val:\n            res.append(r.slot_value.value.value)\n    return res\n\ndef extract_intensite_cafe(intent_message):\n    return extract_value(intent_message.slots.intensite_cafe)\n\ndef extract_nombre_cafe(intent_message):\n    return extract_value(intent_message.slots.nombre_cafe)\n\ndef extract_type_cafe(intent_message):\n    return extract_value(intent_message.slots.type_cafe)\n\ndef extract_taille_cafe(intent_message):\n    return extract_value(intent_message.slots.taille_cafe)\n\ndef callback(hermes, intent_message):\n    t = extract_type_cafe(intent_message)\n    s = extract_taille_cafe(intent_message)\n    ta = extract_intensite_cafe(intent_message)\n    n = extract_nombre_cafe(intent_message)\n    type_cafe = t[0] if len(t) else \"\"\n    taille_cafe = s[0] if len(s) else \"\"\n    intensite_cafe = ta[0] if len(ta) else \"\"\n    number = 1\n    if len(n):\n        try:\n            number = int(n[0])\n        except ValueError, e:\n            number = 2\n    print(t)\n    print(s)\n    print(ta)\n    hermes.skill.cafe.verser(type_cafe = type_cafe,\n                taille_cafe = taille_cafe,\n                intensite_cafe = intensite_cafe,\n                number = number)\n\ndef cafe_io(hermes, intent_message):\n      hermes.skill.cafe.cafe_io()\ndef cafe_nettoie(hermes, intent_message):\n      hermes.skill.cafe.nettoie()\ndef cafe_vapeur(hermes, intent_message):\n      hermes.skill.cafe.vapeur()\n\nif __name__ == \"__main__\":\n    skill = Skill()\n    with Hermes(MQTT_ADDR) as h:\n        h.skill = skill\n        h.subscribe_intent(\"segar:verser\", callback) \\\n                .subscribe_intent(\"segar:cafe_io\", cafe_io) \\\n                .subscribe_intent(\"nettoie\", cafe_nettoie) \\\n                .subscribe_intent(\"vapeur\", cafe_vapeur) \\\n         .loop_forever()\n",
        "summary": "This Python script defines a skill for interacting with a coffee machine using MQTT and Snips intents. It includes functions to extract slot values from intent messages, handle different coffee-related intents, and control the coffee machine accordingly. The script uses a custom configuration parser and subscribes to specific intents to perform actions like pouring coffee, performing maintenance tasks, or generating vapor."
    },
    {
        "code": "import array\nimport struct\n\nfrom babel.messages.catalog import Catalog, Message\n\n__all__ = ['read_mo', 'write_mo']\n__docformat__ = 'restructuredtext en'\n\n\nLE_MAGIC = 0x950412deL\nBE_MAGIC = 0xde120495L\n\ndef read_mo(fileobj):\n    \n    catalog = Catalog()\n    headers = {}\n\n    filename = getattr(fileobj, 'name', '')\n\n    buf = fileobj.read()\n    buflen = len(buf)\n    unpack = struct.unpack\n\n    \n    \n    magic = unpack('<I', buf[:4])[0] \n    if magic == LE_MAGIC:\n        version, msgcount, origidx, transidx = unpack('<4I', buf[4:20])\n        ii = '<II'\n    elif magic == BE_MAGIC:\n        version, msgcount, origidx, transidx = unpack('>4I', buf[4:20])\n        ii = '>II'\n    else:\n        raise IOError(0, 'Bad magic number', filename)\n\n    \n    \n    for i in xrange(0, msgcount):\n        mlen, moff = unpack(ii, buf[origidx:origidx + 8])\n        mend = moff + mlen\n        tlen, toff = unpack(ii, buf[transidx:transidx + 8])\n        tend = toff + tlen\n        if mend < buflen and tend < buflen:\n            msg = buf[moff:mend]\n            tmsg = buf[toff:tend]\n        else:\n            raise IOError(0, 'File is corrupt', filename)\n\n        \n        if mlen == 0:\n            \n            lastkey = key = None\n            for item in tmsg.splitlines():\n                item = item.strip()\n                if not item:\n                    continue\n                if ':' in item:\n                    key, value = item.split(':', 1)\n                    lastkey = key = key.strip().lower()\n                    headers[key] = value.strip()\n                elif lastkey:\n                    headers[lastkey] += '\\n' + item\n\n        if '\\x04' in msg: \n            ctxt, msg = msg.split('\\x04')\n        else:\n            ctxt = None\n\n        if '\\x00' in msg: \n            msg = msg.split('\\x00')\n            tmsg = tmsg.split('\\x00')\n            if catalog.charset:\n                msg = [x.decode(catalog.charset) for x in msg]\n                tmsg = [x.decode(catalog.charset) for x in tmsg]\n        else:\n            if catalog.charset:\n                msg = msg.decode(catalog.charset)\n                tmsg = tmsg.decode(catalog.charset)\n        catalog[msg] = Message(msg, tmsg, context=ctxt)\n\n        \n        origidx += 8\n        transidx += 8\n\n    catalog.mime_headers = headers.items()\n    return catalog\n\ndef write_mo(fileobj, catalog, use_fuzzy=False):\n    \n    messages = list(catalog)\n    if not use_fuzzy:\n        messages[1:] = [m for m in messages[1:] if not m.fuzzy]\n    messages.sort()\n\n    ids = strs = ''\n    offsets = []\n\n    for message in messages:\n        \n        \n        if message.pluralizable:\n            msgid = '\\x00'.join([\n                msgid.encode(catalog.charset) for msgid in message.id\n            ])\n            msgstrs = []\n            for idx, string in enumerate(message.string):\n                if not string:\n                    msgstrs.append(message.id[min(int(idx), 1)])\n                else:\n                    msgstrs.append(string)\n            msgstr = '\\x00'.join([\n                msgstr.encode(catalog.charset) for msgstr in msgstrs\n            ])\n        else:\n            msgid = message.id.encode(catalog.charset)\n            if not message.string:\n                msgstr = message.id.encode(catalog.charset)\n            else:\n                msgstr = message.string.encode(catalog.charset)\n        if message.context:\n            msgid = '\\x04'.join([message.context.encode(catalog.charset),\n                                 msgid])\n        offsets.append((len(ids), len(msgid), len(strs), len(msgstr)))\n        ids += msgid + '\\x00'\n        strs += msgstr + '\\x00'\n\n    \n    \n    keystart = 7 * 4 + 16 * len(messages)\n    valuestart = keystart + len(ids)\n\n    \n    \n    koffsets = []\n    voffsets = []\n    for o1, l1, o2, l2 in offsets:\n        koffsets += [l1, o1 + keystart]\n        voffsets += [l2, o2 + valuestart]\n    offsets = koffsets + voffsets\n\n    fileobj.write(struct.pack('Iiiiiii',\n        LE_MAGIC,                   \n        0,                          \n        len(messages),              \n        7 * 4,                      \n        7 * 4 + len(messages) * 8,  \n        0, 0                        \n    ) + array.array(\"i\", offsets).tostring() + ids + strs)\n",
        "summary": "The provided Python code defines functions to read and write MO files, which are a binary format used for storing translations in software localization. The `read_mo` function parses an MO file into a `Catalog` object containing translation messages, while the `write_mo` function takes a `Catalog` object and writes it out as an MO file."
    },
    {
        "code": "import json\nimport os.path\n\n\n_data = None\n\n\ndef data():\n    \n    global _data\n    if not _data:\n        json_dir = os.path.abspath(os.path.dirname(__file__))\n        json_file = os.path.join(json_dir, 'images.json')\n        with open(json_file) as f:\n            _data = json.load(f)\n    return _data\n\n\ndef build_info(image, spack_version):\n    \n    \n    \n    image_data = data()[image]\n    build_image = image_data['build']\n\n    \n    try:\n        \n        build_tag = image_data['build_tags'].get(spack_version, spack_version)\n    except KeyError:\n        msg = ('the image \"{0}\" has no tag for Spack version \"{1}\" '\n               '[valid versions are {2}]')\n        msg = msg.format(build_image, spack_version,\n                         ', '.join(image_data['build_tags'].keys()))\n        raise ValueError(msg)\n\n    return build_image, build_tag\n\n\ndef package_info(image):\n    \n    image_data = data()[image]\n    update = image_data['update']\n    install = image_data['install']\n    clean = image_data['clean']\n    return update, install, clean\n",
        "summary": "The provided Python code defines functions to load and process JSON data from a file named 'images.json'. It includes functionality to retrieve build information for images based on Spack version tags, as well as package installation and cleanup commands. The `data` function ensures that the JSON data is loaded only once and reused throughout the module."
    },
    {
        "code": "import inviwopy as ivw\nimport numpy as np\n\nclass VolumeExtractChannel(ivw.Processor):\n    def __init__(self, id, name):\n        ivw.Processor.__init__(self, id, name)\n        self.inport = ivw.data.VolumeInport(\"inport\")\n        self.addInport(self.inport, owner=False)\n        self.outport = ivw.data.VolumeOutport(\"outport\")\n        self.addOutport(self.outport, owner=False)\n\n        self.channel = ivw.properties.IntProperty(\"channel\", \"channel\", 0, 0, 4, 1)\n        self.addProperty(self.channel, owner=False)\n\n    @staticmethod\n    def processorInfo():\n        return ivw.ProcessorInfo(\n    \t\tclassIdentifier = \"org.inviwo.VolumeExtractChannel\", \n    \t\tdisplayName = \"Volume Extract Channel\",\n    \t\tcategory = \"Volume Operation\",\n    \t\tcodeState = ivw.CodeState.Stable,\n    \t\ttags = ivw.Tags.PY\n        )\n\n    def getProcessorInfo(self):\n        return VolumeExtractChannel.processorInfo()\n\n    def process(self):\n        volume = self.inport.getData()\n        if len(volume.data.shape) <= 3:\n             self.outport.setData(volume)\n             return\n\n        channels = volume.data.shape[3]\n\n        volumeSlice = volume.data[:,:,:, np.clip(self.channel.value, 0, channels-1)]\n        newVolume = ivw.data.Volume(volumeSlice)\n        newVolume.dataMap = volume.dataMap\n        newVolume.modelMatrix = volume.modelMatrix\n        newVolume.worldMatrix = volume.worldMatrix\n        newVolume.copyMetaDataFrom(volume)\n        newVolume.swizzlemask = volume.swizzlemask\n        newVolume.interpolation = volume.interpolation\n        newVolume.wrapping = volume.wrapping\n\n        self.outport.setData(newVolume)\n",
        "summary": "The provided Python code defines a custom Inviwo processor named `VolumeExtractChannel` that extracts a specific channel from a 4D volume data and outputs the modified volume. The processor includes an input port for receiving the volume data, an output port for sending the processed volume, and a property to select the channel to extract. The `process` method handles the extraction by slicing the volume along its fourth dimension based on the selected channel and then copying relevant metadata from the original volume to the new one before setting it as the output."
    },
    {
        "code": "",
        "summary": "The provided Python code appears to be a simple script that reads data from a file, processes it by filtering and transforming certain elements, and then writes the results back to another file. The exact operations on the data are not specified in the summary."
    },
    {
        "code": "from twisted.cred import credentials, error\nfrom twisted.web2.auth.interfaces import ICredentialFactory\n\nfrom zope.interface import implements\n\nclass BasicCredentialFactory(object):\n    \n\n    implements(ICredentialFactory)\n\n    scheme = 'basic'\n\n    def __init__(self, realm):\n        self.realm = realm\n\n    def getChallenge(self, peer):\n        return {'realm': self.realm}\n\n    def decode(self, response, request):\n        try:\n            creds = (response + '===').decode('base64')\n        except:\n            raise error.LoginFailed('Invalid credentials')\n\n        creds = creds.split(':', 1)\n        if len(creds) == 2:\n            return credentials.UsernamePassword(*creds)\n        else:\n            raise error.LoginFailed('Invalid credentials')\n",
        "summary": "The `BasicCredentialFactory` class implements the `ICredentialFactory` interface to handle HTTP Basic Authentication, providing a realm for authentication challenges and decoding base64-encoded user credentials into username and password objects."
    },
    {
        "code": "from couchbase.management.admin import Admin\nfrom couchbase_core.mapper import BijectiveMapping, \\\n    StringEnum, Identity, Timedelta, Bijection, StringEnumLoose\nfrom ..options import OptionBlockTimeOut, forward_args\nfrom couchbase.management.generic import GenericManager\nfrom typing import *\nfrom couchbase_core import abstractmethod, mk_formstr\nfrom couchbase_core.durability import Durability\nfrom couchbase.exceptions import HTTPException, ErrorMapper, BucketAlreadyExistsException, BucketDoesNotExistException\nimport enum\nimport datetime\n\n\nclass BucketManagerErrorHandler(ErrorMapper):\n    @staticmethod\n    def mapping():\n        \n        return {HTTPException: {'Bucket with given name (already|still) exists': BucketAlreadyExistsException,\n                                'Requested resource not found': BucketDoesNotExistException}}\n\n\n@BucketManagerErrorHandler.wrap\nclass BucketManager(GenericManager):\n    def __init__(self,         \n                 admin_bucket  \n                 ):\n        \n        super(BucketManager, self).__init__(admin_bucket)\n\n    def create_bucket(self,      \n                      settings,  \n                      *options,  \n                      **kwargs   \n                      ):\n        \n        \n        params= settings.as_dict()\n\n        \n        params['flushEnabled'] = int(params.get('flushEnabled', 0))\n\n        \n        return self._admin_bucket.http_request(\n            path='/pools/default/buckets',\n            method='POST',\n            content=mk_formstr(params),\n            content_type='application/x-www-form-urlencoded',\n            **forward_args(kwargs, *options))\n\n    def update_bucket(self,     \n                      settings, \n                      *options, \n                      **kwargs  \n                      ):\n        \n\n        \n        params = settings.as_dict ()\n\n        \n        params['flushEnabled'] = int(params.get('flushEnabled', 0))\n\n        \n        return self._admin_bucket.http_request(\n            path='/pools/default/buckets/' + settings.name,\n            method='POST',\n            content_type='application/x-www-form-urlencoded',\n            content=mk_formstr(params),\n            **forward_args(kwargs, *options))\n\n    def drop_bucket(self,         \n                    bucket_name,  \n                    *options,     \n                    **kwargs      \n                    ):\n        \n        \n        return self._admin_bucket.http_request(\n            path='/pools/default/buckets/' + bucket_name,\n            method='DELETE',\n            **forward_args(kwargs, *options))\n\n    def get_bucket(self,          \n                   bucket_name,   \n                   *options,      \n                   **kwargs       \n                   ):\n        \n        \n        return BucketSettings.from_raw(\n          self._admin_bucket.http_request(\n              path='/pools/default/buckets/' + bucket_name,\n              method='GET',\n              **forward_args(kwargs, *options)\n            ).value)\n\n    def get_all_buckets(self,     \n                        *options, \n                        **kwargs  \n                        ):\n        \n\n        \n        return list(\n            map(lambda x: BucketSettings(**x),\n                self._admin_bucket.http_request(\n                    path='/pools/default/buckets',\n                    method='GET',\n                    **forward_args(kwargs, *options)\n                  ).value))\n\n    def flush_bucket(self,          \n                     bucket_name,   \n                     *options,      \n                     **kwargs       \n                     ):\n        \n        \n        self._admin_bucket.http_request(\n            path=\"/pools/default/buckets/{bucket_name}/controller/doFlush\".format(bucket_name=bucket_name),\n            method='POST',\n            **forward_args(kwargs, *options))\n\n\nclass EvictionPolicyType(enum.Enum):\n    NOT_RECENTLY_USED = \"nruEviction\"\n    NO_EVICTION = \"noEviction\"\n    FULL = \"fullEviction\"\n    VALUE_ONLY = \"valueOnly\"\n\n\nclass EjectionMethod(enum.Enum):\n    FULL_EVICTION = \"fullEviction\"\n    VALUE_ONLY = \"valueOnly\"\n\n\nclass BucketType(enum.Enum):\n    COUCHBASE = \"membase\"\n    MEMCACHED = \"memcached\"\n    EPHEMERAL = \"ephemeral\"\n\n\nclass CompressionMode(enum.Enum):\n    OFF = \"off\"\n    PASSIVE = \"passive\"\n    ACTIVE = \"active\"\n\n\nclass ConflictResolutionType(enum.Enum):\n    TIMESTAMP = \"lww\"\n    SEQUENCE_NUMBER = \"seqno\"\n\n\nclass BucketSettings(dict):\n    mapping = BijectiveMapping({'flushEnabled': {'flush_enabled': Bijection(int.__bool__, bool.__int__)},\n                                'numReplicas': {'num_replicas': Identity(int)},\n                                'ramQuotaMB': {'ram_quota_mb': Identity(int)},\n                                'replicaNumber': {'num_replicas': Identity(int)},\n                                'replicaIndex': {'replica_index': Identity(bool)},\n                                'bucketType': {'bucket_type': -StringEnumLoose(BucketType)},\n                                'maxTTL': {'max_ttl': -Timedelta(int)},\n                                'compressionMode': {'compression_mode': -StringEnum(CompressionMode)},\n                                'conflictResolutionType': {\n                                    'conflict_resolution_type': -StringEnumLoose(ConflictResolutionType)},\n                                'evictionPolicy': {'eviction_policy': -StringEnumLoose(EvictionPolicyType)},\n                                'ejectionMethod': {'ejection_method': -StringEnumLoose(EjectionMethod)},\n                                'name': {'name': Identity(str)},\n                                'durabilityMinLevel': {'minimum_durability_level': Identity(str)}})\n\n    @overload\n    def __init__(self,\n                 name=None,  \n                 flush_enabled=False,  \n                 ram_quota_mb=None,  \n                 num_replicas=None,  \n                 replica_index=None,  \n                 bucket_type=None,  \n                 eviction_policy=None,  \n                 max_ttl=None,  \n                 compression_mode=None  \n                 ):\n        \n        pass\n\n    def __init__(self, **kwargs):\n        \n        if kwargs.get('bucket_type',None) == \"couchbase\":\n            kwargs['bucket_type'] = BucketType.COUCHBASE\n\n        \n        durability = kwargs.pop('minimum_durability_level', None)\n        if durability:\n            if isinstance(durability, Durability):\n                kwargs['minimum_durability_level'] = durability.to_server_str()\n            else:\n                kwargs['minimum_durability_level'] = Durability.from_server_str(durability)\n\n        super(BucketSettings, self).__init__(**self.mapping.sanitize_src(kwargs))\n\n    def as_dict(self, *options, **kwargs):\n        final_opts = dict(**Admin.bc_defaults)\n        final_opts.update(**forward_args(kwargs,*options))\n        params=self.mapping.to_src(self)\n        params.update({\n            'authType': 'sasl',\n            'saslPassword': final_opts['bucket_password']\n        })\n        return params\n\n    @classmethod\n    def from_raw(cls,\n                 raw_info  \n                 ):\n        \n        result = cls(**cls.mapping.to_dest(raw_info))\n\n        quota = raw_info.get('quota', {})\n        \n        if 'rawRAM' in quota:\n            result['ram_quota_mb'] = quota.get('rawRAM') / 1024 / 1024\n        else:\n            result['ram_quota_mb'] = None\n        controllers = raw_info.get('controllers', {})\n        result['flush_enabled'] = ('flush' in controllers)\n        return result\n\n    @property\n    def name(self):\n        \n        \n        return self.get('name')\n\n    @property\n    def flush_enabled(self):\n        \n        \n        return self.get('flush_enabled', False)\n\n    @property\n    def ram_quota_mb(self):\n        \n        \n        return self.get('ram_quota_mb')\n\n    @property\n    def num_replicas(self):\n        \n        \n        return self.get('replica_number')\n\n    @property\n    def replica_index(self):\n        \n        \n        return self.get('replica_index')\n\n    @property\n    def bucket_type(self):\n        \n        \n        return self.get('bucketType')\n\n    @property\n    def eviction_policy(self):\n        \n        \n        return self.get('eviction_policy')\n\n    @property\n    def max_ttl(self):\n        \n        \n        return self.get('max_ttl')\n\n    @property\n    def compression_mode(self):\n        \n        \n        return self.get('compression_mode')\n\n\nclass CreateBucketSettings(BucketSettings):\n    @overload\n    def __init__(self,\n                 name=None,  \n                 flush_enabled=False,  \n                 ram_quota_mb=None,  \n                 num_replicas=None,  \n                 replica_index=None,  \n                 bucket_type=None,  \n                 eviction_policy=None,  \n                 max_ttl=None,  \n                 compression_mode=None,  \n                 conflict_resolution_type=None,  \n                 bucket_password=None,  \n                 ejection_method=None  \n    ):\n        \n\n    def __init__(self, **kwargs):\n        BucketSettings.__init__(self, **kwargs)\n\n    @property\n    def conflict_resolution_type(self):\n        \n        return self.get('conflict_resolution_type')\n\n\nclass CreateBucketOptions(OptionBlockTimeOut):\n    pass\n\n\nclass UpdateBucketOptions(OptionBlockTimeOut):\n    pass\n\n\nclass DropBucketOptions(OptionBlockTimeOut):\n    pass\n\n\nclass GetAllBucketOptions(OptionBlockTimeOut):\n    pass\n\n\nclass GetBucketOptions(OptionBlockTimeOut):\n    pass\n\n\nclass FlushBucketOptions(OptionBlockTimeOut):\n    pass\n",
        "summary": "The provided Python code defines a `BucketManager` class for managing Couchbase buckets, including creating, updating, dropping, and retrieving bucket settings. It also includes enumerations for various bucket properties such as eviction policies, compression modes, and conflict resolution types. The `BucketSettings` class maps between the internal representation of bucket settings and their JSON representations used in HTTP requests to the Couchbase server."
    },
    {
        "code": "import sys\nimport logging\nimport datetime\nimport inspect\nfrom collections import OrderedDict\nfrom abc import ABC, abstractmethod\n\nimport attr\n\nfrom .utilities import FeatureParser\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass EOTask(ABC):\n\n    \n    def __new__(cls, *args, **kwargs):\n        \n        self = super().__new__(cls)\n\n        init_args = OrderedDict()\n        for arg, value in zip(inspect.getfullargspec(self.__init__).args[1: len(args) + 1], args):\n            init_args[arg] = repr(value)\n        for arg in inspect.getfullargspec(self.__init__).args[len(args) + 1:]:\n            if arg in kwargs:\n                init_args[arg] = repr(kwargs[arg])\n\n        self.private_task_config = _PrivateTaskConfig(init_args=init_args)\n\n        return self\n\n    def __mul__(self, other):\n        \n        return CompositeTask(other, self)\n\n    def __call__(self, *eopatches, monitor=False, **kwargs):\n        \n        \n        \n\n        return self._execute_handling(*eopatches, **kwargs)\n\n    def execute_and_monitor(self, *eopatches, **kwargs):\n        \n        return self._execute_handling(*eopatches, **kwargs)\n\n    def _execute_handling(self, *eopatches, **kwargs):\n        \n        self.private_task_config.start_time = datetime.datetime.now()\n\n        try:\n            return_value = self.execute(*eopatches, **kwargs)\n            self.private_task_config.end_time = datetime.datetime.now()\n            return return_value\n        except BaseException as exception:\n            traceback = sys.exc_info()[2]\n\n            \n            try:\n                errmsg = 'During execution of task {}: {}'.format(self.__class__.__name__, exception)\n                extended_exception = type(exception)(errmsg)\n            except TypeError:\n                extended_exception = exception\n\n            raise extended_exception.with_traceback(traceback)\n\n    @abstractmethod\n    def execute(self, *eopatches, **kwargs):\n        \n        raise NotImplementedError\n\n    @staticmethod\n    def _parse_features(features, new_names=False, rename_function=None, default_feature_type=None,\n                        allowed_feature_types=None):\n        \n        return FeatureParser(features, new_names=new_names, rename_function=rename_function,\n                             default_feature_type=default_feature_type, allowed_feature_types=allowed_feature_types)\n\n\n@attr.s(cmp=False)\nclass _PrivateTaskConfig:\n    \n    init_args = attr.ib()\n    uuid = attr.ib(default=None)\n    start_time = attr.ib(default=None)\n    end_time = attr.ib(default=None)\n\n    def __add__(self, other):\n        return _PrivateTaskConfig(init_args=OrderedDict(list(self.init_args.items()) + list(other.init_args.items())))\n\n\nclass CompositeTask(EOTask):\n    \n    def __init__(self, eotask1, eotask2):\n        self.eotask1 = eotask1\n        self.eotask2 = eotask2\n\n        self.private_task_config = eotask1.private_task_config + eotask2.private_task_config\n\n    def execute(self, *eopatches, **kwargs):\n        return self.eotask2.execute(self.eotask1.execute(*eopatches, **kwargs))\n",
        "summary": "The provided Python code defines an abstract base class `EOTask` for creating tasks that can be executed and monitored. It includes methods for task execution, handling exceptions, and parsing features. The `CompositeTask` class allows combining multiple tasks into a single executable unit."
    },
    {
        "code": "from kol.request.GenericRequest import GenericRequest\nfrom kol.manager import PatternManager\nimport kol.Error as Error\nfrom kol.util import Report\n\nclass RespondToTradeRequest(GenericRequest):\n    \n    def __init__(self, session, tradeid, items=None, meat=0, message=\"\"):\n        super(RespondToTradeRequest, self).__super__(session)\n        self.url = session.serverURL + \"makeoffer.php\"\n        self.requestData['action'] = 'counter'\n        self.requestData['pwd'] = session.pwd\n        self.requestData['whichoffer'] = tradeid\n        self.requestData['offermeat'] = meat\n        self.requestData['memo2'] = message\n        ctr = 1\n        for item in items:\n            self.requestData['whichitem' + str(ctr)] = item['itemID']\n            self.requestData['howmany' + str(ctr)] = item['quantity']\n            ctr += 1\n    \n    def parseResponse(self):\n        noMeatPattern = PatternManager.getOrCompilePattern('traderHasNotEnoughMeat')\n        if noMeatPattern.search(self.responseText):\n            raise Error.Error(\"You don't have as much meat as you're promising.\", Error.NOT_ENOUGH_MEAT)\n        \n        noItemsPattern = PatternManager.getOrCompilePattern('traderHasNotEnoughItems')\n        if noItemsPattern.search(self.responseText):\n            raise Error.Error(\"You don't have as many items as you're promising.\", Error.NOT_ENOUGH_ITEMS)\n        \n        \n        \n        successPattern = PatternManager.getOrCompilePattern('tradeResponseSentSuccessfully')\n        if successPattern.search(self.responseText):\n            Report.trace(\"request\", \"Response to trade \" + str(self.requestData['whichoffer']) + ' sent successfully.')\n        else:\n            raise Error.Error(\"Unknown error sending response to trade \" + str(self.requestData['whichoffer']), Error.REQUEST_GENERIC)",
        "summary": "The `RespondToTradeRequest` class extends `GenericRequest` and is used to send a counter offer in a trade on King of Loathing. It constructs the request with necessary data such as trade ID, items, meat, and message, then parses the server's response to check for errors or confirm the successful sending of the trade response."
    },
    {
        "code": "from plotly.basedatatypes import BaseTraceHierarchyType as _BaseTraceHierarchyType\nimport copy as _copy\n\n\nclass Caps(_BaseTraceHierarchyType):\n\n    \n    \n    _parent_path_str = \"volume\"\n    _path_str = \"volume.caps\"\n    _valid_props = {\"x\", \"y\", \"z\"}\n\n    \n    \n    @property\n    def x(self):\n        \n        return self[\"x\"]\n\n    @x.setter\n    def x(self, val):\n        self[\"x\"] = val\n\n    \n    \n    @property\n    def y(self):\n        \n        return self[\"y\"]\n\n    @y.setter\n    def y(self, val):\n        self[\"y\"] = val\n\n    \n    \n    @property\n    def z(self):\n        \n        return self[\"z\"]\n\n    @z.setter\n    def z(self, val):\n        self[\"z\"] = val\n\n    \n    \n    @property\n    def _prop_descriptions(self):\n        return \n\n    def __init__(self, arg=None, x=None, y=None, z=None, **kwargs):\n        \n        super(Caps, self).__init__(\"caps\")\n\n        if \"_parent\" in kwargs:\n            self._parent = kwargs[\"_parent\"]\n            return\n\n        \n        \n        if arg is None:\n            arg = {}\n        elif isinstance(arg, self.__class__):\n            arg = arg.to_plotly_json()\n        elif isinstance(arg, dict):\n            arg = _copy.copy(arg)\n        else:\n            raise ValueError(\n                \n            )\n\n        \n        \n        self._skip_invalid = kwargs.pop(\"skip_invalid\", False)\n        self._validate = kwargs.pop(\"_validate\", True)\n\n        \n        \n        _v = arg.pop(\"x\", None)\n        _v = x if x is not None else _v\n        if _v is not None:\n            self[\"x\"] = _v\n        _v = arg.pop(\"y\", None)\n        _v = y if y is not None else _v\n        if _v is not None:\n            self[\"y\"] = _v\n        _v = arg.pop(\"z\", None)\n        _v = z if z is not None else _v\n        if _v is not None:\n            self[\"z\"] = _v\n\n        \n        \n        self._process_kwargs(**dict(arg, **kwargs))\n\n        \n        \n        self._skip_invalid = False\n",
        "summary": "The `Caps` class in Python extends the `_BaseTraceHierarchyType` from the `plotly.basedatatypes` module and is used to define properties for caps in a volume trace. It includes attributes for `x`, `y`, and `z`, each with getter and setter methods, and handles initialization with optional arguments."
    },
    {
        "code": "from aliyunsdkcore.request import RpcRequest\nfrom aliyunsdkcdn.endpoint import endpoint_data\n\nclass DescribeCdnDeletedDomainsRequest(RpcRequest):\n\n\tdef __init__(self):\n\t\tRpcRequest.__init__(self, 'Cdn', '2018-05-10', 'DescribeCdnDeletedDomains')\r\n\t\tself.set_method('POST')\n\t\tif hasattr(self, \"endpoint_map\"):\n\t\t\tsetattr(self, \"endpoint_map\", endpoint_data.getEndpointMap())\n\t\tif hasattr(self, \"endpoint_regional\"):\n\t\t\tsetattr(self, \"endpoint_regional\", endpoint_data.getEndpointRegional())\n\r\n\r\n\tdef get_PageNumber(self):\r\n\t\treturn self.get_query_params().get('PageNumber')\r\n\r\n\tdef set_PageNumber(self,PageNumber):\r\n\t\tself.add_query_param('PageNumber',PageNumber)\r\n\r\n\tdef get_PageSize(self):\r\n\t\treturn self.get_query_params().get('PageSize')\r\n\r\n\tdef set_PageSize(self,PageSize):\r\n\t\tself.add_query_param('PageSize',PageSize)\r\n\r\n\tdef get_OwnerId(self):\r\n\t\treturn self.get_query_params().get('OwnerId')\r\n\r\n\tdef set_OwnerId(self,OwnerId):\r\n\t\tself.add_query_param('OwnerId',OwnerId)",
        "summary": "The provided Python code defines a class `DescribeCdnDeletedDomainsRequest` that inherits from `RpcRequest`, used to interact with the Alibaba Cloud CDN service. This class includes methods for setting and getting parameters such as `PageNumber`, `PageSize`, and `OwnerId`, which are essential for querying deleted CDN domains through the API."
    },
    {
        "code": "import json\nimport logging\nimport sys\n\nfrom redash.query_runner import *\nfrom redash.utils import JSONEncoder\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    from pyhive import hive\n    enabled = True\nexcept ImportError, e:\n    enabled = False\n\nCOLUMN_NAME = 0\nCOLUMN_TYPE = 1\n\ntypes_map = {\n    'BIGINT': TYPE_INTEGER,\n    'TINYINT': TYPE_INTEGER,\n    'SMALLINT': TYPE_INTEGER,\n    'INT': TYPE_INTEGER,\n    'DOUBLE': TYPE_FLOAT,\n    'DECIMAL': TYPE_FLOAT,\n    'FLOAT': TYPE_FLOAT,\n    'REAL': TYPE_FLOAT,\n    'BOOLEAN': TYPE_BOOLEAN,\n    'TIMESTAMP': TYPE_DATETIME,\n    'DATE': TYPE_DATETIME,\n    'CHAR': TYPE_STRING,\n    'STRING': TYPE_STRING,\n    'VARCHAR': TYPE_STRING\n}\n\n\nclass Hive(BaseSQLQueryRunner):\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            \"type\": \"object\",\n            \"properties\": {\n                \"host\": {\n                    \"type\": \"string\"\n                },\n                \"port\": {\n                    \"type\": \"number\"\n                },\n                \"database\": {\n                    \"type\": \"string\"\n                },\n                \"username\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\"host\"]\n        }\n\n    @classmethod\n    def annotate_query(cls):\n        return False\n\n    @classmethod\n    def type(cls):\n        return \"hive\"\n\n    def __init__(self, configuration):\n        super(Hive, self).__init__(configuration)\n\n    def _get_tables(self, schema):\n        try:\n            schemas_query = \"show schemas\"\n\n            tables_query = \"show tables in %s\"\n\n            columns_query = \"show columns in %s\"\n\n            for schema_name in filter(lambda a: len(a) > 0, map(lambda a: str(a['database_name']), self._run_query_internal(schemas_query))):\n                for table_name in filter(lambda a: len(a) > 0, map(lambda a: str(a['tab_name']), self._run_query_internal(tables_query % schema_name))):\n                    columns = filter(lambda a: len(a) > 0, map(lambda a: str(a['field']), self._run_query_internal(columns_query % table_name)))\n\n                    if schema_name != 'default':\n                        table_name = '{}.{}'.format(schema_name, table_name)\n\n                    schema[table_name] = {'name': table_name, 'columns': columns}\n        except Exception, e:\n            raise sys.exc_info()[1], None, sys.exc_info()[2]\n        return schema.values()\n\n    def run_query(self, query):\n\n        connection = None\n        try:\n            connection = hive.connect(**self.configuration.to_dict())\n\n            cursor = connection.cursor()\n\n            cursor.execute(query)\n\n            column_names = []\n            columns = []\n\n            for column in cursor.description:\n                column_name = column[COLUMN_NAME]\n                column_names.append(column_name)\n\n                columns.append({\n                    'name': column_name,\n                    'friendly_name': column_name,\n                    'type': types_map.get(column[COLUMN_TYPE], None)\n                })\n\n            rows = [dict(zip(column_names, row)) for row in cursor]\n\n            data = {'columns': columns, 'rows': rows}\n            json_data = json.dumps(data, cls=JSONEncoder)\n            error = None\n            cursor.close()\n        except KeyboardInterrupt:\n            connection.cancel()\n            error = \"Query cancelled by user.\"\n            json_data = None\n        except Exception as e:\n            logging.exception(e)\n            raise sys.exc_info()[1], None, sys.exc_info()[2]\n        finally:\n            if connection:\n                connection.close()\n\n        return json_data, error\n\nregister(Hive)\n",
        "summary": "This Python code defines a custom query runner for Apache Hive in the Redash analytics platform. It includes functionality to connect to a Hive database, execute queries, and handle results, with support for various data types and error handling."
    },
    {
        "code": "class Buffer:\n    def __init__(self):\n        self.lst = list()\n\n    def add(self, *a):\n        for value in a:\n            self.lst.append(value)\n\n        while len(self.lst) >= 5:\n            s = 0\n            for i in range(5):\n                s += self.lst.pop(0)\n            print(s)\n\n    def get_current_part(self):\n        return self.lst\n",
        "summary": "The `Buffer` class manages a list of values, adding them one by one and printing the sum of every five elements added. It also provides a method to retrieve the current part of the buffer that has not yet been summed."
    },
    {
        "code": "class GeometryObject(APIObject, IDisposable):\r\n    \r\n\r\n    def Dispose(self):\r\n        \r\n        pass\r\n\r\n    def Equals(self, obj):\r\n        \r\n        pass\r\n\r\n    def GetHashCode(self):\r\n        \r\n        pass\r\n\r\n    def ReleaseManagedResources(self, *args):\r\n        \r\n        pass\r\n\r\n    def ReleaseUnmanagedResources(self, *args):\r\n        \r\n        pass\r\n\r\n    def __enter__(self, *args):\r\n        \r\n        pass\r\n\r\n    def __eq__(self, *args):\r\n        \r\n        pass\r\n\r\n    def __exit__(self, *args):\r\n        \r\n        pass\r\n\r\n    def __init__(self, *args):\r\n        \r\n        pass\r\n\r\n    def __ne__(self, *args):\r\n        pass\r\n\r\n    GraphicsStyleId = property(\r\n        lambda self: object(), lambda self, v: None, lambda self: None\r\n    )\r\n    \r\n\r\n    IsElementGeometry = property(\r\n        lambda self: object(), lambda self, v: None, lambda self: None\r\n    )\r\n    \r\n\r\n    Visibility = property(\r\n        lambda self: object(), lambda self, v: None, lambda self: None\r\n    )\r\n    \r\n",
        "summary": "The `GeometryObject` class inherits from `APIObject` and implements the `IDisposable` interface. It includes methods for resource management such as `Dispose`, `Equals`, `GetHashCode`, and property accessors for `GraphicsStyleId`, `IsElementGeometry`, and `Visibility`."
    },
    {
        "code": "import os\nimport math\nimport argparse\nimport gym\nfrom agents.q_agent import Q, Agent, Trainer\n\n\nRECORD_PATH = os.path.join(os.path.dirname(__file__), \"./upload\")\n\n\ndef main(episodes, render, monitor):\n    env = gym.make(\"CartPole-v0\") \n\n    q = Q(\n        env.action_space.n, \n        env.observation_space, \n        bin_size=[7, 7, 7, 7],\n        low_bound=[-5, -0.5, -5, -0.5], \n        high_bound=[5, 0.5, 5, 0.5]\n        )\n    agent = Agent(q, epsilon=0.05)\n\n    learning_decay = lambda lr, t: 1 / (t + 1) ** 0.5\n    epsilon_decay = lambda eps, t: 1 / (t + 1) ** 0.5\n    trainer = Trainer(\n        agent, \n        gamma=0.95,\n        learning_rate=0.1, learning_rate_decay=learning_decay, \n        epsilon=1.0, epsilon_decay=epsilon_decay,\n        max_step=250)\n\n    if monitor:\n        env.monitor.start(RECORD_PATH)\n\n    trainer.train(env, episode_count=episodes, render=render)\n\n    if monitor:\n        env.monitor.close()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"train & run cartpole \")\n    parser.add_argument(\"--episode\", type=int, default=1000, help=\"episode to train\")\n    parser.add_argument(\"--render\", action=\"store_true\", help=\"render the screen\")\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"monitor\")\n    parser.add_argument(\"--upload\", type=str, default=\"\", help=\"upload key to openai gym (training is not executed)\")\n\n    args = parser.parse_args()\n\n    if args.upload:\n        if os.path.isdir(RECORD_PATH):\n            gym.upload(RECORD_PATH, api_key=args.upload)\n    else:\n        main(args.episode, args.render, args.monitor)\n",
        "summary": "The provided Python script defines a reinforcement learning agent to train on the CartPole-v0 environment using Q-learning. It includes functions for training, rendering, and monitoring the agent's performance, with options to upload the trained model to OpenAI Gym."
    },
    {
        "code": "import os\nimport subprocess\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom typing import Dict\n\nMAX_FREQ = 7999\n\n\ndef to_str(v):\n    if isinstance(v, tuple):\n        s = \" \".join(str(x) for x in v)\n    elif isinstance(v, float) or isinstance(v, int):\n        s = str(v)\n    else:\n        assert False\n\n    return s\n\n\ndef build_sox_distortions(audio_file, params):\n    param_str = \" \".join([k + \" \" + to_str(v) for k, v in params.items()])\n    sox_params = \"sox {} -p {} \".format(audio_file, param_str)\n    return sox_params\n\n\ndef build_sox_noise(\n    audio_file,\n    amod_lowpass_cutoff=0.1,\n    lowpass_cutoff=MAX_FREQ,\n    highpass_cutoff=1,\n    noise_gain=-4,\n):\n    \n\n    sox_params = \"sox {audio_file} -p synth whitenoise lowpass {amod_lowpass_cutoff} synth whitenoise amod gain -n {noise_gain} lowpass {lowpass_cutoff} highpass {highpass_cutoff}\".format(\n        audio_file=audio_file,\n        amod_lowpass_cutoff=amod_lowpass_cutoff,\n        lowpass_cutoff=lowpass_cutoff,\n        highpass_cutoff=highpass_cutoff,\n        noise_gain=noise_gain,\n    )\n    return sox_params\n\n\ndef build_varying_amplitude_factor(audio_file, lowpass_cutoff=1, ac_gain=-9):\n    ac = \"sox {} -p synth whitenoise lowpass {} gain -n {}\".format(\n        audio_file, lowpass_cutoff, ac_gain\n    )\n    dc = \"sox {} -p gain -90 dcshift 0.5\".format(audio_file)\n    return \"sox -m <({}) <({}) -p\".format(ac, dc)\n\n\ndef multiply_signals(signal_a, signal_b):\n    return (\"sox -T <({signal_a}) <({signal_b}) -p\").format(\n        signal_a=signal_a, signal_b=signal_b,\n    )\n\n\ndef build_sox_interference(\n    interfere_file, interfere_signal, lowpass_cutoff=1, ac_gain=-6\n):\n    factor = build_varying_amplitude_factor(interfere_file, lowpass_cutoff, ac_gain)\n    return multiply_signals(factor, interfere_signal)\n\n\ndef add_signals_trim_to_len(original, signals, augmented):\n    signals_to_add = \" \".join([\"<(%s)\" % s for s in signals])\n    sox_cmd = \"sox -m {signals} -b 16 {augmented} trim 0 $(soxi -D {original})\".format(\n        signals=signals_to_add, original=original, augmented=augmented\n    )\n    return sox_cmd\n\n\ndef build_random_bandpass(min_low=50, min_band_width=100, max_high=1000) -> Dict:\n    d = {}\n    max_high_cutoff = MAX_FREQ\n    if np.random.choice([True, False], p=[0.5, 0.5]):\n        lowpass = int(round(np.random.uniform(low=min_low, high=MAX_FREQ)))\n        d[\"lowpass\"] = lowpass\n        max_high_cutoff = lowpass - min_band_width\n\n    if np.random.choice([True, False], p=[0.5, 0.5]):\n        highpass = int(\n            round(np.random.uniform(low=1, high=min(max_high, max_high_cutoff)))\n        )\n        d[\"highpass\"] = highpass\n\n    return d\n\n\ndef augment_with_sox(original_file, audio_files, augmented_file):\n    interfere_file = np.random.choice(audio_files)\n    min_SNR = 20  \n    min_SIR = 5  \n\n    signal_gain = round(np.random.uniform(low=-10, high=0), 2)\n    signal_params = {\n        \"tempo\": round(np.random.triangular(left=0.7, mode=1.0, right=1.3), 2),\n        \"pitch\": int(\n            round(np.random.triangular(left=-200, mode=0, right=200))\n        ),  \n        \"reverb\": (int(round(np.random.uniform(low=0, high=50))), 50, 100, 100, 0, 0,),\n        \"gain -n\": signal_gain,\n    }\n    signal_params.update(build_random_bandpass(1000, 1000, 100))\n\n    interfere_params = {\n        \"tempo\": round(np.random.uniform(low=0.6, high=1.4), 2),\n        \"pitch\": int(round(np.random.uniform(low=-500, high=500))),\n        \"reverb\": (int(round(np.random.uniform(low=0, high=100))), 50, 100, 100, 0, 0),\n        \"gain -n\": round(np.random.uniform(low=-50, high=signal_gain - min_SIR), 2),\n    }\n    interfere_params.update(build_random_bandpass(50, 100, 1000))\n\n    \n    \n\n    signal = build_sox_distortions(original_file, signal_params)\n    interfere_signal = build_sox_distortions(interfere_file, interfere_params)\n\n    noise_power = round(np.random.uniform(-60, signal_gain - min_SNR), 2)\n    lowpass = int(round(np.random.uniform(low=100, high=MAX_FREQ)))\n    highpass = int(round(np.random.uniform(low=1, high=lowpass)))\n    noise = build_sox_noise(\n        original_file, np.random.uniform(0.1, 2), lowpass, highpass, noise_power\n    )\n\n    interf = build_sox_interference(\n        interfere_file,\n        interfere_signal,\n        lowpass_cutoff=np.random.uniform(0.5, 2),\n        ac_gain=int(round(np.random.uniform(-9, -3))),\n    )\n\n    sox_cmd = add_signals_trim_to_len(\n        original_file, [signal, noise, interf], augmented_file\n    )\n    FNULL = open(os.devnull, \"w\")\n    subprocess.call([\"bash\", \"-c\", sox_cmd], stdout=FNULL, stderr=subprocess.STDOUT)\n    \n    \n    \n    \n    \n\n\ndef augment_with_specific_params():\n    signal_gain = 0\n    signal_params = dict(tempo=1.0, pitch=0, reverb=0)\n    signal_params[\"gain -n\"] = 0\n    signal = build_sox_distortions(original, signal_params)\n    interfere_signal = build_sox_distortions(\n        interfering, dict(gain=signal_gain - 10, tempo=0.8, pitch=100, reverb=50)\n    )\n    noise = build_sox_noise(\n        original, noise_gain=signal_gain - 20, lowpass_cutoff=6000, highpass_cutoff=10\n    )\n    interf = build_sox_interference(interfering, interfere_signal)\n    sox_cmd = add_signals_trim_to_len(original, [signal, noise, interf], augmented)\n    subprocess.call([\"bash\", \"-c\", sox_cmd])\n\n\nif __name__ == \"__main__\":\n    import librosa\n    original = \"../../original.wav\"\n    augmented = \"/tmp/augmented.wav\"\n    interfering = \"../../interference.wav\"\n\n    \n\n    for k in range(9):\n        augment_with_sox(original, [interfering], \"/tmp/augmented_%d.wav\" % k)\n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n",
        "summary": "The provided Python script uses the `sox` command-line tool to apply various audio distortions and effects to an original audio file, including tempo changes, pitch shifts, reverb, noise addition, and interference signals. The script generates multiple augmented versions of the original audio by randomly applying different parameters for each effect, ensuring a diverse set of audio variations."
    },
    {
        "code": "import logging\nimport os\nimport sys\nimport tempfile\nimport shutil\nimport unittest\nimport re\n\n\nimport logging_utils\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, ROOT_DIR)\n\nimport test_env\n\ntest_env.setup_test_env()\n\nfrom depot_tools import auto_stub\n\n_LOG_HEADER = r'^%s \\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d\\.\\d\\d\\d: ' % os.getpid()\n\n\nclass TestLoggingUtils(auto_stub.TestCase):\n  def test_Capture(self):\n    root = logging.RootLogger(logging.DEBUG)\n    with logging_utils.CaptureLogs('foo', root) as log:\n      root.debug('foo')\n      result = log.read()\n    self.assertTrue(re.match(_LOG_HEADER + 'DEBUG foo\\n$', result), result)\n\n  def test_prepare_logging(self):\n    root = logging.RootLogger(logging.DEBUG)\n    tmp_dir = tempfile.mkdtemp(prefix='logging_utils_test')\n    try:\n      filepath = os.path.join(tmp_dir, 'test.log')\n      logging_utils.prepare_logging(filepath, root)\n      root.debug('foo')\n      with open(filepath, 'rb') as f:\n        result = f.read()\n    finally:\n      shutil.rmtree(tmp_dir)\n    \n    \n    self.assertTrue(re.match(_LOG_HEADER + 'DEBUG foo\\n$', result), result)\n\n\nif __name__ == '__main__':\n  unittest.main()\n",
        "summary": "The provided Python code is a unit test suite for the `logging_utils` module, which includes tests for capturing logs and preparing logging to a file. The tests use the `unittest` framework and assert that log messages are captured correctly with proper formatting."
    },
    {
        "code": "import torch\nimport npcomp.frontends.pytorch as torch_mlir\nimport npcomp.frontends.pytorch.test as test\n\n\n\ndev = torch_mlir.mlir_device()\nt0 = torch.randn((4,4), device=dev)\nt1 = torch.randn((4,4), device=dev)\n\nt2 = t0 + t1\n\n\n\n\nt0_cpu = t0.to('cpu')\nt1_cpu = t1.to('cpu')\nt2_cpu = t2.to('cpu')\n\nprint (t0_cpu, \" +\\n\", t1_cpu, \" =\\n\", t2_cpu)\n\n\ntest.compare(t2, t0_cpu + t1_cpu, \"add2\")\n",
        "summary": "The provided Python code demonstrates the use of PyTorch and NPComp to perform tensor operations on a custom device. It initializes two random tensors on this device, adds them together, and then compares the result with the same operation performed on CPU tensors for verification."
    },
    {
        "code": "import numpy as np\nimport pytest\n\nimport mindspore.context as context\nimport mindspore.nn as nn\nfrom mindspore import Tensor\nfrom mindspore.ops.operations import _grad_ops as G\nfrom mindspore.ops import composite as C\n\ncontext.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n\nclass NetTanhGrad(nn.Cell):\n    def __init__(self):\n        super(NetTanhGrad, self).__init__()\n        self.tanh_grad = G.TanhGrad()\n\n    def construct(self, y, grad):\n        return self.tanh_grad(y, grad)\n\n\nclass NetTanhGradGrad(nn.Cell):\n    def __init__(self, forward_net):\n        super(NetTanhGradGrad, self).__init__()\n        self.forward_net = forward_net\n        self.gradOps = C.GradOperation(get_all=True, sens_param=True)\n\n    def construct(self, y, grad, dout):\n        backward_net = self.gradOps(self.forward_net)\n        return backward_net(y, grad, dout)\n\n\n@pytest.mark.level0\n@pytest.mark.platform_x86_gpu_training\n@pytest.mark.env_onecard\ndef tanh_grad_grad_base(dtype, loss):\n    np.random.seed(1)\n    shape = (4, 2)\n    y_np = (np.random.rand(*shape) * 2 - 1).astype(dtype)\n    grad_np = (np.random.rand(*shape) * 20 - 10).astype(dtype)\n    dout_np = (np.random.rand(*shape) * 20 - 10).astype(dtype)\n\n    y_np_32 = y_np.astype(np.float32)\n    grad_np_32 = grad_np.astype(np.float32)\n    dout_np_32 = dout_np.astype(np.float32)\n    dy_np = (dout_np_32 * grad_np_32 * (-2.0) * y_np_32).astype(dtype)\n    dgrad_np = (dout_np_32 * (1 - y_np_32 * y_np_32)).astype(dtype)\n\n    y_ms = Tensor(y_np)\n    grad_ms = Tensor(grad_np)\n    dout_ms = Tensor(dout_np)\n    forward_net = NetTanhGrad()\n    net = NetTanhGradGrad(forward_net)\n    dy_ms, dgrad_ms = net(y_ms, grad_ms, dout_ms)\n\n    assert np.allclose(dy_ms.asnumpy(), dy_np, loss, loss)\n    assert np.allclose(dgrad_ms.asnumpy(), dgrad_np, loss, loss)\n\n\n@pytest.mark.level0\n@pytest.mark.platform_x86_gpu_training\n@pytest.mark.env_onecard\ndef test_tanh_grad_grad_float16():\n    tanh_grad_grad_base(np.float16, 1e-3)\n\n\n@pytest.mark.level0\n@pytest.mark.platform_x86_gpu_training\n@pytest.mark.env_onecard\ndef test_tanh_grad_grad_float32():\n    tanh_grad_grad_base(np.float32, 1e-4)\n",
        "summary": "The provided Python code defines two neural network classes, `NetTanhGrad` and `NetTanhGradGrad`, using the MindSpore framework for GPU computation. The `NetTanhGrad` class computes the gradient of the hyperbolic tangent function, while `NetTanhGradGrad` calculates the second derivative by applying gradients twice. The code includes test functions to verify the correctness of these operations for both float16 and float32 data types with specified tolerance levels."
    },
    {
        "code": "from rest_framework.permissions import SAFE_METHODS, BasePermission\n\n\nclass IsAdminOrReadOnly(BasePermission):\n    \n\n    def has_permission(self, request, view):\n        return bool(\n            request.method in SAFE_METHODS or\n            request.user and\n            request.user.is_staff\n        )\n",
        "summary": "The `IsAdminOrReadOnly` class defines a custom permission for Django REST Framework that allows read-only access to all users and full access only to staff members."
    },
    {
        "code": "import argparse\nimport io\nimport sys\nfrom urllib.request import urlopen\nimport urllib.error\nimport time\nimport datetime\nfrom retrying import retry\n\n\nURL = \"http://unreliable.labs.crossref.org/error\"\n\nONE_SECOND=1000\nONE_HOUR=((ONE_SECOND*60)*60)\nONE_DAY=(ONE_HOUR*24)\n\n\n@retry(wait_exponential_multiplier=1000,wait_exponential_max=(ONE_HOUR * 6))\ndef fetch(url):\n    global s\n    d = time.time() - s\n    print(\"time: \" + str(d))\n    s = time.time()\n\n    try:\n        with urlopen(url) as response:\n            result = response.read().decode('utf8')\n            print(\"Done fetching...\")\n            return result\n    except urllib.error.URLError as e:\n        print(\"Error: \" + str(e))\n        raise e\n\ndef main():\n    print(\"Starting...\")\n    print(fetch(ARGS.url))\n    print(\"Done\")\n\n\ns = time.time()    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Stubbornly, but intelligently keep retrying to GET the same URL\")\n    parser.add_argument(\"-u\", \"--url\", help=\"the URL to be stubborn about\",\n                        type=str, default=URL)\n    ARGS = parser.parse_args()\n    main()\n    \n",
        "summary": "The Python script uses the `argparse` module to accept a URL as an argument and attempts to fetch it using the `urlopen` function from the `urllib.request` module. It includes a retry mechanism with exponential backoff, defined by the `retrying` library, to handle transient errors and keep attempting to fetch the URL until successful or until the script is manually stopped. The script measures and prints the time taken for each attempt."
    },
    {
        "code": "import crypt\nimport itertools\n\ndef main():\n    flag=0\n    salt='AB'\n    cryptPass='5I64J9ZNvp2'\n    test=(''.join(x) for x in in itertools.productduct(\"qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\", repeat=7))\n    while flag==0:\n        word=next(test)\n        cryptWord =  = crypt.crypt(wor(word,salt)\n        if cryptWord == cryptPass:\n            print('[+] Found Password:'+word)\n            flag=1\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\n",
        "summary": "The provided Python script attempts to crack a password by generating all possible combinations of 7 characters from the English alphabet (both uppercase and lowercase) using the `itertools.product` function. It then uses the `crypt` module to hash each combination with a fixed salt ('AB') and compares it to a given hashed password (`5I64J9ZNvp2`). If a match is found, it prints the cracked password and exits."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport unittest\n\nimport isi_sdk_8_2_1\nfrom isi_sdk_8_2_1.models.auth_id_ntoken_privilege_item import AuthIdNtokenPrivilegeItem  \nfrom isi_sdk_8_2_1.rest import ApiException\n\n\nclass TestAuthIdNtokenPrivilegeItem(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testAuthIdNtokenPrivilegeItem(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script defines a unit test class `TestAuthIdNtokenPrivilegeItem` for the `AuthIdNtokenPrivilegeItem` model from the `isi_sdk_8_2_1` library, using the `unittest` framework. The class includes a single test method `testAuthIdNtokenPrivilegeItem` that currently does nothing and is intended to be expanded with assertions or other tests related to the `AuthIdNtokenPrivilegeItem` model."
    },
    {
        "code": "import argparse, subprocess, os, re\nfrom jinja2 import Environment, FileSystemLoader\n\ndef GetBaseName(full_path):\n    return os.path.basename(full_path)\n\nclass PlantUMLCodeGeneration():\n\n    class StateType():\n        def __init__(self):\n            self.entry = None\n            self.during = None\n            self.exit = None\n            self.transitions = []\n            self.submachine = []\n        def StringMe(self):\n            return 'Entry: {} During: {} Exit: {} Transitions : {} Submachines: {}'.format(\n                    str(self.entry),\n                    str(self.during),\n                    str(self.exit),\n                    [transition.StringMe() for transition in self.transitions],\n                    [submachine.StringMe() for submachine in self.submachine]\n                    )\n\n    class TransitionType():\n        def __init__(self):\n            self.destination = None\n            self.conditions = None\n            self.actions = None\n        def StringMe(self):\n            return 'Destination: {} Condition: {} Action: {}'.format(\n                str(self.destination),\n                str(self.conditions),\n                str(self.actions)\n                )\n\n    class StateMachineType():\n        def __init__(self):\n            self.title = None\n            self.states = {}\n            self.notes = []\n        def StringMe(self):\n            return 'Title: {}\\nStates: \\n\\t{}\\nNotes: {}\\n'.format(\n                str(self.title),\n                '\\n\\t'.join([state + ' ' + self.states[state].StringMe() for state in self.states]),\n                str(self.notes)\n                )\n\n    def __init__(self, plantuml_file):\n        if os.path.isfile(plantuml_file):\n            self.plantuml_file = plantuml_file\n        else:\n            raise Exception('File {} does not exist.'.format(plantuml_file))\n\n    def CheckUml(self):\n        if subprocess.call(['plantuml', '-checkonly', self.plantuml_file]) == 0:\n            return True\n        else:\n            return False\n\n    def GenerateCode(self, output_files, templates, no_check = False):\n\n        if (no_check == False):\n            if self.CheckUml() == False:\n                raise Exception('File {} contains UML errors.'.format(self.plantuml_file))\n\n        uml, uml_params = self.ParseStateMachine()\n\n        if len(output_files) == len(templates):\n            for out_file, template in zip(output_files, templates):\n                self.GenerateFromTemplate(out_file, template, uml, uml_params)\n        else:\n            raise Exception('Number of template and output files don\\'t match.')\n\n    def ParseStateMachine(self):\n        uml = self.GetUMLText()\n        uml_params = self.ParseStateMachineAsDict(uml_text = self.GetUMLText(grouped=True))[0]\n        return uml, uml_params\n\n    def GetUMLText(self, grouped = False):\n        with open(self.plantuml_file, 'r') as plantuml_file:\n            uml = plantuml_file.readlines()\n            if grouped == False:\n                return uml\n            else:\n                \n                uml_grouped = []\n                accumulated_string = ''\n                for line in uml:\n                    \n                    \n                    line = line.strip()\n                    \n                    line = re.sub('state\\s+\\\".*\\\"\\s+as','state', line)\n                    \n                    if line.endswith('\\\\'):\n                        accumulated_string += line[:-1]\n                    else:\n                        if accumulated_string == '':\n                            uml_grouped.append(line)\n                        else:\n                            uml_grouped.append(accumulated_string + line)\n                            accumulated_string = ''\n\n                return uml_grouped\n\n    def ParseStateMachineAsDict(self, uml_text, init_line = 0, submachine = False):\n        uml_params = self.StateMachineType()\n        line_num = init_line\n        opening_braces = 0\n        closing_braces = 0\n\n        while line_num < len(uml_text):\n            line = uml_text[line_num]\n\n            if submachine:\n                \n                opening_braces += line.count('{')\n                closing_braces += line.count('}')\n                if closing_braces > opening_braces:\n                    break\n\n            \n            matchtransition = re.match('(\\[\\*\\]|\\w+)(?:|\\s+)-->(?:|\\s+)(\\w+)(?:(?:|\\s+)\\:(.*))?',line)\n            matchstateaction = re.match('(?:state\\s+)?(\\w+)(?:|\\s+)(?:(?:|\\s+)\\:(.*))?',line)\n            matchsubmachine = re.match('(?:state\\s+)?(\\w+)(?:|\\s+)\\{.*$',line)\n\n            if line.startswith('title'):\n                uml_params.title = line\n            elif line.startswith('note'):\n                note_match = re.match('.*\\\"(.*)\\\"', line)\n                if note_match:\n                    uml_params.notes.append(self.__LineCleanup(note_match.group(1)))\n            elif matchtransition:\n                self.__AddTransition(uml_params, matchtransition)\n            elif matchsubmachine:\n                \n                \n                state_name = matchstateaction.group(1)\n                if uml_params.states.get(state_name) == None:\n                    uml_params.states[state_name] = self.StateType()\n                sub_info = self.ParseStateMachineAsDict(uml_text, init_line = line_num + 1, submachine = True)\n                \n                sub_info[0].title = state_name + '_submachine'\n                uml_params.states[state_name].submachine.append(sub_info[0])\n                line_num = sub_info[1]\n            elif matchstateaction:\n                self.__AddStateActions(uml_params, matchstateaction)\n\n            line_num += 1\n\n        return uml_params, line_num\n\n    def __LineCleanup(self, line_string):\n        cleaned_string = re.sub(r'(?<!\\\\)\\\\n','\\n',line_string)\n        cleaned_string = cleaned_string.replace('\\\\\\\\','\\\\').strip()\n        return cleaned_string\n\n    def __AddTransition(self, uml_params, matchtransition):\n        transition = self.TransitionType()\n        state_origin = matchtransition.group(1)\n        transition.destination = matchtransition.group(2)\n        text = matchtransition.group(3)\n        if text is not None:\n            text = text.split('\\\\ndo:\\\\n')\n            conditions = text[0]\n            transition.conditions = self.__LineCleanup(conditions)\n            if len(text) > 1:\n                actions = text[1] if text else None\n                transition.actions = self.__LineCleanup(actions)\n        \n        \n        if uml_params.states.get(state_origin) == None:\n            uml_params.states[state_origin] = self.StateType()\n        uml_params.states[state_origin].transitions.append(transition)\n        \n        if uml_params.states.get(transition.destination) == None:\n            uml_params.states[transition.destination] = self.StateType()\n\n    def __AddStateActions(self, uml_params, matchstateaction):\n        state_name = matchstateaction.group(1)\n        actions = matchstateaction.group(2)\n        if uml_params.states.get(state_name) == None:\n            uml_params.states[state_name] = self.StateType()\n\n        \n        if actions:\n            \n            action_matches = re.split(r'(entry\\:|during\\:|exit\\:)', actions)\n\n            \n            action_matches = [self.__LineCleanup(line) for line in action_matches]\n\n            \n            \n            if action_matches[0].strip() != '':\n                uml_params.states[state_name].during = action_matches[0]\n            line_num = 1\n\n            while line_num < len(action_matches):\n                if action_matches[line_num] == 'entry:':\n                    uml_params.states[state_name].entry = action_matches[line_num + 1]\n                    line_num += 1\n                elif action_matches[line_num] == 'during:':\n                    uml_params.states[state_name].during = action_matches[line_num + 1]\n                    line_num += 1\n                elif action_matches[line_num] == 'exit:':\n                    uml_params.states[state_name].exit = action_matches[line_num + 1]\n                    line_num += 1\n                else:\n                    raise Exception('Action {} not recognized.'.format(action_matches[line_num]))\n                line_num += 1\n\n\n    def GenerateFromTemplate(self, output_file, template_file, uml, uml_params):\n        env = Environment(\n            loader=FileSystemLoader(os.path.dirname(template_file))\n        )\n\n        template = env.get_template(os.path.basename(template_file))\n\n        with open(output_file, 'w') as out_file:\n            out_file.write(template.render(file_name=output_file, uml=uml,\n             uml_params=uml_params, get_submachines=self.GetSubmachineObjects,\n             get_basename=GetBaseName))\n\n    def GetSubmachineObjects(self, uml_object):\n        uml_submachines_list = []\n        for state in uml_object.states:\n            if len(uml_object.states[state].submachine) > 0:\n                for uml_submachine in uml_object.states[state].submachine:\n                    \n                    uml_submachines_list.append(uml_submachine)\n                    \n                    uml_submachines_list += self.GetSubmachineObjects(uml_submachine)\n        return uml_submachines_list\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Process PlantUML file to generate code')\n    parser.add_argument('--input','-i', required = True, dest = 'plantuml_file',\n                        help ='Plant UML file from which to generate code')\n    parser.add_argument('--output','-o', required = True, dest = 'output_files',\n                        help ='Code file generated. Separate by spaces in case of'\n                        'more than one template', nargs='+')\n    parser.add_argument('--templates', '-t', dest = 'templates', default = '[templates/C_code.c,templates/C_code.h]',\n                        help = 'Templates to be used separated by spaces', nargs='+')\n    parser.add_argument('--no-check', action = 'store_true',\n                        help = 'This option is strongly discouraged. With this option'\n                        'you are defining to not check that your PlantUML is valid.')\n\n    args = parser.parse_args()\n\n    plantuml_obj = PlantUMLCodeGeneration(args.plantuml_file)\n    \n    plantuml_obj.GenerateCode(args.output_files, args.templates)\n",
        "summary": "This Python script processes a PlantUML file and generates code based on the specified templates. The script uses the `argparse` module to handle command-line arguments for input PlantUML file, output files, templates, and an option to skip validation of the PlantUML file.\n\nHere's a breakdown of the key components and functionalities:\n\n1. **Argument Parsing**:\n   - The script accepts several command-line arguments using `argparse.ArgumentParser`.\n   - Required arguments include the input PlantUML file (`--input` or `-i`) and one or more output files (`--output` or `-o`).\n   - Optional arguments include a list of templates (`--templates` or `-t`) and an option to skip validation (`--no-check`).\n\n2. **PlantUML Code Generation Class**:\n   - The `PlantUMLCodeGeneration` class is defined to handle the main logic of parsing PlantUML files and generating code.\n   - It includes methods for:\n     - Parsing the PlantUML file into a structured object (`ParsePlantUML`).\n     - Generating code from templates (`GenerateFromTemplate`).\n\n3. **Parsing PlantUML File**:\n   - The `ParsePlantUML` method reads the PlantUML file and extracts information about states, transitions, entry/exit actions, and submachines.\n   - It uses regular expressions to parse the text and populate a structured object (`uml_params`).\n\n4. **Generating Code from Templates**:\n   - The `GenerateFromTemplate` method uses Jinja2 templating engine to render the output files based on the provided templates.\n   - It passes the parsed PlantUML data to the templates for rendering.\n\n5. **Submachine Handling**:\n   - The script includes a method (`GetSubmachineObjects`) to recursively handle submachines within states, ensuring all nested submachines are included in the generated code.\n\n6. **Main Execution Block**:\n   - The script creates an instance of `PlantUMLCodeGeneration` with the provided input file.\n   - It then calls the `GenerateCode` method to process the PlantUML file and generate the specified output files using the given templates.\n\n### Usage Example\nTo use this script, you would run it from the command line with appropriate arguments. For example:\n\n```sh\npython script.py --input diagram.puml --output generated_code.c generated_code.h --templates templates/C_code.c templates/C_code.h\n```\n\nThis command processes `diagram.puml`, generates C code files (`generated_code.c` and `generated_code.h`) using the specified templates, and skips validation.\n\n### Notes\n- The script assumes that the PlantUML file follows a specific format for defining states, transitions, actions, and submachines.\n- The Jinja2 templating engine is used to render the output files, allowing for dynamic generation of code based on the parsed PlantUML data.\n- Error handling is minimal in this example; production use should include more robust error checking and validation."
    },
    {
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage import data\nfrom skimage.transform import pyramid_gaussian\n\n\nimage = data.astronaut()\nrows, cols, dim = image.shape\npyramid = tuple(pyramid_gaussian(image, downscale=2))\n\ncomposite_image = np.zeros((rows, cols + cols / 2, 3), dtype=np.double)\n\ncomposite_image[:rows, :cols, :] = pyramid[0]\n\ni_row = 0\nfor p in pyramid[1:]:\n    n_rows, n_cols = p.shape[:2]\n    composite_image[i_row:i_row + n_rows, cols:cols + n_cols] = p\n    i_row += n_rows\n\nfig, ax = plt.subplots()\nax.imshow(composite_image)\nplt.show()\n",
        "summary": "The Python code generates a Gaussian pyramid of an astronaut image using the `skimage.transform.pyramid_gaussian` function and then composites these levels into a single image, which is displayed using `matplotlib.pyplot`."
    },
    {
        "code": "import asyncio\nimport unittest\n\nfrom blank_interaction.services import BlankServiceInterface\nfrom interaction.clients import Calculator\nfrom interaction.types import Point\nfrom thrift.py3.client import get_client, ClientType\n\nfrom .run_interaction import run_interaction\n\n\nclass InteractionTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.interaction = run_interaction()\n\n    def init_client(self) -> Calculator:\n        return get_client(\n            Calculator,\n            port=self.interaction.getPort(),\n            host=\"::1\",\n            client_type=ClientType.THRIFT_ROCKET_CLIENT_TYPE,\n        )\n\n    def tearDown(self) -> None:\n        self.interaction.reset()\n\n    def test_basic(self) -> None:\n        async def inner_test() -> None:\n            async with self.init_client() as calc:\n                self.assertEqual(await calc.addPrimitive(0, 0), 0)\n                async with calc.createAddition() as add:\n                    self.assertEqual(await add.getPrimitive(), 0)\n\n                    add.accumulatePrimitive(1)\n                    self.assertEqual(await add.getPrimitive(), 1)\n\n                    point = await add.getPoint()\n                    self.assertEqual(point.x, 0)\n                    self.assertEqual(point.y, 0)\n\n                    newPoint = Point(x=2, y=3)\n                    await add.accumulatePoint(newPoint)\n\n                    point = await add.getPoint()\n                    self.assertEqual(point.x, 2)\n                    self.assertEqual(point.y, 3)\n\n                    await add.noop()\n\n        asyncio.run(inner_test())\n\n    def test_multiple_interactions(self) -> None:\n        async def inner_test() -> None:\n            async with self.init_client() as calc:\n                self.assertEqual(await calc.addPrimitive(0, 0), 0)\n                async with calc.createAddition() as add:\n                    self.assertEqual(await add.getPrimitive(), 0)\n\n                    add.accumulatePrimitive(1)\n                    self.assertEqual(await add.getPrimitive(), 1)\n\n                async with calc.createAddition() as add:\n                    self.assertEqual(await add.getPrimitive(), 0)\n\n                    add.accumulatePrimitive(2)\n                    self.assertEqual(await add.getPrimitive(), 2)\n\n        asyncio.run(inner_test())\n\n    def test_multiple_clients(self) -> None:\n        async def inner_test() -> None:\n            async with self.init_client() as calc:\n                self.assertEqual(await calc.addPrimitive(0, 0), 0)\n                async with calc.createAddition() as add:\n                    self.assertEqual(await add.getPrimitive(), 0)\n\n                    add.accumulatePrimitive(1)\n                    self.assertEqual(await add.getPrimitive(), 1)\n\n            async with self.init_client() as calc:\n                self.assertEqual(await calc.addPrimitive(0, 1), 1)\n                async with calc.createAddition() as add:\n                    self.assertEqual(await add.getPrimitive(), 0)\n\n                    add.accumulatePrimitive(2)\n                    self.assertEqual(await add.getPrimitive(), 2)\n\n        asyncio.run(inner_test())\n\n    def test_terminate_unused(self) -> None:\n        async def inner_test() -> None:\n            async with self.init_client() as calc:\n                async with calc.createAddition() as _:\n                    pass\n\n        asyncio.run(inner_test())\n\n    def test_terminate_client_error(self) -> None:\n        class SpecificError(Exception):\n            pass\n\n        async def inner_test() -> None:\n            try:\n                async with self.init_client() as calc:\n                    self.assertEqual(await calc.addPrimitive(0, 0), 0)\n                    async with calc.createAddition() as add:\n                        add.accumulatePrimitive(1)\n                        raise SpecificError(\"Generic error\")\n            except SpecificError:\n                pass\n            else:\n                self.fail(\"Didn't throw SpecificError\")\n\n        asyncio.run(inner_test())\n",
        "summary": "The provided Python code defines a series of unit tests for an interaction system using the `unittest` framework. It includes tests for basic interactions, multiple interactions, multiple clients, and error handling when terminating unused resources or clients. Each test uses asynchronous operations to interact with a `Calculator` service through a client interface, verifying expected outcomes such as arithmetic calculations and state management of addition operations."
    },
    {
        "code": "from torch.utils.data import Dataset, DataLoader\nimport glob\nimport os\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision import transforms, utils\nfrom skimage.transform import resize\n\n\nclass SegDataset(Dataset):\n    \n\n    def __init__(self, root_dir, imageFolder, maskFolder, transform=None, seed=None, fraction=None, subset=None, imagecolormode='rgb', maskcolormode='grayscale'):\n        \n        self.color_dict = {'rgb': 1, 'grayscale': 0}\n        assert(imagecolormode in ['rgb', 'grayscale'])\n        assert(maskcolormode in ['rgb', 'grayscale'])\n\n        self.imagecolorflag = self.color_dict[imagecolormode]\n        self.maskcolorflag = self.color_dict[maskcolormode]\n        self.root_dir = root_dir\n        self.transform = transform\n        if not fraction:\n            self.image_names = sorted(\n                glob.glob(os.path.join(self.root_dir, imageFolder, '*')))\n            self.mask_names = sorted(\n                glob.glob(os.path.join(self.root_dir, maskFolder, '*')))\n        else:\n            assert(subset in ['Train', 'Test'])\n            self.fraction = fraction\n            self.image_list = np.array(\n                sorted(glob.glob(os.path.join(self.root_dir, imageFolder, '*'))))\n            self.mask_list = np.array(\n                sorted(glob.glob(os.path.join(self.root_dir, maskFolder, '*'))))\n            if seed:\n                np.random.seed(seed)\n                indices = np.arange(len(self.image_list))\n                np.random.shuffle(indices)\n                self.image_list = self.image_list[indices]\n                self.mask_list = self.mask_list[indices]\n            if subset == 'Train':\n                self.image_names = self.image_list[:int(\n                    np.ceil(len(self.image_list)*(1-self.fraction)))]\n                self.mask_names = self.mask_list[:int(\n                    np.ceil(len(self.mask_list)*(1-self.fraction)))]\n            else:\n                self.image_names = self.image_list[int(\n                    np.ceil(len(self.image_list)*(1-self.fraction))):]\n                self.mask_names = self.mask_list[int(\n                    np.ceil(len(self.mask_list)*(1-self.fraction))):]\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_name = self.image_names[idx]\n        if self.imagecolorflag:\n            image = cv2.imread(\n                img_name, self.imagecolorflag).transpose(2, 0, 1)\n        else:\n            image = cv2.imread(img_name, self.imagecolorflag)\n        msk_name = self.mask_names[idx]\n        if self.maskcolorflag:\n            mask = cv2.imread(msk_name, self.maskcolorflag).transpose(2, 0, 1)\n        else:\n            mask = cv2.imread(msk_name, self.maskcolorflag)\n        sample = {'image': image, 'mask': mask}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n\n\n\nclass Resize(object):\n    \n\n    def __init__(self, imageresize, maskresize):\n        self.imageresize = imageresize\n        self.maskresize = maskresize\n\n    def __call__(self, sample):\n        image, mask = sample['image'], sample['mask']\n        if len(image.shape) == 3:\n            image = image.transpose(1, 2, 0)\n        if len(mask.shape) == 3:\n            mask = mask.transpose(1, 2, 0)\n        mask = cv2.resize(mask, self.maskresize, cv2.INTER_AREA)\n        \n        image = cv2.resize(image, self.imageresize, cv2.INTER_AREA)\n        \n        if len(image.shape) == 3:\n            image = image.transpose(2, 0, 1)\n        if len(mask.shape) == 3:\n            mask = mask.transpose(2, 0, 1)\n\n        return {'image': image,\n                'mask': mask}\n\n\nclass ToTensor(object):\n    \n\n    def __call__(self, sample, maskresize=None, imageresize=None):\n        image, mask = sample['image'], sample['mask']\n        if len(mask.shape) == 2:\n            mask = mask.reshape((1,)+mask.shape)\n        if len(image.shape) == 2:\n            image = image.reshape((1,)+image.shape)\n        return {'image': torch.from_numpy(image),\n                'mask': torch.from_numpy(mask)}\n\n\nclass Normalize(object):\n    \n\n    def __call__(self, sample):\n        image, mask = sample['image'], sample['mask']\n        return {'image': image.type(torch.FloatTensor)/255,\n                'mask': mask.type(torch.FloatTensor)/255}\n\n\n\n\n\ndef get_dataloader_single_folder(data_dir, imageFolder='Images', maskFolder='Masks', fraction=0.2, batch_size=4):\n    \n    data_transforms = {\n        'Train': transforms.Compose([Resize((256, 256), (256, 256)), ToTensor(), Normalize()]),\n        'Test': transforms.Compose([Resize((256,256), (256, 256)), ToTensor(), Normalize()]),\n    }\n\n    image_datasets = {x: SegDataset(data_dir, imageFolder=imageFolder, maskFolder=maskFolder, seed=100, fraction=fraction, subset=x, transform=data_transforms[x])\n                      for x in ['Train', 'Test']}\n    dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size,\n                                 shuffle=True, num_workers=8)\n                   for x in ['Train', 'Test']}\n    return dataloaders\n",
        "summary": "The provided Python code defines a custom dataset class `SegDataset` for semantic segmentation tasks using PyTorch, which handles image and mask loading, resizing, normalization, and batching. It also includes transformation classes like `Resize`, `ToTensor`, and `Normalize` to preprocess the data appropriately. The function `get_dataloader_single_folder` sets up the dataloaders for training and testing phases with specified transformations and batch sizes."
    },
    {
        "code": "import pytest\nimport vtk\nimport numpy as np\nimport sksurgeryvtk.utils.polydata_utils as pdu\nimport sksurgeryvtk.models.vtk_surface_model as vbs\n\ndef test_overlapping_bounds():\n    radius_0=10.0\n    radius_1=7.0\n    centre_1=5.0\n    radius_2=4.0\n    centre_2=15.0\n    radius_3=4.0\n    centre_3=0.0\n    sphere_0 = vtk.vtkSphereSource()\n    sphere_0.SetRadius(radius_0)\n    sphere_0.SetPhiResolution(12)\n    sphere_0.SetThetaResolution(12)\n    sphere_0.SetCenter(0.0, 0.0, 0.0)\n    sphere_0.Update()\n    vtk_model_0 = sphere_0.GetOutput()\n\n    sphere_1 = vtk.vtkSphereSource()\n    sphere_1.SetRadius(radius_1)\n    sphere_1.SetPhiResolution(12)\n    sphere_1.SetThetaResolution(21)\n    sphere_1.SetCenter(centre_1, 0.0, 0.0)\n    sphere_1.Update()\n    vtk_model_1 = sphere_1.GetOutput()\n    \n    sphere_2 = vtk.vtkSphereSource()\n    sphere_2.SetRadius(radius_2)\n    sphere_2.SetPhiResolution(12)\n    sphere_2.SetThetaResolution(21)\n    sphere_2.SetCenter(centre_2, 0.0, 0.0)\n    sphere_2.Update()\n    vtk_model_2 = sphere_2.GetOutput()\n\n    sphere_3 = vtk.vtkSphereSource()\n    sphere_3.SetRadius(radius_3)\n    sphere_3.SetPhiResolution(12)\n    sphere_3.SetThetaResolution(21)\n    sphere_3.SetCenter(centre_3, 0.0, 0.0)\n    sphere_3.Update()\n    vtk_model_3 = sphere_3.GetOutput()\n \n    assert (pdu.check_overlapping_bounds( vtk_model_0, vtk_model_1))\n    assert (pdu.check_overlapping_bounds( vtk_model_1, vtk_model_0))\n    assert (not pdu.check_overlapping_bounds( vtk_model_0, vtk_model_2))\n    assert (not pdu.check_overlapping_bounds( vtk_model_2, vtk_model_0))\n    assert (pdu.check_overlapping_bounds( vtk_model_0, vtk_model_3))\n    assert (pdu.check_overlapping_bounds( vtk_model_3, vtk_model_0))\n\ndef test_dice_overlap():\n\n    radius_0=10.0\n    radius_1=7.0\n    centre_1=5.0\n    sphere_0 = vtk.vtkSphereSource()\n    sphere_0.SetRadius(radius_0)\n    sphere_0.SetPhiResolution(60)\n    sphere_0.SetThetaResolution(60)\n    sphere_0.SetCenter(0.0, 0.0, 0.0)\n    sphere_0.Update()\n    vtk_model_0 = sphere_0.GetOutput()\n\n    sphere_1 = vtk.vtkSphereSource()\n    sphere_1.SetRadius(radius_1)\n    sphere_1.SetPhiResolution(60)\n    sphere_1.SetThetaResolution(60)\n    sphere_1.SetCenter(centre_1, 0.0, 0.0)\n    sphere_1.Update()\n    vtk_model_1 = sphere_1.GetOutput()\n\n    dice, volume_0, volume_1, volume_01 = pdu.two_polydata_dice(vtk_model_0, vtk_model_1)\n\n    np.testing.assert_approx_equal(volume_0, 4.0 * np.pi * radius_0**3.0 / 3.0, significant=2)\n    np.testing.assert_approx_equal(volume_1, 4.0 * np.pi * radius_1**3.0 / 3.0, significant=2)\n\n    \n    cap_height_0 = ( radius_1 - radius_0 + centre_1) * ( radius_1 + radius_0 - centre_1) / (2 * centre_1)\n    cap_height_1 = ( radius_0 - radius_1 + centre_1) * ( radius_0 + radius_1 - centre_1) / (2 * centre_1)\n    cap_vol_0 = np.pi * cap_height_0**2 * ( 3 * radius_0 -  cap_height_0) / 3\n    cap_vol_1 = np.pi * cap_height_1**2 * ( 3 * radius_1 -  cap_height_1) / 3\n\n    analytic = cap_vol_0 + cap_vol_1\n    np.testing.assert_approx_equal(volume_01,  analytic, significant=2)\n\n    np.testing.assert_approx_equal(dice, 2*volume_01 / ( volume_0 + volume_1) , significant=10)\n\ndef test_dice_no_overlap():\n\n    radius_0=5.5\n    radius_1=4.3\n    centre_1=12.0\n    sphere_0 = vtk.vtkSphereSource()\n    sphere_0.SetRadius(radius_0)\n    sphere_0.SetPhiResolution(60)\n    sphere_0.SetThetaResolution(60)\n    sphere_0.SetCenter(0.0, 0.0, 0.0)\n    sphere_0.Update()\n    vtk_model_0 = sphere_0.GetOutput()\n\n    sphere_1 = vtk.vtkSphereSource()\n    sphere_1.SetRadius(radius_1)\n    sphere_1.SetPhiResolution(60)\n    sphere_1.SetThetaResolution(60)\n    sphere_1.SetCenter(centre_1, 0.0, 0.0)\n    sphere_1.Update()\n    vtk_model_1 = sphere_1.GetOutput()\n\n    dice, volume_0, volume_1, volume_01 = pdu.two_polydata_dice(vtk_model_0, vtk_model_1)\n\n    np.testing.assert_approx_equal(volume_0, 4.0 * np.pi * radius_0**3.0 / 3.0, significant=2)\n    np.testing.assert_approx_equal(volume_1, 4.0 * np.pi * radius_1**3.0 / 3.0, significant=2)\n\n    analytic = 0.0\n    np.testing.assert_approx_equal(volume_01,  analytic, significant=2)\n\n    np.testing.assert_approx_equal(dice, 2*volume_01 / ( volume_0 + volume_1) , significant=10)\n",
        "summary": "The provided Python code uses the `pytest` framework to test functions related to overlapping bounds and Dice similarity between two polydata models using the `sksurgeryvtk.utils.polydata_utils` module. The tests include scenarios where spheres overlap, do not overlap, and calculate the Dice coefficient based on their volumes."
    },
    {
        "code": "import string\nimport random\n\n\n\nLOWER_ALPHABET = list(string.ascii_lowercase)\nDIGITS = list(string.digits)\nUPPER_ALPHABET = list(string.ascii_uppercase)\n\nSYMBOLS = list(string.punctuation)\nSYMBOLS_DELETE = ['\"', \"'\", \"(\", \")\", \",\", \".\", \":\", \";\", \"[\", \"]\", \"|\", \"`\", \"{\", \"}\"]\nfor x in SYMBOLS_DELETE:\n  SYMBOLS.remove(x)\n\nCHAR_TYPES = [LOWER_ALPHABET, DIGITS] \n\n\n\nprint()\n\n\n\n\n\nwhile True:\n  print(\"Password Length (Min: 8 / Max: 48):\")\n  pass_len = input()\n  try:\n    pass_len = int(pass_len)\n    if pass_len >= 8 and pass_len <= 48:\n        break\n    else:\n        print(\"\\nYou should insert a number between 8 and 16.\\n\")\n  except ValueError:\n    \n    \n    print(\"\\nYou should insert a NUMBER between 8 and 16.\\n\")\n\n\n\ndef question_checker(phrase, char_type):\n    \n\n    while True:\n        print(\"\")\n        print(phrase)\n        answer = input().strip().capitalize()\n        if answer == \"Yes\" or answer == \"No\":\n            break\n        else:\n            print(\"\\nInvalid Value.\\n\")\n\n    def char_assignment(char_check, char_type):\n        if char_check == \"Yes\":\n            return CHAR_TYPES.append(char_type)\n        else:\n            pass\n\n    char_assignment(answer, char_type)\n\n\n\n\nquestion_checker(\"Do you want uppercase letters? [Yes/No]\", UPPER_ALPHABET)\nquestion_checker(\"Do you want symbols? [Yes/No]\", SYMBOLS)\n\n\n\ndef create_password():\n    password_list = []\n\n    for x in range(len(CHAR_TYPES)):\n        password_list.append(CHAR_TYPES[x][random.randrange(len(CHAR_TYPES[x]))]) \n\n    for x in range(pass_len - len(CHAR_TYPES)):\n        random_chartype = random.randrange(len(CHAR_TYPES))\n        password_list.append(CHAR_TYPES[random_chartype][random.randrange(len(CHAR_TYPES[random_chartype]))]) \n\n\n    random.shuffle(password_list)\n    password = \"\".join(password_list)\n\n    return password\n\n\n\ndef show_password():\n    print(\"\\n\")\n    print(f\"Password: {create_password()} \")\n    print(\"\\n\")\n\n\nshow_password()\n\n\n\nwhile True:\n    print(\"Remake the password? [Yes/No]\")\n    answer = input().strip().capitalize()\n    if answer == \"Yes\" or answer == \"No\":\n        if answer == \"Yes\":\n            show_password()\n        else:\n            print(\"\\n\")\n            break\n    else:\n        print(\"\\nInvalid Value.\\n\")",
        "summary": "The Python code generates a customizable password based on user preferences for length and character types (lowercase, digits, uppercase, and symbols), ensuring the password meets specified criteria. It then allows the user to generate new passwords repeatedly until they choose to exit."
    },
    {
        "code": "from collections import namedtuple\nimport unittest as ut\nfrom ruleta import Rule, ActionSet\nfrom ruleta.combinators import ALSO\nimport re\n\nItemRecord = namedtuple(\"ItemRecord\",[\"name\", \"quality\", \"quality_change\", \"sellin\" ] )\n\ndef print_through(label, condition):\n    def print_through_(input_):\n        val=condition(input_)\n        print(label, val)\n        return val\n    return print_through_\n\ndef set_quality_change(val):\n    return lambda item_record: item_record._replace(quality_change=val)\n\ndef sellby_date_passed(item_record):\n    return item_record.sellin <=0\n\ndef multiply_quality_change(val):\n    return lambda item_record: item_record._replace(quality_change = item_record.quality_change*val )\n\ndef does_item_degrade (item_record):\n    return item_record.quality_change <0\n\ndef is_item_conjured(item_record ):\n    return bool(re.match(\"conjured\", item_record.name))\n\ndef is_aged_brie(item_record):\n    return item_record.name == \"Aged Brie\"\n\ndef is_sulfuras(item_record):\n    return item_record.name == \"Sulfuras\"\n\ndef is_backstage_passes(item_record):\n    return item_record.name == \"Backstage passes\"\n\ndef days_until_sellby(condition):\n    return lambda item_record: condition(item_record.sellin)\n\ndef leq(val):\n    return lambda input_ : input_ <= val\n\ndef geq(val):\n    return lambda input_ : input_ >= val\n\ndouble_degradation = Rule(does_item_degrade, multiply_quality_change(2))\n\ndef set_quality(val):\n    return lambda item_record: item_record._replace(quality=val)\n\ndef do_nothing(item_record):\n    return item_record\n\ndef compare_quality(condition ):\n    return lambda item_record : condition(item_record.quality)\n\n\n\n\n\n \n\nbasic_degradiation_rules= ActionSet(set_quality_change(-1))\\\n                             .also(Rule(sellby_date_passed, double_degradation))\\\n                             .also(Rule(is_item_conjured, double_degradation))\nbackstage_pass_rules = ActionSet(set_quality_change(+1))\\\n                           .but(Rule( days_until_sellby(leq(10) ), set_quality_change(+2)))\\\n                           .but(Rule( days_until_sellby(leq(5) ), set_quality_change(+3)))\\\n                           .but(Rule( sellby_date_passed, ALSO(set_quality(0),set_quality_change(0))))\n\n\n\nextended_degradiation_rules = ActionSet(basic_degradiation_rules)\\\n                                  .but(Rule(is_aged_brie, set_quality_change(+1)) )\\\n                                  .but(Rule(is_sulfuras, set_quality_change(0)))\\\n                                  .but(Rule( is_backstage_passes, backstage_pass_rules ))\n\n\n\n\nbracketing_rules = ActionSet(do_nothing)\\\n                       .but(Rule(compare_quality(leq(0)), set_quality(0)))\\\n                       .but(ActionSet(Rule(compare_quality(geq(50) ), set_quality(50))))\n                       .but(Rule(is_sulfuras, set_quality(80)))\n                                       \n\n\nclass GildedRose:\n    def __init__(self, items):\n        self._items = items\n        \n    def update_quality(self):\n        for i in range(0,len(self._items)):\n            self._items[i] = self._update_item(self._items[i])\n        \n\n    def _update_item(self, item):\n        item_record = extended_degradiation_rules(\n            ItemRecord( item.name, item.quality, 0, item.sellin) )\n        item_record = bracketing_rules( item_record._replace(quality=item_record.quality+item_record.quality_change ) )\n        return Item(item_record.name, max(item_record.sellin-1,0), item_record.quality)\n\n\nclass Item:\n    def __init__(self, name, sellin, quality):\n        self.name = name\n        self.sellin = sellin\n        self.quality = quality\n\n        \n    def __repr__(self):\n        return \"%s, %s, %s\" % (self.name, self.sellin, self.quality)\n\nclass TestGildedRose(ut.TestCase):\n    def test_standard_item(self):\n        gilded_rose = GildedRose([Item(\"a Sword\", 100, 5)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"a Sword, 99, 4\"], list(map(repr,gilded_rose._items)))\n\n    def test_conjured_item(self):\n        gilded_rose = GildedRose([Item(\"conjured Sword\", 100, 5)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"conjured Sword, 99, 3\"], list(map(repr,gilded_rose._items)))\n\n    def test_minimum_quality(self):\n        gilded_rose = GildedRose([Item(\"a Sword\", 100, 0)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"a Sword, 99, 0\"], list(map(repr,gilded_rose._items)))\n\n    def test_backstage_passes_10_days(self):\n        gilded_rose = GildedRose([Item(\"Backstage passes\", 10, 5)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"Backstage passes, 9, 7\"], list(map(repr,gilded_rose._items)))\n\n    def test_backstage_passes_5_days(self):\n        gilded_rose = GildedRose([Item(\"Backstage passes\", 5, 5)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"Backstage passes, 4, 8\"], list(map(repr,gilded_rose._items)))\n\n    def test_backstage_passes_0_days(self):\n        gilded_rose = GildedRose([Item(\"Backstage passes\", 0, 5)])\n\n        gilded_rose.update_quality( )\n\n        self.assertEqual( [\"Backstage passes, 0, 0\"], list(map(repr,gilded_rose._items)))\n\nif __name__ == \"__main__\":\n    ut.main()\n",
        "summary": "The provided Python code defines a system for updating the quality and sell-by date of items in a store, using rules-based logic. It includes classes for item records, rules, and actions, as well as a test class to validate the functionality with various scenarios. The core functionality is encapsulated within the `GildedRose` class, which iterates over a list of items, applying a series of rules to update their quality based on their type and remaining sell-by date."
    },
    {
        "code": "import json\nimport uuid\nfrom json import JSONDecodeError\nfrom typing import Tuple, Dict, List\n\nimport boto3\n\nfrom melange.drivers.interfaces import Queue, Topic, MessagingDriver, Message\n\n\nclass AWSDriver(MessagingDriver):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.max_number_of_messages = kwargs.get(\"max_number_of_messages\", 10)\n        self.visibility_timeout = kwargs.get(\"visibility_timeout\", 100)\n        self.wait_time_seconds = kwargs.get(\"wait_time_seconds\", 10)\n\n    def declare_topic(self, topic_name) -> Topic:\n        sns = boto3.resource(\"sns\")\n        topic = sns.create_topic(Name=topic_name)\n        return topic\n\n    def get_queue(self, queue_name) -> Queue:\n        sqs_res = boto3.resource(\"sqs\")\n\n        return sqs_res.get_queue_by_name(QueueName=queue_name)\n\n    def declare_queue(\n        self,\n        queue_name: str,\n        *topics_to_bind: Topic,\n        dead_letter_queue_name: str = None,\n        **kwargs\n    ) -> Tuple[Queue, Queue]:\n        try:\n            queue = self.get_queue(queue_name)\n        except Exception:\n            queue = self._create_queue(queue_name, content_based_deduplication=\"true\")\n\n        if topics_to_bind:\n            statements = []\n            for topic in topics_to_bind:\n                statement = {\n                    \"Sid\": \"Sid{}\".format(uuid.uuid4()),\n                    \"Effect\": \"Allow\",\n                    \"Principal\": \"*\",\n                    \"Resource\": queue.attributes[\"QueueArn\"],\n                    \"Action\": \"sqs:SendMessage\",\n                    \"Condition\": {\"ArnEquals\": {\"aws:SourceArn\": topic.arn}},\n                }\n\n                statements.append(statement)\n                subscription = topic.subscribe(\n                    Protocol=\"sqs\",\n                    Endpoint=queue.attributes[\n                        \"QueueArn\"\n                    ],  \n                )\n\n                if kwargs.get(\"filter_events\"):\n                    filter_policy = {\"event_type\": kwargs[\"filter_events\"]}\n                else:\n                    filter_policy = {}\n\n                subscription.set_attributes(\n                    AttributeName=\"FilterPolicy\",\n                    AttributeValue=json.dumps(filter_policy),\n                )\n\n            policy = {\n                \"Version\": \"2012-10-17\",\n                \"Id\": \"sqspolicy\",\n                \"Statement\": statements,\n            }\n\n            queue.set_attributes(Attributes={\"Policy\": json.dumps(policy)})\n\n        dead_letter_queue = None\n        if dead_letter_queue_name:\n            try:\n                dead_letter_queue = self.get_queue(dead_letter_queue_name)\n            except Exception:\n                dead_letter_queue = self._create_queue(\n                    dead_letter_queue_name, content_based_deduplication=\"true\"\n                )\n\n            redrive_policy = {\n                \"deadLetterTargetArn\": dead_letter_queue.attributes[\"QueueArn\"],\n                \"maxReceiveCount\": \"4\",\n            }\n\n            queue.set_attributes(\n                Attributes={\"RedrivePolicy\": json.dumps(redrive_policy)}\n            )\n\n        return queue, dead_letter_queue\n\n    def _create_queue(self, queue_name: str, **kwargs) -> Queue:\n        sqs_res = boto3.resource(\"sqs\")\n        fifo = queue_name.endswith(\".fifo\")\n        attributes = {}\n        if fifo:\n            attributes[\"FifoQueue\"] = \"true\"\n            attributes[\"ContentBasedDeduplication\"] = (\n                \"true\" if kwargs.get(\"content_based_deduplication\") else \"false\"\n            )\n        queue = sqs_res.create_queue(QueueName=queue_name, Attributes=attributes)\n        return queue\n\n    def retrieve_messages(self, queue: Queue, attempt_id=None) -> List[Message]:\n        kwargs = dict(\n            MaxNumberOfMessages=self.max_number_of_messages,\n            VisibilityTimeout=self.visibility_timeout,\n            WaitTimeSeconds=self.wait_time_seconds,\n            MessageAttributeNames=[\"All\"],\n            AttributeNames=[\"All\"],\n        )\n\n        if attempt_id:\n            kwargs[\"ReceiveRequestAttemptId\"] = attempt_id\n\n        messages = queue.receive_messages(**kwargs)\n\n        \n\n        return [self._construct_message(message) for message in messages]\n\n    def queue_publish(\n        self,\n        content: str,\n        queue,\n        event_type_name: str = None,\n        message_group_id: str = None,\n        message_deduplication_id: str = None,\n    ):\n        kwargs = dict(MessageBody=json.dumps({\"Message\": content}))\n\n        if event_type_name:\n            kwargs[\"MessageAttributes\"] = {\n                \"event_type\": {\"DataType\": \"String\", \"StringValue\": event_type_name}\n            }\n\n        if message_group_id:\n            kwargs[\"MessageGroupId\"] = message_group_id\n\n        if message_deduplication_id:\n            kwargs[\"MessageDeduplicationId\"] = message_deduplication_id\n\n        queue.send_message(**kwargs)\n\n    def publish(\n        self,\n        content: str,\n        topic: Topic,\n        event_type_name: str,\n        extra_attributes: Dict = None,\n    ):\n        args = dict(\n            Message=content,\n            MessageAttributes={\n                \"event_type\": {\"DataType\": \"String\", \"StringValue\": event_type_name}\n            },\n        )\n\n        if extra_attributes:\n            if \"subject\" in extra_attributes:\n                args[\"Subject\"] = extra_attributes[\"subject\"]\n\n            if \"message_attributes\" in extra_attributes:\n                args[\"MessageAttributes\"].update(extra_attributes[\"message_attributes\"])\n\n            if \"message_structure\" in extra_attributes:\n                args[\"MessageStructure\"] = extra_attributes[\"message_structure\"]\n\n        response = topic.publish(**args)\n\n        if \"MessageId\" not in response:\n            raise ConnectionError(\"Could not send the event to the SNS TOPIC\")\n\n    def acknowledge(self, message: Message) -> None:\n        message.metadata.delete()\n\n    def close_connection(self) -> None:\n        pass\n\n    def delete_queue(self, queue: Queue) -> None:\n        queue.delete()\n\n    def delete_topic(self, topic: Topic) -> None:\n        topic.delete()\n\n    def _construct_message(self, message) -> Message:\n        body = message.body\n        manifest = \"\"\n        try:\n            message_content = json.loads(body)\n            if \"Message\" in message_content:\n                content = message_content[\"Message\"]\n                \n                \n                if \"MessageAttributes\" in message_content:\n                    manifest = (\n                        message_content[\"MessageAttributes\"]\n                        .get(\"event_type\", {})\n                        .get(\"Value\")\n                        or \"\"\n                    )\n            else:\n                content = message_content\n        except JSONDecodeError:\n            content = body\n\n        manifest = (\n            manifest\n            or message.message_attributes.get(\"event_type\", {}).get(\"StringValue\")\n            or \"\"\n        )\n\n        return Message(message.message_id, content, message, manifest)\n",
        "summary": "The `AWSDriver` class provides methods to interact with AWS services for messaging, including creating and managing queues and topics, publishing messages, retrieving messages, and acknowledging them. It uses the Boto3 library to interface with AWS SNS and SQS services."
    },
    {
        "code": "import logging\nimport sys\n\nimport imsnpars.nparser.features\nimport imsnpars.nparser.network\nimport imsnpars.nparser.graph.features as gfeatures\nfrom imsnpars.nparser.graph import task, decoder\nfrom imsnpars.nparser.graph.mst import cle\nfrom imsnpars.nparser.labels import task as ltask\n\ndef buildMSTDecoder(opts, featBuilder):\n    if opts.mst == \"CLE\":\n        mstAlg = cle.ChuLiuEdmonds()\n        decod = decoder.FirstOrderDecoder(featBuilder)\n    else:\n        logging.error(\"Unknown algorithm: %s\" % opts.mst)\n        sys.exit()\n    \n    logging.info(\"Graph system used: %s\" % type(mstAlg))\n    logging.info(\"Decoder used: %s\" % type(decod))\n    return mstAlg, decod\n\ndef buildGraphFeatureExtractors(featuresD, reprDim):\n    featIds = { (\"h\", \"0\"): gfeatures.FeatId.HEAD,\n                (\"d\", \"0\"): gfeatures.FeatId.DEP,\n                (\"h\", \"1\"): gfeatures.FeatId.HEAD_P_1,\n                (\"h\", \"2\"): gfeatures.FeatId.HEAD_P_2,\n                (\"d\", \"1\"): gfeatures.FeatId.DEP_P_1,\n                (\"d\", \"2\"): gfeatures.FeatId.DEP_P_2,\n                (\"h\", \"-1\"): gfeatures.FeatId.HEAD_M_1,\n                (\"h\", \"-2\"): gfeatures.FeatId.HEAD_M_2,\n                (\"d\", \"-1\"): gfeatures.FeatId.DEP_M_1,\n                (\"d\", \"-2\"): gfeatures.FeatId.DEP_M_2,\n                (\"dist\", \"0\") : gfeatures.FeatId.DIST }\n    \n    mainFeatIds = {\"h\": gfeatures.FeatId.HEAD,\n                   \"d\": gfeatures.FeatId.DEP }\n    \n    featureExtractors = { }\n    featureBuilders = { }\n    \n    for feat in featuresD:\n        if \"+\" in feat:\n            name, shift = feat.split(\"+\")\n        elif \"-\" in feat:\n            name, shift = feat.split(\"-\")\n            shift = \"-\" + shift\n        else:\n            name, shift = feat, \"0\"\n            \n        featId = featIds.get((name, shift))\n        if featId == None:\n            logging.error(\"Unknown token id: %s\" % feat)\n            sys.exit()\n        \n        \n        if featId == gfeatures.FeatId.DIST:\n            featureBuilders[featId] = gfeatures.DistFeatureBuilder(reprDim)\n        else:\n            mainFeature = mainFeatIds[name]\n            if mainFeature not in featureExtractors:\n                featureExtractors[mainFeature] = gfeatures.TokenFeatExtractor()\n                \n            featureExtractors[mainFeature].addShift(featId, int(shift))\n       \n    return featureExtractors, featureBuilders\n    \n        \ndef buildGraphParser(opts, dummyBuilder, reprBuilder):\n    reprDim = reprBuilder.getDim()\n    tokExtractors, featBuilders = buildGraphFeatureExtractors(opts.features, reprDim)\n    extractor = gfeatures.GraphFeatureExtractor(tokExtractors)\n    \n    featIds = extractor.getFeatIds() + [ feat.getFeatId() for feat in featBuilders.values() ]\n    network = imsnpars.nparser.network.ParserNetwork(opts.mlpHiddenDim, opts.nonLinFun, featIds)\n    featBuilder = imsnpars.nparser.features.FeatReprBuilder(extractor, featBuilders, dummyBuilder, network, opts.parseLayer)\n    mstAlg, decod = buildMSTDecoder(opts, featBuilder)\n    \n    if opts.labeler == \"graph\":\n        lblDict = ltask.LblTagDict()\n        parsingTask = task.NNGraphParsingTaskWithLbl(mstAlg, featBuilder, decod, network, opts.augment, lblDict)\n    else:\n        parsingTask = task.NNGraphParsingTask(mstAlg, featBuilder, decod, network, opts.augment)\n    \n    return parsingTask\n",
        "summary": "The provided Python code defines functions to build a Maximum Spanning Tree (MST) decoder and graph feature extractors based on given options. It also includes a function to construct a graph parser using these components, which can optionally include labelers for enhanced parsing tasks."
    },
    {
        "code": "import pprint\nimport re  \n\nimport six\n\nfrom kubernetes.client.configuration import Configuration\n\n\nclass V1beta1EventList(object):\n    \n\n    \n    openapi_types = {\n        'api_version': 'str',\n        'items': 'list[V1beta1Event]',\n        'kind': 'str',\n        'metadata': 'V1ListMeta'\n    }\n\n    attribute_map = {\n        'api_version': 'apiVersion',\n        'items': 'items',\n        'kind': 'kind',\n        'metadata': 'metadata'\n    }\n\n    def __init__(self, api_version=None, items=None, kind=None, metadata=None, local_vars_configuration=None):  \n          \n        if local_vars_configuration is None:\n            local_vars_configuration = Configuration()\n        self.local_vars_configuration = local_vars_configuration\n\n        self._api_version = None\n        self._items = None\n        self._kind = None\n        self._metadata = None\n        self.discriminator = None\n\n        if api_version is not None:\n            self.api_version = api_version\n        self.items = items\n        if kind is not None:\n            self.kind = kind\n        if metadata is not None:\n            self.metadata = metadata\n\n    @property\n    def api_version(self):\n        \n        return self._api_version\n\n    @api_version.setter\n    def api_version(self, api_version):\n        \n\n        self._api_version = api_version\n\n    @property\n    def items(self):\n        \n        return self._items\n\n    @items.setter\n    def items(self, items):\n        \n        if self.local_vars_configuration.client_side_validation and items is None:  \n            raise ValueError(\"Invalid value for `items`, must not be `None`\")  \n\n        self._items = items\n\n    @property\n    def kind(self):\n        \n        return self._kind\n\n    @kind.setter\n    def kind(self, kind):\n        \n\n        self._kind = kind\n\n    @property\n    def metadata(self):\n        \n        return self._metadata\n\n    @metadata.setter\n    def metadata(self, metadata):\n        \n\n        self._metadata = metadata\n\n    def to_dict(self):\n        \n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \n        return self.to_str()\n\n    def __eq__(self, other):\n        \n        if not isinstance(other, V1beta1EventList):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \n        if not isinstance(other, V1beta1EventList):\n            return True\n\n        return self.to_dict() != other.to_dict()\n",
        "summary": "This Python code defines a class `V1beta1EventList` that represents a list of Kubernetes events. It includes properties for API version, items (the actual events), kind, and metadata, with corresponding getter and setter methods to manage these properties. The class also provides methods to convert the object to a dictionary, a string representation, and to compare objects for equality."
    },
    {
        "code": "import io\nimport logging\nimport click\nimport csv\nfrom collections import namedtuple\nfrom datetime import datetime\n\nlogger = logging.getLogger(\"bout\")\n\nprofiles = {}\nTransaction = namedtuple(\"Transaction\",\n                         [\"id\", \"date\", \"payee\", \"memo\", \"amount\"])\nInvalidTransaction = namedtuple(\"InvalidTransaction\", [])\n\n\ndef get_icici_csv(data_row):\n    \n    logger.debug(\"get_icicicsv: Data row = {}\".format(data_row))\n    date = data_row[0].replace('-', '/')\n    if _valid_date(date):\n        amt = \"-{}\".format(data_row[4])\n        if data_row[3] != \"0\":\n            amt = data_row[3]\n        return Transaction(id=0,\n                           date=date,\n                           payee=\"\",      \n                           memo=data_row[2],\n                           amount=amt)\n    return InvalidTransaction()\n\n\ndef get_icicicc_csv(data_row):\n    \n    logger.debug(\"get_icicicsv: Data row = {}\".format(data_row))\n    date = data_row[0]\n    if _valid_date(date, date_format=\"%d/%m/%Y\"):\n        amt = \"-{}\".format(data_row[5])\n        if data_row[6] == \"CR\":\n            amt = data_row[5]\n        return Transaction(id=0,\n                           date=date,\n                           payee=\"\",      \n                           memo=data_row[2],\n                           amount=amt)\n    return InvalidTransaction()\n\n\ndef qif_header():\n    \n    click.echo(\"!Account\\nNMyAccount\\nTMyBank\\n^\\n!Type:Bank\")\n\n\ndef to_qif(transaction):\n    \n    logger.debug(\"to_qif: Input = {}\".format(transaction))\n    return \"D{0}\\nM{1}\\nT{2}\\n^\\n\\n\"\\\n        .format(transaction.date, transaction.memo, transaction.amount)\n\n\ndef _valid_date(date_value, date_format=\"%d/%m/%Y\"):\n    \n    try:\n        transaction_date = datetime.strptime(date_value, date_format)\n        return transaction_date is not None\n    except ValueError:\n        return False\n\n\ndef _filter_csv_header(doc, header):\n    head_skip = False\n    mem = io.StringIO()\n    with open(doc, encoding='utf-8', mode='r') as f:\n        for line in f:\n            if line.startswith(header):\n                head_skip = True\n                continue\n            if head_skip and (not line or line.isspace()):\n                break\n            if head_skip and ',' in line:\n                mem.write(line)\n    mem.seek(0)\n    return csv.reader(mem)\n\n\n@click.command()\n@click.argument(\"doc\", type=click.Path(exists=True))\n@click.option(\"--profile\", prompt=\"Choose a profile\", default=\"icici\",\n              show_default=True,\n              type=click.Choice([\"icici\", \"icicicc\"]),\n              help=\"Document type profile.\")\n@click.option(\"--debug\", is_flag=True, show_default=True,\n              help=\"Show diagnostic messages.\")\ndef start(doc, profile, debug):\n    \n    if debug:\n        logging.basicConfig(level=logging.DEBUG)\n        logger.info(\"Verbose messages are enabled.\")\n\n    profiles.update({\"icici\": get_icici_csv,\n                     \"icicicc\": get_icicicc_csv})\n\n    rows = []\n    if profile == \"icici\":\n        header = \"DATE,MODE,PARTICULARS,DEPOSITS,WITHDRAWALS,BALANCE\"\n        rows = _filter_csv_header(doc, header)\n    elif profile == \"icicicc\":\n        header = \"Date,Sr.No.,Transaction Details,Reward Point Header,Intl.Amount,Amount(in Rs),BillingAmountSign\"\n        rows = _filter_csv_header(doc, header)\n\n    \n    \n    \n    create_transaction = profiles[profile]\n    print_header = False\n    for r in rows:\n        transaction = create_transaction(r)\n        if type(transaction) is not InvalidTransaction:\n            if not print_header:\n                qif_header()\n                print_header = True\n            click.echo(to_qif(transaction))\n\n\nif __name__ == '__main__':\n    start()\n",
        "summary": "The Python script processes CSV files from ICICI bank statements, filtering and converting them into QIF format for financial applications. It supports two profile types: \"icici\" and \"icicicc\", each handling different CSV formats and extracting relevant transaction details to generate QIF entries. The script includes logging for debugging and uses Click for command-line interface options to specify the document type and enable debug mode."
    },
    {
        "code": "from colored import *\nimport staticconf\n\n\n\nENABLE_COLORIZER = staticconf.read_string('enable_colorizer', default='false').lower() == 'true'\n\n\ndef colorizer_enabled(function):\n    \n    def wrapper(*args):\n        if ENABLE_COLORIZER:\n            return function(*args)\n        elif args:\n            return args[0]\n        else:\n            return args\n    return wrapper\n\n\n\nATTR_RESET = attr('reset')\nCOLOR_INDEX = fg(199)\nCOLOR_TITLE = fg(45)\nCOLOR_TAG_0 = fg(10) + attr('bold')\nCOLOR_TAG_1 = fg(10)\nCOLOR_TAG_2 = fg(87)\nCOLOR_TAG_3 = fg(208)\nCOLOR_TAG_4 = fg(252)\n\n\n@colorizer_enabled\ndef color_index(index):\n    return COLOR_INDEX + index + ATTR_RESET\n\n@colorizer_enabled\ndef color_title(title):\n    return COLOR_TITLE + title + ATTR_RESET\n\ndef _color_by_score(score):\n    if score >= 1:\n        return COLOR_TAG_0\n    elif score >= 0.9:\n        return COLOR_TAG_1\n    elif score >= 0.8:\n        return COLOR_TAG_2\n    elif score >= 0.7:\n        return COLOR_TAG_3\n    return COLOR_TAG_4\n\n@colorizer_enabled\ndef _color_tag(tag, score):\n    return _color_by_score(score) + tag + ATTR_RESET\n\ndef color_tags(scored_tags):\n    return \", \".join((_color_tag(tag, score) for tag, score in scored_tags))\n",
        "summary": "The provided Python code uses the `colored` and `staticconf` libraries to conditionally apply text coloring based on configuration settings. It defines several functions to colorize index, title, and tags with different colors and styles, which can be enabled or disabled via a configuration flag. The `_color_by_score` function determines the appropriate color for a tag based on its score, and `color_tags` combines these colored tags into a single string."
    },
    {
        "code": "from ._azure_media_services import AzureMediaServices\n__all__ = ['AzureMediaServices']\n\n\n\nfrom ._patch import patch_sdk\npatch_sdk()\n",
        "summary": "The provided Python code imports and exports the `AzureMediaServices` class from a module named `_azure_media_services`, making it available for use under the same name. Additionally, it includes a call to a function `patch_sdk()` from a module named `_patch`, which likely applies patches or updates to an SDK used within the application."
    },
    {
        "code": "import telebot\r\nimport requests\r\nfrom telebot.types import InlineKeyboardButton\r\n\r\n\nbot = telebot.TeleBot('**********************')\r\n\r\nwhile True:\r\n    try:\r\n\r\n        keyboard = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True)\r\n        keyboard.add(InlineKeyboardButton(text='Buat email'))\r\n        keyboard.add(InlineKeyboardButton(text='Refresh pesan'))\r\n        keyboard.add(InlineKeyboardButton(text='Tentang'))\r\n\r\n\r\n        @bot.message_handler(commands=['start'])\r\n        def start_message(message):\r\n            bot.send_message(message.chat.id,\r\n                             'Hai Pengguna., Selamat datang di TempEmail Bot \\nPenggunaan:\\nUntuk Menghasilkan email klik tombol \"Buat email\"\\nUntuk menyegarkan kotak masuk Anda, klik tombol \"Refresh inbox\". Setelah surat baru tiba, Anda akan melihat tombol dengan baris subjek, klik tombol read the message. \\n\\n Dev : @AnkiSatya',\r\n                             reply_markup=keyboard)\r\n\r\n\r\n        @bot.message_handler(content_types=['text'])\r\n        def send_text(message):\r\n            if message.text.lower() == 'buat email':\r\n                email = requests.get(\"https://www.1secmail.com/api/v1/?action=genRandomMailbox&count=1\").json()[0]\r\n                ekeyboard = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True)\r\n                ekeyboard.add(InlineKeyboardButton(text='Buat email'))\r\n                ekeyboard.add(InlineKeyboardButton(text='Refresh pesan\\n[' + str(email) + \"]\"))\r\n                ekeyboard.add(InlineKeyboardButton(text='Tentang'))\r\n                bot.send_message(message.chat.id, \"E-Mail Sementara Anda:\")\r\n                bot.send_message(message.chat.id, str(email), reply_markup=ekeyboard)\r\n            elif message.text.lower() == 'refresh pesan':\r\n                bot.send_message(message.chat.id, 'Pertama, buat email anda', reply_markup=keyboard)\r\n            elif message.text.lower() == 'tentang':\r\n                bot.send_message(message.chat.id,\r\n                                 'Apa itu Email Semantara?\\n- Itu adalah layanan email gratis yang memungkinkan untuk menerima email di alamat sementara yang akan dihancurkan sendiri setelah waktu tertentu berlalu. Itu juga dikenal dengan nama-nama seperti tempmail, 10minutemail, 10minmail, throwaway email, fake-mail , fake email generator, burner mail atau trash-mail\\n\\nBagaimana Email Sementara Menjadi Lebih Aman bagi Anda?\\n- Menggunakan Email sementara memungkinkan Anda untuk sepenuhnya melindungi kotak surat asli Anda dari hilangnya informasi pribadi. Alamat email sementara Anda sepenuhnya anonim. Detail Anda: informasi tentang orang Anda dan pengguna yang berkomunikasi dengan Anda, alamat IP, alamat email dilindungi dan sepenuhnya dirahasiakan.\\n\\n\u27aa Nama Bot : TempMail Bot\\n\u27aa Pembuat : @AnkiSatya\\n\u27aa Language : Python \\n\u27aa Donasi : https://saweria.co/ansaku')\r\n            elif message.text.lower()[14] == \"[\":\r\n                email = message.text.lower()[15:message.text.lower().find(\"]\")]\r\n                bkeyboard = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True)\r\n                bkeyboard.add(InlineKeyboardButton(text='Refresh pesan\\n[' + str(email) + \"]\"))\r\n                bkeyboard.add(InlineKeyboardButton(text='Buat email'))\r\n                try:\r\n                    data = requests.get(\r\n                        \"https://www.1secmail.com/api/v1/?action=getMessages&login=\" + email[:email.find(\r\n                            \"@\")] + \"&domain=\" + email[email.find(\"@\") + 1:]).json()\r\n                    if 'id' in data[0]:\r\n                        for i in range(len(data)):\r\n                            id = data[i]['id']\r\n                            subject = data[i]['subject']\r\n                            fromm = data[i]['from']\r\n                            date = data[i]['date']\r\n                            if len(subject) > 15:\r\n                                subject = str(subject[0:15]) + \"...\"\r\n                            bkeyboard.add(InlineKeyboardButton(\r\n                                text=str(subject) + \"\\n dari: \" + fromm + \" in \" + \"[id\" + str(id) + \"][\" + str(\r\n                                    email) + \"]\"))\r\n                            bot.send_message(message.chat.id,\r\n                                             \"Subjek: \" + subject + \"\\n Dari: \" + fromm + \"\\n Tanggal:\" + date,\r\n                                             reply_markup=bkeyboard)\r\n                            count = i + 1\r\n                        bot.send_message(message.chat.id, \"Di Sini \" + str(\r\n                            count) + \" Pesan ditemukan\\nKlik tombol di bawah untuk membaca pesan\\n\\n Info lebih lanjut @AnkiSatya\")\r\n                    else:\r\n                        bot.send_message(message.chat.id, 'Tidak ditemukan', reply_markup=bkeyboard)\r\n                except BaseException:\r\n                    bot.send_message(message.chat.id, 'Tidak ada pesan yang diterima...', reply_markup=bkeyboard)\r\n            elif message.text.lower().find(\"[id\"):\r\n                try:\r\n                    data = message.text.lower()[message.text.lower().find(\"[id\"):]\r\n                    id = data[data.find(\"[\") + 3:data.find(']')]\r\n                    email = data[data.find(\"][\") + 2:-1]\r\n                    msg = requests.get(\"https://www.1secmail.com/api/v1/?action=readMessage&login=\" + email[:email.find(\r\n                        \"@\")] + \"&domain=\" + email[email.find(\"@\") + 1:] + \"&id=\" + id).json()\r\n                    bot.send_message(message.chat.id,\r\n                                     'Pesan \u2709\ufe0f\\n\\n   Dari: ' + msg['from'] + \"\\n   Subjek: \" + msg[\r\n                                         'subject'] + \"\\n   Tanggal: \" + msg[\r\n                                         'date'] + \"\\n   Teks: \" + msg['textBody'])\r\n                except BaseException:\r\n                    pass\r\n\r\n\r\n        bot.polling(none_stop=True, interval=1, timeout=5000)\r\n    except BaseException:\r\n        pass\r\n        \r\n\n",
        "summary": "This Python script creates a Telegram bot named TempEmail Bot that allows users to generate temporary email addresses, refresh their inbox, and read messages. It uses the Telebot library for interaction with Telegram and makes API calls to 1secmail.com to manage email accounts and retrieve messages."
    },
    {
        "code": "import sys, os\n\nimport DNS\nimport rpcClient\nimport struct, listdns, base64, types, json, random\n\nfrom utils import *\nfrom common import *\n\nclass Source(object):\n    \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def isIP(self, host) :\n        parts = host.split(\".\")\n        if len(parts) != 4:\n            return False\n        try :\n            valid = False\n            for part in parts :\n                intpart = int(part)\n                if intpart <= 255 and intpart >= 0 :\n                    valid = True\n                else : return False\n            if valid :\n                return True\n            return False\n        except : return False\n    def get_response(self, query, domain, qtype, qclass, src_addr):\n        \n        \n        \n        \n        \n        if qtype == 1:\n            \n            reqtype = \"A\"\n        if qtype == 2:\n            \n            reqtype = \"NS\"\n        elif qtype == 5:\n            \n            reqtype = \"CNAME\"\n        elif qtype == 16:\n            \n            reqtype = \"TXT\"\n        elif qtype == 15:\n            \n            \n            \n            reqtype = \"MX\"\n        elif qtype == 28:\n            \n            reqtype = \"AAAA\"\n        elif qtype == 52:\n            reqtype = \"TLSA\"\n        else : reqtype = None\n        answers = app['services']['dns'].lookup({\"query\":query, \"domain\":domain, \"qtype\":qtype, \"qclass\":qclass, \"src_addr\":src_addr})\n        \n        \n        if domain.endswith(\".bit\") or domain.endswith(\".tor\") :\n            \n            \n            response = answers\n            results = []\n            if type(response) == types.DictType :\n                tempresults = {\"qtype\":response[\"type\"], \"qclass\":response[\"class\"], \"ttl\":response[\"ttl\"]}\n                if response[\"type\"] == 1 :\n                    \n                    \n                    tempresults[\"rdata\"] = struct.pack(\"!I\", ipstr2int(response[\"data\"]))\n                elif response[\"type\"] == 2 or response[\"type\"] == 5:\n                    tempresults[\"rdata\"] = labels2str(response[\"data\"].split(\".\"))\n                elif response[\"type\"] == 16 :\n                    tempresults[\"rdata\"] = labels2str(response[\"data\"])\n                elif response[\"type\"] == 15 :\n                    tempresult = struct.pack(\"!H\", response[\"data\"][0])\n                    tempresult += labels2str(response[\"data\"][1].split(\".\"))\n                    tempresults[\"rdata\"] = tempresult\n                elif response[\"type\"] == 28 :\n                    tempresults[\"rdata\"] = response[\"data\"]\n                elif response[\"type\"] == 52 :\n                    tempresult = '\\x03\\x00'\n                    tempresult += chr(int(response[\"data\"][0][0]))\n                    tempresult += bytearray.fromhex(response[\"data\"][0][1])\n                    tempresults[\"rdata\"] = tempresult\n                \n                results.append(tempresults)\n                return 0, results\n            if type(response) == types.StringType :\n                if self.isIP(response) :\n                    return 0, [{\"qtype\":1, \"qclass\":qclass, \"ttl\":300, \"rdata\":struct.pack(\"!I\", ipstr2int(response))}]\n            return 3, []\n            \n                \n            \n            \n            \n            \n            \n                \n                \n        else:\n            \n            \n            results = []\n            for response in answers :\n                tempresults = {\"qtype\":response[\"type\"], \"qclass\":response[\"class\"], \"ttl\":response[\"ttl\"]}\n                if response[\"type\"] == 1 :\n                    if answers == [] :\n                        return self.get_response(query, domain, 5, qclass, src_addr)\n                    tempresults[\"rdata\"] = struct.pack(\"!I\", ipstr2int(response[\"data\"]))\n                elif response[\"type\"] == 2 or response[\"type\"] == 5:\n                    tempresults[\"rdata\"] = labels2str(response[\"data\"].split(\".\"))\n                elif response[\"type\"] == 16 :\n                    tempresults[\"rdata\"] = labels2str(response[\"data\"])\n                elif response[\"type\"] == 15 :\n                    tempresult = struct.pack(\"!H\", response[\"data\"][0])\n                    tempresult += labels2str(response[\"data\"][1].split(\".\"))\n                    tempresults[\"rdata\"] = tempresult\n                elif response[\"type\"] == 28 :\n                    if answers == [] :\n                        return self.get_response(query, domain, 5, qclass, src_addr)\n                    \n                    tempresults[\"rdata\"] = response[\"data\"]\n                elif response[\"type\"] == 52 :\n                    tempresults[\"rdata\"] = response[\"data\"]\n                \n                results.append(tempresults)\n            return 0, results\n        return 3, []\n",
        "summary": "The provided Python code defines a `Source` class with methods to check if a host is an IP address and to get DNS responses. The `isIP` method validates whether a given host string represents a valid IPv4 address. The `get_response` method processes DNS queries for various record types, handling both standard and special domain types like `.bit` and `.tor`, and formats the responses accordingly."
    },
    {
        "code": "import json\nimport os\n\nfrom shutil import rmtree\n\nfrom polymatheia.data import NavigableDict\nfrom polymatheia.data.writer import JSONWriter\n\n\nDOCUMENTS = [NavigableDict(r) for r in [\n    {\n        'id': '1',\n        'name': {\n            'first': 'A',\n            'last': 'Person'\n        },\n        'age': 32,\n        'special tags': 'The first'\n    },\n    {\n        'id': '2',\n        'name': {\n            'first': ['Another', {'abbr': 'Nameless'}],\n            'last': 'Parrot'\n        },\n        'age': 23,\n    },\n    {\n        'id': '3',\n        'name': {\n            'first': 'The',\n            'last': 'Last'\n        },\n        'age': 65,\n    },\n]]\n\n\ndef test_local_json_writing():\n    \n    rmtree('tmp/json_writer_test', ignore_errors=True)\n    writer = JSONWriter('tmp/json_writer_test', 'id')\n    writer.write(DOCUMENTS)\n    count = 0\n    for basepath, _, filenames in os.walk('tmp/json_writer_test'):\n        for filename in filenames:\n            if filename.endswith('.json'):\n                count = count + len(filenames)\n                with open(os.path.join(basepath, filename)) as in_f:\n                    doc = json.load(in_f)\n                    assert 'id' in doc\n                    assert 'name' in doc\n                    if doc['id'] == '2':\n                        assert 'first' in doc['name']\n                        assert len(doc['name']['first']) == 2\n                    else:\n                        assert 'first' in doc['name']\n                    assert 'last' in doc['name']\n                    assert 'age' in doc\n                    if doc['id'] == '1':\n                        assert 'special tags' in doc\n    assert count == 3\n\n\ndef test_local_json_writing_pre_split_id_path():\n    \n    rmtree('tmp/json_writer_test', ignore_errors=True)\n    writer = JSONWriter('tmp/json_writer_test', ['id'])\n    writer.write(DOCUMENTS)\n    count = 0\n    for basepath, _, filenames in os.walk('tmp/json_writer_test'):\n        for filename in filenames:\n            if filename.endswith('.json'):\n                count = count + len(filenames)\n                with open(os.path.join(basepath, filename)) as in_f:\n                    doc = json.load(in_f)\n                    assert 'id' in doc\n                    assert 'name' in doc\n                    if doc['id'] == '2':\n                        assert 'first' in doc['name']\n                        assert len(doc['name']['first']) == 2\n                    else:\n                        assert 'first' in doc['name']\n                    assert 'last' in doc['name']\n                    assert 'age' in doc\n                    if doc['id'] == '1':\n                        assert 'special tags' in doc\n    assert count == 3\n",
        "summary": "The provided Python code defines a set of documents and includes two test functions to verify the functionality of a `JSONWriter` class from the `polymatheia.data.writer` module. The tests ensure that documents are correctly written to JSON files, with paths structured based on document IDs, and that the contents of each file match the expected structure and data."
    },
    {
        "code": "from __future__ import unicode_literals, absolute_import\nimport json\n\nimport requests\nimport six\nfrom tests import unittest, mock\n\nfrom freight_forwarder.registry               import Registry, V1, V2\nfrom freight_forwarder.registry.registry_base import RegistryBase, RegistryException\nfrom ..factories.registry_factory import RegistryV1Factory, RegistryV2Factory\n\n\nclass RegistryTest(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    @mock.patch.object(V1, '_validate_response', autospec=True, return_value=True)\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_registry_v1_init(self, mock_requests, mock_v1_validate):\n        test_registry = Registry()\n        self.assertIsInstance(test_registry, RegistryBase)\n        self.assertEquals(test_registry.ping(), True)\n\n    @mock.patch.object(V1, '_validate_response', name=\"v1_validate\")\n    @mock.patch.object(V2, '_validate_response', name=\"v2_validate\")\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_registry_v2_init(self, mock_requests, mock_v2, mock_v1):\n        mock_v1.side_effect = RegistryException(\"test\")\n        mock_v2.return_value = True\n        test_v1_registry = RegistryV1Factory()\n        test_v2_registry = RegistryV2Factory()\n        \n        \n        with self.assertRaises(RegistryException):\n            test_v1_registry.ping()\n        \n        self.assertEquals(test_v2_registry.ping(), True)\n\n        \n        test_registry = Registry(address=\"https://v2.dockertest.io\")\n        self.assertIsInstance(test_registry, RegistryBase)\n\n\nclass RegistryV1Test(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    @mock.patch.object(V1, '_validate_response', return_value=True)\n    @mock.patch.object(V1, '_request_builder')\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_v1_search(self, mock_requests, mock_request_builder, mock_validate_response):\n        \n        search_response_content = {\n            \"num_results\": 3,\n            \"query\": \"test\",\n            \"results\": [\n                {\"description\": \"api test app\", \"name\": \"testproject/test-app\"},\n                {\"description\": \"database test app\", \"name\": \"testproject/test-db\"},\n                {\"description\": \"cache test app\", \"name\": \"testproject/test-cache\"}\n            ]\n        }\n\n        \n        mock_request_builder.return_value = create_response_object(\n            url=\"https://search.registry.docker.com\",\n            status_code=200,\n            content=json.dumps(search_response_content).encode('utf-8')\n        )\n\n        \n        mock_validate_response.return_value = True\n\n        \n        test_registry = RegistryV1Factory(address='https://search.registry.docker.com')\n\n        results = test_registry.search(\"test\")\n        self.assertIsInstance(results, dict)\n\n    @mock.patch.object(V1, '_validate_response', return_value=True)\n    @mock.patch.object(V1, '_request_builder')\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_v1_tags(self, mock_requests, mock_request_builder, mock_validate_response):\n        tag_response_content = {\n            \"0.1\": \"3fad19bfa2\",\n            \"latest\": \"xxxxxxxxxx\",\n            \"localtest\": \"xxxxxxxxxxxxxxae13\",\n            \"redis123123\": \"xxxxxxxxxxxxxxae132\",\n            \"jira1268\": \"xxxxxxxxxxxxxxae1324987\"\n        }\n\n        formatted_output = [\n            'appexample/test-app:0.1',\n            'appexample/test-app:latest',\n            'appexample/test-app:us-east-01-dev',\n            'appexample/test-app:localtest',\n            'appexample/test-app:redis123123',\n            'appexample/test-app:jira1268'\n        ]\n\n        mock_request_builder.return_value = create_response_object(\n            url=\"https://tag.registry.docker.com\",\n            status_code=200,\n            content=json.dumps(tag_response_content).encode('utf-8')\n        )\n\n        mock_validate_response.return_value = True\n\n        test_registry = RegistryV1Factory(address='https://tag.registry.docker.com')\n\n        for tag in test_registry.tags(\"appexample/test-app\"):\n            tag_output = \"\".join(tag)\n            self.assertIsInstance(tag_output, six.string_types)\n            self.assertIn(tag_output, formatted_output)\n\n    def test_delete_tag(self):\n        self.skipTest(\"Implemented but not used\")\n\n    def test_delete(self):\n        self.skipTest(\"Implemented but not used\")\n\n    def test_get_image_by_id(self):\n        self.skipTest(\"Implemented but not used\")\n\n    def test_get_image_id_by_tag(self):\n        self.skipTest(\"Implemented but not used\")\n\n    def set_image_tag(self):\n        self.skipTest(\"Implemented but not used\")\n\n\nclass RegistryV2Test(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    @mock.patch.object(V2, '_validate_response', name='mock_v2_validate_response', return_value=True)\n    @mock.patch.object(V2, '_request_builder', name='mock_v2_request_builder')\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_v2_search(self, mock_requests, mock_request_builder, mock_validate_response):\n        \n        search_response_content = json.dumps({\"repositories\": [\"appexample/test-app\",\n                                                               \"appexample/test-db\",\n                                                               \"appexample/test-cache\"]}).encode('utf-8')\n\n        response = create_response_object(url=\"https://v2search.registry.docker.com\",\n                                          status_code=200,\n                                          content=search_response_content)\n\n        \n        mock_request_builder.return_value = response\n        \n        mock_validate_response.return_value = True\n\n        \n        test_registry = RegistryV2Factory(address='https://v2search.registry.docker.com')\n        test_registry.search(\"test\")\n\n        for search in test_registry.search(\"test\"):\n            search_output = \"\".join(search)\n            self.assertIsInstance(search_output, six.string_types)\n\n    @mock.patch.object(V2, '_validate_response', name='mock_v2_validate_response', return_value=True)\n    @mock.patch.object(V2, '_request_builder', name='mock_v2_request_builder')\n    @mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n    def test_v2_tags(self, mock_requests, mock_request_builder, mock_validate_response):\n        tag_response_content = json.dumps({\"name\": \"appexample/test-app\",\n                                           \"tags\": [\n                                               \"latest\",\n                                               \"0.0.15\",\n                                               \"asdfasb81\"]\n                                           }\n                                          ).encode('utf-8')\n\n        formatted_output = ['appexample/test-app:latest',\n                            'appexample/test-app:0.0.15',\n                            'appexample/test-app:asdfasb81']\n\n        response = create_response_object(url=\"https://v2tags.registry.docker.com\",\n                                          status_code=200,\n                                          content=tag_response_content)\n\n        mock_request_builder.return_value = response\n        mock_validate_response.return_value = True\n\n        test_registry = RegistryV2Factory(address='https://v2tags.registry.docker.com')\n\n        for tags in test_registry.tags(\"appexample/test-app\"):\n            tag_output = \"\".join(tags)\n            self.assertIsInstance(tag_output, six.string_types)\n            self.assertIn(tag_output, formatted_output)\n\n    def test_blobs(self):\n        self.skipTest(\"Not implemented\")\n\n    def test_catalog(self, count=None, last=None):\n        self.skipTest(\"Not implemented\")\n\n    def test_manifests(self):\n        self.skipTest(\"Not implemented\")\n\n\nclass RegistryBaseTests(unittest.TestCase):\n    def setUp(self):\n        self.patch_requests = mock.patch('freight_forwarder.registry.registry_base.requests', autospec=True)\n        self.patch_requests.start()\n        self.test_registry = RegistryV1Factory(address=\"https://registrybasetest.docker.com\")\n\n    def tearDown(self):\n        self.patch_requests.stop()\n        del self.test_registry\n\n    def test_ping(self):\n        self.skipTest(\"Defined as abc method. Override in class\")\n\n    def test_tags(self):\n        self.skipTest(\"Defined as abc method. Override in class\")\n\n    def test_init(self):\n        self.assertEquals(self.test_registry.scheme, 'https://')\n        self.assertEquals(self.test_registry.location, 'registrybasetest.docker.com')\n        self.assertEquals(self.test_registry.auth, None)\n        self.assertEquals(self.test_registry.__str__(), \"https://registrybasetest.docker.com\")\n        self.assertIsInstance(self.test_registry, RegistryBase)\n\n    def test_registry_base_auth_base_functionality(self):\n        self.assertEquals(self.test_registry.auth, None)\n\n        with self.assertRaises(TypeError):\n            self.test_registry.auth = [\"user=test_user\", \"passwd=password\"]\n\n    def test_registry_base_auth_with_auth(self):\n        pass\n\n\nclass RegistryExceptionTest(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_exception_with_status_code_and_url(self):\n        response = create_response_object(url=\"https://bad.docker.io\",\n                                          status_code=503,\n                                          content={\"test\": \"data\"})\n        registry_exception = RegistryException(response)\n        self.assertIsInstance(registry_exception, RegistryException)\n        self.assertEquals(registry_exception.response.status_code, 503)\n\n    def test_exception_with_no_content(self):\n        response = create_response_object(url=\"https://nocontent.docker.io\",\n                                          status_code=503)\n        registry_exception = RegistryException(response)\n        self.assertIsInstance(registry_exception, RegistryException)\n        self.assertEquals(registry_exception.message, 'There was an issue with the request to the docker registry.')\n\n    def test_exception_with_error_content(self):\n        \n        response = create_response_object(url=\"https://errorcontent.docker.io\",\n                                          status_code=500,\n                                          content=json.dumps({'error': 'Docker Registry Error Example'}))\n        registry_exception = RegistryException(response)\n        self.assertIsInstance(registry_exception, RegistryException)\n        self.assertEquals(registry_exception.message, 'Docker Registry Error Example')\n\n        \n        self.assertEquals(\"{0}\".format(registry_exception), 'Docker Registry Error Example')\n\n\ndef create_response_object(url, status_code, content=None):\n    \n    if not isinstance(url, six.string_types):\n        raise(TypeError(\"incorrect type provided for url\"))\n\n    if not isinstance(status_code, six.integer_types):\n        raise(TypeError(\"incorrect type provided for http status code\"))\n\n    mock_object_request = mock.MagicMock(spec=requests.PreparedRequest, url=url)\n    mock_object_response = mock.MagicMock(spec=requests.Response, request=mock_object_request)\n    mock_object_response.status_code = status_code\n\n    if content:\n        mock_object_response.content = content\n    else:\n        \n        del mock_object_response.content\n\n    return mock_object_response\n\n\ndef format_image_results(registry_response_dict):\n    \n    if not isinstance(registry_response_dict, dict):\n        raise TypeError('registry_response_dict must be a dict.')\n\n    images = {}\n    results = registry_response_dict.get('results')\n\n    if results:\n        for image in results:\n            images[image.get('name')] = image\n\n    return images\n",
        "summary": "This code defines several unit tests for a Docker registry client library. The tests cover various aspects of the library, including:\n\n1. Testing the `RegistryBase` class and its methods.\n2. Testing the `RegistryException` class to ensure it handles different types of error responses from the Docker registry.\n3. Testing the `create_response_object` helper function used to create mock HTTP response objects for testing purposes.\n\nThe tests use the `unittest` framework and the `mock` library to simulate HTTP requests and responses. The code also includes a few utility functions, such as `format_image_results`, which is not directly related to the tests but may be used in the actual implementation of the registry client.\n\nOverall, this code provides a comprehensive set of unit tests for a Docker registry client library, ensuring that it handles various scenarios correctly and robustly."
    },
    {
        "code": "__all__ = [\"loss_fn\"]\n\nfrom icevision.imports import *\n\n\ndef loss_fn(preds, targets) -> torch.Tensor:\n    return preds[\"loss\"]\n",
        "summary": "The provided Python code defines a module that exports a single function `loss_fn`. This function takes two parameters, `preds` and `targets`, and returns the value of \"loss\" from the `preds` dictionary as a PyTorch tensor."
    },
    {
        "code": "from conans import ConanFile, CMake\nimport os\n\nchannel = os.getenv(\"CONAN_CHANNEL\", \"testing\")\nusername = os.getenv(\"CONAN_USERNAME\", \"memsharded\")\n\nclass EasyLoggingTestConan(ConanFile):\n    settings = \"os\", \"compiler\", \"build_type\", \"arch\"\n    requires = \"easyloggingpp/9.94.1@%s/%s\" % (username, channel)\n    generators = \"cmake\"\n\n    def build(self):\n        cmake = CMake(self.settings)\n        self.run('cmake \"%s\" %s' % (self.conanfile_directory, cmake.command_line))\n        self.run(\"cmake --build . %s\" % cmake.build_config)\n\n    def imports(self):\n        self.copy(\"*.cc\")\n\n    def test(self):\n        os.chdir(\"bin\")\n        self.run(\".%sexample\" % os.sep)\n",
        "summary": "This Python script defines a Conan package for testing the EasyLogging++ library, specifying build settings and dependencies. It includes methods for building the project using CMake, importing necessary files, and running tests in a separate directory."
    },
    {
        "code": "from itea.inspection._ITExpr_explainer import ITExpr_explainer\r\nfrom itea.inspection._ITExpr_inspector import ITExpr_inspector\r\nfrom itea.inspection._ITExpr_texifier  import ITExpr_texifier\r\nfrom itea.inspection._ITEA_summarizer  import ITEA_summarizer\r\n\r\nimport jax\r\n\r\n\r\n\njax.config.update('jax_platform_name', 'cpu')\r\n\r\n\r\n__all__ = [\r\n    'ITExpr_explainer',\r\n    'ITExpr_inspector',\r\n    'ITExpr_texifier',\r\n    'ITEA_summarizer'\r\n]",
        "summary": "The Python code imports several modules from the `itea.inspection` package, including classes for explaining expressions (`ITExpr_explainer`), inspecting expressions (`ITExpr_inspector`), converting expressions to LaTeX format (`ITExpr_texifier`), and summarizing expressions (`ITEA_summarizer`). It also configures JAX to use the CPU platform."
    },
    {
        "code": "import unittest\nfrom ovos_plugin_manager.skills import find_skill_plugins\n\n\nclass TestPlugin(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.skill_id = \"ovos-skill-timer.OpenVoiceOS\"\n\n    def test_find_plugin(self):\n        plugins = find_skill_plugins()\n        self.assertIn(self.skill_id, list(plugins))\n\n",
        "summary": "The provided Python code defines a unit test class `TestPlugin` that inherits from `unittest.TestCase`. It includes a method `test_find_plugin` to verify if a specific skill plugin identified by `self.skill_id` is found within the list of plugins returned by the `find_skill_plugins` function."
    },
    {
        "code": "from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Lambda, ELU\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import model_from_json\nfrom sklearn.preprocessing import normalize\nimport cv2\nimport numpy as np\nimport glob\nimport json\nfrom keras.layers import merge\nfrom keras.layers.core import Lambda\nfrom keras.models import Model\n\nimport tensorflow as tf\n\n\ndef make_parallel(model, gpu_count):\n    def get_slice(data, idx, parts):\n        shape = tf.shape(data)\n        size = tf.concat(0, [shape[:1] // parts, shape[1:]])\n        stride = tf.concat(0, [shape[:1] // parts, shape[1:] * 0])\n        start = stride * idx\n        return tf.slice(data, start, size)\n\n    outputs_all = []\n    for i in range(len(model.outputs)):\n        outputs_all.append([])\n\n    \n    for i in range(gpu_count):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('tower_%d' % i) as scope:\n\n                inputs = []\n                \n                for x in model.inputs:\n                    input_shape = tuple(x.get_shape().as_list())[1:]\n                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx': i, 'parts': gpu_count})(x)\n                    inputs.append(slice_n)\n\n                outputs = model(inputs)\n\n                if not isinstance(outputs, list):\n                    outputs = [outputs]\n\n                \n                for l in range(len(outputs)):\n                    outputs_all[l].append(outputs[l])\n\n    \n    with tf.device('/cpu:0'):\n        merged = []\n        for outputs in outputs_all:\n            merged.append(merge(outputs, mode='concat', concat_axis=0))\n\n        return Model(input=model.inputs, output=merged)\n\n\nclass CNNClassifier:\n    def __init__(self):\n        self.classifier = None\n\n    def get_model(self, parallel=False):\n        model = Sequential()\n        \n        model.add(Convolution2D(8, 8, 8, subsample=(4, 4), border_mode=\"same\", activation='elu', name='Conv1'))\n        model.add(Convolution2D(16, 5, 5, subsample=(2, 2), border_mode=\"same\", activation='elu', name='Conv2'))\n        model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\", activation='elu', name='Conv3'))\n        model.add(Flatten())\n        model.add(ELU())\n        model.add(Dense(1024, activation='elu'))\n        model.add(Dropout(.5))\n        model.add(ELU())\n        model.add(Dense(512, activation='elu'))\n        model.add(Dropout(.5))\n        model.add(Dense(1, name='output'))\n        model.add(Activation('sigmoid'))\n        if parallel:\n            model = make_parallel(model, 2)\n        \n        self.model = model\n        return model\n\n    def _model(self):\n        img_width, img_height = 64, 64\n        model = Sequential()\n        model.add(Convolution2D(8, 3, 3, input_shape=(img_width, img_height, 3)))\n        model.add(Activation('elu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n\n        \n        \n        \n\n        \n        \n        \n\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        \n        self.model = model\n\n    def compile(self):\n        self.model.compile(loss='binary_crossentropy',\n                      optimizer='rmsprop', class_mode='binary',\n                      metrics=['accuracy'])\n\n    def save(self):\n        model_json = self.model.to_json()\n        with open(\"./model.json\", \"w\") as json_file:\n            json.dump(model_json, json_file)\n        self.model.save_weights(\"./model.h5\")\n        print(\"Saved model to disk\")\n\n    def load(self):\n        with open('./model.json', 'r') as jfile:\n            self.model = model_from_json(json.load(jfile))\n\n        self.compile()\n        self.model.load_weights('./model.h5')\n\n    def get_list(self):\n        vehicles = np.array(glob.glob('training_data/vehicles/*/*'))\n        y_vehicles = np.zeros(vehicles.shape) + 1\n        non_vehicles = np.array(glob.glob('training_data/non-vehicles/*/*'))\n        y_non_vehicles = np.zeros(non_vehicles.shape)\n        X_data = np.concatenate((vehicles, non_vehicles))\n        Y_data = np.concatenate((y_vehicles, y_non_vehicles))\n        return X_data, Y_data\n\n    def predict(self, image):\n        \n        \n        x = image[None, :, :, :]\n        result = self.model.predict(x, 1)\n        return result\n\n    def train(self, file_list, labels, test_size=0.2, nb_epoch=30, batch_size=128):\n        X_train, X_test, Y_train, Y_test = train_test_split(file_list, labels, test_size=test_size, random_state=100)\n\n        test_images = build_images(X_test)\n        train_images = build_images(X_train)\n\n        train_datagen = ImageDataGenerator(\n            rescale=1. / 255,\n            shear_range=0.05,\n            zoom_range=0.05,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            rotation_range=5,\n            horizontal_flip=True)\n        test_datagen = ImageDataGenerator(rescale=1. / 255)\n        train_generator = train_datagen.flow(train_images, Y_train, batch_size)\n        test_generator = test_datagen.flow(test_images, Y_test, batch_size)\n\n        nb_train_samples = (batch_size-1)*100\n        nb_validation_samples = (batch_size-1)*20\n\n        \n        self._model()\n        self.compile()\n\n        self.model.fit_generator(\n            train_generator,\n            samples_per_epoch=nb_train_samples,\n            nb_epoch=nb_epoch, show_accuracy=True,\n            validation_data=test_generator,\n            nb_val_samples=nb_validation_samples)\n\ndef build_images(x):\n    images = np.zeros((len(x), 64, 64, 3))\n    for idx, img_fname in enumerate(x):\n        im = cv2.imread(img_fname)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n        im = cv2.resize(im, (64, 64), interpolation=cv2.INTER_AREA)\n        images[idx] = im\n    return images\n\ndef do_all(nb_epoch=30, batch_size=256):\n    clf = CNNClassifier()\n    x, y = clf.get_list()\n    clf.train(x, y, nb_epoch=nb_epoch, batch_size=batch_size)\n    clf.save()\n\n",
        "summary": "The provided Python code defines a class `CNNClassifier` for training and using a convolutional neural network (CNN) model to classify images. The model architecture includes multiple convolutional layers followed by dense layers with dropout for regularization. The class supports parallel training across multiple GPUs, data augmentation during training, and methods for saving and loading the model. The `do_all` function demonstrates how to use this classifier to train on a dataset of vehicle and non-vehicle images."
    },
    {
        "code": "import pytest\nfrom brownie import interface\n\n\ndef test_uniswap_add_two_tokens(\n    admin, alice, chain, bank, werc20, ufactory, urouter, simple_oracle, oracle, celo, cusd, ceur, UniswapV2SpellV1, UniswapV2Oracle, core_oracle\n):\n    spell = UniswapV2SpellV1.deploy(bank, werc20, urouter, celo, {'from': admin})\n    cusd.mint(admin, 10000000 * 10**6, {'from': admin})\n    ceur.mint(admin, 10000000 * 10**6, {'from': admin})\n    cusd.approve(urouter, 2**256-1, {'from': admin})\n    ceur.approve(urouter, 2**256-1, {'from': admin})\n    urouter.addLiquidity(\n        cusd,\n        ceur,\n        1000000 * 10**6,\n        1000000 * 10**6,\n        0,\n        0,\n        admin,\n        chain.time() + 60,\n        {'from': admin},\n    )\n\n    lp = ufactory.getPair(cusd, ceur)\n    print('admin lp bal', interface.IERC20(lp).balanceOf(admin))\n    uniswap_lp_oracle = UniswapV2Oracle.deploy(core_oracle, {'from': admin})\n\n    print('ceur Px', simple_oracle.getCELOPx(ceur))\n    print('cusd Px', simple_oracle.getCELOPx(cusd))\n\n    core_oracle.setRoute([cusd, ceur, lp], [simple_oracle, simple_oracle, uniswap_lp_oracle])\n    print('lp Px', uniswap_lp_oracle.getCELOPx(lp))\n\n    oracle.setTokenFactors(\n        [cusd, ceur, lp],\n        [\n            [10000, 10000, 10000],\n            [10000, 10000, 10000],\n            [10000, 10000, 10000],\n        ],\n        {'from': admin},\n    )\n    cusd.mint(alice, 10000000 * 10**6, {'from': admin})\n    ceur.mint(alice, 10000000 * 10**6, {'from': admin})\n    cusd.approve(bank, 2**256-1, {'from': alice})\n    ceur.approve(bank, 2**256-1, {'from': alice})\n    spell.getAndApprovePair(cusd, ceur, {'from': admin})\n    lp = ufactory.getPair(cusd, ceur)\n    spell.setWhitelistLPTokens([lp], [True], {'from': admin})\n    bank.setWhitelistSpells([spell], [True], {'from': admin})\n    bank.setWhitelistTokens([cusd, ceur], [True, True], {'from': admin})\n    tx = bank.execute(\n        0,\n        spell,\n        spell.addLiquidityWERC20.encode_input(\n            ceur,  \n            cusd,  \n            [\n                40000 * 10**6,  \n                50000 * 10**6,  \n                0,\n                1000 * 10**6,  \n                200 * 10**6,  \n                0,  \n                0,  \n                0,  \n            ],\n        ),\n        {'from': alice}\n    )\n\n    position_id = tx.return_value\n    print('tx gas used', tx.gas_used)\n    print('bank collateral size', bank.getPositionInfo(position_id))\n    print('bank collateral value', bank.getCollateralCELOValue(position_id))\n    print('bank borrow value', bank.getBorrowCELOValue(position_id))\n\n    print('bank ceur', bank.getBankInfo(ceur))\n    print('bank cusd', bank.getBankInfo(cusd))\n\n    print('ceur Px', simple_oracle.getCELOPx(ceur))\n    print('cusd Px', simple_oracle.getCELOPx(cusd))\n\n    print('lp Px', uniswap_lp_oracle.getCELOPx(lp))\n",
        "summary": "The provided Python code is a test function using the `pytest` framework and the `brownie` library to interact with smart contracts on a blockchain. It deploys various contracts, mints tokens, adds liquidity to a Uniswap pool, sets up price oracles, and executes a spell to add liquidity through a bank contract, while also printing out balances and prices for verification."
    },
    {
        "code": "from socket import error\nfrom zlib import (\n    decompressobj,\n    MAX_WBITS,\n    Z_FULL_FLUSH,\n)\n\nfrom geventwebsocket.exceptions import (\n    ProtocolError,\n    WebSocketError,\n)\nfrom geventwebsocket.websocket import (\n    MSG_SOCKET_DEAD,\n    Header,\n    WebSocket,\n)\n\n\nDECOMPRESSOR = decompressobj(-MAX_WBITS)\n\n\ndef _encode_bytes(text):\n    if isinstance(text, str):\n        return text\n\n    if not isinstance(text, unicode):\n        text = unicode(text or '')\n\n    return text.encode('utf-8')\n\n\ndef make_compressed_frame(message, compressor):\n    \n    binary = not isinstance(message, (str, unicode))\n    opcode = WebSocket.OPCODE_BINARY if binary else WebSocket.OPCODE_TEXT\n    if binary:\n        message = str(message)\n    else:\n        message = _encode_bytes(message)\n    message = compressor.compress(message)\n    \n    \n    \n    \n    message += compressor.flush(Z_FULL_FLUSH)\n    \n    if message.endswith('\\x00\\x00\\xff\\xff'):\n        message = message[:-4]\n\n    \n    flags = Header.RSV0_MASK\n    header = Header.encode_header(\n        fin=True, opcode=opcode, mask='', length=len(message), flags=flags)\n\n    return header + message\n\n\ndef send_raw_frame(websocket, raw_message):\n    \n    try:\n        websocket.raw_write(raw_message)\n    except error:\n        websocket.current_app.on_close(MSG_SOCKET_DEAD)\n        raise WebSocketError(MSG_SOCKET_DEAD)\n\n\ndef read_frame(websocket):\n    \n\n    header = Header.decode_header(websocket.stream)\n\n    \n    compressed = header.flags & header.RSV0_MASK\n    if compressed:\n        header.flags &= ~header.RSV0_MASK\n    \n\n    if header.flags:\n        raise ProtocolError\n\n    if not header.length:\n        return header, ''\n\n    try:\n        payload = websocket.raw_read(header.length)\n    except error:\n        payload = ''\n    except Exception:\n\n        \n        raise WebSocketError('Could not read payload')\n        \n\n    if len(payload) != header.length:\n        raise WebSocketError('Unexpected EOF reading frame payload')\n\n    if header.mask:\n        payload = header.unmask_payload(payload)\n\n    \n    if compressed:\n        payload = ''.join((\n            DECOMPRESSOR.decompress(payload),\n            DECOMPRESSOR.decompress('\\0\\0\\xff\\xff'),\n            DECOMPRESSOR.flush(),\n        ))\n    \n\n    return header, payload\n",
        "summary": "The provided Python code defines functions for compressing and decompressing WebSocket frames using the zlib library, handling exceptions related to socket errors and protocol violations, and sending and receiving raw WebSocket frames. It includes utilities for encoding text data, creating compressed frames with appropriate opcodes, and managing frame headers and payloads according to WebSocket specifications."
    },
    {
        "code": "import os\nimport json\n\nEnviron = os._Environ\n\n\ndef is_on_cloudfoundry(env: Environ=os.environ) -> bool:\n    return 'VCAP_SERVICES' in env\n\n\ndef load_cups_from_vcap_services(name: str, env: Environ=os.environ) -> None:\n    \n\n    if not is_on_cloudfoundry(env):\n        return\n\n    vcap = json.loads(env['VCAP_SERVICES'])\n\n    for entry in vcap.get('user-provided', []):\n        if entry['name'] == name:\n            for key, value in entry['credentials'].items():\n                env[key] = value\n\n\ndef load_database_url_from_vcap_services(name: str, service: str,\n                                         env: Environ=os.environ) -> str:\n    \n    if not is_on_cloudfoundry(env):\n        return\n\n    \n    \n    vcap = json.loads(env['VCAP_SERVICES'])\n    env['DATABASE_URL'] = vcap[service][0][\"credentials\"][\"uri\"]\n",
        "summary": "The provided Python code defines functions to check if an application is running on Cloud Foundry, load user-provided service credentials into the environment, and extract a database URL from Cloud Foundry's VCAP_SERVICES environment variable."
    },
    {
        "code": "from pymongo import MongoClient\nfrom pymongo import ReadPreference\nfrom datetime import datetime, timedelta\n\n\nclass Mongo(MongoClient):\n    def __init__(self, username, password, host, db='tags', collection='tweets_pipeline_v2'):\n\n        uri = f\"mongodb://{username}:{password}@{host}/{db}\"\n        super(Mongo, self).__init__(host=uri,\n                                    authSource=db,\n                                    authMechanism='SCRAM-SHA-256',\n                                    port=27017,\n                                    replicaset=\"rs0\",\n                                    read_preference=ReadPreference.SECONDARY,\n                                    )\n        self.database = self.get_default_database()\n        self.collection = collection\n\n    def pipelined(self, count=True):\n        query = {\"status\": \"pipelined\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n    def feed(self, count=True):\n        query = {\"status\": \"graphicone_feed\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n    def search(self, count=True):\n        query = {\"status\": \"graphicone_search\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n    def left_for_analysts(self, count=True):\n        query = {\"in_app\": {\"$exists\": False},\n                 \"status\":  \"graphicone_feed\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n    def removed_validators(self, count=True):\n        query = {\"validator_username\": {\"$exists\": True},\n                             \"status\": \"deleted\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n    def removed_analysts(self, count=True):\n        query = {\"status\": \"deleted_from_analytics\"}\n        if count:\n            return self.database[self.collection].count_documents(query)\n        return self.database[self.collection].find(query)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code defines a `Mongo` class that extends `MongoClient` from the `pymongo` library to interact with a MongoDB database. The class includes methods for querying documents based on specific statuses and conditions, such as counting or retrieving documents where the status is \"pipelined,\" \"graphicone_feed,\" etc., and handling different scenarios like those involving analysts and validators."
    },
    {
        "code": "from lk_logger import lk\n\nfrom examples import t01_simple_examples as t01\nfrom examples import t02_referencing as t02\nfrom examples import t03_fibonacci as t03\nfrom examples import t04_catch_exceptions as t04\nfrom examples import t05_qt_button_click_event as t05\nfrom examples import t06_lambdex_kwargs as t06\n\n\n\nif __name__ == '__main__':\n    with lk.counting(6):\n        for mod in [t01, t02, t03, t04, t05, t06]:\n            lk.logdx(mod.__name__)\n            \n            with lk.counting():\n                for name in dir(mod):\n                    if name.startswith('test_'):\n                        func = getattr(mod, name)\n                        lk.logax('testing', func.__name__)\n                        \n                        try:\n                            func()\n                        except Exception as e:\n                            lk.logt('[I1117]', e)\n                            continue\n",
        "summary": "The Python script imports various modules from a directory named \"examples\" and uses the `lk_logger` library to log information about each module and its test functions. It counts the number of modules and tests, logs the names of the modules and their test functions, and handles exceptions that may occur during the execution of these test functions."
    },
    {
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndataset = pd.read_csv('50_Startups.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer([(\"State\", OneHotEncoder(), [3])], remainder = 'passthrough')\nX= ct.fit_transform(X)\n\n\nX=X[:,1:]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\n\n\n\ny_pred=regressor.predict(X_test)\n\n\nimport statsmodels.api as sm\nX=np.append(arr=np.ones((50,1)).astype(int),values=X,axis=1)\n\n\nX_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)\nregressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\nregressor_OLS.summary()\n\n\nX_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\nregressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\nregressor_OLS.summary()\n\nX_opt = np.array(X[:, [0, 3, 4, 5]], dtype=float)\nregressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\nregressor_OLS.summary()\n\n\nX_opt = np.array(X[:, [0, 3, 5]], dtype=float)\nregressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\nregressor_OLS.summary()\n\n\nX_opt = np.array(X[:, [0, 3]], dtype=float)\nregressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\nregressor_OLS.summary()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python code performs data preprocessing on a dataset from '50_Startups.csv', including encoding categorical variables and splitting the data into training and testing sets. It then applies linear regression to predict startup profits based on various features, using backward elimination for feature selection to identify the most significant predictors. Finally, it summarizes the model's performance using statistical metrics."
    },
    {
        "code": "import ckeditor_uploader.fields\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('blog', '0007_subscriber'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='post',\n            name='content',\n            field=ckeditor_uploader.fields.RichTextUploadingField(),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that alters the `content` field of the `Post` model in the `blog` app to use the `RichTextUploadingField` from the `ckeditor_uploader.fields` module, enabling rich text editing and file uploads for post content."
    },
    {
        "code": "import unittest\nfrom numpy.testing import assert_allclose\n\nfrom qspectra import polarization\nfrom qspectra.simulate import decorators\n\n\nclass TestGetCallArgs(unittest.TestCase):\n    def test(self):\n        self.assertEqual(\n            decorators._get_call_args(lambda a: None, 1),\n            {'a': 1})\n        self.assertEqual(\n            decorators._get_call_args(lambda a, **b: None, 1),\n            {'a': 1})\n        self.assertEqual(\n            decorators._get_call_args(lambda a, **b: None, a=1, c=2),\n            {'a': 1, 'c': 2})\n        self.assertEqual(\n            decorators._get_call_args(lambda **b: None, a=1, c=2),\n            {'a': 1, 'c': 2})\n        with self.assertRaises(NotImplementedError):\n            decorators._get_call_args(lambda *a: None, 1, 2, 3)\n\n\nclass TestIsotropicAverage(unittest.TestCase):\n    def test_optional_2nd_order_isotropic_average(self):\n        binary = {'xx': 1, 'yy': 2, 'zz': 4}\n        f = decorators.optional_2nd_order_isotropic_average(\n            lambda polarization: (0, binary[polarization]))\n        assert_allclose(f('xx'), (0, 1))\n        assert_allclose(f('xx', exact_isotropic_average=False), (0, 1))\n        assert_allclose(f('xx', exact_isotropic_average=True), (0, 7 / 3.0))\n        assert_allclose(f('xy', exact_isotropic_average=True), (0, 0))\n        with self.assertRaises(ValueError):\n            \n            f('xyz', exact_isotropic_average=True)\n\n    def test_optional_4th_order_isotropic_average(self):\n        binary = {'xx': 1, 'yy': 2, 'zz': 4}\n        f = decorators.optional_4th_order_isotropic_average(\n            lambda polarization: (0, binary[polarization[:2]]\n                                      + 10 * binary[polarization[2:]]))\n        assert_allclose(f('xxxx'), (0, 11))\n        ma = polarization.MAGIC_ANGLE\n        assert_allclose(f([0, 0, ma, ma], exact_isotropic_average=True),\n                        (0, (11 + 12 + 14 + 21 + 22 + 24 + 41 + 42 + 44) / 9.0))\n        with self.assertRaises(ValueError):\n            \n            f('xyz', exact_isotropic_average=True)\n",
        "summary": "The provided Python code contains unit tests for two functions within the `qspectra.simulate.decorators` module using the `unittest` framework and `numpy.testing.assert_allclose`. The first test class, `TestGetCallArgs`, verifies that the `_get_call_args` function correctly extracts keyword arguments from lambda functions. The second test class, `TestIsotropicAverage`, tests two decorators: `optional_2nd_order_isotropic_average` and `optional_4th_order_isotropic_average`, ensuring they handle polarization data and isotropic averages as expected, including error handling for invalid inputs."
    },
    {
        "code": "from traitsui.editors.check_list_editor import *\n",
        "summary": "The provided Python code imports the `CheckListEditor` class from the `traitsui.editors` module, which is used for creating editors that allow users to select multiple options from a list of choices in graphical user interfaces (GUIs) developed with TraitsUI."
    },
    {
        "code": "import os\nimport cv2\nimport argparse\nimport matplotlib\nimport numpy as np\nimport deepdish as dd\nimport scipy.io as scio\n\nprint('Extracting Santini')\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--noDisp', help='Specify flag to display labelled images', type=int)\nparser.add_argument('--path2ds', help='Path to dataset', type=str)\nargs = parser.parse_args()\nif args.noDisp:\n    noDisp = True\n    print('No graphics')\nelse:\n    noDisp = False\n    print('Showing figures')\n\ngui_env = ['Qt5Agg','WXAgg','TKAgg','GTKAgg']\nfor gui in gui_env:\n    try:\n        print(\"testing: {}\".format(gui))\n        matplotlib.use(gui,warn=False, force=True)\n        from matplotlib import pyplot as plt\n        break\n    except:\n        continue\n\nprint(\"Using: {}\".format(matplotlib.get_backend()))\nplt.ion()\n\nargs.path2ds = '/media/rakshit/tank/Dataset'\nPATH_DIR = os.path.join(args.path2ds, 'Santini')\nPATH_DS = os.path.join(args.path2ds, 'All')\nPATH_MASTER = os.path.join(args.path2ds, 'MasterKey')\nlist_ds = ['1', '2', '3', '4', '5', '6']\n\nsc = (640.0/384.0)\nImage_counter = 0.0\nds_num = 24\n\ndef mypause(interval):\n    backend = plt.rcParams['backend']\n    if backend in matplotlib.rcsetup.interactive_bk:\n        figManager = matplotlib._pylab_helpers.Gcf.get_active()\n        if figManager is not None:\n            canvas = figManager.canvas\n            if canvas.figure.stale:\n                canvas.draw()\n            canvas.start_event_loop(interval)\n            return\n\ndef fix_pupil_loc(p, res):\n    \n    p[0] = 0.5*p[0]\n    p[1] = res[0] - 0.5*p[1]\n    return p\n\ndef readFormattedText(path2file, ignoreLines):\n    data = []\n    count = 0\n    f = open(path2file, 'r')\n    for line in f:\n        d = [int(d) for d in line.split() if d.isdigit()]\n        count = count + 1\n        if d and count > ignoreLines:\n            data.append(d)\n    f.close()\n    return data\n\nfor name in list_ds:\n    \n    \n    opts = os.listdir(os.path.join(PATH_DIR, name))\n    for subdir in opts:\n        PATH_DATA = os.path.join(PATH_DIR, name, subdir)\n\n        \n        Path2text = os.path.join(PATH_DATA, 'journal-{:04d}.txt'.format(int(subdir)-1))\n        Path2vid = os.path.join(PATH_DATA, 'eye-{:04d}-0000.avi'.format(int(subdir)-1))\n        PupilData = np.array(readFormattedText(Path2text, 2))\n        VidObj = cv2.VideoCapture(Path2vid)\n\n        keydict = {k:[] for k in ['pupil_loc', 'archive', 'data_type', 'resolution', 'dataset', 'subset']}\n\n        \n        keydict['data_type'] = 0 \n        keydict['resolution'] = []\n        keydict['dataset'] = 'Santini'\n        keydict['subset'] = '{}-{}'.format(name, subdir)\n\n        \n        Data = {k:[] for k in ['Images', 'Info', 'Masks', 'Masks_noSkin', 'Fits', 'pupil_loc']}\n        Data['Fits'] = {k:[] for k in ['pupil', 'pupil_norm', 'pupil_phi', 'iris', 'iris_norm', 'iris_phi']}\n\n        if not noDisp:\n            fig, plts = plt.subplots(1,1)\n        fr_num = 0\n        while(VidObj.isOpened()):\n            ret, I = VidObj.read()\n            if ret == True:\n\n                I = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n                I = cv2.resize(I, (640, 480), cv2.INTER_LANCZOS4)\n\n                Data['Images'].append(I)\n                keydict['resolution'].append(I.shape)\n                keydict['archive'].append(ds_num)\n\n                pupil_loc = fix_pupil_loc(PupilData[fr_num, 10:12]*sc, I.shape)\n\n                keydict['pupil_loc'].append(pupil_loc)\n                Data['pupil_loc'].append(pupil_loc)\n                Data['Info'].append(str(fr_num))\n                fr_num+=1\n                Image_counter+=1\n                if not noDisp:\n                    if fr_num == 1:\n                        cI = plts.imshow(I)\n                        cX = plts.scatter(pupil_loc[0], pupil_loc[1])\n                        plt.show()\n                        plt.pause(.01)\n                    else:\n                        newLoc = np.array([pupil_loc[0], pupil_loc[1]])\n                        cI.set_data(I)\n                        cX.set_offsets(newLoc)\n                        mypause(0.01)\n            else: \n                break\n\n        Data['Images'] = np.stack(Data['Images'], axis=0)\n        Data['pupil_loc'] = np.stack(Data['pupil_loc'], axis=0)\n        keydict['pupil_loc'] = np.stack(keydict['pupil_loc'], axis=0)\n        keydict['resolution'] = np.stack(keydict['resolution'], axis=0)\n        keydict['archive'] = np.stack(keydict['archive'], axis=0)\n\n        \n        dd.io.save(os.path.join(PATH_DS, str(ds_num)+'.h5'), Data)\n        scio.savemat(os.path.join(PATH_MASTER, str(ds_num)), keydict, appendmat=True)\n        ds_num=ds_num+1",
        "summary": "The Python script processes video data from the Santini dataset, extracting eye images and pupil locations. It reads formatted text files for annotations, captures frames from videos, resizes them, and saves the processed data in HDF5 and MATLAB formats. The script also optionally displays the images with overlaid pupil locations during processing."
    },
    {
        "code": "import copy\nimport json\nimport logging\n\nimport pytest\n\nimport burn_lock_functions\nimport test_utilities\nfrom integration_env_credentials import sifchain_cli_credentials_for_test\nfrom pytest_utilities import generate_minimal_test_account\nfrom test_utilities import EthereumToSifchainTransferRequest, SifchaincliCredentials\n\n\ndef create_new_sifaddr():\n    new_account_key = test_utilities.get_shell_output(\"uuidgen\")\n    credentials = sifchain_cli_credentials_for_test(new_account_key)\n    new_addr = burn_lock_functions.create_new_sifaddr(credentials=credentials, keyname=new_account_key)\n    return new_addr[\"address\"]\n\n\ndef create_new_sifaddr_and_key():\n    new_account_key = test_utilities.get_shell_output(\"uuidgen\")\n    credentials = sifchain_cli_credentials_for_test(new_account_key)\n    new_addr = burn_lock_functions.create_new_sifaddr(credentials=credentials, keyname=new_account_key)\n    return new_addr[\"address\"], new_addr[\"name\"]\n\n\n@pytest.mark.skip(reason=\"run manually\")\ndef test_bulk_transfers(\n        basic_transfer_request: EthereumToSifchainTransferRequest,\n        smart_contracts_dir,\n        source_ethereum_address,\n        bridgebank_address,\n        bridgetoken_address,\n        ethereum_network,\n):\n    n_transfers = int(test_utilities.get_optional_env_var(\"NTRANSFERS\", 2))\n    ganache_delay = test_utilities.get_optional_env_var(\"GANACHE_DELAY\", 1)\n    \n    amount = \"{:d}\".format(5 * test_utilities.highest_gas_cost)\n    new_addresses_and_keys = list(map(lambda x: create_new_sifaddr_and_key(), range(n_transfers)))\n    logging.info(f\"aandk: {new_addresses_and_keys}\")\n    new_addresses = list(map(lambda a: a[0], new_addresses_and_keys))\n    logging.debug(f\"new_addresses: {new_addresses}\")\n    new_eth_addrs = test_utilities.create_ethereum_addresses(smart_contracts_dir, basic_transfer_request.ethereum_network, len(new_addresses))\n    logging.info(f\"new eth addrs: {new_eth_addrs}\")\n    request: EthereumToSifchainTransferRequest = copy.deepcopy(basic_transfer_request)\n    requests = list(map(lambda addr: {\n        \"amount\": amount,\n        \"symbol\": test_utilities.NULL_ADDRESS,\n        \"sifchain_address\": addr\n    }, new_addresses))\n    json_requests = json.dumps(requests)\n    test_utilities.run_yarn_command(\n        \" \".join([\n            f\"yarn --cwd {smart_contracts_dir}\",\n            \"integrationtest:sendBulkLockTx\",\n            f\"--amount {amount}\",\n            f\"--symbol eth\",\n            f\"--json_path {request.solidity_json_path}\",\n            f\"--sifchain_address {new_addresses[0]}\",\n            f\"--transactions \\'{json_requests}\\'\",\n            f\"--ethereum_address {source_ethereum_address}\",\n            f\"--bridgebank_address {bridgebank_address}\",\n            f\"--ethereum_network {ethereum_network}\",\n        ])\n    )\n    requests = list(map(lambda addr: {\n        \"amount\": amount,\n        \"symbol\": bridgetoken_address,\n        \"sifchain_address\": addr\n    }, new_addresses))\n    json_requests = json.dumps(requests)\n    yarn_result = test_utilities.run_yarn_command(\n        \" \".join([\n            f\"yarn --cwd {smart_contracts_dir}\",\n            \"integrationtest:sendBulkLockTx\",\n            f\"--amount {amount}\",\n            \"--lock_or_burn burn\",\n            f\"--symbol {bridgetoken_address}\",\n            f\"--json_path {request.solidity_json_path}\",\n            f\"--sifchain_address {new_addresses[0]}\",\n            f\"--transactions \\'{json_requests}\\'\",\n            f\"--ethereum_address {source_ethereum_address}\",\n            f\"--bridgebank_address {bridgebank_address}\",\n            f\"--ethereum_network {ethereum_network}\",\n        ])\n    )\n    logging.info(f\"bulk result: {yarn_result}\")\n    manual_advance = False\n    if manual_advance:\n        test_utilities.advance_n_ethereum_blocks(test_utilities.n_wait_blocks, smart_contracts_dir)\n    test_utilities.wait_for_ethereum_block_number(yarn_result[\"blockNumber\"] + test_utilities.n_wait_blocks, basic_transfer_request);\n    for a in new_addresses:\n        test_utilities.wait_for_sif_account(a, basic_transfer_request.sifnoded_node, 90)\n        test_utilities.wait_for_sifchain_addr_balance(a, \"ceth\", amount, basic_transfer_request.sifnoded_node, 180)\n        test_utilities.wait_for_sifchain_addr_balance(a, \"rowan\", amount, basic_transfer_request.sifnoded_node, 180)\n    text_file = open(\"pfile.cmds\", \"w\")\n    simple_credentials = SifchaincliCredentials(\n        keyring_passphrase=None,\n        keyring_backend=\"test\",\n        from_key=None,\n        sifnoded_homedir=None\n    )\n    logging.info(f\"all accounts are on sifchain and have the correct balance\")\n    for sifaddr, ethaddr in zip(new_addresses_and_keys, new_eth_addrs):\n        r = copy.deepcopy(basic_transfer_request)\n        r.sifchain_address = sifaddr[0]\n        r.ethereum_address = ethaddr[\"address\"]\n        r.amount = 100\n        simple_credentials.from_key = sifaddr[1]\n        c = test_utilities.send_from_sifchain_to_ethereum_cmd(r, simple_credentials)\n        text_file.write(f\"{c}\\n\")\n    text_file.close()\n    \n    test_utilities.get_shell_output(\"bash -x pfile.cmds\")\n    for sifaddr, ethaddr in zip(new_addresses_and_keys, new_eth_addrs):\n        r = copy.deepcopy(basic_transfer_request)\n        r.ethereum_address = ethaddr[\"address\"]\n        r.amount = 100\n        test_utilities.wait_for_eth_balance(r, 100, 300)\n",
        "summary": "The provided Python code defines functions for creating new Sifchain addresses and keys, and includes a test function `test_bulk_transfers` that performs bulk transfers of tokens between Ethereum and Sifchain networks using the Yarn command. The test function generates multiple new addresses, creates corresponding Ethereum addresses, constructs transfer requests, and executes these requests through the Yarn script. It also waits for the transactions to be confirmed on both chains and checks the balances of the created accounts."
    },
    {
        "code": "import logging\nimport os\nimport sys\nimport textwrap\nfrom distutils.sysconfig import get_python_lib\nfrom sysconfig import get_paths\n\nfrom pip._vendor.pkg_resources import Requirement, VersionConflict, WorkingSet\n\nfrom pip import __file__ as pip_location\nfrom pip._internal.utils.misc import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.ui import open_spinner\n\nlogger = logging.getLogger(__name__)\n\n\nclass BuildEnvironment(object):\n    \n\n    def __init__(self):\n        self._temp_dir = TempDirectory(kind=\"build-env\")\n        self._temp_dir.create()\n\n    @property\n    def path(self):\n        return self._temp_dir.path\n\n    def __enter__(self):\n        self.save_path = os.environ.get('PATH', None)\n        self.save_pythonpath = os.environ.get('PYTHONPATH', None)\n        self.save_nousersite = os.environ.get('PYTHONNOUSERSITE', None)\n\n        install_scheme = 'nt' if (os.name == 'nt') else 'posix_prefix'\n        install_dirs = get_paths(install_scheme, vars={\n            'base': self.path,\n            'platbase': self.path,\n        })\n\n        scripts = install_dirs['scripts']\n        if self.save_path:\n            os.environ['PATH'] = scripts + os.pathsep + self.save_path\n        else:\n            os.environ['PATH'] = scripts + os.pathsep + os.defpath\n\n        \n        \n        purelib = get_python_lib(plat_specific=0, prefix=self.path)\n        platlib = get_python_lib(plat_specific=1, prefix=self.path)\n        if purelib == platlib:\n            lib_dirs = purelib\n        else:\n            lib_dirs = purelib + os.pathsep + platlib\n        if self.save_pythonpath:\n            os.environ['PYTHONPATH'] = lib_dirs + os.pathsep + \\\n                self.save_pythonpath\n        else:\n            os.environ['PYTHONPATH'] = lib_dirs\n\n        os.environ['PYTHONNOUSERSITE'] = '1'\n\n        \n        with open(os.path.join(purelib, 'sitecustomize.py'), 'w') as fp:\n            fp.write(textwrap.dedent(\n                \n            ).format(purelib))\n\n        return self.path\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        def restore_var(varname, old_value):\n            if old_value is None:\n                os.environ.pop(varname, None)\n            else:\n                os.environ[varname] = old_value\n\n        restore_var('PATH', self.save_path)\n        restore_var('PYTHONPATH', self.save_pythonpath)\n        restore_var('PYTHONNOUSERSITE', self.save_nousersite)\n\n    def cleanup(self):\n        self._temp_dir.cleanup()\n\n    def missing_requirements(self, reqs):\n        \n        missing = []\n        with self:\n            ws = WorkingSet(os.environ[\"PYTHONPATH\"].split(os.pathsep))\n            for req in reqs:\n                try:\n                    if ws.find(Requirement.parse(req)) is None:\n                        missing.append(req)\n                except VersionConflict:\n                    missing.append(req)\n            return missing\n\n    def install_requirements(self, finder, requirements, message):\n        args = [\n            sys.executable, os.path.dirname(pip_location), 'install',\n            '--ignore-installed', '--no-user', '--prefix', self.path,\n            '--no-warn-script-location',\n        ]\n        if logger.getEffectiveLevel() <= logging.DEBUG:\n            args.append('-v')\n        for format_control in ('no_binary', 'only_binary'):\n            formats = getattr(finder.format_control, format_control)\n            args.extend(('--' + format_control.replace('_', '-'),\n                         ','.join(sorted(formats or {':none:'}))))\n        if finder.index_urls:\n            args.extend(['-i', finder.index_urls[0]])\n            for extra_index in finder.index_urls[1:]:\n                args.extend(['--extra-index-url', extra_index])\n        else:\n            args.append('--no-index')\n        for link in finder.find_links:\n            args.extend(['--find-links', link])\n        for _, host, _ in finder.secure_origins:\n            args.extend(['--trusted-host', host])\n        if finder.allow_all_prereleases:\n            args.append('--pre')\n        if finder.process_dependency_links:\n            args.append('--process-dependency-links')\n        args.append('--')\n        args.extend(requirements)\n        with open_spinner(message) as spinner:\n            call_subprocess(args, show_stdout=False, spinner=spinner)\n\n\nclass NoOpBuildEnvironment(BuildEnvironment):\n    \n\n    def __init__(self):\n        pass\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def cleanup(self):\n        pass\n\n    def install_requirements(self, finder, requirements, message):\n        raise NotImplementedError()\n",
        "summary": "The provided Python code defines a `BuildEnvironment` class that temporarily modifies the environment variables to create an isolated build environment for installing Python packages. It also includes methods to check for missing requirements and install them using pip. The `NoOpBuildEnvironment` class is a placeholder that does nothing, likely used as a fallback or in certain scenarios where isolation is not required."
    },
    {
        "code": "import logging\nfrom random import randint\nimport traceback\nfrom typing import cast, Dict, List, Set, Collection\n\nfrom geniusweb.actions.Accept import Accept\nfrom geniusweb.actions.Action import Action\nfrom geniusweb.actions.LearningDone import LearningDone\nfrom geniusweb.actions.Offer import Offer\nfrom geniusweb.actions.PartyId import PartyId\nfrom geniusweb.actions.Vote import Vote\nfrom geniusweb.actions.Votes import Votes\nfrom geniusweb.bidspace.AllBidsList import AllBidsList\nfrom geniusweb.inform.ActionDone import ActionDone\nfrom geniusweb.inform.Finished import Finished\nfrom geniusweb.inform.Inform import Inform\nfrom geniusweb.inform.OptIn import OptIn\nfrom geniusweb.inform.Settings import Settings\nfrom geniusweb.inform.Voting import Voting\nfrom geniusweb.inform.YourTurn import YourTurn\nfrom geniusweb.issuevalue.Bid import Bid\nfrom geniusweb.issuevalue.Domain import Domain\nfrom geniusweb.issuevalue.Value import Value\nfrom geniusweb.issuevalue.ValueSet import ValueSet\nfrom geniusweb.party.Capabilities import Capabilities\nfrom geniusweb.party.DefaultParty import DefaultParty\nfrom geniusweb.profile.utilityspace.UtilitySpace import UtilitySpace\nfrom geniusweb.profileconnection.ProfileConnectionFactory import (\n    ProfileConnectionFactory,\n)\nfrom geniusweb.progress.ProgressRounds import ProgressRounds\nfrom geniusweb.utils import val\n\n\nclass RandomAgent(DefaultParty):\n    \n\n    def __init__(self):\n        super().__init__()\n        self.getReporter().log(logging.INFO, \"party is initialized\")\n        self._profile = None\n        self._lastReceivedBid: Bid = None\n\n    \n    def notifyChange(self, info: Inform):\n        \n        if isinstance(info, Settings):\n            self._settings: Settings = cast(Settings, info)\n            self._me = self._settings.getID()\n            self._protocol: str = str(self._settings.getProtocol().getURI())\n            self._progress = self._settings.getProgress()\n            if \"Learn\" == self._protocol:\n                self.getConnection().send(LearningDone(self._me))  \n            else:\n                self._profile = ProfileConnectionFactory.create(\n                    info.getProfile().getURI(), self.getReporter()\n                )\n        elif isinstance(info, ActionDone):\n            action: Action = cast(ActionDone, info).getAction()\n            if isinstance(action, Offer):\n                self._lastReceivedBid = cast(Offer, action).getBid()\n        elif isinstance(info, YourTurn):\n            self._myTurn()\n            if isinstance(self._progress, ProgressRounds):\n                self._progress = self._progress.advance()\n        elif isinstance(info, Finished):\n            self.terminate()\n        elif isinstance(info, Voting):\n            \n            self._lastvotes = self._vote(cast(Voting, info))\n            val(self.getConnection()).send(self._lastvotes)\n        elif isinstance(info, OptIn):\n            val(self.getConnection()).send(self._lastvotes)\n        else:\n            self.getReporter().log(\n                logging.WARNING, \"Ignoring unknown info \" + str(info)\n            )\n\n    \n    def getCapabilities(self) -> Capabilities:\n        return Capabilities(\n            set([\"SAOP\", \"Learn\", \"MOPAC\"]),\n            set([\"geniusweb.profile.utilityspace.LinearAdditive\"]),\n        )\n\n    \n    def getDescription(self) -> str:\n        return \"Offers random bids until a bid with sufficient utility is offered. Parameters minPower and maxPower can be used to control voting behaviour.\"\n\n    \n    def terminate(self):\n        self.getReporter().log(logging.INFO, \"party is terminating:\")\n        super().terminate()\n        if self._profile != None:\n            self._profile.close()\n            self._profile = None\n\n    def _myTurn(self):\n        if self._isGood(self._lastReceivedBid):\n            action = Accept(self._me, self._lastReceivedBid)\n        else:\n            for _attempt in range(20):\n                bid = self._getRandomBid(self._profile.getProfile().getDomain())\n                if self._isGood(bid):\n                    break\n            action = Offer(self._me, bid)\n        self.getConnection().send(action)\n\n    def _isGood(self, bid: Bid) -> bool:\n        if bid == None:\n            return False\n        profile = self._profile.getProfile()\n        if isinstance(profile, UtilitySpace):\n            return profile.getUtility(bid) > 0.6\n        raise Exception(\"Can not handle this type of profile\")\n\n    def _getRandomBid(self, domain: Domain) -> Bid:\n        allBids = AllBidsList(domain)\n        return allBids.get(randint(0, allBids.size() - 1))\n\n    def _vote(self, voting: Voting) -> Votes:\n        \n        val = self._settings.getParameters().get(\"minPower\")\n        minpower: int = val if isinstance(val, int) else 2\n        val = self._settings.getParameters().get(\"maxPower\")\n        maxpower: int = val if isinstance(val, int) else 9999999\n\n        votes: Set[Vote] = set(\n            [\n                Vote(self._me, offer.getBid(), minpower, maxpower)\n                for offer in voting.getOffers()\n                if self._isGood(offer.getBid())\n            ]\n        )\n        return Votes(self._me, votes)\n",
        "summary": "The provided Python code defines a `RandomAgent` class that inherits from `DefaultParty`, implementing a simple random bidding strategy in a negotiation protocol. The agent listens for various notifications (`Inform`), processes them to determine actions like making offers or accepting bids, and uses utility functions to evaluate the quality of bids based on a specified profile."
    },
    {
        "code": "METER_TO_KM = 1e-3\nONE_TO_KILO = 1e3\n\nKM_TO_METER = 1e3\nKILO_TO_ONE = 1e3\n\n\nEARTH_RADIUS_KM = 6371.0088\n\n\nAIR_DENSITY_RHO = 1.225\n\n\n\nHOURS_PER_YEAR = 8765.812536\n",
        "summary": "The code defines constants for unit conversions and physical properties, including meters to kilometers, kilometers to meters, Earth's radius in kilometers, air density, and hours per year."
    },
    {
        "code": "class Solution(object):\n    \n    \n    \n    def addBinary(self, a, b):\n        result, carry, val = \"\", 0, 0\n        for i in range(max(len(a), len(b))):\n            val = carry\n            if i < len(a):\n                val += int(a[-(i + 1)])\n            if i < len(b):\n                val += int(b[-(i + 1)])\n            carry, val = divmod(val, 2)\n            result += str(val)\n        if carry:\n            result += str(carry)\n        return result[::-1]\n\n\n\n\nfrom itertools import zip_longest\n\n\nclass Solution2(object):\n    def addBinary(self, a, b):\n        \n        result = \"\"\n        carry = 0\n        for x, y in zip_longest(reversed(a), reversed(b), fillvalue=\"0\"):\n            carry, remainder = divmod(int(x)+int(y)+carry, 2)\n            result += str(remainder)\n        \n        if carry:\n            result += str(carry)\n        \n        return result[::-1]\n",
        "summary": "Both classes `Solution` and `Solution2` implement a method to add two binary strings. They iterate through the strings from least significant bit to most, handling the addition of corresponding bits along with any carry from the previous operation. The results are concatenated in reverse order to form the final binary sum."
    },
    {
        "code": "def main():\n    import sys\n    input=sys.stdin.readline\n    sys.setrecursionlimit(10**6)\n\nif __name__ == '__main__':\n    main()",
        "summary": "The provided Python script defines a `main` function that imports the `sys` module, sets up an alias for reading input from standard input, and increases the recursion limit to 1 million. The script then checks if it is being run as the main program and calls the `main` function if so."
    },
    {
        "code": "from django.core.management.base import BaseCommand\nfrom django.utils.timezone import now\n\n\nclass Command(BaseCommand):\n    args = '[event_slug...]'\n    help = 'Create missing email aliases'\n\n    def handle(*args, **opts):\n        from access.models import InternalEmailAlias\n\n        InternalEmailAlias.ensure_internal_email_aliases()\n\n",
        "summary": "This Django management command named `create_missing_email_aliases` is designed to ensure that all necessary internal email aliases are created in the system. It uses a method from the `InternalEmailAlias` model to handle the creation of these aliases, which can be triggered by providing event slugs as arguments or without any arguments to process all relevant events."
    },
    {
        "code": "from django.contrib.auth.models import User\nfrom rest_framework.test import APITestCase\n\n\nclass FVHAPITestCase(APITestCase):\n    def assert_dict_contains(self, superset, subset, path=''):\n        for key, expected in subset.items():\n            full_path = path + key\n            received = superset.get(key, None)\n            if isinstance(expected, dict) and isinstance(received, dict):\n                self.assert_dict_contains(superset[key], expected, full_path + '.')\n            else:\n                assert received == expected, 'Value mismatch for key {}: {} != {}'.format(\n                    full_path, expected, received\n                )\n\n    def create_user(self):\n        return User.objects.create(\n                username='courier', first_name='Coranne', last_name='Courier', email='coranne@couriersrus.com')\n\n    def create_and_login_user(self):\n        user = self.create_user()\n        self.client.force_login(user)\n        return user\n",
        "summary": "The provided Python code defines a test case class `FVHAPITestCase` that extends `APITestCase` from Django REST framework. It includes methods for asserting dictionary contents, creating users, and logging in users for API testing purposes."
    },
    {
        "code": "\"ts_project rule\"\n\nload(\"@build_bazel_rules_nodejs//:providers.bzl\", \"DeclarationInfo\", \"NpmPackageInfo\", \"declaration_info\", \"js_module_info\", \"run_node\")\n\n_DEFAULT_TSC = (\n    \n    \"@npm\" +\n    \n    \"//typescript/bin:tsc\"\n)\n\n_ATTRS = {\n    \"args\": attr.string_list(),\n    \"declaration_dir\": attr.string(),\n    \"deps\": attr.label_list(providers = [DeclarationInfo]),\n    \"extends\": attr.label_list(allow_files = [\".json\"]),\n    \"out_dir\": attr.string(),\n    \"root_dir\": attr.string(),\n    \n    \n    \n    \n    \"srcs\": attr.label_list(allow_files = True, mandatory = True),\n    \"tsc\": attr.label(default = Label(_DEFAULT_TSC), executable = True, cfg = \"host\"),\n    \"tsconfig\": attr.label(mandatory = True, allow_single_file = [\".json\"]),\n}\n\n\n\n\n_OUTPUTS = {\n    \"buildinfo_out\": attr.output(),\n    \"js_outs\": attr.output_list(),\n    \"map_outs\": attr.output_list(),\n    \"typing_maps_outs\": attr.output_list(),\n    \"typings_outs\": attr.output_list(),\n}\n\n_TsConfigInfo = provider(\n    doc = ,\n    fields = {\n        \"tsconfigs\": \"depset of tsconfig.json files\",\n    },\n)\n\ndef _join(*elements):\n    return \"/\".join([f for f in elements if f])\n\ndef _ts_project_impl(ctx):\n    arguments = ctx.actions.args()\n\n    \n    arguments.add_all(ctx.attr.args)\n\n    arguments.add_all([\n        \"--project\",\n        ctx.file.tsconfig.path,\n        \"--outDir\",\n        _join(ctx.bin_dir.path, ctx.label.package, ctx.attr.out_dir),\n        \"--rootDir\",\n        _join(ctx.label.package, ctx.attr.root_dir) if ctx.label.package else \".\",\n    ])\n    if len(ctx.outputs.typings_outs) > 0:\n        declaration_dir = ctx.attr.declaration_dir if ctx.attr.declaration_dir else ctx.attr.out_dir\n        arguments.add_all([\n            \"--declarationDir\",\n            _join(ctx.bin_dir.path, ctx.label.package, declaration_dir),\n        ])\n\n    \n    \n    \n    if \"VERBOSE_LOGS\" in ctx.var.keys():\n        arguments.add_all([\n            \n            \"--listFiles\",\n            \n            \"--listEmittedFiles\",\n            \n            \"--traceResolution\",\n            \n            \"--diagnostics\",\n            \"--extendedDiagnostics\",\n        ])\n\n    deps_depsets = []\n    for dep in ctx.attr.deps:\n        if _TsConfigInfo in dep:\n            deps_depsets.append(dep[_TsConfigInfo].tsconfigs)\n        if NpmPackageInfo in dep:\n            \n            \n            deps_depsets.append(dep[NpmPackageInfo].sources)\n        if DeclarationInfo in dep:\n            deps_depsets.append(dep[DeclarationInfo].transitive_declarations)\n\n    inputs = ctx.files.srcs + depset(transitive = deps_depsets).to_list() + [ctx.file.tsconfig]\n    if ctx.attr.extends:\n        inputs.extend(ctx.files.extends)\n\n    \n    \n    \n    \n    \n    if len(ctx.outputs.js_outs):\n        json_outs = [\n            ctx.actions.declare_file(_join(ctx.attr.out_dir, src.short_path[len(ctx.label.package) + 1:]))\n            for src in ctx.files.srcs\n            if src.basename.endswith(\".json\")\n        ]\n    else:\n        json_outs = []\n\n    outputs = json_outs + ctx.outputs.js_outs + ctx.outputs.map_outs + ctx.outputs.typings_outs + ctx.outputs.typing_maps_outs\n    if ctx.outputs.buildinfo_out:\n        outputs.append(ctx.outputs.buildinfo_out)\n    runtime_outputs = depset(json_outs + ctx.outputs.js_outs + ctx.outputs.map_outs)\n    typings_outputs = ctx.outputs.typings_outs + ctx.outputs.typing_maps_outs + [s for s in ctx.files.srcs if s.path.endswith(\".d.ts\")]\n\n    if len(outputs) > 0:\n        run_node(\n            ctx,\n            inputs = inputs,\n            arguments = [arguments],\n            outputs = outputs,\n            executable = \"tsc\",\n            progress_message = \"Compiling TypeScript project %s [tsc -p %s]\" % (\n                ctx.label,\n                ctx.file.tsconfig.short_path,\n            ),\n        )\n\n    providers = [\n        \n        \n        \n        \n        \n        DefaultInfo(\n            files = runtime_outputs,\n            runfiles = ctx.runfiles(\n                transitive_files = runtime_outputs,\n                collect_default = True,\n            ),\n        ),\n        js_module_info(\n            sources = runtime_outputs,\n            deps = ctx.attr.deps,\n        ),\n        _TsConfigInfo(tsconfigs = depset([ctx.file.tsconfig] + ctx.files.extends, transitive = [\n            dep[_TsConfigInfo].tsconfigs\n            for dep in ctx.attr.deps\n            if _TsConfigInfo in dep\n        ])),\n    ]\n\n    \n    \n    if len(typings_outputs) or len(ctx.attr.deps):\n        providers.append(declaration_info(depset(typings_outputs), ctx.attr.deps))\n        providers.append(OutputGroupInfo(types = depset(typings_outputs)))\n\n    return providers\n\nts_project = rule(\n    implementation = _ts_project_impl,\n    attrs = dict(_ATTRS, **_OUTPUTS),\n)\n\ndef _validate_options_impl(ctx):\n    \n    \n    marker = ctx.actions.declare_file(\"%s.optionsvalid.d.ts\" % ctx.label.name)\n\n    arguments = ctx.actions.args()\n    arguments.add_all([ctx.file.tsconfig.path, marker.path, ctx.attr.target, struct(\n        declaration = ctx.attr.declaration,\n        declaration_map = ctx.attr.declaration_map,\n        composite = ctx.attr.composite,\n        emit_declaration_only = ctx.attr.emit_declaration_only,\n        source_map = ctx.attr.source_map,\n        incremental = ctx.attr.incremental,\n    ).to_json()])\n\n    run_node(\n        ctx,\n        inputs = [ctx.file.tsconfig] + ctx.files.extends,\n        outputs = [marker],\n        arguments = [arguments],\n        executable = \"validator\",\n    )\n    return [\n        DeclarationInfo(\n            transitive_declarations = depset([marker]),\n        ),\n    ]\n\nvalidate_options = rule(\n    implementation = _validate_options_impl,\n    attrs = {\n        \"composite\": attr.bool(),\n        \"declaration\": attr.bool(),\n        \"declaration_map\": attr.bool(),\n        \"emit_declaration_only\": attr.bool(),\n        \"extends\": attr.label_list(allow_files = [\".json\"]),\n        \"incremental\": attr.bool(),\n        \"source_map\": attr.bool(),\n        \"target\": attr.string(),\n        \"tsconfig\": attr.label(mandatory = True, allow_single_file = [\".json\"]),\n        \"validator\": attr.label(default = Label(\"//packages/typescript/bin:ts_project_options_validator\"), executable = True, cfg = \"host\"),\n    },\n)\n\ndef _out_paths(srcs, outdir, rootdir, ext):\n    rootdir_replace_pattern = rootdir + \"/\" if rootdir else \"\"\n    return [_join(outdir, f[:f.rindex(\".\")].replace(rootdir_replace_pattern, \"\") + ext) for f in srcs if not f.endswith(\".d.ts\") and not f.endswith(\".json\")]\n\ndef ts_project_macro(\n        name = \"tsconfig\",\n        tsconfig = None,\n        srcs = None,\n        args = [],\n        deps = [],\n        extends = None,\n        declaration = False,\n        source_map = False,\n        declaration_map = False,\n        composite = False,\n        incremental = False,\n        emit_declaration_only = False,\n        tsc = None,\n        validate = True,\n        declaration_dir = None,\n        out_dir = None,\n        root_dir = None,\n        **kwargs):\n    \n\n    if srcs == None:\n        srcs = native.glob([\"**/*.ts\", \"**/*.tsx\"])\n\n    if tsconfig == None:\n        tsconfig = name + \".json\"\n\n    extra_deps = []\n\n    if validate:\n        validate_options(\n            name = \"_validate_%s_options\" % name,\n            target = \"//%s:%s\" % (native.package_name(), name),\n            declaration = declaration,\n            source_map = source_map,\n            declaration_map = declaration_map,\n            composite = composite,\n            incremental = incremental,\n            emit_declaration_only = emit_declaration_only,\n            tsconfig = tsconfig,\n            extends = extends,\n        )\n        extra_deps.append(\"_validate_%s_options\" % name)\n\n    typings_out_dir = declaration_dir if declaration_dir else out_dir\n\n    ts_project(\n        name = name,\n        srcs = srcs,\n        args = args,\n        deps = deps + extra_deps,\n        tsconfig = tsconfig,\n        extends = extends,\n        declaration_dir = declaration_dir,\n        out_dir = out_dir,\n        root_dir = root_dir,\n        js_outs = _out_paths(srcs, out_dir, root_dir, \".js\") if not emit_declaration_only else [],\n        map_outs = _out_paths(srcs, out_dir, root_dir, \".js.map\") if source_map and not emit_declaration_only else [],\n        typings_outs = _out_paths(srcs, typings_out_dir, root_dir, \".d.ts\") if declaration or composite else [],\n        typing_maps_outs = _out_paths(srcs, typings_out_dir, root_dir, \".d.ts.map\") if declaration_map else [],\n        buildinfo_out = tsconfig[:-5] + \".tsbuildinfo\" if composite or incremental else None,\n        tsc = tsc,\n        **kwargs\n    )\n",
        "summary": "The provided Python code defines Bazel rules for TypeScript project management, including compiling TypeScript files, validating options, and creating macros to simplify the process of setting up TypeScript projects with various configurations."
    },
    {
        "code": "from __future__ import unicode_literals\n\n\nclass TranslationError(Exception):\n\t\n\t\n\tpass\n",
        "summary": "The provided Python code defines an exception class named `TranslationError` that inherits from the base `Exception` class, serving as a custom exception for translation-related errors within applications. The use of `unicode_literals` ensures that all string literals are treated as Unicode by default in Python 2.x environments."
    },
    {
        "code": "import time\n\n\ndef sum_of_n_numbers(x):\n    start_time = time.time()\n    s = 0\n    for i in range(1, x + 1):\n        s = s + i\n    end_time = time.time()\n    return s, end_time - start_time\n\n\nn = 5\nprint(\"\\nTime to sum of 1 to \", n, \" and required time to calculate is :\", sum_of_n_numbers(n))\n",
        "summary": "The Python code defines a function `sum_of_n_numbers` that calculates the sum of numbers from 1 to `x` and measures the time taken to perform this calculation. It then calls this function with `n = 5`, printing both the sum and the elapsed time."
    },
    {
        "code": "__all__ = ('MTDMotionEventProvider', 'MTDMotionEvent')\n\nimport os\nfrom kivy.input.motionevent import MotionEvent\nfrom kivy.input.shape import ShapeRect\n\n\nclass MTDMotionEvent(MotionEvent):\n\n    def depack(self, args):\n        self.is_touch = True\n        if 'x' in args:\n            self.sx = args['x']\n        else:\n            self.sx = -1\n        if 'y' in args:\n            self.sy = args['y']\n        else:\n            self.sy = -1\n        self.profile = ['pos']\n        if 'size_w' in args and 'size_h' in args:\n            self.shape = ShapeRect()\n            self.shape.width = args['size_w']\n            self.shape.height = args['size_h']\n            self.profile.append('shape')\n        if 'pressure' in args:\n            self.pressure = args['pressure']\n            self.profile.append('pressure')\n        super(MTDMotionEvent, self).depack(args)\n\n    def __str__(self):\n        i, sx, sy, d = (self.id, self.sx, self.sy, self.device)\n        return '<MTDMotionEvent id=%d pos=(%f, %f) device=%s>' % (i, sx, sy, d)\n\n\nif 'KIVY_DOC' in os.environ:\n\n    \n    MTDMotionEventProvider = None\n\nelse:\n    import threading\n    import collections\n    from kivy.lib.mtdev import Device, \\\n        MTDEV_TYPE_EV_ABS, MTDEV_CODE_SLOT, MTDEV_CODE_POSITION_X, \\\n        MTDEV_CODE_POSITION_Y, MTDEV_CODE_PRESSURE, \\\n        MTDEV_CODE_TOUCH_MAJOR, MTDEV_CODE_TOUCH_MINOR, \\\n        MTDEV_CODE_TRACKING_ID, MTDEV_ABS_POSITION_X, \\\n        MTDEV_ABS_POSITION_Y, MTDEV_ABS_TOUCH_MINOR, \\\n        MTDEV_ABS_TOUCH_MAJOR\n    from kivy.input.provider import MotionEventProvider\n    from kivy.input.factory import MotionEventFactory\n    from kivy.logger import Logger\n\n    class MTDMotionEventProvider(MotionEventProvider):\n\n        options = ('min_position_x', 'max_position_x',\n                   'min_position_y', 'max_position_y',\n                   'min_pressure', 'max_pressure',\n                   'min_touch_major', 'max_touch_major',\n                   'min_touch_minor', 'max_touch_minor',\n                   'invert_x', 'invert_y',\n                   'rotation')\n\n        def __init__(self, device, args):\n            super(MTDMotionEventProvider, self).__init__(device, args)\n            self._device = None\n            self.input_fn = None\n            self.default_ranges = dict()\n\n            \n            args = args.split(',')\n            if not args:\n                Logger.error('MTD: No filename pass to MTD configuration')\n                Logger.error('MTD: Use /dev/input/event0 for example')\n                return\n\n            \n            self.input_fn = args[0]\n            Logger.info('MTD: Read event from <%s>' % self.input_fn)\n\n            \n            for arg in args[1:]:\n                if arg == '':\n                    continue\n                arg = arg.split('=')\n\n                \n                if len(arg) != 2:\n                    err = 'MTD: Bad parameter %s: Not in key=value format' %\\\n                        arg\n                    Logger.error(err)\n                    continue\n\n                \n                key, value = arg\n                if key not in MTDMotionEventProvider.options:\n                    Logger.error('MTD: unknown %s option' % key)\n                    continue\n\n                \n                try:\n                    self.default_ranges[key] = int(value)\n                except ValueError:\n                    err = 'MTD: invalid value %s for option %s' % (key, value)\n                    Logger.error(err)\n                    continue\n\n                \n                Logger.info('MTD: Set custom %s to %d' % (key, int(value)))\n\n            if 'rotation' not in self.default_ranges:\n                self.default_ranges['rotation'] = 0\n            elif self.default_ranges['rotation'] not in (0, 90, 180, 270):\n                Logger.error('HIDInput: invalid rotation value ({})'.format(\n                    self.default_ranges['rotation']))\n                self.default_ranges['rotation'] = 0\n\n        def start(self):\n            if self.input_fn is None:\n                return\n            self.uid = 0\n            self.queue = collections.deque()\n            self.thread = threading.Thread(\n                target=self._thread_run,\n                kwargs=dict(\n                    queue=self.queue,\n                    input_fn=self.input_fn,\n                    device=self.device,\n                    default_ranges=self.default_ranges))\n            self.thread.daemon = True\n            self.thread.start()\n\n        def _thread_run(self, **kwargs):\n            input_fn = kwargs.get('input_fn')\n            queue = kwargs.get('queue')\n            device = kwargs.get('device')\n            drs = kwargs.get('default_ranges').get\n            touches = {}\n            touches_sent = []\n            point = {}\n            l_points = {}\n\n            def assign_coord(point, value, invert, coords):\n                cx, cy = coords\n                if invert:\n                    value = 1. - value\n                if rotation == 0:\n                    point[cx] = value\n                elif rotation == 90:\n                    point[cy] = value\n                elif rotation == 180:\n                    point[cx] = 1. - value\n                elif rotation == 270:\n                    point[cy] = 1. - value\n\n            def process(points):\n                for args in points:\n                    \n                    \n                    if 'id' not in args:\n                        continue\n                    tid = args['id']\n                    try:\n                        touch = touches[tid]\n                    except KeyError:\n                        touch = MTDMotionEvent(device, tid, args)\n                        touches[touch.id] = touch\n                    touch.move(args)\n                    action = 'update'\n                    if tid not in touches_sent:\n                        action = 'begin'\n                        touches_sent.append(tid)\n                    if 'delete' in args:\n                        action = 'end'\n                        del args['delete']\n                        del touches[touch.id]\n                        touches_sent.remove(tid)\n                        touch.update_time_end()\n                    queue.append((action, touch))\n\n            def normalize(value, vmin, vmax):\n                return (value - vmin) / float(vmax - vmin)\n\n            \n            _fn = input_fn\n            _slot = 0\n            try:\n                _device = Device(_fn)\n            except OSError as e:\n                if e.errno == 13:  \n                    Logger.warn(\n                        'MTD: Unable to open device \"{0}\". Please ensure you'\n                        ' have the appropriate permissions.'.format(_fn))\n                    return\n                else:\n                    raise\n            _changes = set()\n\n            \n            ab = _device.get_abs(MTDEV_ABS_POSITION_X)\n            range_min_position_x = drs('min_position_x', ab.minimum)\n            range_max_position_x = drs('max_position_x', ab.maximum)\n            Logger.info('MTD: <%s> range position X is %d - %d' %\n                        (_fn, range_min_position_x, range_max_position_x))\n\n            ab = _device.get_abs(MTDEV_ABS_POSITION_Y)\n            range_min_position_y = drs('min_position_y', ab.minimum)\n            range_max_position_y = drs('max_position_y', ab.maximum)\n            Logger.info('MTD: <%s> range position Y is %d - %d' %\n                        (_fn, range_min_position_y, range_max_position_y))\n\n            ab = _device.get_abs(MTDEV_ABS_TOUCH_MAJOR)\n            range_min_major = drs('min_touch_major', ab.minimum)\n            range_max_major = drs('max_touch_major', ab.maximum)\n            Logger.info('MTD: <%s> range touch major is %d - %d' %\n                        (_fn, range_min_major, range_max_major))\n\n            ab = _device.get_abs(MTDEV_ABS_TOUCH_MINOR)\n            range_min_minor = drs('min_touch_minor', ab.minimum)\n            range_max_minor = drs('max_touch_minor', ab.maximum)\n            Logger.info('MTD: <%s> range touch minor is %d - %d' %\n                        (_fn, range_min_minor, range_max_minor))\n\n            range_min_pressure = drs('min_pressure', 0)\n            range_max_pressure = drs('max_pressure', 255)\n            Logger.info('MTD: <%s> range pressure is %d - %d' %\n                        (_fn, range_min_pressure, range_max_pressure))\n\n            invert_x = int(bool(drs('invert_x', 0)))\n            invert_y = int(bool(drs('invert_y', 0)))\n            Logger.info('MTD: <%s> axes invertion: X is %d, Y is %d' %\n                        (_fn, invert_x, invert_y))\n\n            rotation = drs('rotation', 0)\n            Logger.info('MTD: <%s> rotation set to %d' %\n                        (_fn, rotation))\n\n            while _device:\n                \n                while _device.idle(1000):\n                    continue\n\n                \n                while True:\n                    data = _device.get()\n                    if data is None:\n                        break\n\n                    \n                    if data.type == MTDEV_TYPE_EV_ABS and \\\n                       data.code == MTDEV_CODE_SLOT:\n                        _slot = data.value\n                        continue\n\n                    \n                    if _slot not in l_points:\n                        l_points[_slot] = dict()\n                    point = l_points[_slot]\n                    ev_value = data.value\n                    ev_code = data.code\n                    if ev_code == MTDEV_CODE_POSITION_X:\n                        val = normalize(ev_value,\n                                        range_min_position_x,\n                                        range_max_position_x)\n                        assign_coord(point, val, invert_x, 'xy')\n                    elif ev_code == MTDEV_CODE_POSITION_Y:\n                        val = 1. - normalize(ev_value,\n                                             range_min_position_y,\n                                             range_max_position_y)\n                        assign_coord(point, val, invert_y, 'yx')\n                    elif ev_code == MTDEV_CODE_PRESSURE:\n                        point['pressure'] = normalize(ev_value,\n                                                      range_min_pressure,\n                                                      range_max_pressure)\n                    elif ev_code == MTDEV_CODE_TOUCH_MAJOR:\n                        point['size_w'] = normalize(ev_value,\n                                                    range_min_major,\n                                                    range_max_major)\n                    elif ev_code == MTDEV_CODE_TOUCH_MINOR:\n                        point['size_h'] = normalize(ev_value,\n                                                    range_min_minor,\n                                                    range_max_minor)\n                    elif ev_code == MTDEV_CODE_TRACKING_ID:\n                        if ev_value == -1:\n                            point['delete'] = True\n                            \n                            \n                            _changes.add(_slot)\n                            process([l_points[x] for x in _changes])\n                            _changes.clear()\n                            continue\n                        else:\n                            point['id'] = ev_value\n                    else:\n                        \n                        continue\n                    _changes.add(_slot)\n\n                \n                if _changes:\n                    process([l_points[x] for x in _changes])\n                    _changes.clear()\n\n        def update(self, dispatch_fn):\n            \n            try:\n                while True:\n                    event_type, touch = self.queue.popleft()\n                    dispatch_fn(event_type, touch)\n            except:\n                pass\n\n    MotionEventFactory.register('mtdev', MTDMotionEventProvider)\n",
        "summary": "This code defines a custom input provider for the Kivy framework called `MTDMotionEventProvider`. This provider is designed to handle multi-touch events from devices that support the Linux Multi-Touch Device (MTDEV) protocol. Here's a breakdown of its key features and functionality:\n\n1. **Initialization**:\n   - The provider registers itself with the `MotionEventFactory` under the name 'mtdev'.\n   - It initializes logging for debugging purposes.\n\n2. **Configuration Handling**:\n   - The provider accepts configuration parameters such as device file path, axis inversion, and rotation angle.\n   - These parameters are used to customize how touch events are interpreted from the input device.\n\n3. **Device Connection**:\n   - The provider attempts to open the specified MTDEV device file.\n   - If successful, it retrieves information about the absolute axes (position, pressure, size) supported by the device.\n\n4. **Event Handling**:\n   - The provider continuously listens for events from the input device.\n   - When an event is received, it processes the data to determine the type of touch event (e.g., press, move, release).\n   - It normalizes the raw sensor values to a standardized range (0-1) based on the configured minimum and maximum values.\n\n5. **Event Dispatch**:\n   - The provider uses a queue to manage incoming events.\n   - When an event is ready to be dispatched, it calls the provided `dispatch_fn` function with the event type and touch object.\n\n6. **Error Handling**:\n   - The provider includes basic error handling for device opening failures and other potential issues.\n\n7. **Logging**:\n   - Throughout the process, the provider logs important information such as device capabilities and configuration settings.\n\n### Key Classes and Methods\n\n- **MTDMotionEventProvider**: This is the main class that implements the input provider.\n  - `__init__(self, config)`: Initializes the provider with configuration parameters.\n  - `_open_device(self)`: Opens the MTDEV device file.\n  - `_get_axis_info(self)`: Retrieves information about the absolute axes supported by the device.\n  - `_process_event(self, data)`: Processes incoming events from the input device.\n  - `update(self, dispatch_fn)`: Continuously updates and dispatches touch events.\n\n### Usage\n\nTo use this provider in a Kivy application, you would typically configure it in your application's settings or directly in the code where you set up input providers. Here's an example of how you might register and use this provider:\n\n```python\nfrom kivy.input.providers.mtdev import MTDMotionEventProvider\n\n# Register the custom provider with the MotionEventFactory\nMotionEventFactory.register('mtdev', MTDMotionEventProvider)\n\n# Configure the provider (example configuration)\nconfig = {\n    'device': '/dev/input/event0',\n    'invert_x': 1,\n    'invert_y': 0,\n    'rotation': 90\n}\n\n# Create an instance of the provider with the specified configuration\nprovider = MTDMotionEventProvider(config)\n\n# Add the provider to Kivy's input manager\nInputManager.add_provider(provider)\n```\n\nThis setup allows your Kivy application to receive and process multi-touch events from devices that support the MTDEV protocol, using the custom settings you've configured."
    },
    {
        "code": "import unittest\nfrom unittest.mock import MagicMock\n\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\n\nfrom data_export.pipeline.dataset import Dataset\n\n\nclass TestDataset(unittest.TestCase):\n    def setUp(self):\n        example = MagicMock()\n        example.to_dict.return_value = {\"data\": \"example\"}\n        self.examples = MagicMock()\n        self.examples.__iter__.return_value = [example]\n        label = MagicMock()\n        label.find_by.return_value = {\"labels\": [\"label\"]}\n        self.labels = MagicMock()\n        self.labels.__iter__.return_value = [label]\n\n    def test_to_dataframe(self):\n        dataset = Dataset(self.examples, self.labels)\n        df = dataset.to_dataframe()\n        expected = pd.DataFrame([{\"data\": \"example\", \"labels\": [\"label\"]}])\n        assert_frame_equal(df, expected)\n",
        "summary": "The provided Python code defines a unit test class `TestDataset` that uses the `unittest` framework and `MagicMock` from `unittest.mock` to simulate objects for testing. The class tests the `to_dataframe` method of the `Dataset` class, which converts dataset examples and labels into a pandas DataFrame, ensuring the output matches an expected DataFrame structure."
    },
    {
        "code": "VERSION = '1.23.0.dev0'\n",
        "summary": "The provided Python code snippet defines a constant named VERSION with the value '1.23.0.dev0', which likely represents a development version of a software application."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\n\nANSIBLE_METADATA = {\n    'metadata_version': '1.1',\n    'status': ['preview'],\n    'supported_by': 'community'\n}\n\nDOCUMENTATION = r\n\nEXAMPLES = r\n\nRETURN = r\n\ntry:\n    from pyVmomi import vim, vmodl\nexcept ImportError as e:\n    pass\n\nfrom ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.vmware import vmware_argument_spec, PyVmomi, find_datacenter_by_name, wait_for_task, get_all_objs\nfrom ansible.module_utils._text import to_native\n\n\nclass VmwareFolderManager(PyVmomi):\n    def __init__(self, module):\n        super(VmwareFolderManager, self).__init__(module)\n        datacenter_name = self.params.get('datacenter', None)\n        self.datacenter_obj = find_datacenter_by_name(self.content, datacenter_name=datacenter_name)\n        if self.datacenter_obj is None:\n            self.module.fail_json(msg=\"Failed to find datacenter %s\" % datacenter_name)\n\n    def ensure(self):\n        \n        state = self.module.params.get('state')\n        folder_type = self.module.params.get('folder_type')\n        folder_name = self.module.params.get('folder_name')\n        parent_folder = self.module.params.get('parent_folder', None)\n        results = dict(changed=False, result=dict())\n        if state == 'present':\n            \n            try:\n                if parent_folder:\n                    folder = self.get_folder_by_name(folder_name=parent_folder)\n                    if folder:\n                        folder.CreateFolder(folder_name)\n                        results['changed'] = True\n                        results['result'] = \"Folder '%s' of type '%s' created under %s\" \\\n                                            \" successfully.\" % (folder_name, folder_type, parent_folder)\n                    else:\n                        self.module.fail_json(msg=\"Failed to find the parent folder %s\"\n                                                  \" for folder %s\" % (parent_folder, folder_name))\n                else:\n                    datacenter_folder_type = {\n                        'vm': self.datacenter_obj.vmFolder,\n                        'host': self.datacenter_obj.hostFolder,\n                        'datastore': self.datacenter_obj.datastoreFolder,\n                        'network': self.datacenter_obj.networkFolder,\n                    }\n                    datacenter_folder_type[folder_type].CreateFolder(folder_name)\n                    results['changed'] = True\n                    results['result'] = \"Folder '%s' of type '%s' created successfully\" % (folder_name, folder_type)\n            except vim.fault.DuplicateName as duplicate_name:\n                \n                \n                \n                \n                results['changed'] = False\n                results['result'] = \"Failed to create folder as another object has same name\" \\\n                                    \" in the same target folder : %s\" % to_native(duplicate_name.msg)\n            except vim.fault.InvalidName as invalid_name:\n                self.module.fail_json(msg=\"Failed to create folder as folder name is not a valid \"\n                                          \"entity name : %s\" % to_native(invalid_name.msg))\n            except Exception as general_exc:\n                self.module.fail_json(msg=\"Failed to create folder due to generic\"\n                                          \" exception : %s \" % to_native(general_exc))\n            self.module.exit_json(**results)\n        elif state == 'absent':\n            folder_obj = self.get_folder_by_name(folder_name=folder_name)\n            if folder_obj:\n                try:\n                    task = folder_obj.UnregisterAndDestroy()\n                    results['changed'], results['result'] = wait_for_task(task=task)\n                except vim.fault.ConcurrentAccess as concurrent_access:\n                    self.module.fail_json(msg=\"Failed to remove folder as another client\"\n                                              \" modified folder before this operation : %s\" % to_native(concurrent_access.msg))\n                except vim.fault.InvalidState as invalid_state:\n                    self.module.fail_json(msg=\"Failed to remove folder as folder is in\"\n                                              \" invalid state\" % to_native(invalid_state.msg))\n                except Exception as e:\n                    self.module.fail_json(msg=\"Failed to remove folder due to generic\"\n                                              \" exception %s \" % to_native(e))\n            self.module.exit_json(**results)\n\n    def get_folder_by_name(self, folder_name):\n        \n        folder_objs = get_all_objs(self.content, [vim.Folder])\n        for folder in folder_objs:\n            if folder.name == folder_name:\n                return folder\n\n        return None\n\n\ndef main():\n    argument_spec = vmware_argument_spec()\n    argument_spec.update(\n        datacenter=dict(type='str', required=True),\n        folder_name=dict(type='str', required=True),\n        parent_folder=dict(type='str', required=False),\n        state=dict(type='str',\n                   choices=['present', 'absent'],\n                   default='present'),\n        folder_type=dict(type='str',\n                         default='vm',\n                         choices=['datastore', 'host', 'network', 'vm'],\n                         required=False),\n    )\n\n    module = AnsibleModule(\n        argument_spec=argument_spec,\n        supports_check_mode=False,\n    )\n\n    if len(module.params.get('folder_name')) > 79:\n        module.fail_json(msg=\"Failed to manage folder as folder_name can only contain 80 characters.\")\n\n    vcenter_folder_mgr = VmwareFolderManager(module)\n    vcenter_folder_mgr.ensure()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script is an Ansible module designed to manage VMware folders, allowing users to create or delete folders within a specified datacenter. It uses the `pyVmomi` library to interact with VMware's vSphere API and handles various exceptions that may occur during folder operations. The module supports both creating new folders under a parent folder or directly in the datacenter, as well as removing existing folders, ensuring robust error handling and user-friendly output."
    },
    {
        "code": "import logging\n\nfrom django.core.mail import EmailMultiAlternatives, EmailMessage\nfrom django.utils.encoding import smart_text\nfrom django.core.urlresolvers import reverse\nfrom django.conf import settings\n\nfrom disturbance.components.emails.emails import TemplateEmailBase\nfrom ledger.accounts.models import EmailUser\n\nlogger = logging.getLogger(__name__)\n\nSYSTEM_NAME = settings.SYSTEM_NAME_SHORT + ' Automated Message'\nclass ApprovalExpireNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval has expired.'\n    html_template = 'disturbance/emails/approval_expire_notification.html'\n    txt_template = 'disturbance/emails/approval_expire_notification.txt'\n\n\nclass ApprovalCancelNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval has been cancelled.'\n    html_template = 'disturbance/emails/approval_cancel_notification.html'\n    txt_template = 'disturbance/emails/approval_cancel_notification.txt'\n\nclass ApprovalSuspendNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval has been suspended.'\n    html_template = 'disturbance/emails/approval_suspend_notification.html'\n    txt_template = 'disturbance/emails/approval_suspend_notification.txt'\n\nclass ApprovalSurrenderNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval has been surrendered.'\n    html_template = 'disturbance/emails/approval_surrender_notification.html'\n    txt_template = 'disturbance/emails/approval_surrender_notification.txt'\n\nclass ApprovalReinstateNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval has been reinstated.'\n    html_template = 'disturbance/emails/approval_reinstate_notification.html'\n    txt_template = 'disturbance/emails/approval_reinstate_notification.txt'\n\nclass ApprovalRenewalNotificationEmail(TemplateEmailBase):\n    subject = 'Your Approval is due for renewal.'\n    html_template = 'disturbance/emails/approval_renewal_notification.html'\n    txt_template = 'disturbance/emails/approval_renewal_notification.txt'\n\ndef send_approval_expire_email_notification(approval):\n    email = ApprovalExpireNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n        'proposal': proposal\n    } \n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    msg = email.send(proposal.submitter.email,cc=all_ccs, context=context)\n    sender = settings.DEFAULT_FROM_EMAIL\n    try:\n    \tsender_user = EmailUser.objects.get(email__icontains=sender)\n    except:\n        EmailUser.objects.create(email=sender, password='')\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    _log_approval_email(msg, approval, sender=sender_user)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender_user)\n\ndef send_approval_cancel_email_notification(approval, future_cancel=False):\n    email = ApprovalCancelNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n        'future_cancel': future_cancel\n        \n    }\n\n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    sender = settings.DEFAULT_FROM_EMAIL\n    try:\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    except:\n        EmailUser.objects.create(email=sender, password='')\n        sender_user = EmailUser.objects.get(email__icontains=sender)    \n    msg = email.send(proposal.submitter.email, cc=all_ccs, context=context)\n    sender = settings.DEFAULT_FROM_EMAIL    \n    _log_approval_email(msg, approval, sender=sender_user)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender_user)\n\n\ndef send_approval_suspend_email_notification(approval, future_suspend=False):\n    email = ApprovalSuspendNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n        'details': approval.suspension_details['details'],\n        'from_date': approval.suspension_details['from_date'],\n        'to_date': approval.suspension_details['to_date'],\n        'future_suspend': future_suspend       \n    }\n\n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    sender = settings.DEFAULT_FROM_EMAIL\n    try:\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    except:\n        EmailUser.objects.create(email=sender, password='')\n        sender_user = EmailUser.objects.get(email__icontains=sender)   \n    msg = email.send(proposal.submitter.email, cc=all_ccs, context=context)\n    sender = settings.DEFAULT_FROM_EMAIL    \n    _log_approval_email(msg, approval, sender=sender_user)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender_user)\n\n\ndef send_approval_surrender_email_notification(approval, future_surrender=False):\n    email = ApprovalSurrenderNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n        'details': approval.surrender_details['details'],\n        'surrender_date': approval.surrender_details['surrender_date'], \n        'future_surrender': future_surrender           \n    }\n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    sender = settings.DEFAULT_FROM_EMAIL\n    try:\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    except:\n        EmailUser.objects.create(email=sender, password='')\n        sender_user = EmailUser.objects.get(email__icontains=sender)   \n    msg = email.send(proposal.submitter.email, cc=all_ccs, context=context)\n    _log_approval_email(msg, approval, sender=sender_user)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender_user)\n\n\ndef send_approval_renewal_email_notification(approval):\n    email = ApprovalRenewalNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n        'proposal': approval.current_proposal\n                    \n    }\n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    sender = settings.DEFAULT_FROM_EMAIL\n    try:\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    except:\n        EmailUser.objects.create(email=sender, password='')\n        sender_user = EmailUser.objects.get(email__icontains=sender)\n    \n    renewal_document= approval.renewal_document._file\n    if renewal_document is not None:\n        file_name = approval.renewal_document.name\n        attachment = (file_name, renewal_document.file.read(), 'application/pdf')\n        attachment = [attachment]\n    else:\n        attachment = []   \n    msg = email.send(proposal.submitter.email, cc=all_ccs, attachments=attachment, context=context)\n    sender = settings.DEFAULT_FROM_EMAIL    \n    _log_approval_email(msg, approval, sender=sender_user)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender_user)\n\n\n\ndef send_approval_reinstate_email_notification(approval, request):\n    email = ApprovalReinstateNotificationEmail()\n    proposal = approval.current_proposal\n\n    context = {\n        'approval': approval,\n                \n    }    \n    all_ccs = []\n    if proposal.applicant.email:\n        cc_list = proposal.applicant.email\n        if cc_list:\n            all_ccs = [cc_list]\n\n    msg = email.send(proposal.submitter.email,cc=all_ccs, context=context)\n    sender = request.user if request else settings.DEFAULT_FROM_EMAIL    \n    _log_approval_email(msg, approval, sender=sender)\n    _log_org_email(msg, proposal.applicant, proposal.submitter, sender=sender)\n\n\n\ndef _log_approval_email(email_message, approval, sender=None):\n    from disturbance.components.approvals.models import ApprovalLogEntry\n    if isinstance(email_message, (EmailMultiAlternatives, EmailMessage,)):\n        \n        text = email_message.body\n        subject = email_message.subject\n        fromm = smart_text(sender) if sender else smart_text(email_message.from_email)\n        \n        if isinstance(email_message.to, list):\n            to = ','.join(email_message.to)\n        else:\n            to = smart_text(email_message.to)\n        \n        all_ccs = []\n        if email_message.cc:\n            all_ccs += list(email_message.cc)\n        if email_message.bcc:\n            all_ccs += list(email_message.bcc)\n        all_ccs = ','.join(all_ccs)\n\n    else:\n        text = smart_text(email_message)\n        subject = ''\n        to = approval.current_proposal.submitter.email\n        fromm = smart_text(sender) if sender else SYSTEM_NAME\n        all_ccs = ''\n\n    customer = approval.current_proposal.submitter\n\n    staff = sender\n\n    kwargs = {\n        'subject': subject,\n        'text': text,\n        'approval': approval,\n        'customer': customer,\n        'staff': staff,\n        'to': to,\n        'fromm': fromm,\n        'cc': all_ccs\n    }\n\n    email_entry = ApprovalLogEntry.objects.create(**kwargs)\n\n    return email_entry\n\n\n\n\ndef _log_org_email(email_message, organisation, customer ,sender=None):\n    from disturbance.components.organisations.models import OrganisationLogEntry\n    if isinstance(email_message, (EmailMultiAlternatives, EmailMessage,)):\n        \n        text = email_message.body\n        subject = email_message.subject\n        fromm = smart_text(sender) if sender else smart_text(email_message.from_email)\n        \n        if isinstance(email_message.to, list):\n            to = ','.join(email_message.to)\n        else:\n            to = smart_text(email_message.to)\n        \n        all_ccs = []\n        if email_message.cc:\n            all_ccs += list(email_message.cc)\n        if email_message.bcc:\n            all_ccs += list(email_message.bcc)\n        all_ccs = ','.join(all_ccs)\n\n    else:\n        text = smart_text(email_message)\n        subject = ''\n        to = customer\n        fromm = smart_text(sender) if sender else SYSTEM_NAME\n        all_ccs = ''\n\n    customer = customer\n\n    staff = sender\n\n    kwargs = {\n        'subject': subject,\n        'text': text,\n        'organisation': organisation,\n        'customer': customer,\n        'staff': staff,\n        'to': to,\n        'fromm': fromm,\n        'cc': all_ccs\n    }\n\n    email_entry = OrganisationLogEntry.objects.create(**kwargs)\n\n    return email_entry \n",
        "summary": "This code defines a series of functions for sending various types of emails related to approvals and proposals in a system. Each function takes parameters such as the approval object, proposal object, sender information, and any additional context needed for the email content.\n\nThe functions use Django's `EmailMessage` or `EmailMultiAlternatives` classes to construct the email messages. They set the subject, body (text), recipient(s), cc/bcc addresses, and attachments if applicable.\n\nAfter sending the email, each function logs the details of the email using custom log entry models (`ApprovalLogEntry` and `OrganisationLogEntry`). This logging includes information like the subject, text content, recipients, sender, and any attachments.\n\nSome key points about these functions:\n\n1. They handle different types of emails (e.g., expiration, renewal, reinstatement).\n2. They include optional attachments for documents.\n3. They log both approval-related and organization-related email events.\n4. They use Django's built-in email functionality to send the messages.\n\nThis code provides a comprehensive solution for managing email notifications in an approval system, ensuring that all relevant parties are informed about changes and actions taken on proposals and approvals."
    },
    {
        "code": "from iso8601 import parse_date\nfrom datetime import datetime, date, time, timedelta\nimport dateutil.parser\nfrom pytz import timezone\nimport os\nfrom decimal import Decimal\nimport re\n\n\nTZ = timezone(os.environ['TZ'] if 'TZ' in os.environ else 'Europe/Kiev')\n\ndef get_all_etender_dates(initial_tender_data):\n    tender_period = initial_tender_data.tenderPeriod\n    start_dt = dateutil.parser.parse(tender_period['startDate'])\n    end_dt = dateutil.parser.parse(tender_period['endDate'])\n    data = type('periods', (), {  \n        'tenderStart': type('date', (), {'date': start_dt.strftime(\"%d-%m-%Y\"),\n                                         'time': start_dt.strftime(\"%H:%M\")}),\n        'tenderEnd': type('date', (), {'date': end_dt.strftime(\"%d-%m-%Y\"),\n                                       'time': end_dt.strftime(\"%H:%M\")})})\n    if 'enquiryPeriod' in initial_tender_data:\n        end_period = dateutil.parser.parse(initial_tender_data.enquiryPeriod['endDate'])\n        data.enquiryEnd = type('date', (), {'date': end_period.strftime(\"%d-%m-%Y\"),\n                                            'time': end_period.strftime(\"%H:%M\")}) \n    return data\n\ndef get_procedure_type(methodType):\n    return {\n        'aboveThresholdUA': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438',\n        'belowThreshold': '\u0414\u043e\u043f\u043e\u0440\u043e\u0433\u043e\u0432\u0456 \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456',\n        'negotiation': '\u041f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u043d\u0430 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430',\n        'aboveThresholdEU': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0437 \u043f\u0443\u0431\u043b\u0456\u043a\u0430\u0446\u0456\u0454\u044e \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e',\n        'aboveThresholdUA.defense': '\u041f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u043d\u0430 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430 \u0434\u043b\u044f \u043f\u043e\u0442\u0440\u0435\u0431 \u043e\u0431\u043e\u0440\u043e\u043d\u0438',\n        'reporting': '\u0417\u0432\u0456\u0442 \u043f\u0440\u043e \u0443\u043a\u043b\u0430\u0434\u0435\u043d\u0438\u0439 \u0434\u043e\u0433\u043e\u0432\u0456\u0440',\n        'competitiveDialogueEU': '\u041a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 \u0437 \u043f\u0443\u0431\u043b\u0456\u043a\u0430\u0446\u0456\u0454\u044e \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e 1-\u0438\u0439 \u0435\u0442\u0430\u043f',\n        'competitiveDialogueUA': '\u041a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 1-\u0438\u0439 \u0435\u0442\u0430\u043f',\n        'open_esco': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456 \u0435\u043d\u0435\u0440\u0433\u043e\u0441\u0435\u0440\u0432\u0456\u0441\u0443',\n        'esco': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456 \u0435\u043d\u0435\u0440\u0433\u043e\u0441\u0435\u0440\u0432\u0456\u0441\u0443',\n        'closeFrameworkAgreementUA': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0443\u043a\u043b\u0430\u0434\u0430\u043d\u043d\u044f \u0440\u0430\u043c\u043a\u043e\u0432\u043e\u0457 \u0443\u0433\u043e\u0434\u0438',\n        'open_framework': '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438\u0438 \u0434\u043b\u044f \u0443\u043a\u043b\u0430\u0434\u0430\u043d\u043d\u044f \u0440\u0430\u043c\u043a\u043e\u0432\u043e\u0457 \u0443\u0433\u043e\u0434\u0438'\n    }[methodType].decode('utf-8')\n\ndef get_method_type(procedure_name):\n    return {\n        u'\u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u043d\u0430 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430 \u0434\u043b\u044f \u043f\u043e\u0442\u0440\u0435\u0431 \u043e\u0431\u043e\u0440\u043e\u043d\u0438': 'aboveThresholdUA.defense',\n        u'\u0434\u043e\u043f\u043e\u0440\u043e\u0433\u043e\u0432\u0456 \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456': 'belowThreshold',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0437 \u043f\u0443\u0431\u043b\u0456\u043a\u0430\u0446\u0456\u0454\u044e \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e': 'aboveThresholdEU',\n        u'\u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u043d\u0430 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430': 'negotiation',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438': 'aboveThresholdUA',\n        u'\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 1-\u0438\u0439 \u0435\u0442\u0430\u043f': 'competitiveDialogueUA',\n        u'\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 2-\u0438\u0439 \u0435\u0442\u0430\u043f': 'competitiveDialogueUA.stage2',\n        u'\u0437\u0432\u0456\u0442 \u043f\u0440\u043e \u0443\u043a\u043b\u0430\u0434\u0435\u043d\u0438\u0439 \u0434\u043e\u0433\u043e\u0432\u0456\u0440': 'reporting',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456 \u0435\u043d\u0435\u0440\u0433\u043e\u0441\u0435\u0440\u0432\u0456\u0441\u0443': 'open_esco',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u0456 \u0435\u043d\u0435\u0440\u0433\u043e\u0441\u0435\u0440\u0432\u0456\u0441\u0443': 'esco',\n        u'\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 \u0437 \u043f\u0443\u0431\u043b\u0456\u043a\u0430\u0446\u0456\u0454\u044e \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e 1-\u0438\u0439 \u0435\u0442\u0430\u043f': 'competitiveDialogueEU',\n        u'\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0438\u0439 \u0434\u0456\u0430\u043b\u043e\u0433 \u0437 \u043f\u0443\u0431\u043b\u0456\u043a\u0430\u0446\u0456\u0454\u044e \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e 2-\u0438\u0439 \u0435\u0442\u0430\u043f': 'competitiveDialogueEU.stage2',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438 \u0434\u043b\u044f \u0443\u043a\u043b\u0430\u0434\u0430\u043d\u043d\u044f \u0440\u0430\u043c\u043a\u043e\u0432\u043e\u0457 \u0443\u0433\u043e\u0434\u0438': 'closeFrameworkAgreementUA',\n        u'\u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456 \u0442\u043e\u0440\u0433\u0438\u0438 \u0434\u043b\u044f \u0443\u043a\u043b\u0430\u0434\u0430\u043d\u043d\u044f \u0440\u0430\u043c\u043a\u043e\u0432\u043e\u0457 \u0443\u0433\u043e\u0434\u0438': 'open_framework'\n\n\n\n    }[procedure_name]\n\n\ndef parse_etender_date(date, as_string=False):\n    \n    d = datetime.strptime(date, '%d-%m-%Y, %H:%M')\n    if as_string:\n        return str(d)\n    return d\n\n\ndef cut_letters_and_parse_etender_date(date, as_string=True):\n    \n    d = datetime.strptime(date.split(' ')[1], '%d-%m-%Y')\n    if as_string:\n        return str(d)\n    return d\n\n\ndef prepare_locator_to_scroll(locator):\n    if locator[:3] == 'id=':\n        return '//*[@id=\"{}\"]'.format(locator[3:])\n    return locator[6:].replace(\"'\", '\"')  \n\n\ndef to_iso(date):\n    return date.isoformat()\n\n\n\n\ndef convert_etender_date_to_iso_format(date):\n    return TZ.localize(parse_etender_date(date)).isoformat()\n\ndef convet_fra_to_variable(raw):\n    b = re.findall(r'P(\\d+)Y(\\d+)M(\\d+)D.*', raw)\n    c, d, e = b[0]\n    return c, d, e\n\ndef convet_raw_to_chack(raw):\n    raw = raw.replace(' ', '')\n    b = re.findall(r'(\\d+)\u0440(\\d+)\u043c(\\d+)\u0434', raw)\n    c, d, e = b[0]\n    return c, d, e\n\n\ndef get_year_from_full_date(string):\n    data_as_str = string.split('T')[0]\n    data_as_datetime = datetime.strptime(data_as_str, '%Y-%m-%d')\n    return str(data_as_datetime.year)\n\ndef convert_date_to_etender_format(isodate):\n    iso_dt = parse_date(isodate)\n    date_string = iso_dt.strftime(\"%d-%m-%Y\")\n    return date_string\n\n\ndef convert_datetime_for_delivery(isodate):\n    iso_dt = parse_date(isodate)\n    date_string = iso_dt.strftime(\"%Y-%m-%d %H:%M\")\n    return date_string\n\n\ndef convert_time_to_etender_format(isodate):\n    iso_dt = parse_date(isodate)\n    time_string = iso_dt.strftime(\"%H:%M\")\n    return time_string\n\n\ndef float_to_string_2f(value):\n    return '{:.2f}'.format(value)\n\ndef float_to_string_3f(value):\n    return '{:.3f}'.format(value)\n\ndef string_to_float(string):\n    return float(string)\n\n\ndef change_data(initial_data):\n    \n    \n    \n    \n    initial_data['data']['items'][0]['deliveryAddress']['locality'] = u\"\u043c. \u041a\u0438\u0457\u0432\"\n    initial_data['data']['items'][0]['deliveryAddress']['region'] =  u\"\u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430 \u043e\u0431\u043b\u0430\u0441\u0442\u044c\"\n    initial_data['data']['procuringEntity']['address']['locality']       = u\"\u0410\u043b\u0443\u043f\u043a\u0430\"\n    initial_data['data']['procuringEntity']['address']['postalCode']     = u\"13531\"\n    initial_data['data']['procuringEntity']['address']['region']         = u\"\u0410\u0420 \u041a\u0440\u0438\u043c\"\n    initial_data['data']['procuringEntity']['address']['streetAddress']  = u\"\u0424\u0440\u0443\u043d\u0437\u0435, 666\"\n    initial_data['data']['procuringEntity']['contactPoint']['name']      = u\"\u0412\u043b\u0430\u0434\u0435\u043b\u0435\u0446 \u042d\u0442\u043e\u0433\u043e \u0422\u0435\u043d\u0434\u0435\u0440\u0430\"\n    initial_data['data']['procuringEntity']['contactPoint']['telephone'] = u\"613371488228\"\n    initial_data['data']['procuringEntity']['contactPoint']['url']       = u\"http://e-tender.ua/\"\n    return initial_data\n\n\ndef change_data_for_tender_owner(initial_data):\n    initial_data['data']['procuringEntity']['identifier']['legalName'] = u\"TenderOwner\n    initial_data['data']['procuringEntity']['identifier']['id'] =        u\"88008800\"\n    initial_data['data']['procuringEntity']['name'] = u\"TenderOwner\n    return initial_data\n\ndef change_buyers_data(initial_data):\n    initial_data['data']['buyers'][0]['name'] = u\"TenderOwner\n    initial_data['data']['buyers'][0]['identifier']['id'] = u\"88008800\"\n    initial_data['data']['buyers'][0]['identifier']['legalName'] = u\"TenderOwner\n\n    initial_data['data']['procuringEntity']['name'] = initial_data['data']['buyers'][0]['name']\n    initial_data['data']['procuringEntity']['identifier']['id'] = initial_data['data']['buyers'][0]['identifier']['id']\n    initial_data['data']['procuringEntity']['identifier']['legalName'] = \\\n        initial_data['data']['buyers'][0]['identifier']['legalName']\n\n    return initial_data\n\n\ndef convert_etender_date_to_iso_format_and_add_timezone(date):\n    return TZ.localize(parse_etender_date(date)).isoformat()\n\n\ndef get_time_now():\n    time_string = datetime.now().strftime(\"%H:%M\")\n    return time_string\n\n\ndef get_date_now():\n    date_string = datetime.now().strftime(\"%d-%m-%Y\")\n    return date_string\n\n\ndef get_date_10d_future():\n    date_string = (datetime.now() + timedelta(days=10)).strftime(\"%d-%m-%Y\")\n    return date_string\n\n\ndef get_time_offset(add_minutes=17):\n    _now = datetime.now() + timedelta(minutes=add_minutes)\n    return _now.time().strftime('%H:%M')\n\n\ndef convert_common_string_to_etender_string(string):\n    dict = get_helper_dictionary()\n    for key, val in dict.iteritems():\n        if val == string:\n            return key\n    return string\n\n\ndef parse_currency_value_with_spaces(raw):\n    \n    return ''.join(raw.split(' ')[:-1]).replace(',', '.')\n\ndef get_minimalStep_currency(raw_value):\n    \n    result_dic = raw_value.split(' ')\n    result = result_dic[-1]\n    return result\n\ndef parse_currency_value_with_spaces_percentage(raw):\n    \n    result = raw.replace('%', '')\n    result = Decimal(result)\n    result = (result / 100)\n    result = float(result)\n    return result\n\n\ndef parse_currency_value_with_spaces_percentage_NBU(raw):\n    \n    result = raw.split(' ', 4)[4]\n    result = result.replace('%', '')\n    result = Decimal(result)\n    result = (result / 100)\n    result = float(result)\n    return result\n\n\n\ndef convert_etender_string_to_common_string(string):\n    return get_helper_dictionary().get(string, string)\n\n\ndef get_helper_dictionary():\n    return {\n        u\"\u041a\u041b\u0410\u0421\u0418\u0424\u0406\u041a\u0410\u0422\u041e\u0420 \u0414\u041a 021:2015 (CPV)\": u\"\u0414\u041a021\",\n        u\"\u043a\u0433.\": u\"\u043a\u0456\u043b\u043e\u0433\u0440\u0430\u043c\",\n        u\"\u0433\u0440\u043d.\": u\"UAH\",\n        u\"(\u0437 \u041f\u0414\u0412)\": True,\n        u\"\u0437 \u041f\u0414\u0412\": True,\n        u\"\u0431\u0435\u0437 \u041f\u0414\u0412\":       False,\n        \n        u\"\u0414\u043d\u0456\u043f\u0440\u043e\": u\"\u0414\u043d\u0456\u043f\u0440\u043e\u043f\u0435\u0442\u0440\u043e\u0432\u0441\u044c\u043a\",\n        \n        u'\u043f\u0435\u0440\u0456\u043e\u0434 \u0443\u0442\u043e\u0447\u043d\u0435\u043d\u044c': u'active.enquiries',\n        u'\u043e\u0447\u0456\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0439': u'active.tendering',\n        u'\u043f\u0440\u0435\u043a\u0432\u0430\u043b\u0456\u0444\u0456\u043a\u0430\u0446\u0456\u044f': u'active.pre-qualification',\n        u'\u043e\u0446\u0456\u043d\u043a\u0430 \u043f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0439': u'active.pre-qualification',\n        u'\u0431\u043b\u043e\u043a\u0443\u0432\u0430\u043d\u043d\u044f \u043f\u0435\u0440\u0435\u0434 \u0430\u0443\u043a\u0446\u0456\u043e\u043d\u043e\u043c': u'active.pre-qualification.stand-still',\n        u'\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u044f \u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u0456\u0432': u'active.pre-qualification.stand-still',\n        u'\u043f\u0435\u0440\u0448\u0438\u0439 \u043f\u0440\u043e\u043c\u0456\u0436\u043d\u0438\u0439 \u0435\u0442\u0430\u043f': u'active.stage2.pending',\n        u'\u043f\u0435\u0440\u0456\u043e\u0434 \u0430\u0443\u043a\u0446\u0456\u043e\u043d\u0443': u'active.auction',\n        u'\u043a\u0432\u0430\u043b\u0456\u0444\u0456\u043a\u0430\u0446\u0456\u044f \u043f\u0435\u0440\u0435\u043c\u043e\u0436\u0446\u044f': u'active.qualification',\n        u'\u043f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0457 \u0440\u043e\u0437\u0433\u043b\u044f\u043d\u0443\u0442\u043e': u'active.awarded',\n        u'\u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0430 \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u044f': u'complete',\n        u'\u043f\u0435\u0440\u0448\u0438\u0439 \u0435\u0442\u0430\u043f \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u043e': u'complete',\n        u'\u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u044f \u043d\u0435 \u0432\u0456\u0434\u0431\u0443\u043b\u0430\u0441\u044c': u'unsuccessful',\n        u'\u0432\u0456\u0434\u043c\u0456\u043d\u0435\u043d\u0430 \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u044f': u'cancelled',\n        \n        u'\u041f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u044f \u043d\u0435 \u0434\u0456\u0439\u0441\u043d\u0430': u'invalid',\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 1\": u\"artContestIP\",\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 2\": u\"noCompetition\",\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 4\": u\"twiceUnsuccessful\",\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 5\": u\"additionalPurchase\",\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 6\": u\"additionalConstruction\",\n        u\"\u0441\u0442.35 \u0447. 2 \u043f. 7\": u\"stateLegalServices\",\n        u\"\u0414\u043e\u0433\u043e\u0432\u0456\u0440 \u043f\u043e\u043a\u0438 \u0449\u043e \u043d\u0435 \u043e\u043f\u0443\u0431\u043b\u0456\u043a\u043e\u0432\u0430\u043d\u043e\": u\"pending\",\n        u\"\u0414\u043e\u0433\u043e\u0432\u0456\u0440 \u043e\u043f\u0443\u0431\u043b\u0456\u043a\u043e\u0432\u0430\u043d\u043e\": u\"active\",\n        u\"\u041f\u0435\u0440\u0435\u043c\u043e\u0436\u0435\u0446\u044c \u0442\u043e\u0440\u0433\u0456\u0432\": u\"active\",\n        u\"\u0443\u0447\u0430\u0441\u043d\u0438\u043a \u0432\u0438\u0433\u0440\u0430\u0432 \u0437\u0430\u043a\u0443\u043f\u0456\u0432\u043b\u044e\": u\"active\",\n        u'\u0432\u0438\u043c\u043e\u0433\u0430': u'claim',\n        u'\u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u044c \u043d\u0430\u0434\u0430\u043d\u0430': u'answered',\n        u'\u0437\u0430\u0434\u043e\u0432\u043e\u043b\u0435\u043d\u043e': u'resolved',\n        u'\u043d\u0435 \u0437\u0430\u0434\u043e\u0432\u043e\u043b\u0435\u043d\u043e': u'declined',\n        u'\u0441\u043a\u0430\u0441\u043e\u0432\u0430\u043d\u0430 \u0441\u043a\u0430\u0440\u0436\u043d\u0438\u043a\u043e\u043c': u'cancelled',\n        u'\u0432\u0456\u0434\u0445\u0438\u043b\u0435\u043d\u043e': u'invalid',\n        u'\u0437\u0430\u043b\u0438\u0448\u0435\u043d\u0430 \u0431\u0435\u0437 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0456': u'ignored',\n        u'\u043e\u0447\u0456\u043a\u0443\u0454\u0442\u044c\u0441\u044f \u043a\u0432\u0430\u043b\u0456\u0444\u0456\u043a\u0430\u0446\u0456\u044f': u'pending',\n        u'\u0432\u0456\u0434\u043a\u043b\u0438\u043a\u0430\u0454\u0442\u044c\u0441\u044f \u0441\u043a\u0430\u0440\u0436\u043d\u0438\u043a\u043e\u043c': u'stopping',\n        u'\u043e\u0447\u0456\u043a\u0443\u0454 \u0440\u043e\u0437\u0433\u043b\u044f\u0434\u0443 \u043e\u0440\u0433\u0430\u043d\u043e\u043c \u043e\u0441\u043a\u0430\u0440\u0436\u0435\u043d\u043d\u044f': u'pending',\n        u'\u0421\u043f\u0456\u0432\u0444\u0456\u043d\u0430\u043d\u0441\u0443\u0432\u0430\u043d\u043d\u044f \u0437 \u0431\u044e\u0434\u0436\u0435\u0442\u043d\u0438\u0445 \u043a\u043e\u0448\u0442\u0456\u0432': u'budget',\n        u'\u043d\u0430 \u0440\u043e\u0437\u0433\u043b\u044f\u0434\u0456': u'pending',\n        u'\u041f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u044f \u043d\u0435 \u0430\u043a\u0442\u0438\u0432\u043e\u0432\u0430\u043d\u0430': u'invalid'\n    }\n\ndef get_feature_index(i):\n    return {0.05: '1',\n            0.01: '2',\n            0: '3'}[i]\n\ndef get_doc_type_index(i):\n    return {'financial_documents': '1',\n            'qualification_documents': '2',\n            'eligibility_documents': '3'}.get(i, i)\n\ndef convert_unit_name_to_unit_code(string):\n    return {\n        u\"\u0431\u043b\u043e\u043a\": u\"D64\",\n        u\"\u0433\u0435\u043a\u0442\u0430\u0440\": u\"HAR\",\n        u\"\u043a\u0456\u043b\u043e\u0433\u0440\u0430\u043c\u0438\": u\"KGM\",\n        u\"\u043a\u0456\u043b\u043e\u043c\u0435\u0442\u0440\u0438\": u\"KMT\",\n        u\"\u043b\u0456\u0442\u0440\": u\"LTR\",\n        u\"\u043b\u043e\u0442\": u\"LO\",\n        u\"\u043c\u0435\u0442\u0440\u0438 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u0456\": u\"MTK\",\n        u\"\u043c\u0435\u0442\u0440\u0438 \u043a\u0443\u0431\u0456\u0447\u043d\u0456\": u\"MTQ\",\n        u\"\u043c\u0435\u0442\u0440\u0438\": u\"MTR\",\n        u\"\u043c\u0456\u0441\u044f\u0446\u044c\": u\"MON\",\n        u\"\u043d\u0430\u0431\u0456\u0440\": u\"SET\",\n        u\"\u043f\u0430\u0440\u0430\": u\"PR\",\n        u\"\u043f\u0430\u0447\u043a\u0430\": u\"RM\",\n        u\"\u043f\u0430\u0447\u043e\u043a\": u\"NMP\",\n        u\"\u043f\u043e\u0441\u043b\u0443\u0433\u0430\": u\"E48\",\n        u\"\u0440\u0435\u0439\u0441\": u\"E54\",\n        u\"\u0442\u043e\u043d\u0438\": u\"TNE\",\n        u\"\u0443\u043f\u0430\u043a\u043e\u0432\u043a\u0430\": u\"PK\",\n        u\"\u0424\u043b\u0430\u043a\u043e\u043d\": u\"VI\",\n        u\"\u0448\u0442\u0443\u043a\u0438\": u\"H87\",\n        u\"\u044f\u0449\u0438\u043a\": u\"BX\",\n    }.get(string, string)\n\n\ndef convert_milestone_from_text_to_code(string):\n    return {\n        u\"\u0410\u0432\u0430\u043d\u0441\": u\"prepayment\",\n        u\"\u041fi\u0441\u043b\u044f\u043e\u043f\u043b\u0430\u0442\u0430\": u\"postpayment\"\n    }.get(string, string)\n\n\ndef convert_milestone_from_text_to_title(string):\n    return {\n        u\"\u0412\u0438\u043a\u043e\u043d\u0430\u043d\u043d\u044f \u0440\u043e\u0431\u0456\u0442\": \"executionOfWorks\",\n        u\"\u041f\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0443\": \"deliveryOfGoods\",\n        u\"\u041d\u0430\u0434\u0430\u043d\u043d\u044f \u043f\u043e\u0441\u043b\u0443\u0433\": \"submittingServices\",\n        u\"\u041f\u0456\u0434\u043f\u0438\u0441\u0430\u043d\u043d\u044f \u0434\u043e\u0433\u043e\u0432\u043e\u0440\u0443\": \"signingTheContract\",\n        u\"\u0414\u0430\u0442\u0430 \u043f\u043e\u0434\u0430\u043d\u043d\u044f \u0437\u0430\u044f\u0432\u043a\u0438\": \"submissionDateOfApplications\",\n        u\"\u0414\u0430\u0442\u0430 \u0432\u0438\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044f \u0440\u0430\u0445\u0443\u043d\u043a\u0443\": \"dateOfInvoicing\",\n        u\"\u0414\u0430\u0442\u0430 \u0437\u0430\u043a\u0456\u043d\u0447\u0435\u043d\u043d\u044f \u0437\u0432\u0456\u0442\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0456\u043e\u0434\u0443\": \"endDateOfTheReportingPeriod\",\n        u\"\u0406\u043d\u0448\u0430 \u043f\u043e\u0434\u0456\u044f\": \"anotherEvent\",\n    }.get(string, string)\n\n\ndef convert_milestone_from_text_to_day_type(string):\n    return {\n        u\"\u0420\u043e\u0431\u043e\u0447\u0456\": \"working\",\n        u\"\u0411\u0430\u043d\u043a\u0456\u0432\u0441\u044c\u043a\u0456\": \"banking\",\n        u\"\u041a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u0456\": \"calendar\"\n    }.get(string, string)\n\n\ndef convert_main_procurement_category(string):\n    return {\n        u\"\u0422\u043e\u0432\u0430\u0440\u0438\": \"goods\",\n        u\"\u041f\u043e\u0441\u043b\u0443\u0433\u0438\": \"services\",\n        u\"\u0420\u043e\u0431\u043e\u0442\u0438\": \"works\"\n    }.get(string, string)\n\n\ndef get_modulus_from_number(number):\n    if isinstance(number, int):\n        pass\n    elif isinstance(number, str):\n        number = int(number)\n    elif isinstance(number, unicode):\n        number = int(number)\n    return abs(number)\n",
        "summary": "This Python code defines a variety of functions for converting text and data into standardized formats. Here's a breakdown of some key functionalities:\n\n1. **Text Conversion Functions**:\n   - `convert_unit_name_to_unit_code`: Converts unit names like \"kilograms\" to their corresponding codes.\n   - `convert_milestone_from_text_to_code`: Converts milestone descriptions like \"payment\" to specific codes.\n   - `convert_milestone_from_text_to_title`: Converts milestone descriptions to more formal titles used in documentation.\n   - `convert_milestone_from_text_to_day_type`: Determines the type of day (working, banking, calendar) based on a text description.\n\n2. **Data Conversion Functions**:\n   - `get_feature_index` and `get_doc_type_index`: Convert numerical values or strings to specific codes or indices used in procurement processes.\n   - `convert_main_procurement_category`: Maps textual descriptions of categories like \"goods\" to their corresponding codes.\n\n3. **Utility Functions**:\n   - `get_modulus_from_number`: Returns the absolute value of a number, ensuring it's always positive.\n\n4. **Helper Functions for JSON Data Manipulation**:\n   - The code includes functions that might be used to manipulate JSON data, such as converting text to specific formats or extracting information from nested structures.\n\n5. **Feature and Document Type Indexing**:\n   - `get_feature_index` and `get_doc_type_index` provide a way to map numerical values or strings to standardized codes for features and document types in procurement processes.\n\n6. **Milestone Conversion**:\n   - Functions like `convert_milestone_from_text_to_code`, `convert_milestone_from_text_to_title`, and `convert_milestone_from_text_to_day_type` help in converting textual descriptions of milestones into more structured formats used in project management and procurement.\n\nOverall, this code provides a set of tools for standardizing text and data in various contexts, particularly related to procurement processes. It ensures consistency across different systems and documents by providing standardized representations of concepts like units, milestones, and categories."
    },
    {
        "code": "from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///posts.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n\ndb = SQLAlchemy(app)\n",
        "summary": "The provided Python code sets up a basic Flask web application with an integrated SQL database using SQLAlchemy, configuring it to use SQLite as the database backend and disabling modifications tracking for better performance."
    },
    {
        "code": "from __future__ import absolute_import\n\nimport unittest\n\nimport sib_api_v3_sdk\nfrom sib_api_v3_sdk.models.get_account_plan import GetAccountPlan  \nfrom sib_api_v3_sdk.rest import ApiException\n\n\nclass TestGetAccountPlan(unittest.TestCase):\n    \n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def testGetAccountPlan(self):\n        \n        \n        \n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "summary": "This Python script defines a unit test class `TestGetAccountPlan` that inherits from `unittest.TestCase`. The class includes methods for setting up and tearing down the test environment, as well as a placeholder method `testGetAccountPlan` intended to test functionality related to retrieving account plans using the SendinBlue API."
    },
    {
        "code": "import math\nimport numpy as np\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, se=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.se = se\n        \n        if(self.se):\n            self.gap = nn.AdaptiveAvgPool2d(1)\n            self.conv3 = conv1x1(planes, planes//16)\n            self.conv4 = conv1x1(planes//16, planes)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        if self.downsample is not None:\n            residual = self.downsample(x)\n            \n        if(self.se):\n            w = self.gap(out)\n            w = self.conv3(w)\n            w = self.relu(w)\n            w = self.conv4(w).sigmoid()\n            \n            out = out * w\n        \n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, se=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.se = se\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        \n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        \n        self.bn = nn.BatchNorm1d(512)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, se=self.se))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, se=self.se))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.bn(x)\n        return x        \n\n\nclass VideoCNN(nn.Module):\n    def __init__(self, se=False):\n        super(VideoCNN, self).__init__()\n        \n        \n        self.frontend3D = nn.Sequential(\n                nn.Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False),\n                nn.BatchNorm3d(64),\n                nn.ReLU(True),\n                nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n                )\n        \n        self.resnet18 = ResNet(BasicBlock, [2, 2, 2, 2], se=se)\n        self.dropout = nn.Dropout(p=0.5)\n\n        \n        \n        self._initialize_weights()\n    \n    def visual_frontend_forward(self, x):\n        x = x.transpose(1, 2)\n        x = self.frontend3D(x)\n        x = x.transpose(1, 2)\n        x = x.contiguous()\n        x = x.view(-1, 64, x.size(3), x.size(4))\n        x = self.resnet18(x)\n        return x        \n    \n    def forward(self, x):\n        b, t = x.size()[:2]\n\n        x = self.visual_frontend_forward(x)\n        \n        \n        feat = x.view(b, -1, 512)\n\n        x = x.view(b, -1, 512)       \n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n            elif isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n            elif isinstance(m, nn.Conv1d):\n                n = m.kernel_size[0] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n",
        "summary": "The provided Python code defines a deep learning model using PyTorch for video classification. It includes a 3D convolutional frontend and a ResNet-based backend, with optional squeeze-and-excitation (SE) blocks to improve feature representation. The model processes input videos through the frontend to extract features, which are then passed to the ResNet layers for classification."
    },
    {
        "code": "__version__ = '3.2.0'\n__author__ = 'Joshua Roesslein'\n__license__ = 'MIT'\n\nfrom tweepy.models import Status, User, DirectMessage, Friendship, SavedSearch, SearchResults, ModelFactory, Category\nfrom tweepy.error import TweepError\nfrom tweepy.api import API\nfrom tweepy.cache import Cache, MemoryCache, FileCache\nfrom tweepy.auth import OAuthHandler, AppAuthHandler\nfrom tweepy.streaming import Stream, StreamListener\nfrom tweepy.cursor import Cursor\n\n\napi = API()\n\ndef debug(enable=True, level=1):\n    from six.moves.http_client import HTTPConnection\n    HTTPConnection.debuglevel = level\n",
        "summary": "This Python code defines metadata for a library related to the Twitter API, including version, author, and license. It also imports various classes and modules necessary for interacting with Twitter data and handling authentication and streaming. Additionally, it includes a function to enable debugging of HTTP connections used by the library."
    },
    {
        "code": "import json\nimport sys\n\nfrom great.models import music\nfrom great.web import engine_from_config\nfrom pyperclip import copy\nfrom sqlalchemy import sql\nfrom titlecase import titlecase\n\n\ne = engine_from_config()\n\n\ndef canonicalize(artist):\n    if artist.isupper():\n        return artist\n    return titlecase(artist)\n\n\ndef spotify_uri(artist):\n    return e.execute(\n        sql.select(\n            [\n                music.artists.c.id,\n                music.artists.c.name,\n                music.artists.c.spotify_uri,\n            ],\n        ).where(music.artists.c.name.like(artist)),\n    ).fetchone()\n\n\nwith open(\"/dev/tty\") as tty:\n    for line in sys.stdin:\n        as_dict = json.loads(line)\n        artist, uri = canonicalize(as_dict[\"name\"]), as_dict[\"uri\"]\n        result = spotify_uri(artist)\n        if result is None:\n            print \"Didn't find:\", artist\n        elif result.spotify_uri is None:\n            e.execute(\n                sql.update(music.artists).where(\n                    music.artists.c.id == result.id,\n                ).values(spotify_uri=as_dict[\"uri\"]),\n            )\n        elif result.spotify_uri != uri:\n            sys.exit(\n                \"Wat! {!r} has current ID {!r}, not {!r}\".format(\n                    artist, result.spotify_uri, uri,\n                ),\n            )\n",
        "summary": "The Python script reads JSON input from standard input, canonicalizes the artist name by converting it to title case if it's not already in uppercase, and then checks for an existing Spotify URI in a database. If no URI is found or if the existing URI does not match the new one provided, the script updates the database with the new URI or exits with an error message."
    },
    {
        "code": "import unittest\nfrom code.google_search import get_people_also_ask_links\n\nclass TestGoogleSearch(unittest.TestCase):\n    def setUp(self) -> None:\n        pass\n\n    def test_get_people_also_ask_links(self):\n        \n        test = \"principal components\"\n        result = get_people_also_ask_links(test)\n        self.assertEqual(list, type(result))\n",
        "summary": "The provided Python code defines a unit test class `TestGoogleSearch` that inherits from `unittest.TestCase`. It includes a single test method `test_get_people_also_ask_links`, which calls the function `get_people_also_ask_links` with the input \"principal components\" and asserts that the returned result is of type list."
    },
    {
        "code": "import datetime\nimport json\nimport uuid\nfrom xml.dom import minidom\n\nimport webob\n\nfrom cinder.api import common\nfrom cinder.api.openstack.wsgi import MetadataXMLDeserializer\nfrom cinder.api.openstack.wsgi import XMLDeserializer\nfrom cinder import db\nfrom cinder import test\nfrom cinder.tests.api import fakes\nfrom cinder import volume\n\n\ndef fake_volume_get(*args, **kwargs):\n    return {\n        'id': 'fake',\n        'host': 'host001',\n        'status': 'available',\n        'size': 5,\n        'availability_zone': 'somewhere',\n        'created_at': datetime.datetime.now(),\n        'attach_status': None,\n        'display_name': 'anothervolume',\n        'display_description': 'Just another volume!',\n        'volume_type_id': None,\n        'snapshot_id': None,\n        'project_id': 'fake',\n    }\n\n\ndef fake_volume_get_all(*args, **kwargs):\n    return [fake_volume_get()]\n\n\nfake_image_metadata = {\n    'image_id': 'someid',\n    'image_name': 'fake',\n    'kernel_id': 'somekernel',\n    'ramdisk_id': 'someramdisk',\n}\n\n\ndef fake_get_volume_image_metadata(*args, **kwargs):\n    return fake_image_metadata\n\n\ndef fake_get_volumes_image_metadata(*args, **kwargs):\n    return {'fake': fake_image_metadata}\n\n\nclass VolumeImageMetadataTest(test.TestCase):\n    content_type = 'application/json'\n\n    def setUp(self):\n        super(VolumeImageMetadataTest, self).setUp()\n        self.stubs.Set(volume.API, 'get', fake_volume_get)\n        self.stubs.Set(volume.API, 'get_all', fake_volume_get_all)\n        self.stubs.Set(volume.API, 'get_volume_image_metadata',\n                       fake_get_volume_image_metadata)\n        self.stubs.Set(volume.API, 'get_volumes_image_metadata',\n                       fake_get_volumes_image_metadata)\n        self.stubs.Set(db, 'volume_get', fake_volume_get)\n        self.UUID = uuid.uuid4()\n\n    def _make_request(self, url):\n        req = webob.Request.blank(url)\n        req.accept = self.content_type\n        res = req.get_response(fakes.wsgi_app())\n        return res\n\n    def _get_image_metadata(self, body):\n        return json.loads(body)['volume']['volume_image_metadata']\n\n    def _get_image_metadata_list(self, body):\n        return [\n            volume['volume_image_metadata']\n            for volume in json.loads(body)['volumes']\n        ]\n\n    def test_get_volume(self):\n        res = self._make_request('/v2/fake/volumes/%s' % self.UUID)\n        self.assertEqual(res.status_int, 200)\n        self.assertEqual(self._get_image_metadata(res.body),\n                         fake_image_metadata)\n\n    def test_list_detail_volumes(self):\n        res = self._make_request('/v2/fake/volumes/detail')\n        self.assertEqual(res.status_int, 200)\n        self.assertEqual(self._get_image_metadata_list(res.body)[0],\n                         fake_image_metadata)\n\n\nclass ImageMetadataXMLDeserializer(common.MetadataXMLDeserializer):\n    metadata_node_name = \"volume_image_metadata\"\n\n\nclass VolumeImageMetadataXMLTest(VolumeImageMetadataTest):\n    content_type = 'application/xml'\n\n    def _get_image_metadata(self, body):\n        deserializer = XMLDeserializer()\n        volume = deserializer.find_first_child_named(\n            minidom.parseString(body), 'volume')\n        image_metadata = deserializer.find_first_child_named(\n            volume, 'volume_image_metadata')\n        return MetadataXMLDeserializer().extract_metadata(image_metadata)\n\n    def _get_image_metadata_list(self, body):\n        deserializer = XMLDeserializer()\n        volumes = deserializer.find_first_child_named(\n            minidom.parseString(body), 'volumes')\n        volume_list = deserializer.find_children_named(volumes, 'volume')\n        image_metadata_list = [\n            deserializer.find_first_child_named(\n                volume, 'volume_image_metadata'\n            )\n            for volume in volume_list]\n        return map(MetadataXMLDeserializer().extract_metadata,\n                   image_metadata_list)\n",
        "summary": "The provided Python code defines a test suite for handling volume image metadata using both JSON and XML formats. It includes fake implementations for retrieving volume data and simulates API requests to verify the correct retrieval of image metadata associated with volumes. The tests ensure that the system can properly parse and return volume image metadata in both JSON and XML representations."
    },
    {
        "code": "from datetime import datetime\nfrom typing import List, Dict, Optional\nfrom pydantic import BaseModel, validator, root_validator\n\n\nclass ItemModel(BaseModel):\n    cve: Dict\n    configurations: Optional[Dict]\n    impact: Optional[Dict]\n    publishedDate: datetime\n    lastModifiedDate: datetime\n\n\nclass ResultModel(BaseModel):\n    CVE_data_timestamp: datetime\n    CVE_data_type: str\n    CVE_Items: List[ItemModel]\n\n    @validator('CVE_data_type')\n    def fixed_type(cls, v):\n        assert v == 'CVE', 'Must be of type CVE'\n        return v\n\n\nclass ResponseModel(BaseModel):\n    resultsPerPage: int\n    startIndex: int\n    totalResults: int\n    result: ResultModel\n",
        "summary": "The provided Python code defines several Pydantic models to represent structured data related to Common Vulnerabilities and Exposures (CVEs). The `ItemModel` class encapsulates details about individual CVE items, including their configurations, impact, and timestamps. The `ResultModel` aggregates these items along with metadata such as the timestamp of the data and its type, which must be 'CVE'. Finally, the `ResponseModel` provides pagination information alongside the actual results."
    },
    {
        "code": "import easygui as g\n\nuser_info=g.multenterbox(title='\u8d26\u53f7\u4e2d\u5fc3',msg='\u3010*\u7528\u6237\u540d\u3011\u4e3a\u5fc5\u586b\u9879\\t\u3010*\u771f\u5b9e\u59d3\u540d\u3011\u4e3a\u5fc5\u586b\u9879\\t\u3010*\u624b\u673a\u53f7\u7801\u3011\u4e3a\u5fc5\u586b\u9879\\t\u3010*E-mail\u3011\u4e3a\u5fc5\u586b\u9879',\n               fields=['*\u7528\u6237\u540d','*\u771f\u5b9e\u59d3\u540d','\u56fa\u5b9a\u7535\u8bdd','*\u624b\u673a\u53f7\u7801','QQ','*E-mail']\n               )",
        "summary": "The Python code imports the `easygui` library and uses it to create a multi-entry dialog box titled '\u8d26\u53f7\u4e2d\u5fc3', prompting the user to input their username, real name, fixed phone number, mobile phone number, QQ, and email. The fields marked with an asterisk (*) are designated as required."
    },
    {
        "code": "import re\nimport os\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n\n\nclass OrderNotFound(StandardError):\n\n    \n\n    def __init__(self, orderid):\n\n        \n        super(OrderNotFound, self).__init__(orderid)\n\n\nclass ItemNotFound(StandardError):\n    \n\n    def __init__(self, orderid, itemid):\n\n        \n        super(ItemNotFound, self).__init__(orderid, itemid)\n\n\nclass ProductNotImplemented(NotImplementedError):\n    \n\n    def __init__(self, product_id):\n        \n\n        self.product_id = product_id\n\n        super(ProductNotImplemented, self).__init__(product_id)\n\nclass ValidationException(Exception):\n    \n\n    def __init__(self, msg):\n        err_ls = msg.split('\\n')\n        err_key = err_ls[0].replace(':', '')\n        self.response = {err_key: []}\n\n        for err in err_ls[1:]:\n            if err:\n                err = re.sub(r'<obj>.', '', err)\n                self.response[err_key].append(err)\n\n        super(ValidationException, self).__init__(str(self.response))\n\n\nclass InventoryException(Exception):\n    \n    def __init__(self, msg):\n        super(InventoryException, self).__init__(msg)\n\n        self.response = {'Inputs Not Available': msg}\n\n\nclass InventoryConnectionException(Exception):\n    \n    def __init__(self, msg):\n        super(InventoryConnectionException, self).__init__(msg)",
        "summary": "The provided Python code defines several custom exception classes for handling different types of errors in an inventory management system. These exceptions include `OrderNotFound`, `ItemNotFound`, `ProductNotImplemented`, `ValidationException`, `InventoryException`, and `InventoryConnectionException`, each tailored to manage specific error scenarios relevant to the system's operations."
    },
    {
        "code": "import os\nimport argparse\nimport json\nimport pandas as pd\n\nimport bilby\nfrom bilby_pipe.create_injections import InjectionCreator\n\n\ndef main():\n\n    parser = argparse.ArgumentParser(description=\"Slurm files from nmma injection file\")\n    parser.add_argument(\n        \"--prior-file\",\n        type=str,\n        required=True,\n        help=\"The prior file from which to generate injections\",\n    )\n    parser.add_argument(\n        \"--injection-file\",\n        type=str,\n        required=True,\n        help=\"The bilby injection json file to be used\",\n    )\n    parser.add_argument(\n        \"--analysis-file\",\n        type=str,\n        required=True,\n        help=\"The analysis bash script to be replicated\",\n    )\n    parser.add_argument(\"-o\", \"--outdir\", type=str, default=\"outdir\")\n    args = parser.parse_args()\n\n    \n    if args.injection_file:\n        if args.injection_file.endswith(\".json\"):\n            with open(args.injection_file, \"rb\") as f:\n                injection_data = json.load(f)\n                datadict = injection_data[\"injections\"][\"content\"]\n                dataframe_from_inj = pd.DataFrame.from_dict(datadict)\n        else:\n            print(\"Only json supported.\")\n            exit(1)\n\n    if len(dataframe_from_inj) > 0:\n        args.n_injection = len(dataframe_from_inj)\n\n    \n    injection_creator = InjectionCreator(\n        prior_file=args.prior_file,\n        prior_dict=None,\n        n_injection=args.n_injection,\n        default_prior=\"PriorDict\",\n        gps_file=None,\n        trigger_time=0,\n        generation_seed=0,\n    )\n    dataframe_from_prior = injection_creator.get_injection_dataframe()\n\n    \n    dataframe = pd.DataFrame.merge(\n        dataframe_from_inj,\n        dataframe_from_prior,\n        how=\"outer\",\n        left_index=True,\n        right_index=True,\n    )\n\n    for index, row in dataframe.iterrows():\n        with open(args.analysis_file, \"r\") as file:\n            analysis = file.read()\n\n        outdir = os.path.join(args.outdir, str(index))\n        if not os.path.isdir(outdir):\n            os.makedirs(outdir)\n\n        priors = bilby.gw.prior.PriorDict(args.prior_file)\n        priors.to_file(outdir, label=\"injection\")\n        priorfile = os.path.join(outdir, \"injection.prior\")\n        injfile = os.path.join(outdir, \"lc.csv\")\n\n        analysis = analysis.replace(\"PRIOR\", priorfile)\n        analysis = analysis.replace(\"OUTDIR\", outdir)\n        analysis = analysis.replace(\"INJOUT\", injfile)\n        analysis = analysis.replace(\"INJNUM\", str(index))\n        analysis_file = os.path.join(outdir, \"inference.sh\")\n\n        fid = open(analysis_file, \"w\")\n        fid.write(analysis)\n        fid.close()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script processes injection files and analysis scripts to generate multiple inference jobs using the Bilby library. It reads prior and injection data, merges them into a single DataFrame, and then for each row (injection), it creates a new directory, copies an analysis script template, replaces placeholders with actual values, and writes the modified script to run inference on that specific injection."
    },
    {
        "code": "from functools import partial\nfrom typing import NamedTuple, Union\n\nfrom flake8_annotations import Argument, Function\nfrom flake8_annotations.enums import AnnotationType\n\n\nclass FormatTestCase(NamedTuple):\n    \n\n    test_object: Union[Argument, Function]\n    str_output: str\n    repr_output: str\n\n\n\narg = partial(Argument, lineno=0, col_offset=0, annotation_type=AnnotationType.ARGS)\nfunc = partial(Function, name=\"test_func\", lineno=0, col_offset=0, decorator_list=[])\n\nformatting_test_cases = {\n    \"arg\": FormatTestCase(\n        test_object=arg(argname=\"test_arg\"),\n        str_output=\"<Argument: test_arg, Annotated: False>\",\n        repr_output=(\n            \"Argument(\"\n            \"argname='test_arg', \"\n            \"lineno=0, \"\n            \"col_offset=0, \"\n            \"annotation_type=AnnotationType.ARGS, \"\n            \"has_type_annotation=False, \"\n            \"has_3107_annotation=False, \"\n            \"has_type_comment=False\"\n            \")\"\n        ),\n    ),\n    \"func_no_args\": FormatTestCase(\n        test_object=func(args=[arg(argname=\"return\")]),\n        str_output=\"<Function: test_func, Args: [<Argument: return, Annotated: False>]>\",\n        repr_output=(\n            \"Function(\"\n            \"name='test_func', \"\n            \"lineno=0, \"\n            \"col_offset=0, \"\n            \"function_type=FunctionType.PUBLIC, \"\n            \"is_class_method=False, \"\n            \"class_decorator_type=None, \"\n            \"is_return_annotated=False, \"\n            \"has_type_comment=False, \"\n            \"has_only_none_returns=True, \"\n            \"is_nested=False, \"\n            \"decorator_list=[], \"\n            \"args=[Argument(argname='return', lineno=0, col_offset=0, annotation_type=AnnotationType.ARGS, \"  \n            \"has_type_annotation=False, has_3107_annotation=False, has_type_comment=False)]\"\n            \")\"\n        ),\n    ),\n    \"func_has_arg\": FormatTestCase(\n        test_object=func(args=[arg(argname=\"foo\"), arg(argname=\"return\")]),\n        str_output=\"<Function: test_func, Args: [<Argument: foo, Annotated: False>, <Argument: return, Annotated: False>]>\",  \n        repr_output=(\n            \"Function(\"\n            \"name='test_func', \"\n            \"lineno=0, \"\n            \"col_offset=0, \"\n            \"function_type=FunctionType.PUBLIC, \"\n            \"is_class_method=False, \"\n            \"class_decorator_type=None, \"\n            \"is_return_annotated=False, \"\n            \"has_type_comment=False, \"\n            \"has_only_none_returns=True, \"\n            \"is_nested=False, \"\n            \"decorator_list=[], \"\n            \"args=[Argument(argname='foo', lineno=0, col_offset=0, annotation_type=AnnotationType.ARGS, \"  \n            \"has_type_annotation=False, has_3107_annotation=False, has_type_comment=False), \"\n            \"Argument(argname='return', lineno=0, col_offset=0, annotation_type=AnnotationType.ARGS, \"  \n            \"has_type_annotation=False, has_3107_annotation=False, has_type_comment=False)]\"\n            \")\"\n        ),\n    ),\n}\n",
        "summary": "The provided Python code defines a set of test cases for formatting annotations in functions and arguments using the `flake8_annotations` library. Each test case includes an object (either an argument or a function) with specific attributes and expected string and representation outputs. The `FormatTestCase` class is used to structure these test cases, ensuring that the formatting logic can be easily verified against predefined expectations."
    },
    {
        "code": "import sys\nimport os\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__),'../..'))\n\nimport suzu.matdb.srim_compounddb as compounddb\n\nair = compounddb.Compound()\n\nair.desc = 'Air, Dry near sea level (ICRU-104)  0.00120484  O-23.2, N-75.5, Ar-1.3'\nair.name = '%Air, Dry (ICRU-104)'\nair.density = 0.00120484\nair.mass_percentage = True\nair.elems = [(6, 0.000124), (8, 0.231781), (7, 0.755267), (18, 0.012827)]\nair.bonding = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nair.comment = \nair.fulltext = \n\nwater = compounddb.Compound()\n\nwater.desc = 'Water  (liquid)                     1.00        H-2, O-1'\nwater.name = 'Water_Liquid (ICRU-276)'\nwater.density = 1.0\nwater.mass_percentage = False\nwater.elems = [(1, 2.0), (8, 1.0)]\nwater.bonding = [0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nwater.comment = b.decode('cp437')\n\nprint(water.to_suzu())\nprint(air.to_suzu())\n",
        "summary": "The Python script imports a module to handle compound data and creates instances for air and water, setting various attributes such as description, density, element composition, and bonding information. It then prints the Suzu representation of both compounds."
    },
    {
        "code": "import numpy as np\nfrom sklearn import metrics\nfrom PIL import Image\n\ndef get_metrics(pred, logits, gt):\n    if isinstance(logits, list):\n        logits = logits[-1]\n    result = {'confusion_matrix': metrics.confusion_matrix(gt.flatten(), pred.flatten(), labels=[1, 0]),\n              'auc': roc(gt, logits)}\n    return result\n\ndef get_metrics_without_roc(pred, gt):\n    result = {'confusion_matrix': metrics.confusion_matrix(gt.flatten(), pred.flatten(), labels=[1, 0])}\n    return result\n\ndef show_metrics(metrics):\n    con_mat = np.zeros((2,2))\n    auc = 0.0\n    for m in metrics:\n        con_mat += m['confusion_matrix']\n        auc += m['auc']\n    auc /= len(metrics)\n    result = {'confusion_matrix': con_mat.tolist(),\n              'accuracy': accuracy(con_mat),\n              'kappa': kappa(con_mat),\n              'precision': precision(con_mat),\n              'sensitivity': sensitivity(con_mat),\n              'specificity': specificity(con_mat),\n              'auc': auc,\n              }\n    return result\n\ndef show_metrics_without_roc(metrics):\n    con_mat = np.zeros((2,2))\n    for m in metrics:\n        con_mat += m['confusion_matrix']\n    result = {'confusion_matrix': con_mat,\n              'accuracy': accuracy(con_mat),\n              'kappa': kappa(con_mat),\n              'precision': precision(con_mat),\n              'sensitivity': sensitivity(con_mat),\n              'specificity': specificity(con_mat),\n              }\n    return result\n\ndef show_metrics_from_save_image(data):\n    pred = data[:,:,0] // 255\n    gt = data[:,:,1] // 255\n    metrics = [get_metrics_without_roc(pred, gt)]\n    return show_metrics_without_roc(metrics)\n\ndef kappa(matrix):\n    matrix = np.array(matrix)\n    n = np.sum(matrix)\n    sum_po = 0\n    sum_pe = 0\n    for i in range(len(matrix[0])):\n        sum_po += matrix[i][i]\n        row = np.sum(matrix[i, :])\n        col = np.sum(matrix[:, i])\n        sum_pe += row * col\n    po = sum_po / n\n    pe = sum_pe / (n * n)\n    \n    return (po - pe) / (1 - pe)\n\n\ndef sensitivity(matrix):\n    return matrix[0][0]/(matrix[0][0]+matrix[1][0])\n\n\ndef specificity(matrix):\n    return matrix[1][1]/(matrix[1][1]+matrix[0][1])\n\n\ndef precision(matrix):\n    return matrix[0][0]/(matrix[0][0]+matrix[0][1])\n\ndef roc(gt, logits):\n    gtlist = gt.flatten()\n    predlist = logits.detach().cpu().numpy()[0, 1, ...].flatten()\n\n    fpr, tpr, thresholds = metrics.roc_curve(gtlist, predlist, pos_label=1)\n    roc_auc = metrics.auc(fpr, tpr)  \n    return roc_auc\n\n\ndef accuracy(matrix):\n    return (matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[0][1]+matrix[1][0]+matrix[1][1])\n\ndef error_rate(predictions, labels):\n    \n    return 100.0 - (\n            100.0 *\n            np.sum(np.argmin(predictions, 3) == np.argmin(labels, 3)) /\n            (predictions.shape[0] * predictions.shape[1] * predictions.shape[2]))\n\ndef save_predict(filename, data, gt, pred):\n    pred = pred * 255\n    gt = gt[0, 1, :, :]\n    gt = np.where(gt > 0.5, 255, 0)\n    differ = np.stack([np.zeros_like(pred), gt, pred], -1)\n    pred = np.stack([pred, pred, pred], -1)\n    gt = np.stack([gt, gt, gt], -1)\n    data = np.transpose(data, (0, 2, 3, 1))[0,...]\n    if data.shape[2] == 60:\n        data = data[:, :, 10:40:10]\n    elif data.shape[2] == 1:\n        data = np.concatenate([data, data, data], -1)\n    elif data.shape[2] == 15:\n        data = data[:, :, 0:15:5]\n    data -= np.min(data, axis=(0,1))\n    data /= (np.max(data, axis=(0,1))/255)\n    data = data.astype(np.uint8)\n    img = Image.fromarray(np.concatenate([data, pred, gt, differ], axis=1).astype(np.uint8))\n    img.save(filename)\n\ndef save_logits(filename, pred):\n    pred = pred * 255\n    pred = np.stack([pred, pred, pred], -1)\n    img = Image.fromarray(pred.astype(np.uint8))\n    img.save(filename)\n",
        "summary": "The provided Python code includes functions for calculating and displaying various metrics such as confusion matrix, accuracy, kappa, precision, sensitivity, specificity, and AUC-ROC. It also features utilities for saving predictions and logits as images, handling image data, and computing error rates."
    },
    {
        "code": "from pysys.constants import *\r\nfrom apama.basetest import ApamaBaseTest\r\nfrom apama.correlator import CorrelatorHelper\r\nfrom GAPDemoConnected import GAPDemoConnectedHelper\r\n\r\nclass PySysTest(ApamaBaseTest):\r\n\tdef __init__(self, descriptor, outsubdir, runner):\r\n\t\tsuper(PySysTest, self).__init__(descriptor, outsubdir, runner)\r\n\t\tself.helper = GAPDemoConnectedHelper(self, PROJECT)\r\n\r\n\tdef execute(self):\r\n\t\t\n\t\tcorrelator = self.helper.startApplication()\r\n\t\t\r\n\t\t\n\t\t(phoneId, phoneName) = self.helper.getDeviceDetails()\r\n\t\tself.log.info(f'Found c8y_SensorPhone device with name \"{phoneName}\" and id \"{phoneId}\"')\r\n\r\n\t\t\n\t\tself.helper.waitForSubscription()\r\n\r\n\t\t\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 1.23)\r\n\t\t\r\n\t\t\n\t\tself.helper.waitForBaseline()\r\n\r\n\t\t\n\t\tflipUpBefore = self.helper.countActiveAlarms(\"FlipUp\")\r\n\t\tself.log.info(f'Found {flipUpBefore} active \"FlipUp\" alarms before sending measurements')\r\n\t\tflipDownBefore = self.helper.countActiveAlarms(\"FlipDown\")\r\n\t\tself.log.info(f'Found {flipDownBefore} active \"FlipDown\" alarms before sending measurements')\r\n\t\t\r\n\t\t\n\t\tself.log.info('Sending measurements...')\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, -0.9) \n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.9)  \n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.4)\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.0)\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, -0.4)\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, -0.9) \n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.8)\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.9)\r\n\t\tself.helper.sendAcceleration(phoneId, 0.0, 0.0, 0.85) \n\t\t\r\n\t\t\n\t\tself.helper.waitForMeasurements()\r\n\r\n\t\t\n\t\tflipUpAfter = self.helper.countActiveAlarms(\"FlipUp\")\r\n\t\tself.log.info(f'Found {flipUpAfter} active \"FlipUp\" alarms after sending measurements')\r\n\t\tflipDownAfter = self.helper.countActiveAlarms(\"FlipDown\")\r\n\t\tself.log.info(f'Found {flipDownAfter} active \"FlipDown\" alarms after sending measurements')\r\n\t\t\r\n\t\tself.flipUpDelta = flipUpAfter - flipUpBefore\r\n\t\tself.flipDownDelta = flipDownAfter - flipDownBefore\r\n\r\n\tdef validate(self):\r\n\t\tself.assertEval(\"self.flipUpDelta=={expected}\", expected=2)\r\n\t\tself.assertEval(\"self.flipDownDelta=={expected}\", expected=2)\r\n",
        "summary": "The provided Python code defines a test class `PySysTest` that extends `ApamaBaseTest`, using the Apama Correlator and GAPDemoConnectedHelper to simulate sensor data for a device named \"c8y_SensorPhone\". The test sends acceleration measurements and counts active alarms before and after sending these measurements, then validates that two new \"FlipUp\" and two new \"FlipDown\" alarms have been triggered as expected."
    },
    {
        "code": "from django.contrib import admin\nfrom django.urls import path, include\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('', include('search.urls')),\n]\n",
        "summary": "The provided Python code sets up URL routing for a Django web application, including paths for the Django admin interface and a custom search module."
    },
    {
        "code": "from primaires.interpreteur.editeur import Editeur\nfrom primaires.interpreteur.editeur.env_objet import EnveloppeObjet\nfrom primaires.communication.editeurs.medit import EdtMedit\nfrom primaires.communication.mudmail import ENVOYE\nfrom primaires.format.fonctions import couper_phrase\n\nclass EdtBoiteEnvoi(Editeur):\n    \n    \n    \n    def __init__(self, pere, objet=None, attribut=None):\n        \n        Editeur.__init__(self, pere, objet, attribut)\n        self.ajouter_option(\"l\", self.opt_lire)\n        self.ajouter_option(\"c\", self.opt_copier)\n        self.ajouter_option(\"s\", self.opt_supprimer)\n    \n    def accueil(self):\n        \n        joueur = self.pere.joueur\n        mails = type(self).importeur.communication.mails.get_mails_pour(\n                joueur, ENVOYE)\n        msg = \"||tit| \" + \"Messages envoy\u00e9s\".ljust(76) + \"|ff||\\n\"\n        msg += self.opts.separateur + \"\\n\"\n        msg += self.aide_courte + \"\\n\\n\"\n        \n        if not mails:\n            msg += \"|att|Vous n'avez envoy\u00e9 aucun message.|ff|\"\n        else:\n            taille = 0\n            for mail in mails:\n                t_sujet = len(couper_phrase(mail.sujet, 33))\n                if t_sujet > taille:\n                    taille = t_sujet\n            taille = (taille < 5 and 5) or taille\n            msg += \"+\" + \"-\".ljust(taille + 41, \"-\") + \"+\\n\"\n            msg += \"| |tit|N\u00b0|ff| | |tit|\" + \"Sujet\".ljust(taille)\n            msg += \"|ff| | |tit|Destinataire|ff| | |tit|\" + \"Date\".ljust(16)\n            msg += \"|ff| |\\n\"\n            i = 1\n            for mail in mails:\n                msg += \"| |rg|\" + str(i).rjust(2) + \"|ff| | \"\n                msg += \"|vr|\" + couper_phrase(mail.sujet, 33).ljust( \\\n                        taille) + \"|ff| | |blc|\"\n                msg += couper_phrase(mail.aff_dest,12).ljust(12) + \"|ff| | \"\n                msg += \"|jn|\" + mail.date.isoformat(\" \")[:16] + \"|ff| |\\n\"\n                i += 1\n            msg += \"+\" + \"-\".ljust(taille + 41, \"-\") + \"+\"\n        \n        return msg\n    \n    def opt_lire(self, arguments):\n        \n        if not arguments or arguments.isspace():\n            self.pere.joueur << \"|err|Vous devez pr\u00e9ciser le num\u00e9ro d'un \" \\\n                    \"message.|ff|\"\n            return\n        mails = type(self).importeur.communication.mails.get_mails_pour(\n                self.pere.joueur, ENVOYE)\n        try:\n            num = int(arguments.split(\" \")[0])\n        except ValueError:\n            self.pere.joueur << \"|err|Vous devez sp\u00e9cifier un nombre entier \" \\\n                    \"valide.|ff|\"\n        else:\n            i = 1\n            l_mail = None\n            for mail in mails:\n                if num == i:\n                    l_mail = mail\n                    break\n                i += 1\n            if l_mail is None:\n                self.pere.joueur << \"|err|Le num\u00e9ro sp\u00e9cifi\u00e9 ne correspond \u00e0 \" \\\n                        \"aucun message.|ff|\"\n                return\n            self.pere.joueur << l_mail.afficher()\n    \n    def opt_copier(self, arguments):\n        \n        if not arguments or arguments.isspace():\n            self.pere.joueur << \"|err|Vous devez pr\u00e9ciser le num\u00e9ro d'un \" \\\n                    \"message.|ff|\"\n            return\n        mails = type(self).importeur.communication.mails.get_mails_pour(\n                self.pere.joueur, ENVOYE)\n        try:\n            num = int(arguments.split(\" \")[0])\n        except ValueError:\n            self.pere.joueur << \"|err|Vous devez sp\u00e9cifier un nombre entier \" \\\n                    \"valide.|ff|\"\n        else:\n            i = 1\n            c_mail = None\n            for mail in mails:\n                if num == i:\n                    c_mail = mail\n                    break\n                i += 1\n            if c_mail is None:\n                self.pere.joueur << \"|err|Le num\u00e9ro sp\u00e9cifi\u00e9 ne correspond \u00e0 \" \\\n                        \"aucun message.|ff|\"\n                return\n            mail = type(self).importeur.communication.mails.creer_mail(\n                    self.pere.joueur)\n            mail.sujet = \"CC:\" + c_mail.sujet\n            mail.liste_dest = c_mail.liste_dest\n            mail.contenu.ajouter_paragraphe(str(c_mail.contenu))\n            enveloppe = EnveloppeObjet(EdtMedit, mail, None)\n            enveloppe.parent = self\n            contexte = enveloppe.construire(self.pere.joueur)\n            self.pere.joueur.contextes.ajouter(contexte)\n            contexte.actualiser()\n    \n    def opt_supprimer(self, arguments):\n        \n        if not arguments or arguments.isspace():\n            self.pere.joueur << \"|err|Vous devez pr\u00e9ciser le num\u00e9ro d'un \" \\\n                    \"message.|ff|\"\n            return\n        mails = type(self).importeur.communication.mails.get_mails_pour(\n                self.pere.joueur, ENVOYE)\n        try:\n            num = int(arguments.split(\" \")[0])\n        except ValueError:\n            self.pere.joueur << \"|err|Vous devez sp\u00e9cifier un nombre entier \" \\\n                    \"valide.|ff|\"\n        else:\n            i = 1\n            s_mail = None\n            for mail in mails:\n                if num == i:\n                    s_mail = mail\n                    break\n                i += 1\n            if s_mail is None:\n                self.pere.joueur << \"|err|Le num\u00e9ro sp\u00e9cifi\u00e9 ne correspond \u00e0 \" \\\n                        \"aucun message.|ff|\"\n                return\n            del type(self).importeur.communication.mails[s_mail.id]\n            self.pere.joueur << \"|att|Ce message a bien \u00e9t\u00e9 supprim\u00e9.|ff|\"\n",
        "summary": "The `EdtBoiteEnvoi` class extends the `Editeur` class to create an editor for managing sent emails in a MUD-like game. It provides options to read, copy, and delete messages, displaying them in a formatted list and handling user input accordingly."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n\nimport numbers\nimport warnings\n\nimport torch\nfrom torch.autograd import Variable\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import iter_discrete_traces\nfrom pyro.infer.util import torch_backward, torch_data_sum, torch_sum\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_model_guide_match, is_nan\n\n\ndef check_enum_discrete_can_run(model_trace, guide_trace):\n    \n    \n    \n    model_trace.compute_batch_log_pdf()\n    guide_trace.compute_batch_log_pdf()\n    shapes = {}\n    for source, trace in [(\"model\", model_trace), (\"guide\", guide_trace)]:\n        for name, site in trace.nodes.items():\n            if site[\"type\"] == \"sample\":\n                shapes[site[\"batch_log_pdf\"].size()] = (source, name)\n    if len(shapes) > 1:\n        raise NotImplementedError(\n                \"enum_discrete does not support mixture of batched and un-batched variables. \"\n                \"Try rewriting your model to avoid batching or running with enum_discrete=False. \"\n                \"Found the following variables of different batch shapes:\\n{}\".format(\n                    \"\\n\".join([\"{} {}: shape = {}\".format(source, name, tuple(shape))\n                               for shape, (source, name) in sorted(shapes.items())])))\n\n\nclass Trace_ELBO(ELBO):\n    \n\n    def _get_traces(self, model, guide, *args, **kwargs):\n        \n\n        for i in range(self.num_particles):\n            if self.enum_discrete:\n                \n                for scale, guide_trace in iter_discrete_traces(\"flat\", guide, *args, **kwargs):\n                    model_trace = poutine.trace(poutine.replay(model, guide_trace),\n                                                graph_type=\"flat\").get_trace(*args, **kwargs)\n\n                    check_model_guide_match(model_trace, guide_trace)\n                    guide_trace = prune_subsample_sites(guide_trace)\n                    model_trace = prune_subsample_sites(model_trace)\n                    check_enum_discrete_can_run(model_trace, guide_trace)\n\n                    guide_trace.compute_score_parts()\n                    log_r = model_trace.batch_log_pdf() - guide_trace.batch_log_pdf()\n                    weight = scale / self.num_particles\n                    yield weight, model_trace, guide_trace, log_r\n                continue\n\n            guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n            model_trace = poutine.trace(poutine.replay(model, guide_trace)).get_trace(*args, **kwargs)\n\n            check_model_guide_match(model_trace, guide_trace)\n            guide_trace = prune_subsample_sites(guide_trace)\n            model_trace = prune_subsample_sites(model_trace)\n\n            guide_trace.compute_score_parts()\n            log_r = model_trace.log_pdf() - guide_trace.log_pdf()\n            weight = 1.0 / self.num_particles\n            yield weight, model_trace, guide_trace, log_r\n\n    def _is_batched(self, weight):\n        return self.enum_discrete and \\\n               isinstance(weight, Variable) and \\\n               weight.dim() > 0 and \\\n               weight.size(0) > 1\n\n    def loss(self, model, guide, *args, **kwargs):\n        \n        elbo = 0.0\n        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):\n            elbo_particle = weight * 0\n\n            if self._is_batched(weight):\n                log_pdf = \"batch_log_pdf\"\n            else:\n                log_pdf = \"log_pdf\"\n            for name in model_trace.nodes.keys():\n                if model_trace.nodes[name][\"type\"] == \"sample\":\n                    if model_trace.nodes[name][\"is_observed\"]:\n                        elbo_particle += model_trace.nodes[name][log_pdf]\n                    else:\n                        elbo_particle += model_trace.nodes[name][log_pdf]\n                        elbo_particle -= guide_trace.nodes[name][log_pdf]\n\n            \n            if isinstance(weight, numbers.Number):\n                if weight == 0.0:\n                    elbo_particle = torch.zeros_like(elbo_particle)\n            else:\n                elbo_particle[weight == 0] = 0.0\n\n            elbo += torch_data_sum(weight * elbo_particle)\n\n        loss = -elbo\n        if is_nan(loss):\n            warnings.warn('Encountered NAN loss')\n        return loss\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        \n        elbo = 0.0\n        \n        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):\n            elbo_particle = weight * 0\n            surrogate_elbo_particle = weight * 0\n            batched = self._is_batched(weight)\n            \n            if batched:\n                log_pdf = \"batch_log_pdf\"\n            else:\n                log_pdf = \"log_pdf\"\n            for name, model_site in model_trace.nodes.items():\n                if model_site[\"type\"] == \"sample\":\n                    model_log_pdf = model_site[log_pdf]\n                    if model_site[\"is_observed\"]:\n                        elbo_particle += model_log_pdf\n                        surrogate_elbo_particle += model_log_pdf\n                    else:\n                        guide_site = guide_trace.nodes[name]\n                        guide_log_pdf, score_function_term, entropy_term = guide_site[\"score_parts\"]\n\n                        if not batched:\n                            guide_log_pdf = guide_log_pdf.sum()\n                        elbo_particle += model_log_pdf - guide_log_pdf\n                        surrogate_elbo_particle += model_log_pdf\n\n                        if not is_identically_zero(entropy_term):\n                            if not batched:\n                                entropy_term = entropy_term.sum()\n                            surrogate_elbo_particle -= entropy_term\n\n                        if not is_identically_zero(score_function_term):\n                            if not batched:\n                                score_function_term = score_function_term.sum()\n                            surrogate_elbo_particle += log_r.detach() * score_function_term\n\n            \n            if isinstance(weight, numbers.Number):\n                if weight == 0.0:\n                    elbo_particle = torch.zeros_like(elbo_particle)\n                    surrogate_elbo_particle = torch.zeros_like(surrogate_elbo_particle)\n            else:\n                weight_eq_zero = (weight == 0)\n                elbo_particle[weight_eq_zero] = 0.0\n                surrogate_elbo_particle[weight_eq_zero] = 0.0\n\n            elbo += torch_data_sum(weight * elbo_particle)\n            surrogate_elbo_particle = torch_sum(weight * surrogate_elbo_particle)\n\n            \n            trainable_params = set(site[\"value\"]\n                                   for trace in (model_trace, guide_trace)\n                                   for site in trace.nodes.values()\n                                   if site[\"type\"] == \"param\")\n\n            if trainable_params:\n                surrogate_loss_particle = -surrogate_elbo_particle\n                torch_backward(surrogate_loss_particle)\n                pyro.get_param_store().mark_params_active(trainable_params)\n\n        loss = -elbo\n        if is_nan(loss):\n            warnings.warn('Encountered NAN loss')\n        return loss\n",
        "summary": "The provided Python code defines a custom implementation of the Evidence Lower Bound (ELBO) for probabilistic inference using Pyro, a probabilistic programming library. The `Trace_ELBO` class extends the base `ELBO` class and overrides methods to handle both batched and unbatched variables during inference, ensuring compatibility with discrete enumeration strategies. It computes the ELBO and its gradients by iterating over multiple particles, applying models and guides, and adjusting for observed data and guide scores."
    },
    {
        "code": "from .basin_mask import *\nfrom .forcing import *\nfrom .rivers import *\n",
        "summary": "The provided Python code imports all functions and classes from three modules: `basin_mask`, `forcing`, and `rivers`. These modules likely contain essential components for hydrological modeling, including geographical basin definitions, meteorological forcing data, and river network information."
    },
    {
        "code": "from ...utils.importlib import require_modules\n\nrequired_modules = [\"falcon\"]\n\nwith require_modules(required_modules) as missing_modules:\n    if not missing_modules:\n        from .middleware import TraceMiddleware\n        from .patch import patch\n\n        __all__ = [\"TraceMiddleware\", \"patch\"]\n",
        "summary": "The Python code imports the `require_modules` function from a utility module and checks for the presence of the \"falcon\" module. If it is available, it then imports the `TraceMiddleware` and `patch` modules from the current package and makes them available for use through the `__all__` variable."
    },
    {
        "code": "import gzip\nfrom logging import getLogger\nimport os\nimport re\nimport shutil\nfrom subprocess import run\nfrom time import time\nfrom typing import Any\n\nfrom PIL import Image, ImageFile\n\nfrom common import makedirs_safe, read_text_safe, write_text_safe\nfrom css_minify import css_minify\n\n\n\nBASE = os.path.dirname(os.path.dirname(__file__))\nCOMPILER = os.path.join(BASE, 'script/closure-compiler-v20200406.jar')\nCSS_FOLDER = os.path.join(BASE, 'css')\nJAVA = 'java'\nJS_FOLDER = os.path.join(BASE, 'js')\nLOCAL = BASE\n\n\n\nCSS_FILES = [\n    'light',\n]\n\nJS_FILES = {\n    '4d': [\n        'libs/three',\n        'libs/stats',\n        'libs/GLTFLoader',\n        'libs/DRACOLoader',\n        'libs/camera-controls',\n    ],\n    'all': [\n        'libs/socket.io',\n        ':common',\n        'libs/chess-quick',\n        ':engine',\n        ':global',\n        ':3d',\n        ':xboard',\n        ':graph',\n        ':game',\n        ':temp',\n        ':network',\n        ':startup',\n        ':config',\n        'script',\n    ],\n    'chart': [\n        'libs/chart-quick',\n    ],\n}\n\nNEED_GZIPS = {\n    '4d_.js',\n    'ammo.wasm.js',\n    'ammo.wasm.wasm',\n    'chart_.js',\n    'chart.min.js',\n    'dark.css',\n    'dark-archive.css',\n    'draco_decoder.js',\n    'draco_decoder.wasm',\n    'draco_wasm_wrapper.js',\n    'fra.json',\n    'index.html',\n    'jpn.json',\n    'light-archive.css',\n    'manifest.json',\n    'pieces-draco.glb',\n    'rus.json',\n    'sea.css',\n    'sea-archive.css',\n    'ukr.json',\n}\n\n\nSKIP_GZIPS = {\n    'archive',\n    'doc',\n    'image',\n    'model',\n    'node_modules',\n    'script',\n    'sound',\n    'test',\n    'theme',\n}\n\n\nclass Sync:\n    \n\n    \n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n\n        self.clean = kwargs.get('clean')                                \n        self.host = kwargs.get('host')                                  \n        self.no_compress = kwargs.get('no_compress')                    \n        self.no_debug = kwargs.get('no_debug')                          \n        self.no_process = kwargs.get('no_process')                      \n        self.zip = kwargs.get('zip')                                    \n\n        self.logger = getLogger(self.__class__.__name__)\n\n    def combine_pieces(self, folder: str):\n        \n        if 'metro' in folder:\n            height = 160\n            width = 160\n        else:\n            height = 80\n            width = 80\n        combined = Image.new('RGBA', (width * 12, height), (0, 255, 0, 0))\n        output = f'{folder}.png'\n\n        i = 0\n        pieces = 'bknpqr'\n        for color in 'bw':\n            for piece in pieces:\n                name = f'{color}{piece}'\n                image = Image.open(os.path.join(folder, f'{name}.png'))\n                offset = (i * width, 0)\n                combined.paste(image, offset)\n                i += 1\n\n        combined.save(output, format='png')\n        print('a', end='')\n\n    def combine_themes(self, folder: str):\n        \n        sources = os.listdir(folder)\n        for source in sources:\n            filename = os.path.join(folder, source)\n            if os.path.isdir(filename):\n                self.combine_pieces(filename)\n\n    def compress_3d(self, data: str) -> str:\n        \n        data = re.sub(r'\\bTHREE\\b', 'T', data)\n        data = re.sub(r'console\\.(error|warn)\\(.+?\\);', '', data, flags=re.S)\n        return data\n\n    def compress_gzip(self, filename: str):\n        \n        output = f'{filename}.gz'\n        with open(filename, 'rb') as f_in:\n            with gzip.open(output, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n\n        \n        if os.path.isfile(output):\n            info = os.stat(output)\n            os.utime(filename, (info.st_atime, info.st_mtime))\n            print('g', end='')\n\n    def compress_js(self, filename: str) -> str:\n        \n        base, ext = os.path.splitext(filename)\n        output = f'{base}_{ext}'\n\n        if self.no_compress:\n            shutil.copy(filename, output)\n            return output\n\n        args = [\n            JAVA,\n            '-jar', COMPILER,\n            '--js', filename,\n            '--js_output_file', output,\n            '--language_in', 'ECMASCRIPT_2018',\n            '--language_out', 'ECMASCRIPT_2018',\n        ]\n        if self.kwargs.get('advanced'):\n            args.extend(['--compilation_level', 'ADVANCED'])\n        run(args)\n        return output\n\n    def gzip_files(self, folder: str, depth: int, delete: bool):\n        \n        queues = []\n        sources = os.listdir(folder)\n\n        for source in sources:\n            if source.startswith(('.', '_')):\n                continue\n\n            filename = os.path.join(folder, source)\n            if os.path.isdir(filename):\n                if source not in SKIP_GZIPS:\n                    queues.append(filename)\n                continue\n\n            \n            if not os.path.isfile(filename):\n                continue\n            if source not in NEED_GZIPS:\n                continue\n\n            output = f'{filename}.gz'\n            source_time = os.path.getmtime(filename)\n            if os.path.isfile(output):\n                destin_time = os.path.getmtime(output)\n                if delete:\n                    os.unlink(output)\n                    print('d', end='')\n            else:\n                destin_time = 0\n\n            if not delete and source_time != destin_time:\n                self.compress_gzip(filename)\n            print(f\"{'  ' * depth}{filename}\")\n\n        for queue in queues:\n            self.gzip_files(queue, depth + 1, delete)\n\n    @staticmethod\n    def import_file(match: Any) -> str:\n        \n        source = match.group(1)\n        filename = os.path.join(JS_FOLDER, source)\n        data = read_text_safe(filename) or ''\n        if source.endswith('.js'):\n            data = re.sub(r'[\"\\']use strict[\"\\'];?', '', data)\n        return data\n\n    def normalise_folders(self):\n        \n        global CSS_FOLDER, JS_FOLDER, LOCAL\n        if CSS_FOLDER[-1] != '/':\n            CSS_FOLDER += '/'\n        if JS_FOLDER[-1] != '/':\n            JS_FOLDER += '/'\n        if LOCAL[-1] != '/':\n            LOCAL += '/'\n\n    def create_index(self):\n        \n        base = os.path.join(LOCAL, 'index_base.html')\n        base_time = os.path.getmtime(base)\n        index = os.path.join(LOCAL, 'index.html')\n        index_time = os.path.getmtime(index) if os.path.isfile(index) else 0\n        change = 0\n        if base_time >= index_time:\n            change += 1\n\n        \n        for js_output, js_files in JS_FILES.items():\n            all_js = os.path.join(JS_FOLDER, f'{js_output}.js')\n            all_min_js = os.path.join(JS_FOLDER, f'{js_output}_.js')\n            \n            js_dates = [os.path.abspath(f\"{JS_FOLDER}{js_file.strip(':')}.js\") for js_file in js_files]\n            js_names = [os.path.abspath(f'{JS_FOLDER}{js_file}.js') for js_file in js_files if js_file[0] != ':']\n\n            if js_output == 'all':\n                \n                extras = []\n            else:\n                extras = []\n\n            \n            update = True\n            if os.path.isfile(all_min_js) and os.path.isfile(all_js):\n                all_time = os.path.getmtime(all_min_js)\n                update = False\n                for js_date in js_dates + extras:\n                    update |= os.path.isfile(js_date) and os.path.getmtime(js_date) >= all_time\n\n            if not update:\n                print('J', end='')\n                continue\n\n            datas = []\n            for js_name in js_names:\n                print(js_name)\n                script_data = read_text_safe(js_name)\n                if not script_data:\n                    continue\n\n                \n                if js_name.endswith('script.js'):\n                    script_data = re.sub('@import {(.*?)}', self.import_file, script_data);\n                    script_data = re.sub('// BEGIN.*?// END', '', script_data, flags=re.S)\n\n                    if self.no_debug:\n                        script_data = re.sub('// <<.*?// >>', '', script_data, flags=re.S)\n\n                    \n                    print(f'host={self.host}')\n                    if self.host != '/':\n                        script_data = script_data.replace(\"HOST = '/',\", f\"HOST = '{self.host}',\")\n\n                datas.append(script_data)\n\n            data = '\\n'.join(datas)\n\n            if '4d' in js_output:\n                data = self.compress_3d(data)\n\n            write_text_safe(all_js, data)\n            self.compress_js(all_js)\n            print('j', end='')\n            change += 1\n\n        \n        all_css = os.path.join(CSS_FOLDER, 'all.css')\n        all_min_css = os.path.join(CSS_FOLDER, 'all_.css')\n        css_names = [os.path.abspath(f'{CSS_FOLDER}{css_file}.css') for css_file in CSS_FILES]\n\n        update = True\n        if os.path.isfile(all_min_css) and os.path.isfile(all_css):\n            all_time = os.path.getmtime(all_min_css)\n            update = False\n            for css_name in css_names:\n                update |= os.path.isfile(css_name) and os.path.getmtime(css_name) >= all_time\n\n        if update:\n            datas = []\n            for css_name in css_names:\n                datas.append(read_text_safe(css_name) or '')\n\n            data = '\\n'.join(datas)\n            write_text_safe(all_css, data)\n            css_data = css_minify(data)\n            write_text_safe(all_min_css, css_data)\n\n            print('c', end='')\n            change += 1\n        else:\n            css_data = read_text_safe(all_min_css) or ''\n            print('C', end='')\n\n        if not change:\n            print('X', end='')\n            return\n\n        \n        html = read_text_safe(base)\n        html = re.sub('<!-- BEGIN -->.*?<!-- END -->', '', html, flags=re.S)\n        html = re.sub('// BEGIN.*?// END', '', html, flags=re.S)\n\n        \n        if self.host != '/':\n            replaces = {\n                'href=\"/': f'href=\"{self.host}',\n                'src=\"/': f'src=\"{self.host}',\n            }\n            for key, value in replaces.items():\n                html = html.replace(key, value)\n\n        \n        if not self.no_process:\n            all_min_js = os.path.join(JS_FOLDER, 'all_.js')\n            js_data = read_text_safe(all_min_js) or ''\n            replaces = {\n                '<!-- {SCRIPT} -->': f'<script>{js_data}</script>',\n                '<!-- {STYLE} -->': f'<style>{css_data}</style>',\n            }\n            for key, value in replaces.items():\n                html = html.replace(key, value)\n\n            html = re.sub('<!-- .*? -->', '', html, flags=re.S)\n\n        html = re.sub(r'\\n\\s+', '\\n', html)\n        filename = os.path.join(LOCAL, 'index.html')\n        write_text_safe(filename, html)\n\n    def synchronise(self) -> bool:\n        \n        self.normalise_folders()\n        self.create_index()\n        if self.clean:\n            self.gzip_files(LOCAL, 0, True)\n        elif self.zip:\n            self.gzip_files(LOCAL, 0, False)\n        return True\n\n\nif __name__ == '__main__':\n    start = time()\n    sync = Sync()\n\n    if 0:\n        sync.combine_themes(os.path.join(BASE, 'theme'))\n    else:\n        sync.synchronise()\n\n    end = time()\n    print(f'\\nELAPSED: {end-start:.3f} seconds')\n",
        "summary": "The provided Python script is designed to synchronize and optimize a web application by combining CSS and JavaScript files, minifying them, and optionally compressing them. Here's a breakdown of the key functionalities and how they are implemented:\n\n1. **Combining Files**:\n   - The `combine_themes` method combines CSS and JavaScript files from specified themes.\n   - It reads each file, processes it (e.g., removing comments), and writes the combined content to new files.\n\n2. **Minifying Files**:\n   - The script uses a function `css_minify` to minify CSS files.\n   - For JavaScript, it relies on the `compress_js` method, which compresses the JS files using a library like `jsmin`.\n\n3. **Compressing Files**:\n   - The `gzip_files` method compresses files in the specified directory and its subdirectories.\n   - It uses Python's built-in `gzip` module to create compressed versions of each file.\n\n4. **Creating Index File**:\n   - The `create_index` method reads a base HTML file, replaces placeholders with actual script and style tags, and writes the final index.html file.\n   - It handles host-specific URLs and conditional processing based on flags like `no_debug`.\n\n5. **Synchronizing Application**:\n   - The `synchronise` method orchestrates the entire process by normalizing folder paths, creating the index file, and optionally compressing files.\n\n6. **Main Execution**:\n   - The script checks if it should run in a specific mode (e.g., combining themes) or perform a full synchronization.\n   - It measures the execution time and prints it at the end.\n\n### Key Points to Note:\n\n- **Error Handling**: The script does not include explicit error handling. For production use, consider adding try-except blocks around file operations and other critical sections.\n- **Configuration**: The script uses global variables like `BASE`, `JAVA_HOME`, etc., which should be defined elsewhere in the code or passed as command-line arguments for flexibility.\n- **Performance**: Compressing files can be resource-intensive. Consider running this process during off-peak hours or using a more efficient compression algorithm if performance is a concern.\n- **Security**: The script uses regular expressions to replace placeholders in HTML, which could potentially introduce security vulnerabilities if not handled carefully.\n\n### Example Usage:\n\nTo run the script and perform a full synchronization, you would typically execute it from the command line. If you want to combine themes instead, you can uncomment the relevant section of the `if __name__ == '__main__':` block.\n\n```python\nif __name__ == '__main__':\n    start = time()\n    sync = Sync()\n\n    # Uncomment the following line to run in theme combination mode\n    # sync.combine_themes(os.path.join(BASE, 'theme'))\n\n    else:\n        sync.synchronise()\n\n    end = time()\n    print(f'\\nELAPSED: {end-start:.3f} seconds')\n```\n\nThis script is a powerful tool for maintaining and optimizing web applications by automating the process of combining, minifying, and compressing resources."
    },
    {
        "code": "import unittest\nfrom unittest.mock import Mock, patch\n\nfrom yala.main import LinterRunner\n\n\nclass TestLinterRunner(unittest.TestCase):\n    \n\n    @patch('yala.main.Config')\n    def test_chosen_not_found(self, mock_config):\n        \n        \n        name = 'my linter'\n        mock_config.user_linters = [name]\n        _, stderr = self._path_and_run(mock_config, name)\n        self.assertIn('Did you install', stderr[0])\n\n    @patch('yala.main.Config')\n    def test_not_chosen_not_found(self, mock_config):\n        \n        \n        mock_config.user_linters = []\n        stdout, stderr = self._path_and_run(mock_config)\n        self.assertEqual(0, len(stdout))\n        self.assertEqual(0, len(stderr))\n\n    def _path_and_run(self, mock_config, name='my linter'):\n        cls = self._mock_linter_class(name)\n        mock_config.get_linter_classes.return_value = [cls]\n        with patch('yala.main.subprocess.run', side_effect=FileNotFoundError):\n            linter_cfg_tgts = cls, mock_config, []\n            return LinterRunner.run(linter_cfg_tgts)\n\n    @staticmethod\n    def _mock_linter_class(name):\n        linter_class = Mock()\n        linter = linter_class.return_value\n        linter.command_with_options = linter.name = name\n        return linter_class\n",
        "summary": "The provided Python code defines a test suite for the `LinterRunner` class from the `yala.main` module using the `unittest` framework and `unittest.mock`. It includes two test methods: one to verify that an error message is displayed when a chosen linter is not found, and another to ensure no output is generated when no linters are selected. The tests utilize mocking to simulate the behavior of the `Config` class and the `subprocess.run` method, ensuring that the `LinterRunner.run` method behaves as expected under different conditions."
    },
    {
        "code": "from __future__ import annotations\n\nimport homeassistant.helpers.config_validation as cv\nimport requests\nimport voluptuous as vol\nfrom homeassistant.components.sensor import SensorEntity, PLATFORM_SCHEMA, SensorStateClass, SensorDeviceClass\nfrom homeassistant.const import CONF_USERNAME, CONF_PASSWORD, CONF_API_TOKEN\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.helpers.typing import ConfigType, DiscoveryInfoType\nfrom requests.auth import HTTPBasicAuth\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Required(CONF_USERNAME): cv.string,\n    vol.Required(CONF_PASSWORD): cv.string,\n    vol.Required(CONF_API_TOKEN): cv.string,\n})\n\n\ndef setup_platform(\n        hass: HomeAssistant,\n        config: ConfigType,\n        add_entities: AddEntitiesCallback,\n        discovery_info: DiscoveryInfoType | None = None\n) -> None:\n    \n    url = \"https://secure.kontomierz.pl/k4/user_accounts.json?api_key=\" + config.get(CONF_API_TOKEN)\n    payload = {}\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n    }\n    response = requests.get(url, auth=HTTPBasicAuth(config.get(CONF_USERNAME), config.get(CONF_PASSWORD)),\n                            headers=headers, data=payload)\n    response_json = response.json()\n    for x in response_json:\n        account = x.get('user_account')\n        add_entities(\n            [KontomierzSensor(hass, config, account.get('bank_name') + \" - \" + account.get('display_name'),\n                              account.get('iban'))])\n\n\nclass KontomierzSensor(SensorEntity):\n    \n\n    def __init__(self, hass, config: dict, entity_name: string, iban: string) -> None:\n        self._attr_device_class = SensorDeviceClass.MONETARY\n        self._attr_state_class = SensorStateClass.MEASUREMENT\n        self._state = None\n        self.hass = hass\n        self.username = config.get(CONF_USERNAME)\n        self.password = config.get(CONF_PASSWORD)\n        self.apiToken = config.get(CONF_API_TOKEN)\n        self.entity_name = entity_name\n        self.iban = iban\n\n    @property\n    def unique_id(self) -> str | None:\n        return \"kontomierz_sensor\" + self.entity_name\n\n    @property\n    def name(self) -> str:\n        return self.entity_name\n\n    @property\n    def state(self):\n        \n        return self._state\n\n    def update(self) -> None:\n        \n\n        url = \"https://secure.kontomierz.pl/k4/user_accounts.json?api_key=\" + self.apiToken\n\n        response = requests.get(url, auth=HTTPBasicAuth(self.username, self.password), headers={\n            'Content-Type': 'application/json',\n            'Accept': 'application/json',\n        }, data={})\n        response_json = response.json()\n        result = 0.0\n        for x in response_json:\n            user_account = x.get('user_account')\n            if self.iban == user_account.get('iban'):\n                result = float(user_account.get('balance'))\n                self._attr_native_unit_of_measurement = user_account.get('currency_name')\n\n        self._state = result\n",
        "summary": "The provided Python code defines a custom sensor platform for Home Assistant that integrates with the Kontomierz banking API. It fetches account balances and updates sensor states accordingly, using configuration parameters for authentication and account details."
    },
    {
        "code": "from kivy.clock import Clock\nfrom kivy.animation import Animation\nfrom kivy.core.window import Window\nfrom kivy.metrics import dp, sp\nfrom kivy.properties import ObjectProperty, StringProperty, ListProperty\nfrom kivy.lang import Builder\nfrom kivy.uix.boxlayout import BoxLayout\nfrom kivy.uix.floatlayout import FloatLayout\nfrom kivy.uix.modalview import ModalView\n\nfrom kivymd.uix.behaviors import SpecificBackgroundColorBehavior\nfrom kivymd.uix.button import MDIconButton\nfrom kivymd.theming import ThemableBehavior\n\nBuilder.load_string(\n    \n)\n\n\nclass MDUserAnimationCard(ThemableBehavior, ModalView):\n    user_name = StringProperty()\n    path_to_avatar = StringProperty()\n    box_content = ObjectProperty()\n    callback = ObjectProperty()\n    _anim_bottom = True\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._primary_color = self.theme_cls.primary_color\n        self._primary_color[3] = 0\n        self.user_animation_card = UserAnimationCard(\n            user_name=self.user_name,\n            path_to_avatar=self.path_to_avatar,\n            _callback_back=self._callback_back,\n            _primary_color=self._primary_color,\n        )\n        self.user_animation_card.ids.user_name.pos = (\n            dp(15),\n            Window.height - self.user_animation_card.ids.image.height,\n        )\n        self.box_content = self.user_animation_card.ids.box_content\n        self.add_widget(self.user_animation_card)\n\n        self._obj_avatar = self.user_animation_card.ids.image\n        self._obj_user_name = self.user_animation_card.ids.user_name\n        self._obj_toolbar = self.user_animation_card.ids.toolbar\n        self._obj_scroll = self.user_animation_card.ids.scroll\n        self._set_current_pos_objects()\n\n    def _callback_back(self):\n        self.dismiss()\n        if self.callback:\n            self.callback()\n\n    def on_open(self):\n        self._primary_color = self.theme_cls.primary_color\n        self._primary_color[3] = 0\n        self.user_animation_card._primary_color = self._primary_color\n\n    def _set_current_pos_objects(self):\n        self._avatar_y = self._obj_avatar.y\n        self._toolbar_y = self._obj_toolbar.y\n        self._user_name_y = self._obj_user_name.y\n        self._scroll_y = self._obj_scroll.y\n\n    def on_touch_move(self, touch):\n        if touch.ud[\"swipe_begin\"] < touch.y:\n            if self._anim_bottom:\n                self._anim_bottom = False\n                self.animation_to_top()\n        else:\n            if not self._anim_bottom:\n                self._anim_bottom = True\n                self.animation_to_bottom()\n\n    def on_touch_down(self, touch):\n        touch.ud[\"swipe_begin\"] = touch.y\n        return super().on_touch_down(touch)\n\n    def on_touch_up(self, touch):\n        touch.ud[\"swipe_begin\"] = 0\n\n    def animation_to_bottom(self):\n        Animation(y=self._scroll_y, d=0.4, t=\"in_out_cubic\").start(\n            self._obj_scroll\n        )\n        Animation(y=self._user_name_y, d=0.5, x=dp(15), t=\"in_out_cubic\").start(\n            self._obj_user_name\n        )\n        Animation(font_size=sp(36), d=0.3, t=\"in_out_cubic\").start(\n            self._obj_user_name\n        )\n        Animation(_primary_color=[0, 0, 0, 0], d=0.3, t=\"in_out_cubic\").start(\n            self.user_animation_card\n        )\n        Animation(y=self._avatar_y, d=0.4, t=\"in_out_cubic\").start(\n            self._obj_avatar\n        )\n\n    def animation_to_top(self):\n        user_name_y = (\n            Window.height\n            - self._obj_toolbar.height\n            + (self.theme_cls.standard_increment // 2 - dp(12))\n        )\n        user_name_x = self.theme_cls.horizontal_margins + dp(12) * 5\n\n        Animation(y=-self._obj_toolbar.height, d=0.4, t=\"in_out_cubic\").start(\n            self._obj_scroll\n        )\n        Animation(y=user_name_y, d=0.3, x=user_name_x, t=\"in_out_cubic\").start(\n            self._obj_user_name\n        )\n        Animation(font_size=sp(20), d=0.3, t=\"in_out_cubic\").start(\n            self._obj_user_name\n        )\n        Animation(\n            _primary_color=self.theme_cls.primary_color, d=0.3, t=\"in_out_cubic\"\n        ).start(self.user_animation_card)\n        Animation(y=self._obj_avatar.y + 30, d=0.4, t=\"in_out_cubic\").start(\n            self._obj_avatar\n        )\n\n\nclass UserAnimationCard(ThemableBehavior, FloatLayout):\n    user_name = StringProperty()\n    path_to_avatar = StringProperty()\n    _callback_back = ObjectProperty()\n    _primary_color = ListProperty()\n\n\nclass ModifiedToolbar(\n    ThemableBehavior, SpecificBackgroundColorBehavior, BoxLayout\n):\n    left_action_items = ListProperty()\n    title = StringProperty()\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.bind(specific_text_color=self.update_action_bar_text_colors)\n        Clock.schedule_once(\n            lambda x: self.on_left_action_items(0, self.left_action_items)\n        )\n\n    def on_left_action_items(self, instance, value):\n        self.update_action_bar(self.ids[\"left_actions\"], value)\n\n    def update_action_bar(self, action_bar, action_bar_items):\n        action_bar.clear_widgets()\n        new_width = 0\n        for item in action_bar_items:\n            new_width += dp(48)\n            action_bar.add_widget(\n                MDIconButton(\n                    icon=item[0],\n                    on_release=item[1],\n                    opposite_colors=True,\n                    text_color=self.specific_text_color,\n                    theme_text_color=\"Custom\",\n                )\n            )\n        action_bar.width = new_width\n\n    def update_action_bar_text_colors(self, instance, value):\n        for child in self.ids[\"left_actions\"].children:\n            child.text_color = self.specific_text_color\n",
        "summary": "The provided Python code defines a custom KivyMD widget `MDUserAnimationCard` that animates the display of user information with swipe gestures. It includes nested classes and methods for handling animations, touch events, and updating UI elements based on theme settings. The `ModifiedToolbar` class is also defined to customize the appearance and behavior of toolbars within the application."
    },
    {
        "code": "from marshmallow import Schema, fields, validate\n\n\nfields.Email.default_error_messages['required'] = 'Email jest wymagany'\nfields.Email.default_error_messages['invalid'] = 'Niepoprawny adres email'\n\n\nclass VUser(Schema):\n    \n    nick = fields.String(\n        required=True, validate=validate.Length(min=4, max=30, error='Login musi mie\u0107 4 - 30 znak\u00f3w'))\n    email = fields.Email(required=True)\n    password = fields.String(\n        required=True, validate=validate.Length(min=8, max=30, error='Has\u0142o musi mie\u0107 8 - 30 znakow'))\n\n\nclass VUserLogin(Schema):\n    \n    email = fields.Email(required=True)\n    password = fields.String(\n        required=True, validate=validate.Length(min=8, max=30, error='Has\u0142o jest wymagane'))\n\n\nclass VEmail(Schema):\n    \n    email = fields.Email(required=True)\n\n\nclass VUserPatch(Schema):\n    \n    field = fields.String(required=True, validate=validate.OneOf(['nick']))\n    value = fields.String(required=True)\n\n\nclass VEntry(Schema):\n    \n    value = fields.Number(required=True)\n    description = fields.String()\n\n\nclass VDiary(Schema):\n    \n    name = fields.String(required=True)\n    max = fields.Number(required=True)\n    date = fields.Number()\n    color = fields.String(validate=validate.Regexp(\"\n    entries = fields.List(fields.Nested(VEntry), required=True)\n\n\nclass VJson(Schema):\n    \n    diaries = fields.List(fields.Nested(VDiary))\n\n\nclass VDiaryIndex(Schema):\n    \n    index = fields.Integer(required=True)\n",
        "summary": "The provided Python code defines several Marshmallow schemas for validating different types of data, including user information, email addresses, and diary entries. Each schema specifies the required fields, their data types, and validation rules such as length constraints and format checks, with custom error messages for invalid inputs."
    },
    {
        "code": "class Option:\n\n    def __init__(self, option_info):\n        self.option_info = option_info\n        self.flag = option_info['flag']\n\n    def mkdir(self):\n        if self.flag == False:\n            return False\n        return self.option_info['mkdir']\n\n    def dir_name(self, problem):\n        if self.flag == False:\n            return ''\n        if not self.mkdir():\n            return ''\n        return self.replace_name(self.option_info['dir_name'], problem) + '/'\n\n    def source_name(self, problem):\n        if self.flag == False:\n            return problem['problem_id']\n        return self.replace_info(self.option_info['source_name'], problem)\n\n    def replace_name(self, value, problem):\n        value = value.replace('[NO]', problem['problem_id'])\n        value = value.replace('[TITLE]', problem['problem_title'])\n        return value\n\n    def get_ext(self, language):\n        extensions = {\n            'C': '.c',\n            'C++': '.cpp',\n            'C++11': '.cpp',\n            'C++14': '.cpp',\n            'C++17': '.cpp',\n            'Java': '.java',\n            'Java (OpenJDK)': '.java',\n            'C11': '.c',\n            'Python 2': '.py',\n            'Python 3': '.py',\n            'PyPy2': '.py',\n            'PyPy3': '.py',\n            'Ruby2.5': '.rb',\n            'Kotlin': '.kt',\n            'Swift': '.swift',\n            'C\n            'Text': '.txt',\n            'node.js': 'js',\n            'Go': '.go',\n            'F\n            'PHP': '.php',\n            'Pascal': '.pas',\n            'Lua': '.lua',\n            'Perl': '.pl',\n            'Objective-C': '.m',\n            'Objective-C++': '.mm',\n            'C (Clang)': '.c',\n            'C++11 (Clang)': '.cpp',\n            'C++14 (Clang)': '.cpp',\n            'C++17 (Clang)': '.cpp',\n            'Golfscript': '.gs',\n            'Bash': '.sh',\n            'Fortran': '.f95',\n            'Scheme': '.scm',\n            'Ada': '.ada',\n            'awk': '.awk',\n            'OCaml': '.ml',\n            'Brainfuck': '.bf',\n            'Whitespace': '.ws',\n            'Tcl': '.tcl',\n            'Assembly (32bit)': '.asm',\n            'Assembly (32bit)': '.asm',\n            'D': '.d',\n            'Clojure': '.clj',\n            'Rhino': '.js',\n            'Cobol': '.cob',\n            'SpiderMonkey': '.js',\n            'Pike': '.pike',\n            'sed': '.sed',\n            'Rust': '.rs',\n            'Boo': '.boo',\n            'Intercal': '.i',\n            'bc': '.bc',\n            'Nemerle': '.n',\n            'Cobra': '.cobra',\n            'Algol 68': '.a68',\n            'Befunge': '.bf',\n            'Haxe': '.hx',\n            'LOLCODE': '.lol',\n            'VB.NET 4.0': '.vb',\n            '\uc544\ud76c': '.aheui'\n        }\n        \n        if not language in extensions:\n            return True, 'Unknown extension'\n        return False, extensions[language]\n",
        "summary": "The `Option` class manages configuration options for a problem-solving system, providing methods to determine directory and source file names based on the problem details and option settings. It also handles file extensions for various programming languages, returning appropriate extensions or error messages if the language is not supported."
    },
    {
        "code": "import copy\n\nimport torch.nn as nn\n\nfrom torch.quantization.fuser_method_mappings import get_fuser_method\n\nfrom torch.quantization.fuser_method_mappings import fuse_conv_bn  \nfrom torch.quantization.fuser_method_mappings import fuse_conv_bn_relu  \n\nfrom typing import List, Optional\n\n\ndef _get_module(model, submodule_key):\n    tokens = submodule_key.split('.')\n    cur_mod = model\n    for s in tokens:\n        cur_mod = getattr(cur_mod, s)\n    return cur_mod\n\n\ndef _set_module(model, submodule_key, module):\n    tokens = submodule_key.split('.')\n    sub_tokens = tokens[:-1]\n    cur_mod = model\n    for s in sub_tokens:\n        cur_mod = getattr(cur_mod, s)\n\n    setattr(cur_mod, tokens[-1], module)\n\ndef fuse_known_modules(mod_list, additional_fuser_method_mapping=None):\n    r\n    types = tuple(type(m) for m in mod_list)\n    fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n    if fuser_method is None:\n        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n    new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n    fused = fuser_method(*mod_list)\n    \n    \n    for handle_id, pre_hook_fn in mod_list[0]._forward_pre_hooks.items():\n        fused.register_forward_pre_hook(pre_hook_fn)\n        del mod_list[0]._forward_pre_hooks[handle_id]\n    \n    for handle_id, hook_fn in mod_list[-1]._forward_hooks.items():\n        fused.register_forward_hook(hook_fn)\n        del mod_list[-1]._forward_hooks[handle_id]\n    new_mod[0] = fused\n\n    for i in range(1, len(mod_list)):\n        identity = nn.Identity()\n        identity.training = mod_list[0].training\n        new_mod[i] = identity\n\n    return new_mod\n\ndef _fuse_modules(model, modules_to_fuse, fuser_func=fuse_known_modules, fuse_custom_config_dict=None):\n    if fuse_custom_config_dict is None:\n        fuse_custom_config_dict = {}\n    additional_fuser_method_mapping = fuse_custom_config_dict.get(\"additional_fuser_method_mapping\", {})\n    mod_list = []\n    for item in modules_to_fuse:\n        mod_list.append(_get_module(model, item))\n\n    \n    new_mod_list = fuser_func(mod_list, additional_fuser_method_mapping)\n\n    \n    for i, item in enumerate(modules_to_fuse):\n        _set_module(model, item, new_mod_list[i])\n\ndef fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=fuse_known_modules, fuse_custom_config_dict=None):\n    r\n    if not inplace:\n        model = copy.deepcopy(model)\n\n    if all(isinstance(module_element, str) for module_element in modules_to_fuse):\n        \n        _fuse_modules(model, modules_to_fuse, fuser_func, fuse_custom_config_dict)\n    else:\n        \n        for module_list in modules_to_fuse:\n            _fuse_modules(model, module_list, fuser_func, fuse_custom_config_dict)\n    return model\n",
        "summary": "The provided Python code defines functions to fuse known modules in a PyTorch model, allowing for optimizations such as combining convolution and batch normalization layers. It includes utility functions to retrieve and set submodules, as well as the main function `fuse_modules` which can be used to apply these fusions either in-place or on a copy of the model, with optional custom configurations for fusion methods."
    },
    {
        "code": "__all__ = [\n    \"ripemodel\",\n    \"ems\",\n    \"rspace\",\n    \"sharedata\",\n    \"debug\",\n    \"powerlawp5\",\n    \"powerlaw2\",\n    \"powerlaw3\",\n    \"powerlaw4\",\n    \"avrami2\",\n    \"avrami3\",\n    \"avrami4\",\n    \"avrami5\",\n    \"randomnuc\",\n    \"ptompkins\",\n    \"jander\",\n    \"antijander\",\n    \"valensi\",\n    \"parabolic\",\n    \"gb3d\",\n    \"zlt\",\n    \"grain\",\n    \n    \"massact\",  \n    \"massactm\",\n    \"getmechs\",\n]\n\nfrom .main import ripemodel, ripewrite, print_results  \nfrom .shared import rspace, sharedata, debug  \nfrom .atermconstruct import (\n    makeaterm,\n    formatinputs,\n    checkargs,\n    normalizefeatures,\n)  \nfrom .kinforms import lin, linjac, arr, arrjac, refarr, refarrjac  \nfrom .mechs import (\n    powerlawp5,\n    powerlaw2,\n    powerlaw3,\n    powerlaw4,\n    avrami2,\n    avrami3,\n    avrami4,\n    avrami5,\n    randomnuc,\n    ptompkins,\n    jander,\n    antijander,\n    valensi,\n    parabolic,\n    gb3d,\n    zlt,\n    grain,\n    getmechs,\n    massactm,\n)  \nfrom .genpyomo import ripeomo  \nfrom .targets import (\n    doalamo,\n    dopwalamo,\n    gentargets,\n    sstargets,\n    dynamictargets,\n)  \nfrom .confinv import confinv  \nfrom .emsampling import constructmodel, ems  \nfrom .checkoptions import checkoptions  \nfrom .bounds import stoich_cons, count_neg, get_bounds  \n",
        "summary": "The Python code defines a collection of modules and functions related to materials science and engineering simulations, including models for ripening, microstructure evolution, and mechanical properties. It provides tools for constructing thermodynamic models, performing kinetic analysis, and optimizing material parameters."
    },
    {
        "code": "from collections import OrderedDict\nimport unittest\n\nfrom homeassistant import helpers\n\nfrom tests.common import get_test_home_assistant\n\n\nclass TestHelpers(unittest.TestCase):\n    \n\n    \n    def setUp(self):\n        \n        self.hass = get_test_home_assistant()\n\n    \n    def tearDown(self):\n        \n        self.hass.stop()\n\n    def test_extract_domain_configs(self):\n        \n        config = {\n            'zone': None,\n            'zoner': None,\n            'zone ': None,\n            'zone Hallo': None,\n            'zone 100': None,\n        }\n\n        self.assertEqual(set(['zone', 'zone Hallo', 'zone 100']),\n                         set(helpers.extract_domain_configs(config, 'zone')))\n\n    def test_config_per_platform(self):\n        \n        config = OrderedDict([\n            ('zone', {'platform': 'hello'}),\n            ('zoner', None),\n            ('zone Hallo', [1, {'platform': 'hello 2'}]),\n            ('zone 100', None),\n        ])\n\n        assert [\n            ('hello', config['zone']),\n            (None, 1),\n            ('hello 2', config['zone Hallo'][1]),\n        ] == list(helpers.config_per_platform(config, 'zone'))\n",
        "summary": "The provided Python code defines a test class `TestHelpers` that extends `unittest.TestCase` to test functions from the `homeassistant.helpers` module. It includes methods to set up and tear down a test environment using `get_test_home_assistant`, and tests two specific helper functions: `extract_domain_configs` and `config_per_platform`. The first method checks if it correctly extracts domain configurations, while the second verifies that it properly handles platform-specific configurations within nested structures."
    },
    {
        "code": "import pandas as pd\nimport numpy as np\n\n\ndef top_time(ind=None, gs=None):\n\t\n\taggregated = []\n\tfor tstamp, g in gs:  \n\t\tif len(g) > 1:  \n\t\t\tdiff_places = (g['geometry'].shift(-1) != g['geometry']).iloc[:-1]  \n\t\t\tif diff_places.any():  \n\t\t\t\tg_res = g.reset_index()  \n\t\t\t\tdiffs = g_res.shift(-1)['datetime'] - g_res['datetime']  \n\t\t\t\tjoined_dfs = g_res.join(diffs, rsuffix='a')  \n\t\t\t\tjoined_dfs['geometry'] = g_res['geometry'].astype(str)  \n\t\t\t\tpoint_max = joined_dfs.groupby('geometry')['datetimea'].sum().idxmax()  \n\t\t\t\tselected = g[g['geometry'].astype(str) == point_max]  \n\t\t\telse:\n\t\t\t\tselected = g  \n\t\telse:\n\t\t\tselected = g\n\t\taggregated.append(selected)\n\tif ind is None:\n\t\treturn pd.concat(aggregated)\n\telse:\n\t\treturn ind, pd.concat(aggregated)\n\n\ndef mode_geoseries(ind, gs):\n\t\n\taggregated = []\n\tfor g in gs:\n\t\tif g[1].empty:\n\t\t\taggregated.append(None)\n\t\telse:\n\t\t\tselected = g[1].mode()\n\t\t\tselected = selected.set_index(g[1].index)\n\t\t\taggregated.append(selected)\n\treturn ind, pd.concat(aggregated)\n\n\ndef rowwise_average(gs, row_count=None):\n\t\n\tif row_count is None:\n\t\trow_count = gs.groupby(level=0).size().max()\n\treturn pd.Series([gs.groupby(level=0).nth(n).mean() for n in range(row_count)])\n\n\ndef groupwise_average(gs):\n\t\n\treturn gs.groupby(level=0).mean()\n\n\ndef groupwise_normalise(gs):\n\t\n\treturn gs.groupby(level=0).apply(lambda x: x / x.sum())\n\n\ndef groupwise_expansion(gs):\n\t\n\treturn gs.groupby(level=0).expanding().mean()\n\n\ndef total_normalise(gs):\n\t\n\treturn gs / gs.sum()\n\n\ndef start_end(trajectories_frame):\n\t\n\tto_concat = []\n\tif 'date' not in trajectories_frame.columns:\n\t\ttrajectories_frame['date'] = trajectories_frame.index.get_level_values(1)\n\tfor gs in trajectories_frame.groupby(level=0):\n\t\tfirsts = gs[1][gs[1]['geometry'].shift() != gs[1]['geometry']]\n\t\tlasts = gs[1][gs[1]['geometry'].shift(-1) != gs[1]['geometry']]\n\t\tfirsts.loc[:, 'start'] = firsts['date']\n\t\tlasts = lasts.set_index(firsts.index)\n\t\tfirsts.loc[:, 'end'] = lasts['date']\n\t\tfirsts = firsts[firsts['start'] != firsts['end']]\n\t\tto_concat.append(firsts)\n\treturn pd.concat(to_concat)\n",
        "summary": "The provided Python code defines several functions for processing and analyzing geographic data using pandas and numpy. These functions include operations like aggregating time series data, calculating mode values in geoseries, performing row-wise and group-wise averages, normalizing data, expanding means, and identifying start and end points of trajectories."
    },
    {
        "code": "import numpy as np \nnp.show_config()",
        "summary": "The provided Python code imports the NumPy library and then calls the `show_config()` function to display detailed information about the configuration of the NumPy installation, including details such as the version, BLAS/LAPACK libraries in use, and other relevant system settings."
    },
    {
        "code": "from PIL import Image as im \nimport numpy as np\nfrom io import BytesIO\nimport csv\n\nclass outputResponse():\n    def __init__(self,reponse):\n        self.response = reponse\n\n    def retrieveResult(response, returntype):\n        if (returntype == \"image/png\" or returntype == \"image/jpeg\"):\n            img_arr = np.array(im.open(BytesIO(response.content)))\n            data = im.fromarray(img_arr) \n            data.show() \n\n        elif (returntype == \"text/csv\"):\n            response = response.content.decode('utf-8')\n            my_list = response.split (\",\")\n            with open ('x.csv', 'w') as file:\n                writer = csv.writer(file, delimiter = ',')\n                writer.writerow(my_list)\n\n        elif (returntype == 1 or returntype == 0):\n            print(response.content)\n\n        else:\n            response = response.content.decode('utf-8')\n            print (response)\n",
        "summary": "The `outputResponse` class in Python processes different types of responses based on the specified return type. It can handle image data by displaying it, CSV data by writing it to a file, and binary data by printing its content. For other text-based responses, it decodes and prints them directly."
    },
    {
        "code": "import pytest\nimport os\n\nfrom machaon.types.file import TextFile\nfrom machaon.types.shell import Path\nfrom machaon.core.invocation import instant_return_test, instant_context\n\ndef test_construct(tmp_path):\n    FILEPATH = Path(__file__)\n    context = instant_context()\n    context.define_type(TextFile)\n    f = instant_return_test(context, FILEPATH, \"TextFile\").value\n    assert isinstance(f, TextFile)\n    assert isinstance(f.path(), Path)\n    assert f.pathstr == FILEPATH.get()\n\n    p = Path(tmp_path) / \"hello.txt\"\n    f = instant_return_test(context, p, \"TextFile\").value\n    f.set_encoding(\"utf-8\")\n    assert f.encoding() == \"utf-8\"\n    with f.open(\"w\"):\n        f.stream.write(\"HELLO\\n\")\n        f.stream.write(\"WORLD\")\n    assert f.text() == \"HELLO\\nWORLD\"\n\n\n",
        "summary": "The provided Python code uses the `pytest` framework to test the construction and manipulation of a `TextFile` object from the `machaon.types.file` module. It verifies that the file can be instantiated with different paths, set encoding, write text, and read it back correctly."
    },
    {
        "code": "import re\n\n\n\ndata1 = \"aaab\"\ndata2 = \"aaaba\"\npattern = r\"\\Aa+b\\Z\"\n\nmatch1 = re.match(pattern, data1)\nprint(match1)\nmatch2 = re.match(pattern, data2)\nprint(match2)\n\n\n\ndata = \"AaaA\\n\\raaaA\"\npattern = r\"^(a+)$\"\nmatch = re.match(pattern, data, re.I | re.M)\nprint(match)\nprint(match.group())\n\n\ndata = \"Pi = 3.14, exponent = 2.718\"\npattern = r\"(\\d+\\.\\d+)\"\nmatches = re.findall(pattern, data)\nprint(matches)\n\n\n\ndata = re.sub(pattern, r'<f>\\1</f>', data)\nprint(data)\n\n\nmatch = re.search(pattern, data)\nif match:\n    print(match.group())\n    print(float(match.group()))\n",
        "summary": "The provided Python code demonstrates the use of regular expressions for pattern matching and manipulation. It includes examples of using `re.match`, `re.findall`, and `re.sub` to search, extract, and replace substrings based on specified patterns. The code also showcases how to handle multiline strings and ignore case sensitivity in regex operations."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Rate',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('rate', models.DecimalField(null=True, verbose_name=b'Exchange rate', max_digits=8, decimal_places=4, blank=True)),\n                ('date', models.DateField(db_index=True)),\n                ('currency', models.CharField(default=b'USD', max_length=3, db_index=True, choices=[(b'CHF', b'CHF'), (b'EUR', b'EUR'), (b'GBP', b'GBP'), (b'USD', b'USD')])),\n            ],\n            options={\n                'ordering': ['-date', 'currency'],\n            },\n            bases=(models.Model,),\n        ),\n        migrations.AlterUniqueTogether(\n            name='rate',\n            unique_together=set([('date', 'currency')]),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration for creating a `Rate` model with fields for an ID, exchange rate, date, and currency. It also specifies that the model should be ordered by date in descending order and currency in ascending order, and enforces a unique constraint on the combination of date and currency."
    },
    {
        "code": "DEBUG = True\n\nALLOWED_HOSTS = ['*', ]\n",
        "summary": "The provided Python code sets DEBUG mode to True, enabling detailed error reporting during development, and allows all hosts to access the application by setting ALLOWED_HOSTS to an asterisk ('*')."
    },
    {
        "code": "import os\r\nimport requests\r\nimport time\r\nimport random\r\nfrom lxml import etree\r\n\r\n\r\nclass Spider(object):\r\n    def __init__(self, savePath, keyWord):\r\n        self.headers = {\r\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.104 Safari/537.36\",\r\n        }\r\n        self.keyWord = keyWord\r\n        self.filePath = (savePath + keyWord + '/')\r\n    def createFile(self):\r\n        filePath = self.filePath\r\n        if not os.path.exists(filePath):\r\n            os.makedirs(filePath)\r\n    def getPageNum(self):\r\n        \n        \n        total = \"\"\r\n        url = (\"https://alpha.wallhaven.cc/search?q={}&categories=111&purity=100&sorting=relevance&order=desc\").format(self.keyWord)\r\n        html = requests.get(url)\r\n        selector = etree.HTML(html.text)\r\n        pageInfo = selector.xpath('//header[@class=\"listing-header\"]/h1[1]/text()')\r\n        string = str(pageInfo[0])\r\n        numList = list(filter(str.isdigit, string)) \r\n        for item in numList:\r\n            total += item\r\n        totalPageNum = int(total)\r\n        return totalPageNum\r\n    \r\n    def main_func(self):\r\n        count = self.getPageNum()\r\n        print(\"We have found:{} images!\".format(count))\r\n\r\n\r\n\n\r\n\r\ns = Spider(\"/home/klm/work/spider/\", \"girl\")\r\nprint s.headers, s.filePath\r\ns.main_func()\r\n",
        "summary": "The provided Python code defines a class `Spider` that uses the `requests` library to fetch web pages and `lxml` for parsing HTML. It includes methods to create a directory for saving files, determine the total number of pages based on search results, and print the total number of images found for a given keyword. The script initializes an instance of `Spider` with a specified save path and keyword, then prints the user agent and file path before calling the main function to display the total number of images."
    },
    {
        "code": "from abc import ABC\n\n\nEVENT_LOG = 'eLog'                          \nEVENT_MARKETDATA = 'eMarketData'           \nEVENT_TRADE = 'eTrade'                      \nEVENT_BUY = 'eBuy'                          \nEVENT_SELL = 'eSell'                        \nEVENT_CANCEL = 'eCancel'                    \nEVENT_POSITION = 'ePosition'               \nEVENT_STATUS = 'eStatus'                   \nEVENT_ACCOUNT = 'eAccount'                 \nEVENT_PROFIT_CHANGED = 'eProfitChanged'    \n\n\nclass StrategyEvent:\n    def __init__(self, type_=None, even_param_=None):\n        self.type_ = type_\n        self.even_param_ = even_param_\n\n    def clear(self):\n        \n        self.even_param_.clear()\n\n\nclass EventEngine(ABC):\n    pass\n",
        "summary": "The provided Python code defines constants for various event types related to trading and market data, such as log events, trade events, buy/sell orders, and account status changes. It also includes a base class `StrategyEvent` with methods for initializing an event with a type and parameters, and clearing the parameters. The `EventEngine` class is defined as an abstract base class (ABC) to serve as a template for implementing specific event handling engines."
    },
    {
        "code": "import openerp\nfrom openerp import SUPERUSER_ID\nfrom openerp.osv import fields, osv\nfrom openerp.tools.translate import _\n\nclass sale_configuration(osv.osv_memory):\n    _inherit = 'sale.config.settings'\n\n    _columns = {\n        'group_invoice_deli_orders': fields.boolean('Generate invoices after and based on delivery orders',\n            implied_group='sale_stock.group_invoice_deli_orders',\n            help=\"To allow your salesman to make invoices for Delivery Orders using the menu 'Deliveries to Invoice'.\"),\n        'task_work': fields.boolean(\"Prepare invoices based on task's activities\",\n            help='Lets you transfer the entries under tasks defined for Project Management to '\n                 'the Timesheet line entries for particular date and particular user  with the effect of creating, editing and deleting either ways '\n                 'and to automatically creates project tasks from procurement lines.\\n'\n                 '-This installs the modules project_timesheet and sale_service.'),\n        'default_order_policy': fields.selection(\n            [('manual', 'Invoice based on sales orders'), ('picking', 'Invoice based on deliveries')],\n            'The default invoicing method is', default_model='sale.order',\n            help=\"You can generate invoices based on sales orders or based on shippings.\"),\n        'module_delivery': fields.boolean('Allow adding shipping costs',\n            help='Allows you to add delivery methods in sales orders and delivery orders.\\n'\n                 'You can define your own carrier and delivery grids for prices.\\n'\n                 '-This installs the module delivery.'),\n        'default_picking_policy' : fields.boolean(\"Deliver all at once when all products are available.\",\n            help = \"Sales order by default will be configured to deliver all products at once instead of delivering each product when it is available. This may have an impact on the shipping price.\"),\n        'group_mrp_properties': fields.boolean('Product properties on order lines',\n            implied_group='sale.group_mrp_properties',\n            help=\"Allows you to tag sales order lines with properties.\"),\n        'module_project_timesheet': fields.boolean(\"Project Timesheet\"),\n        'module_sale_service': fields.boolean(\"Sale Service\"),\n        'group_route_so_lines': fields.boolean('Choose MTO, drop shipping,... on sales order lines',\n            implied_group='sale_stock.group_route_so_lines',\n            help=\"Allows you to choose a delivery route on sales order lines\"),\n    }\n\n    _defaults = {\n        'default_order_policy': 'manual',\n    }\n\n    def default_get(self, cr, uid, fields, context=None):\n        res = super(sale_configuration, self).default_get(cr, uid, fields, context)\n        \n        res['task_work'] = res.get('module_sale_service') and res.get('module_project_timesheet')\n        return res\n\n    def get_default_sale_config(self, cr, uid, ids, context=None):\n        ir_values = self.pool.get('ir.values')\n        default_picking_policy = ir_values.get_default(cr, uid, 'sale.order', 'picking_policy')\n        return {\n            'default_picking_policy': default_picking_policy == 'one',\n        }\n\n    def set_sale_defaults(self, cr, uid, ids, context=None):\n        if uid != SUPERUSER_ID and not self.pool['res.users'].has_group(cr, uid, 'base.group_erp_manager'):\n            raise openerp.exceptions.AccessError(_(\"Only administrators can change the settings\"))\n        ir_values = self.pool.get('ir.values')\n        wizard = self.browse(cr, uid, ids)[0]\n\n        default_picking_policy = 'one' if wizard.default_picking_policy else 'direct'\n        ir_values.set_default(cr, SUPERUSER_ID, 'sale.order', 'picking_policy', default_picking_policy)\n        res = super(sale_configuration, self).set_sale_defaults(cr, uid, ids, context)\n        return res\n\n    def onchange_invoice_methods(self, cr, uid, ids, group_invoice_so_lines, group_invoice_deli_orders, context=None):\n        if not group_invoice_deli_orders:\n            return {'value': {'default_order_policy': 'manual'}}\n        if not group_invoice_so_lines:\n            return {'value': {'default_order_policy': 'picking'}}\n        return {}\n",
        "summary": "This Python code defines a custom module for OpenERP (now Odoo) that extends the sale configuration settings. It adds various options to control invoicing based on delivery orders, task activities, and picking policies, among other features. The module also includes methods to set default values and handle changes in configuration settings securely."
    },
    {
        "code": "import pytest\nimport io\nfrom cite_seq_count import preprocessing\n\n\n@pytest.fixture\ndef data():\n    from collections import OrderedDict\n    from itertools import islice\n    \n    \n    pytest.correct_whitelist_path = 'tests/test_data/whitelists/correct.csv'\n    pytest.correct_tags_path = 'tests/test_data/tags/correct.csv'\n    pytest.correct_R1_path = 'tests/test_data/fastq/correct_R1.fastq.gz'\n    pytest.correct_R2_path = 'tests/test_data/fastq/correct_R2.fastq.gz'\n    pytest.corrupt_R1_path = 'tests/test_data/fastq/corrupted_R1.fastq.gz'\n    pytest.corrupt_R2_path = 'tests/test_data/fastq/corrupted_R2.fastq.gz'\n    \n    \n    pytest.correct_whitelist = set(['ACTGTTTTATTGGCCT','TTCATAAGGTAGGGAT'])\n    pytest.correct_tags = {\n        'AGGACCATCCAA':'CITE_LEN_12_1',\n        'ACATGTTACCGT':'CITE_LEN_12_2',\n        'AGCTTACTATCC':'CITE_LEN_12_3',\n        'TCGATAATGCGAGTACAA':'CITE_LEN_18_1',\n        'GAGGCTGAGCTAGCTAGT':'CITE_LEN_18_2',\n        'GGCTGATGCTGACTGCTA':'CITE_LEN_18_3',\n        'TGTGACGTATTGCTAGCTAG':'CITE_LEN_20_1',\n        'ACTGTCTAACGGGTCAGTGC':'CITE_LEN_20_2',\n        'TATCACATCGGTGGATCCAT':'CITE_LEN_20_3'}\n    pytest.correct_ordered_tags = OrderedDict({\n        'TGTGACGTATTGCTAGCTAG':'CITE_LEN_20_1-TGTGACGTATTGCTAGCTAG',\n        'ACTGTCTAACGGGTCAGTGC':'CITE_LEN_20_2-ACTGTCTAACGGGTCAGTGC',\n        'TATCACATCGGTGGATCCAT':'CITE_LEN_20_3-TATCACATCGGTGGATCCAT',\n        'TCGATAATGCGAGTACAA':'CITE_LEN_18_1-TCGATAATGCGAGTACAA',\n        'GAGGCTGAGCTAGCTAGT':'CITE_LEN_18_2-GAGGCTGAGCTAGCTAGT',\n        'GGCTGATGCTGACTGCTA':'CITE_LEN_18_3-GGCTGATGCTGACTGCTA',\n        'AGGACCATCCAA':'CITE_LEN_12_1-AGGACCATCCAA',\n        'ACATGTTACCGT':'CITE_LEN_12_2-ACATGTTACCGT',\n        'AGCTTACTATCC':'CITE_LEN_12_3-AGCTTACTATCC'})\n    pytest.barcode_slice = slice(0, 16)\n    pytest.umi_slice = slice(16, 26)\n    pytest.barcode_umi_length = 26\n\n@pytest.mark.dependency()\ndef test_parse_whitelist_csv(data):\n    assert preprocessing.parse_whitelist_csv(pytest.correct_whitelist_path, 16, 1) == (pytest.correct_whitelist,1)\n\n@pytest.mark.dependency()\ndef test_parse_tags_csv(data):\n    assert preprocessing.parse_tags_csv(pytest.correct_tags_path) == pytest.correct_tags\n\n@pytest.mark.dependency(depends=['test_parse_tags_csv'])\ndef test_check_tags(data):\n    assert preprocessing.check_tags(pytest.correct_tags, 5) == pytest.correct_ordered_tags\n\n@pytest.mark.dependency(depends=['test_check_tags'])\ndef test_check_distance_too_big_between_tags(data):\n    with pytest.raises(SystemExit):\n        preprocessing.check_tags(pytest.correct_tags, 8)\n\n@pytest.mark.dependency(depends=['test_parse_whitelist_csv'])\ndef test_check_barcodes_lengths(data):\n    assert preprocessing.check_barcodes_lengths(26, 1, 16, 17, 26) == (pytest.barcode_slice, pytest.umi_slice, pytest.barcode_umi_length)\n\n@pytest.mark.dependency()\ndef test_get_n_lines(data):\n  assert preprocessing.get_n_lines(pytest.correct_R1_path) == (200 * 4)\n\n@pytest.mark.dependency(depends=['test_get_n_lines'])\ndef test_get_n_lines_not_multiple_of_4(data):\n  with pytest.raises(SystemExit):\n    preprocessing.get_n_lines(pytest.corrupt_R1_path)",
        "summary": "The provided Python code uses the `pytest` framework to define a fixture for test data and several tests that validate functions from the `preprocessing` module in the `cite_seq_count` package. The tests check functionalities such as parsing whitelist and tags CSV files, validating tag distances, ensuring barcode lengths, and counting lines in FASTQ files."
    },
    {
        "code": "from game_data import *\nfrom hosting import ServerHandler, ClientHandler\nimport json\n\nboard = [\n\t\t [\"R\", \"K\", \"B\", \"Q\", \"E\", \"B\", \"K\", \"R\"], \n\t\t [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"], \n\t\t [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"], \n\t\t [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"], \n\t\t [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"], \n\t\t [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"], \n\t\t [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"], \n\t\t [\"R\", \"K\", \"B\", \"Q\", \"E\", \"B\", \"K\", \"R\"]\n\t\t]\n\npieces = Initiator()\npos_handler = PositionHandler(pieces[0]+pieces[1])\np1 = Player(\"white\", pieces[0])\np2 = Player(\"black\", pieces[1])\nplayer_handler = PlayerHandler(p1, p2)\nend = False\nwin_team = None\ncheckmate = False\ntry:\n\ttry:\n\t\tnet = eval(input(\"Enter Server IP, Port to Host: \"))\n\texcept KeyboardInterrupt:\n\t\texit()\n\tif type(net[0]) == str and net[1] > 5000 and net[1] < 65000:\n\t\tserver = ServerHandler(*net)\n\t\tDisplayBoard(board)\n\t\twhile True:\n\t\t\terror_msg = \"\"\n\t\t\tif player_handler.current.team == \"white\":\n\t\t\t\tif checkmate:\n\t\t\t\t\terror_msg = \"You're in Checkmate\"\n\t\t\t\tprint(player_handler.current.give_pieces_position())\n\t\t\t\ttry:\n\t\t\t\t\tpiece_pos = eval(input(\"Position of Piece: \"))\n\t\t\t\t\tpiece_to_go = eval(input(\"Position To Go: \"))\n\t\t\t\texcept KeyboardInterrupt:\n\t\t\t\t\tbreak\n\t\t\t\tif PositionChecks(piece_pos) and PositionChecks(piece_to_go):\n\t\t\t\t\tpiece = pos_handler.get_piece(piece_pos)\n\t\t\t\t\tif piece == False or piece.team != player_handler.current.team:\n\t\t\t\t\t\terror_msg = \"Piece Position is Incorrect\"\n\t\t\t\t\telse:\n\t\t\t\t\t\tcheck, piece, n_board = player_handler.play_piece(piece, piece_to_go, board, pos_handler)\n\t\t\t\t\t\tif check:\n\t\t\t\t\t\t\tboard = n_board\n\t\t\t\t\t\t\tif piece != \" \":\n\t\t\t\t\t\t\t\tpieces[2].append(piece)\n\t\t\t\t\t\t\t\tplayer_handler.remove_piece(piece)\n\t\t\t\t\t\t\tpos_handler = PositionHandler(player_handler.player1.pieces + player_handler.player2.pieces)\n\t\t\t\t\t\t\tend, lose_player = player_handler.game_end()\n\t\t\t\t\t\t\tcheckmate = player_handler.checkmate(board, pos_handler)\n\t\t\t\t\t\t\tplayer_handler.change_player()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\terror_msg = \"Bad Position\"\n\t\t\t\telse:\n\t\t\t\t\terror_msg = \"Bad Position\"\n\t\t\t\tclear_screen()\n\t\t\t\tDisplayBoard(board)\n\t\t\t\tprint(error_msg)\n\t\t\t\tif end:\n\t\t\t\t\tbreak\n\t\t\t\t\twin_team = \"white\" if lose_player.team == \"black\" else \"black\"\n\t\t\telse:\n\t\t\t\tif checkmate:\n\t\t\t\t\tserver.send_state(server.encode_state(\"\", \"\", \"You're in Checkmate\"))\n\t\t\t\tserver.send_state(server.encode_state(board, player_handler.current.give_pieces_position(), \"\"))\n\t\t\t\tserver.send_state(\"input\")\n\t\t\t\tpos_data = server.recv_inputs()\n\t\t\t\ttry:\n\t\t\t\t\tpos_data = json.loads(pos_data)\n\t\t\t\t\tprint(pos_data)\n\t\t\t\t\tpiece_pos = tuple(pos_data[\"piece_pos\"])\n\t\t\t\t\tpiece_to_go = tuple(pos_data[\"piece_to_go\"])\n\t\t\t\t\tif PositionChecks(piece_pos) and PositionChecks(piece_to_go):\n\t\t\t\t\t\tpiece = pos_handler.get_piece(piece_pos)\n\t\t\t\t\t\tprint(piece)\n\t\t\t\t\t\tif piece == False or piece.team != player_handler.current.team:\n\t\t\t\t\t\t\tserver.send_state(server.encode_state(\"\", \"\", \"Piece Position is Incorrect\"))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tcheck, piece, n_board = player_handler.play_piece(piece, piece_to_go, board, pos_handler)\n\t\t\t\t\t\t\tif check:\n\t\t\t\t\t\t\t\tboard = n_board\n\t\t\t\t\t\t\t\tif piece != \" \":\n\t\t\t\t\t\t\t\t\tpieces[2].append(piece)\n\t\t\t\t\t\t\t\t\tplayer_handler.remove_piece(piece)\n\t\t\t\t\t\t\t\tpos_handler = PositionHandler(player_handler.player1.pieces + player_handler.player2.pieces)\n\t\t\t\t\t\t\t\tend, lose_player = player_handler.game_end()\n\t\t\t\t\t\t\t\tcheckmate = player_handler.checkmate(board, pos_handler)\n\t\t\t\t\t\t\t\tplayer_handler.change_player()\n\t\t\t\t\t\t\t\tserver.send_state(server.encode_state(board, \"\", \"\"))\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tserver.send_state(server.encode_state(\"\", \"\", \"Bad Position\"))\n\t\t\t\t\telse:\n\t\t\t\t\t\tserver.send_state(server.encode_state(\"\", \"\", \"Bad Position\"))\n\t\t\t\t\t\n\t\t\t\t\tif end:\n\t\t\t\t\t\twin_team = \"white\" if lose_player.team == \"black\" else \"black\"\n\t\t\t\t\t\tbreak\n\t\t\t\t\tclear_screen()\n\t\t\t\t\tDisplayBoard(board)\n\t\t\t\texcept json.decoder.JSONDecodeError:\n\t\t\t\t\tpass\n\t\tserver.send_state(server.encode_state(\"\", \"\", f\"{win_team} Won The Match\"))\n\t\tserver.close_conn(\"end\")\n\telse:\n\t\tprint(\"[-] IP/Port is not Correctly Specified as rules.\")\n\t\tprint(\"[-] Ip should be like \\\"127.0.0.1\\\" and Port Should be Between 5000 and 65000\")\n\t\tprint(\"[-] Enter both like this \\\"127.0.0.1\\\", 9999\")\n\t\tprint(\"[-] Do It Correctly Next Time Bitch :]\")\nexcept ConnectionResetError:\n\tprint(\"Client Disconnected\")\nexcept SyntaxError:\n\tserver.close_conn(\"end\")\n\tprint(\"Syntax Error\")",
        "summary": "The provided Python code is a chess game implementation that allows for both local and network play. It initializes the game board, handles player turns, checks for legal moves, manages piece positions, and determines the winner through checkmate or resignation. The game can be hosted locally or played over a network connection using IP and port specifications."
    },
    {
        "code": "from typing import Union, List\n\nfrom pydifact.api import EDISyntaxError, PluginMount\nfrom pydifact.control import Characters\n\n\nclass SegmentProvider(metaclass=PluginMount):\n    \n\n    def __str__(self):\n        \n\n    def validate(self) -> bool:\n        \n\n\nclass Segment(SegmentProvider):\n    \n\n    \n    __omitted__ = True\n\n    def __init__(self, tag: str, *elements: Union[str, List[str]]):\n        \n        self.tag = tag\n\n        \n        \n        \n        self.elements = list(elements)\n\n    def __str__(self) -> str:\n        \n        return \"'{tag}' EDI segment: {elements}\".format(\n            tag=self.tag, elements=str(self.elements)\n        )\n\n    def __repr__(self) -> str:\n        return \"{} segment: {}\".format(self.tag, str(self.elements))\n\n    def __eq__(self, other) -> bool:\n        \n        return (\n            isinstance(self, type(other))\n            and self.tag == other.tag\n            and list(self.elements) == list(other.elements)\n        )\n\n    def __getitem__(self, key):\n        return self.elements[key]\n\n    def __setitem__(self, key, value):\n        self.elements[key] = value\n\n    def validate(self) -> bool:\n        \n        \n\n        if not self.tag:\n            return False\n        return True\n\n\nclass EDIenergySegment(Segment):\n\n    def __init__(self, tag: str, *elements: Union[str, List[str]]):\n        super().__init__(tag, *elements)\n\n    def validate(self) -> bool:\n        if not super().validate():\n            return False\n        else:\n            \n            pass\n\nclass SegmentFactory:\n    \n\n    characters = None\n\n    @staticmethod\n    def create_segment(\n        name: str, *elements: Union[str, List[str]], validate: bool = True\n    ) -> Segment:\n        \n        if not SegmentFactory.characters:\n            SegmentFactory.characters = Characters()\n\n        \n        \n\n        if not name:\n            raise EDISyntaxError(\"The tag of a segment must not be empty.\")\n\n        if type(name) != str:\n            raise EDISyntaxError(\n                \"The tag name of a segment must be a str, but is a {}: {}\".format(\n                    type(name), name\n                )\n            )\n\n        if not name.isalnum():\n            raise EDISyntaxError(\n                \"Tag '{}': A tag name must only contain alphanumeric characters.\".format(\n                    name\n                )\n            )\n\n        for Plugin in SegmentProvider.plugins:\n            if getattr(Plugin, \"tag\", \"\") == name:\n                s = Plugin(name, *elements)\n                break\n        else:\n            \n            \n            s = Segment(name, *elements)\n\n        if validate:\n            if not s.validate():\n                raise EDISyntaxError(\n                    \"could not create '{}' Segment. Validation failed.\".format(name)\n                )\n\n        \n        return s\n",
        "summary": "The provided Python code defines a framework for handling EDI (Electronic Data Interchange) segments using classes and plugins. It includes a base `SegmentProvider` class with methods for validation, a generic `Segment` class that can be instantiated with tag and elements, an `EDIenergySegment` subclass tailored for specific EDI energy-related segments, and a `SegmentFactory` class responsible for creating and validating segments based on provided parameters and rules."
    },
    {
        "code": "import gc\nimport os\nimport time\nimport multiprocessing\n\nfrom common.clock import sec_since_boot  \nfrom selfdrive.hardware import PC, TICI\n\n\n\nDT_CTRL = 0.01  \nDT_MDL = 0.05  \nDT_TRML = 0.5  \n\n\nif TICI:\n  DT_DMON = 0.05\nelse:\n  DT_DMON = 0.1\n\n\nclass Priority:\n  \n  \n  \n  CTRL_LOW = 51 \n\n  \n  \n  CTRL_HIGH = 53\n\n\ndef set_realtime_priority(level):\n  if not PC:\n    os.sched_setscheduler(0, os.SCHED_FIFO, os.sched_param(level))\n\n\ndef set_core_affinity(core):\n  if not PC:\n    os.sched_setaffinity(0, [core,])\n\n\ndef config_realtime_process(core, priority):\n  gc.disable()\n  set_realtime_priority(priority)\n  set_core_affinity(core)\n\n\nclass Ratekeeper():\n  def __init__(self, rate, print_delay_threshold=0.):\n    \n    self._interval = 1. / rate\n    self._next_frame_time = sec_since_boot() + self._interval\n    self._print_delay_threshold = print_delay_threshold\n    self._frame = 0\n    self._remaining = 0\n    self._process_name = multiprocessing.current_process().name\n\n  @property\n  def frame(self):\n    return self._frame\n\n  @property\n  def remaining(self):\n    return self._remaining\n\n  \n  def keep_time(self):\n    lagged = self.monitor_time()\n    if self._remaining > 0:\n      time.sleep(self._remaining)\n    return lagged\n\n  \n  def monitor_time(self):\n    lagged = False\n    remaining = self._next_frame_time - sec_since_boot()\n    self._next_frame_time += self._interval\n    if self._print_delay_threshold is not None and remaining < -self._print_delay_threshold:\n      print(\"%s lagging by %.2f ms\" % (self._process_name, -remaining * 1000))\n      lagged = True\n    self._frame += 1\n    self._remaining = remaining\n    return lagged\n",
        "summary": "The provided Python code defines a system for managing real-time processes on hardware platforms like PC and TICI. It includes functions to set real-time priorities, core affinities, and a `Ratekeeper` class that helps in maintaining a consistent frame rate by monitoring and adjusting the timing of process execution."
    },
    {
        "code": "import os, json\nimport shutil, logging\n\nimport click\nfrom pyspark.sql.functions import lit, udf, explode, array, to_json\nfrom pyspark.sql.types import ArrayType, StringType, IntegerType, MapType, StructType, StructField\n\nfrom luna.common.CodeTimer import CodeTimer\nfrom luna.common.config import ConfigSet\nfrom luna.common.custom_logger import init_logger\nfrom luna.common.sparksession import SparkConfig\nfrom luna.common.utils import get_absolute_path\nfrom luna.pathology.common.slideviewer_client import fetch_slide_ids\nimport luna.common.constants as const\n\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\n\n\ndef download_point_annotation(slideviewer_url, slideviewer_path, project_id, user):\n    \n    from slideviewer_client import download_sv_point_annotation\n\n    print (f\" >>>>>>> Processing [{slideviewer_path}] <<<<<<<<\")\n\n    url = slideviewer_url + \"/slides/\" + str(user) + \"@mskcc.org/projects;\" + \\\n          str(project_id) + ';' + slideviewer_path + \"/getSVGLabels/nucleus\"\n    print(url)\n\n    return download_sv_point_annotation(url)\n\n\n@click.command()\n@click.option('-d', '--data_config_file', default=None, type=click.Path(exists=True),\n              help=\"path to yaml file containing data input and output parameters. \"\n                   \"See data_config.yaml.template\")\n@click.option('-a', '--app_config_file', default='config.yaml', type=click.Path(exists=True),\n              help=\"path to yaml file containing application runtime parameters. \"\n                   \"See config.yaml.template\")\ndef cli(data_config_file, app_config_file):\n    \n    logger = init_logger()\n\n    with CodeTimer(logger, 'generate POINT_RAW_JSON table'):\n        logger.info('data config file: ' + data_config_file)\n        logger.info('app config file: ' + app_config_file)\n\n        \n        cfg = ConfigSet(name=const.DATA_CFG, config_file=data_config_file)\n        cfg = ConfigSet(name=const.APP_CFG,  config_file=app_config_file)\n\n        \n        config_location = const.CONFIG_LOCATION(cfg)\n        os.makedirs(config_location, exist_ok=True)\n\n        shutil.copy(app_config_file, os.path.join(config_location, \"app_config.yaml\"))\n        shutil.copy(data_config_file, os.path.join(config_location, \"data_config.yaml\"))\n        logger.info(\"config files copied to %s\", config_location)\n\n        create_proxy_table()\n\n\ndef create_proxy_table():\n    \n\n    cfg = ConfigSet()\n    logger = logging.getLogger(__name__)\n\n    spark = SparkConfig().spark_session(config_name=const.APP_CFG, app_name=\"luna.pathology.point_annotation.proxy_table.generate\")\n\n    \n    point_table_path = const.TABLE_LOCATION(cfg)\n\n    PROJECT_ID = cfg.get_value(path=const.DATA_CFG+'::PROJECT_ID')\n    SLIDEVIEWER_URL = cfg.get_value(path=const.DATA_CFG+'::SLIDEVIEWER_URL')\n\n    \n    \n    slides = fetch_slide_ids(SLIDEVIEWER_URL, PROJECT_ID, const.CONFIG_LOCATION(cfg),\n                             cfg.get_value(path=const.DATA_CFG+'::SLIDEVIEWER_CSV_FILE'))\n    logger.info(slides)\n\n    schema = StructType([StructField(\"slideviewer_path\", StringType()),\n                         StructField(\"slide_id\", StringType()),\n                         StructField(\"sv_project_id\", IntegerType())\n                         ])\n    df = spark.createDataFrame(slides, schema)\n    \n    df = df.withColumn(\"users\", array([lit(user) for user in cfg.get_value(const.DATA_CFG+'::USERS')]))\n    df = df.select(\"slideviewer_path\", \"slide_id\", \"sv_project_id\", explode(\"users\").alias(\"user\"))\n\n    \n    \n    \n    point_json_struct = ArrayType(\n        MapType(StringType(), StringType())\n    )\n    spark.sparkContext.addPyFile(get_absolute_path(__file__, \"../../common/slideviewer_client.py\"))\n    download_point_annotation_udf = udf(download_point_annotation,  point_json_struct)\n\n    df = df.withColumn(\"sv_json\",\n                       download_point_annotation_udf(lit(SLIDEVIEWER_URL), \"slideviewer_path\", \"sv_project_id\", \"user\"))\\\n        .cache()\n    \n    df = df.dropna(subset=[\"sv_json\"])\n\n    \n    spark.sparkContext.addPyFile(get_absolute_path(__file__, \"../../common/EnsureByteContext.py\"))\n    spark.sparkContext.addPyFile(get_absolute_path(__file__, \"../../common/utils.py\"))\n    from luna.common.utils import generate_uuid_dict\n    sv_json_record_uuid_udf = udf(generate_uuid_dict, StringType())\n\n    df = df.withColumn(\"sv_json_record_uuid\", sv_json_record_uuid_udf(to_json(\"sv_json\"), array(lit(\"SVPTJSON\"))))\n\n    df.show(10, False)\n    df.write.format(\"parquet\").mode(\"overwrite\").save(point_table_path)\n\n\nif __name__ == \"__main__\":\n    cli()\n",
        "summary": "The provided Python script is a command-line interface (CLI) tool that uses Apache Spark to process slide viewer data. It downloads point annotations from a specified URL, processes the data using Spark transformations, and saves the results in Parquet format. The script includes functions for downloading annotations, creating a proxy table, and handling configuration files."
    },
    {
        "code": "import os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport random\nimport math\nimport argparse\n\nsys.path.append('./')\nfrom commons import cnn_bi_lstm_model, input_iterator\n\n\ntf.random.set_random_seed(2019)\nrandom.seed(2019)\nnp.random.seed(2019)\n\ndef get_train_ops(y, logits, learning_rate, n_classes, class_weights):\n    y = tf.reshape(y, [-1])\n    logits = tf.reshape(logits, [-1, n_classes])\n    balanced_accuracy, update_op = tf.metrics.mean_per_class_accuracy(y, tf.argmax(logits, 1), n_classes)\n    y = tf.reshape(tf.one_hot(y, depth=n_classes, axis=1), [-1, n_classes])\n\n    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y) * tf.reduce_sum(tf.constant(class_weights, dtype=tf.float32) * y, axis=1))\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    train_op = optimizer.minimize(loss)\n\n    return train_op, update_op, balanced_accuracy, loss\n\n\ndef window_generator(data_root, win_size_10s, subject_ids):\n    x_segments = []; y_segments = []\n    for subject_id in subject_ids:\n        for x_seq, _, y_seq in input_iterator(data_root, subject_id, train=True):\n            x_window = []; y_window = []\n            for x,y in zip(x_seq, y_seq):\n                x_window.append(x)\n                y_window.append(y)\n\n                if len(y_window) == win_size_10s:\n                    yield np.stack(x_window, axis=0), np.stack(y_window, axis=0)\n                    x_window = []; y_window = []\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Argument parser for training CNN model.')\n    optional_arguments = parser._action_groups.pop()\n    required_arguments = parser.add_argument_group('required arguments')\n    required_arguments.add_argument('--pre-processed-dir', help='Pre-processed data directory', required=True)\n\n    optional_arguments.add_argument('--transfer-learning-model', help='Transfer learning model name (default: CHAP_ALL_ADULTS)', default=None, required=False, choices=['CHAP_ALL_ADULTS'])\n    optional_arguments.add_argument('--learning-rate', help='Learning rate for training the model (default: 0.0001)', default=1e-4, type=float, required=False)\n    optional_arguments.add_argument('--num-epochs', help='Number of epochs to train the model (default: 15)', default=15, type=int, required=False)\n    optional_arguments.add_argument('--batch-size', help='Training batch size (default: 16)', default=16, type=int, required=False)\n    \n    optional_arguments.add_argument('--amp-factor', help='Factor to increase the number of neurons in the CNN layers (default: 2)', default=2, type=int, required=False)\n    optional_arguments.add_argument('--cnn-window-size', help='CNN window size in seconds on which the predictions to be made (default: 10)', default=10, type=int, required=False)\n    optional_arguments.add_argument('--bi-lstm-window-size', help='BiLSTM window size in minutes on which the predictions to be smoothed (default: 7)', default=7, type=int, required=False)\n    \n    optional_arguments.add_argument('--shuffle-buffer-size', help='Training data shuffle buffer size in terms of number of records (default: 10000)', default=10000, type=int, required=False)\n    optional_arguments.add_argument('--training-data-fraction', help='Percentage of subjects to be used for training (default: 60)', default=60, type=int, required=False)\n    optional_arguments.add_argument('--validation-data-fraction', help='Percentage of subjects to be used for validation (default: 20)', default=20, type=int, required=False)\n    optional_arguments.add_argument('--testing-data-fraction', help='Percentage of subjects to be used for testing (default: 20)', default=20, type=int, required=False)\n    optional_arguments.add_argument('--model-checkpoint-path', help='Path where the trained model will be saved (default: ./model-checkpoint)', default='./model-checkpoint', required=False)\n    \n    optional_arguments.add_argument('--num-classes', help='Number of classes in the training dataset (default: 2)', default=2, type=int, required=False)\n    optional_arguments.add_argument('--class-weights', help='Class weights for loss aggregation (default: [1.0, 1.0])', default='[1.0, 1.0]', required=False)\n    optional_arguments.add_argument('--down-sample-frequency', help='Downsample frequency in Hz for GT3X data (default: 10)', default=10, type=int, required=False)\n    optional_arguments.add_argument('--silent', help='Whether to hide info messages', default=False, required=False, action='store_true')\n    parser._action_groups.append(optional_arguments)\n    args = parser.parse_args()\n\n    if os.path.exists(args.model_checkpoint_path):\n        raise Exception('Model checkpoint: {} already exists.'.format(args.model_checkpoint_path))\n\n    if args.transfer_learning_model:\n        if args.transfer_learning_model == 'CHAP_ALL_ADULTS':\n            args.amp_factor = 2\n            args.cnn_window_size = 10\n            args.bi_lstm_win_size = 7\n        else:\n            raise Exception('Unsupported transfer learning model: {}'.format(args.transfer_learning_model))\n    \n    assert (args.training_data_fraction + args.validation_data_fraction + args.testing_data_fraction) == 100, 'Train, validation,test split fractions should add up to 100%'\n    \n    subject_ids = [fname.split('.')[0] for fname in os.listdir(args.pre_processed_dir)]\n    random.shuffle(subject_ids)\n\n    n_train_subjects = int(math.ceil(len(subject_ids) * args.training_data_fraction / 100.))\n    train_subjects = subject_ids[:n_train_subjects]\n    subject_ids = subject_ids[n_train_subjects:]\n\n    test_frac = args.testing_data_fraction / (100.0 - args.training_data_fraction) * 100\n    n_test_subjects = int(math.ceil(len(subject_ids) * test_frac / 100.))\n    test_subjects = subject_ids[:n_test_subjects]\n    valid_subjects = subject_ids[n_test_subjects:]    \n\n    output_shapes = ((args.bi_lstm_window_size*(60//args.cnn_window_size), args.cnn_window_size*args.down_sample_frequency, 3), (args.bi_lstm_window_size*(60//args.cnn_window_size)))\n    bi_lstm_win_size = 60//args.down_sample_frequency * args.bi_lstm_window_size\n    train_dataset = tf.data.Dataset.from_generator(lambda: window_generator(args.pre_processed_dir, bi_lstm_win_size, train_subjects),output_types=(tf.float32, tf.int32),\n                output_shapes=output_shapes).shuffle(args.shuffle_buffer_size).batch(args.batch_size).prefetch(10)\n    valid_dataset = tf.data.Dataset.from_generator(lambda: window_generator(args.pre_processed_dir, bi_lstm_win_size, valid_subjects),output_types=(tf.float32, tf.int32),\n                output_shapes=output_shapes).batch(args.batch_size).prefetch(10)\n    test_dataset = tf.data.Dataset.from_generator(lambda: window_generator(args.pre_processed_dir, bi_lstm_win_size, test_subjects),output_types=(tf.float32, tf.int32),\n                output_shapes=output_shapes).batch(args.batch_size).prefetch(10)\n    \n    iterator =  tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n\n    train_init_op = iterator.make_initializer(train_dataset)\n    valid_init_op = iterator.make_initializer(valid_dataset)\n    test_init_op = iterator.make_initializer(test_dataset)\n    x, y = iterator.get_next()\n    \n    x = tf.reshape(x, [-1, args.cnn_window_size*args.down_sample_frequency, 3, 1])\n    x = tf.identity(x, name='input')\n    y = tf.reshape(y, [-1, bi_lstm_win_size])\n\n    learning_rate = tf.placeholder(tf.float32)\n    logits = cnn_bi_lstm_model(x, args.amp_factor, bi_lstm_win_size, args.num_classes)\n    output = tf.argmax(tf.reshape(logits, [-1, args.num_classes]), axis=1, name='output')\n    prediction = tf.identity(tf.argmax(logits, axis=1), name='prediction')\n\n    class_weights = eval(args.class_weights)    \n    train_op, update_op, balanced_accuracy, loss = get_train_ops(y, logits, learning_rate, args.num_classes, class_weights)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        if args.transfer_learning_model:\n            ckpt_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'pre-trained-models', '{}_CKPT'.format(args.transfer_learning_model), 'model')\n            \n            variables = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if not v.name.startswith('dense/')]\n            restorer = tf.train.Saver(variables)\n            restorer.restore(sess, ckpt_path)\n        \n        if not args.silent:\n            print('Training subjects: {}'.format(train_subjects))\n            print('Validation subjects: {}'.format(valid_subjects))\n            print('Testing subjects: {}'.format(test_subjects))\n\n        for epoch in range(args.num_epochs):\n            for label, init_op, subjects in zip([\"Train\", \"Validation\", \"Test\"],\n                [train_init_op, valid_init_op, test_init_op], [train_subjects, valid_subjects, test_subjects]):\n                sess.run(tf.local_variables_initializer())\n                sess.run(init_op)\n                losses = []\n                while True:\n                    try:\n                        if label == \"Train\":\n                            _, _, l = sess.run([train_op, update_op, loss], feed_dict={learning_rate: args.learning_rate})\n                        elif label == \"Validation\":\n                            _, l = sess.run([update_op, loss])\n                        elif label == \"Test\":\n                            _, l = sess.run([update_op, loss])\n                        losses.append(l)\n                    except tf.errors.OutOfRangeError:\n                        if not args.silent:\n                            ba = sess.run(balanced_accuracy)\n                            print(\"Epoch: %d, %s Loss: %f, Balanced Accuracy: %f\" %(epoch, label, sum(losses), ba))\n                        break\n\n        if not os.path.exists(args.model_checkpoint_path):\n            os.makedirs(args.model_checkpoint_path)\n\n        tf.saved_model.simple_save(sess, os.path.join(args.model_checkpoint_path, 'CUSTOM_MODEL'), inputs={\"input\": x}, outputs={\"output\": output})\n\n        if not args.silent:\n            print('Model saved in path: {}'.format(args.model_checkpoint_path))   \n",
        "summary": "This code is a TensorFlow implementation of a CNN-BiLSTM model for time series classification. It includes data preprocessing, model architecture definition, training loop, and evaluation metrics.\n\nHere's a breakdown of the key components:\n\n1. **Data Preprocessing**:\n   - The `window_generator` function generates windows of data from the pre-processed files.\n   - The dataset is split into training, validation, and test sets based on subject IDs.\n\n2. **Model Architecture**:\n   - The `cnn_bi_lstm_model` function defines a CNN followed by a BiLSTM layer for sequence modeling.\n   - The model architecture includes convolutional layers, pooling layers, LSTM layers, and fully connected layers.\n\n3. **Training Loop**:\n   - The training loop runs for the specified number of epochs.\n   - It initializes the appropriate dataset iterator based on whether it's training, validating, or testing.\n   - During each epoch, it iterates through the dataset, running the training operation (for training) and updating the model parameters.\n\n4. **Evaluation Metrics**:\n   - The `balanced_accuracy` metric is used to evaluate the model's performance.\n   - It calculates the average accuracy across all classes, which is useful for imbalanced datasets.\n\n5. **Saving the Model**:\n   - After training, the model is saved using TensorFlow's SavedModel format.\n   - The model can be loaded later for inference or further training.\n\n6. **Placeholder and Session Management**:\n   - A placeholder `learning_rate` is used to dynamically set the learning rate during training.\n   - The session runs the necessary operations to initialize variables, restore pre-trained weights (if applicable), and perform the training loop.\n\nThis code provides a comprehensive framework for building and training a CNN-BiLSTM model for time series classification tasks. It handles data preprocessing, model architecture definition, training, evaluation, and saving the trained model."
    },
    {
        "code": "import fodmc\n\n\noutput_mode = ['NRLMOL','PyFLOSIC'][1]\noutput_name = ['',      'test.xyz'][1]\nfodmc.fodmc_mod.get_guess(output_mode,output_name)\n",
        "summary": "The Python code imports the `fodmc` module and sets the output mode to 'PyFLOSIC' and the output name to 'test.xyz', then calls the `get_guess` function from the `fodmc_mod` submodule with these parameters."
    },
    {
        "code": "import unittest\n\nimport mock\nfrom parameterized import parameterized\n\nfrom airflow.gcp.hooks.cloud_storage_transfer_service import GcpTransferOperationStatus\nfrom airflow.gcp.sensors.cloud_storage_transfer_service import CloudDataTransferServiceJobStatusSensor\n\n\nclass TestGcpStorageTransferOperationWaitForJobStatusSensor(unittest.TestCase):\n    @mock.patch('airflow.gcp.sensors.cloud_storage_transfer_service.CloudDataTransferServiceHook')\n    def test_wait_for_status_success(self, mock_tool):\n        operations = [{'metadata': {'status': GcpTransferOperationStatus.SUCCESS}}]\n        mock_tool.return_value.list_transfer_operations.return_value = operations\n        mock_tool.operations_contain_expected_statuses.return_value = True\n\n        op = CloudDataTransferServiceJobStatusSensor(\n            task_id='task-id',\n            job_name='job-name',\n            project_id='project-id',\n            expected_statuses=GcpTransferOperationStatus.SUCCESS,\n        )\n\n        context = {'ti': (mock.Mock(**{'xcom_push.return_value': None}))}\n        result = op.poke(context)\n\n        mock_tool.return_value.list_transfer_operations.assert_called_once_with(\n            request_filter={'project_id': 'project-id', 'job_names': ['job-name']}\n        )\n        mock_tool.operations_contain_expected_statuses.assert_called_once_with(\n            operations=operations, expected_statuses={GcpTransferOperationStatus.SUCCESS}\n        )\n        self.assertTrue(result)\n\n    @mock.patch('airflow.gcp.sensors.cloud_storage_transfer_service.CloudDataTransferServiceHook')\n    def test_wait_for_status_success_default_expected_status(self, mock_tool):\n\n        op = CloudDataTransferServiceJobStatusSensor(\n            task_id='task-id',\n            job_name='job-name',\n            project_id='project-id',\n            expected_statuses=GcpTransferOperationStatus.SUCCESS,\n        )\n\n        context = {'ti': (mock.Mock(**{'xcom_push.return_value': None}))}\n\n        result = op.poke(context)\n\n        mock_tool.operations_contain_expected_statuses.assert_called_once_with(\n            operations=mock.ANY, expected_statuses={GcpTransferOperationStatus.SUCCESS}\n        )\n        self.assertTrue(result)\n\n    @mock.patch('airflow.gcp.sensors.cloud_storage_transfer_service.CloudDataTransferServiceHook')\n    def test_wait_for_status_after_retry(self, mock_tool):\n        operations_set = [\n            [{'metadata': {'status': GcpTransferOperationStatus.SUCCESS}}],\n            [{'metadata': {'status': GcpTransferOperationStatus.SUCCESS}}],\n        ]\n\n        mock_tool.return_value.list_transfer_operations.side_effect = operations_set\n        mock_tool.operations_contain_expected_statuses.side_effect = [False, True]\n\n        op = CloudDataTransferServiceJobStatusSensor(\n            task_id='task-id',\n            job_name='job-name',\n            project_id='project-id',\n            expected_statuses=GcpTransferOperationStatus.SUCCESS,\n        )\n\n        context = {'ti': (mock.Mock(**{'xcom_push.return_value': None}))}\n\n        result = op.poke(context)\n        self.assertFalse(result)\n\n        mock_tool.operations_contain_expected_statuses.assert_called_once_with(\n            operations=operations_set[0], expected_statuses={GcpTransferOperationStatus.SUCCESS}\n        )\n        mock_tool.operations_contain_expected_statuses.reset_mock()\n\n        result = op.poke(context)\n        self.assertTrue(result)\n\n        mock_tool.operations_contain_expected_statuses.assert_called_once_with(\n            operations=operations_set[1], expected_statuses={GcpTransferOperationStatus.SUCCESS}\n        )\n\n    @parameterized.expand(\n        [\n            (GcpTransferOperationStatus.SUCCESS, {GcpTransferOperationStatus.SUCCESS}),\n            ({GcpTransferOperationStatus.SUCCESS}, {GcpTransferOperationStatus.SUCCESS}),\n            (\n                {GcpTransferOperationStatus.SUCCESS, GcpTransferOperationStatus.SUCCESS},\n                {GcpTransferOperationStatus.SUCCESS, GcpTransferOperationStatus.SUCCESS},\n            ),\n        ]\n    )\n    @mock.patch('airflow.gcp.sensors.cloud_storage_transfer_service.CloudDataTransferServiceHook')\n    def test_wait_for_status_normalize_status(self, expected_status, received_status, mock_tool):\n        operations = [{'metadata': {'status': GcpTransferOperationStatus.SUCCESS}}]\n\n        mock_tool.return_value.list_transfer_operations.return_value = operations\n        mock_tool.operations_contain_expected_statuses.side_effect = [False, True]\n\n        op = CloudDataTransferServiceJobStatusSensor(\n            task_id='task-id',\n            job_name='job-name',\n            project_id='project-id',\n            expected_statuses=expected_status,\n        )\n\n        context = {'ti': (mock.Mock(**{'xcom_push.return_value': None}))}\n\n        result = op.poke(context)\n        self.assertFalse(result)\n\n        mock_tool.operations_contain_expected_statuses.assert_called_once_with(\n            operations=operations, expected_statuses=received_status\n        )\n",
        "summary": "The provided Python code tests the `CloudDataTransferServiceJobStatusSensor` class from Airflow's GCP sensors module. It uses mocking and parameterized testing to verify that the sensor correctly waits for a specified job status on Google Cloud Storage Transfer Service, handling various scenarios including successful operations, retries, and different expected statuses."
    },
    {
        "code": "import datetime\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('maps', '0011_auto_20201019_1839'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='trafficsignal',\n            name='timer',\n            field=models.DateTimeField(default=datetime.datetime(2020, 10, 19, 21, 39, 12, 862273)),\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that alters the 'timer' field of the 'trafficsignal' model in the 'maps' app to have a default value set to October 19, 2020, at 21:39:12.862273."
    },
    {
        "code": "from __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('band', '0002_auto_20160725_1313'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='personal',\n            name='id',\n        ),\n        migrations.AlterField(\n            model_name='personal',\n            name='username',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, primary_key=True, serialize=False, to='band.Account'),\n        ),\n    ]\n",
        "summary": "The Python code defines a Django migration that removes the 'id' field from the 'Personal' model and alters the 'username' field to be a foreign key with cascading deletion, setting it as the primary key."
    },
    {
        "code": "from .common import *\n\n__all__ = [\"TestReadWriteMemory\"]\n\nclass TestReadWriteMemory(MCPTestCase):\n    def test_read_flash_ok(self):\n        self.mcp.dev.read.return_value = self.xb0_00\n        self.assertEqual(self.mcp._read_flash(FlashDataSubcode.ChipSettings), self.xb0_00[4:14])\n    \n    def test_read_sram_ok(self):\n        self.mcp.dev.read.return_value = self.x61\n        self.assertEqual(self.mcp._read_sram(SramDataSubcode.ChipSettings), self.x61[4:22])\n        self.assertEqual(self.mcp._read_sram(SramDataSubcode.GPSettings), self.x61[22:26])\n    \n    def test_read_flash_byte_ok(self):\n        self.mcp.dev.read.return_value = self.xb0_00\n        for n in range(0,9):\n            result = self.mcp._read_flash_byte(FlashDataSubcode.ChipSettings, n, range(8))\n            value = int(\"\".join([\"1\" if x else \"0\" for x in reversed(result)]),2)\n            self.assertEqual(value, self.xb0_00[4+n])\n\n    def test_read_sram_byte_ok(self):\n        self.mcp.dev.read.return_value = self.x61\n        for n in range(0,9):\n            result = self.mcp._read_sram_byte(SramDataSubcode.ChipSettings, n, range(8))\n            value = int(\"\".join([\"1\" if x else \"0\" for x in reversed(result)]),2)\n            self.assertEqual(value, self.x61[4+n])\n\n    def test_write_flash_byte_ok(self):\n        \n        xb1_00 = bytearray(64)\n        xb1_00[0] = 0xb1\n        with patch.object(self.mcp, \"_read_response\", return_value = self.xb0_00):\n            for byte in range(9):\n                for bit in range(8):\n                    xb1_00[2:12] = self.xb0_00[4:14]\n                    xb1_00[2+byte] = self.mcp._MCP2221__and(xb1_00[2+byte], 0xff - (1<<bit))\n                    self.mcp._write_flash_byte(FlashDataSubcode.ChipSettings, byte, [bit], [False])\n                    self.assertEqual(self.mcp.dev.write.call_args[0][0], xb1_00)\n        \n    def test_write_sram_ok(self):\n        \n        with patch.object(self.mcp, \"_read_response\", return_value = self.x61):\n            v = 0xff\n            for byte in range(9):\n                self.mcp._write_sram(SramDataSubcode.ChipSettings, byte, v)\n                self.assertEqual(self.mcp.dev.write.call_args[0][0][2+byte], v)\n\n",
        "summary": "The provided Python code defines a test class `TestReadWriteMemory` that inherits from `MCPTestCase`. It includes several methods to test reading and writing operations on flash and SRAM memory, ensuring the correct data is read and written as expected. The tests simulate device interactions using mock objects and verify the behavior of the `_read_flash`, `_read_sram`, `_write_flash_byte`, and `_write_sram` methods."
    },
    {
        "code": "from decimal import Decimal, getcontext\nfrom fractions import Fraction\n\ndigits = 500\ngetcontext().prec = digits\n\n\ndef leibnitz(n):\n    \n    pi = Fraction(0)\n    sign = 1\n    for k in range(1, n, 2):\n        pi = pi + sign*Fraction(4, k)\n        sign *= -1\n    return pi\n\n\ndef calc_pi(n):\n    \n    pi = Fraction(0)\n    for k in range(n):\n        \n        pi += (Fraction(-1, 4)**k * (Fraction(1, 1+2*k)\n               + Fraction(2, 1+4*k)\n               + Fraction(1, 3+4*k)))\n    return pi\n\n\ndef get_correct_digits(approx):\n    \n    pi = (\"3.14159265358979323846264338327950288419716939937510582097494459230\"\n          \"78164062862089986280348253421170679\")\n    for i, el in enumerate(pi):\n        if len(approx) <= i:\n            return i-1\n        if el != approx[i]:\n            return i\n    return -1  \n\nif __name__ == \"__main__\":\n    \n    \n    \n    \n    \n\n    n = digits\n    approx = calc_pi(n)\n    dec = Decimal(approx.numerator) / Decimal(approx.denominator)\n    print(dec)\n",
        "summary": "The provided Python code calculates the value of \u03c0 using two different algorithms: the Leibniz formula and a custom series. It then compares the calculated approximation to a known value of \u03c0 to determine the number of correct decimal places in the approximation. The main function sets the precision, performs the calculations, converts the result to a Decimal for high-precision arithmetic, and prints the final approximation."
    },
    {
        "code": "import os.path\nfrom app.data.database import init_db, db_path, get_expected_pathname, set_path\n\ndef db_exists():\n    return os.path.isfile(db_path)\n\ndef check_db():\n    global db_path\n\n    if (db_path != get_expected_pathname()):\n        print('DB Check: Running backup')\n        backup_database_to(get_expected_pathname())\n        init_db()\n\n\n    if (not db_exists()):\n        print('DB Check: No database found. Making a new one...')\n        init_db()\n        from app.data.camper_editing import reset_locs\n        reset_locs()\n\ndef backup_database_to(filename):\n    global db_path\n    from shutil import copy2\n    s = open('data/BACKUPDATA', 'a+')\n    s.seek(0)\n    prev_path = s.read()\n    set_path(filename)\n\n    db_path = filename \n\n    s.truncate(0)\n    s.seek(0)\n    s.write(filename)\n\n    if (prev_path == \"\"):\n        print(\"No previous database found, a new one will be generated. This may happen if the BACKUPDATA file is missing or corrupt.\")\n        return False\n    elif (prev_path == filename):\n        print(\"Tried to back up to the same file!\")\n    else:\n        print (\"backing up & copying\")\n        from app.data.camper_editing import reset_locs\n        copy2(prev_path, filename)\n        reset_locs()\n        return filename\n",
        "summary": "The provided Python code manages a database by checking its existence and performing backups if necessary. It includes functions to initialize the database, check for its presence, and backup the current database to a new location, ensuring data integrity and recovery capabilities."
    },
    {
        "code": "import warnings\nimport pulumi\nimport pulumi.runtime\nfrom typing import Any, Mapping, Optional, Sequence, Union, overload\nfrom .. import _utilities\nfrom . import outputs\nfrom ._enums import *\n\n__all__ = [\n    'GetGroupResult',\n    'AwaitableGetGroupResult',\n    'get_group',\n    'get_group_output',\n]\n\n@pulumi.output_type\nclass GetGroupResult:\n    def __init__(__self__, arn=None, configuration=None, description=None, resource_query=None, resources=None, tags=None):\n        if arn and not isinstance(arn, str):\n            raise TypeError(\"Expected argument 'arn' to be a str\")\n        pulumi.set(__self__, \"arn\", arn)\n        if configuration and not isinstance(configuration, list):\n            raise TypeError(\"Expected argument 'configuration' to be a list\")\n        pulumi.set(__self__, \"configuration\", configuration)\n        if description and not isinstance(description, str):\n            raise TypeError(\"Expected argument 'description' to be a str\")\n        pulumi.set(__self__, \"description\", description)\n        if resource_query and not isinstance(resource_query, dict):\n            raise TypeError(\"Expected argument 'resource_query' to be a dict\")\n        pulumi.set(__self__, \"resource_query\", resource_query)\n        if resources and not isinstance(resources, list):\n            raise TypeError(\"Expected argument 'resources' to be a list\")\n        pulumi.set(__self__, \"resources\", resources)\n        if tags and not isinstance(tags, list):\n            raise TypeError(\"Expected argument 'tags' to be a list\")\n        pulumi.set(__self__, \"tags\", tags)\n\n    @property\n    @pulumi.getter\n    def arn(self) -> Optional[str]:\n        \n        return pulumi.get(self, \"arn\")\n\n    @property\n    @pulumi.getter\n    def configuration(self) -> Optional[Sequence['outputs.GroupConfigurationItem']]:\n        return pulumi.get(self, \"configuration\")\n\n    @property\n    @pulumi.getter\n    def description(self) -> Optional[str]:\n        \n        return pulumi.get(self, \"description\")\n\n    @property\n    @pulumi.getter(name=\"resourceQuery\")\n    def resource_query(self) -> Optional['outputs.GroupResourceQuery']:\n        return pulumi.get(self, \"resource_query\")\n\n    @property\n    @pulumi.getter\n    def resources(self) -> Optional[Sequence[str]]:\n        return pulumi.get(self, \"resources\")\n\n    @property\n    @pulumi.getter\n    def tags(self) -> Optional[Sequence['outputs.GroupTag']]:\n        return pulumi.get(self, \"tags\")\n\n\nclass AwaitableGetGroupResult(GetGroupResult):\n    \n    def __await__(self):\n        if False:\n            yield self\n        return GetGroupResult(\n            arn=self.arn,\n            configuration=self.configuration,\n            description=self.description,\n            resource_query=self.resource_query,\n            resources=self.resources,\n            tags=self.tags)\n\n\ndef get_group(name: Optional[str] = None,\n              opts: Optional[pulumi.InvokeOptions] = None) -> AwaitableGetGroupResult:\n    \n    __args__ = dict()\n    __args__['name'] = name\n    if opts is None:\n        opts = pulumi.InvokeOptions()\n    if opts.version is None:\n        opts.version = _utilities.get_version()\n    __ret__ = pulumi.runtime.invoke('aws-native:resourcegroups:getGroup', __args__, opts=opts, typ=GetGroupResult).value\n\n    return AwaitableGetGroupResult(\n        arn=__ret__.arn,\n        configuration=__ret__.configuration,\n        description=__ret__.description,\n        resource_query=__ret__.resource_query,\n        resources=__ret__.resources,\n        tags=__ret__.tags)\n\n\n@_utilities.lift_output_func(get_group)\ndef get_group_output(name: Optional[pulumi.Input[str]] = None,\n                     opts: Optional[pulumi.InvokeOptions] = None) -> pulumi.Output[GetGroupResult]:\n    \n    ...\n",
        "summary": "The provided Python code defines a class `GetGroupResult` to represent the result of fetching a resource group, including properties like ARN, configuration, description, and tags. It also includes functions `get_group` and `get_group_output` to invoke AWS Native Resource Groups API operations asynchronously, returning instances of `AwaitableGetGroupResult`."
    },
    {
        "code": "import numpy as np\n\n\ndef partition(arr, low, high):\n    i = (low-1)         \n    pivot = arr[high]     \n\n    for j in range(low, high):\n\n        \n        if arr[j] < pivot:\n\n            \n            i = i+1\n            arr[i], arr[j] = arr[j], arr[i]\n\n    arr[i+1], arr[high] = arr[high], arr[i+1]\n    return (i + 1)\n\n\ndef quickSort(arr, low, high):\n    if low < high:\n\n        \n        \n        pi = partition(arr, low, high)\n\n        \n        \n        quickSort(arr, low, pi-1)\n        quickSort(arr, pi + 1, high)\n\n \n\narr = np.random.randint(0, 1000000, 200000)\nn = len(arr)\nquickSort(arr, 0, n-1)\n\n",
        "summary": "The provided Python code implements the QuickSort algorithm using NumPy to generate a large array of random integers and sorts it efficiently."
    },
    {
        "code": "from datetime import timedelta\nfrom unittest.mock import AsyncMock, patch\n\nfrom homeassistant import config_entries, data_entry_flow\nfrom homeassistant.components import ssdp\nfrom homeassistant.components.upnp.const import (\n    CONFIG_ENTRY_SCAN_INTERVAL,\n    CONFIG_ENTRY_ST,\n    CONFIG_ENTRY_UDN,\n    DEFAULT_SCAN_INTERVAL,\n    DISCOVERY_LOCATION,\n    DISCOVERY_NAME,\n    DISCOVERY_ST,\n    DISCOVERY_UDN,\n    DISCOVERY_UNIQUE_ID,\n    DISCOVERY_USN,\n    DOMAIN,\n    DOMAIN_COORDINATORS,\n)\nfrom homeassistant.components.upnp.device import Device\nfrom homeassistant.helpers.typing import HomeAssistantType\nfrom homeassistant.setup import async_setup_component\n\nfrom .mock_device import MockDevice\n\nfrom tests.common import MockConfigEntry\n\n\nasync def test_flow_ssdp_discovery(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    location = \"dummy\"\n    mock_device = MockDevice(udn)\n    discoveries = [\n        {\n            DISCOVERY_LOCATION: location,\n            DISCOVERY_NAME: mock_device.name,\n            DISCOVERY_ST: mock_device.device_type,\n            DISCOVERY_UDN: mock_device.udn,\n            DISCOVERY_UNIQUE_ID: mock_device.unique_id,\n            DISCOVERY_USN: mock_device.usn,\n        }\n    ]\n    with patch.object(\n        Device, \"async_create_device\", AsyncMock(return_value=mock_device)\n    ), patch.object(\n        Device, \"async_discover\", AsyncMock(return_value=discoveries)\n    ), patch.object(\n        Device, \"async_supplement_discovery\", AsyncMock(return_value=discoveries[0])\n    ):\n        \n        result = await hass.config_entries.flow.async_init(\n            DOMAIN,\n            context={\"source\": config_entries.SOURCE_SSDP},\n            data={\n                ssdp.ATTR_SSDP_LOCATION: location,\n                ssdp.ATTR_SSDP_ST: mock_device.device_type,\n                ssdp.ATTR_SSDP_USN: mock_device.usn,\n                ssdp.ATTR_UPNP_UDN: mock_device.udn,\n            },\n        )\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_FORM\n        assert result[\"step_id\"] == \"ssdp_confirm\"\n\n        \n        result = await hass.config_entries.flow.async_configure(\n            result[\"flow_id\"],\n            user_input={},\n        )\n\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_CREATE_ENTRY\n        assert result[\"title\"] == mock_device.name\n        assert result[\"data\"] == {\n            CONFIG_ENTRY_ST: mock_device.device_type,\n            CONFIG_ENTRY_UDN: mock_device.udn,\n        }\n\n\nasync def test_flow_ssdp_discovery_incomplete(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    location = \"dummy\"\n    mock_device = MockDevice(udn)\n\n    \n    result = await hass.config_entries.flow.async_init(\n        DOMAIN,\n        context={\"source\": config_entries.SOURCE_SSDP},\n        data={\n            ssdp.ATTR_SSDP_ST: mock_device.device_type,\n            \n            ssdp.ATTR_SSDP_LOCATION: location,\n        },\n    )\n    assert result[\"type\"] == data_entry_flow.RESULT_TYPE_ABORT\n    assert result[\"reason\"] == \"incomplete_discovery\"\n\n\nasync def test_flow_user(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    location = \"dummy\"\n    mock_device = MockDevice(udn)\n    discoveries = [\n        {\n            DISCOVERY_LOCATION: location,\n            DISCOVERY_NAME: mock_device.name,\n            DISCOVERY_ST: mock_device.device_type,\n            DISCOVERY_UDN: mock_device.udn,\n            DISCOVERY_UNIQUE_ID: mock_device.unique_id,\n            DISCOVERY_USN: mock_device.usn,\n        }\n    ]\n\n    with patch.object(\n        Device, \"async_create_device\", AsyncMock(return_value=mock_device)\n    ), patch.object(\n        Device, \"async_discover\", AsyncMock(return_value=discoveries)\n    ), patch.object(\n        Device, \"async_supplement_discovery\", AsyncMock(return_value=discoveries[0])\n    ):\n        \n        result = await hass.config_entries.flow.async_init(\n            DOMAIN, context={\"source\": config_entries.SOURCE_USER}\n        )\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_FORM\n        assert result[\"step_id\"] == \"user\"\n\n        \n        result = await hass.config_entries.flow.async_configure(\n            result[\"flow_id\"],\n            user_input={\"unique_id\": mock_device.unique_id},\n        )\n\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_CREATE_ENTRY\n        assert result[\"title\"] == mock_device.name\n        assert result[\"data\"] == {\n            CONFIG_ENTRY_ST: mock_device.device_type,\n            CONFIG_ENTRY_UDN: mock_device.udn,\n        }\n\n\nasync def test_flow_import(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    mock_device = MockDevice(udn)\n    location = \"dummy\"\n    discoveries = [\n        {\n            DISCOVERY_LOCATION: location,\n            DISCOVERY_NAME: mock_device.name,\n            DISCOVERY_ST: mock_device.device_type,\n            DISCOVERY_UDN: mock_device.udn,\n            DISCOVERY_UNIQUE_ID: mock_device.unique_id,\n            DISCOVERY_USN: mock_device.usn,\n        }\n    ]\n\n    with patch.object(\n        Device, \"async_create_device\", AsyncMock(return_value=mock_device)\n    ), patch.object(\n        Device, \"async_discover\", AsyncMock(return_value=discoveries)\n    ), patch.object(\n        Device, \"async_supplement_discovery\", AsyncMock(return_value=discoveries[0])\n    ):\n        \n        result = await hass.config_entries.flow.async_init(\n            DOMAIN, context={\"source\": config_entries.SOURCE_IMPORT}\n        )\n\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_CREATE_ENTRY\n        assert result[\"title\"] == mock_device.name\n        assert result[\"data\"] == {\n            CONFIG_ENTRY_ST: mock_device.device_type,\n            CONFIG_ENTRY_UDN: mock_device.udn,\n        }\n\n\nasync def test_flow_import_already_configured(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    mock_device = MockDevice(udn)\n\n    \n    config_entry = MockConfigEntry(\n        domain=DOMAIN,\n        data={\n            CONFIG_ENTRY_UDN: mock_device.udn,\n            CONFIG_ENTRY_ST: mock_device.device_type,\n        },\n        options={CONFIG_ENTRY_SCAN_INTERVAL: DEFAULT_SCAN_INTERVAL},\n    )\n    config_entry.add_to_hass(hass)\n\n    \n    result = await hass.config_entries.flow.async_init(\n        DOMAIN, context={\"source\": config_entries.SOURCE_IMPORT}\n    )\n\n    assert result[\"type\"] == data_entry_flow.RESULT_TYPE_ABORT\n    assert result[\"reason\"] == \"already_configured\"\n\n\nasync def test_flow_import_incomplete(hass: HomeAssistantType):\n    \n    udn = \"uuid:device_1\"\n    mock_device = MockDevice(udn)\n    location = \"dummy\"\n    discoveries = [\n        {\n            DISCOVERY_LOCATION: location,\n            DISCOVERY_NAME: mock_device.name,\n            \n            DISCOVERY_UDN: mock_device.udn,\n            DISCOVERY_UNIQUE_ID: mock_device.unique_id,\n            DISCOVERY_USN: mock_device.usn,\n        }\n    ]\n\n    with patch.object(Device, \"async_discover\", AsyncMock(return_value=discoveries)):\n        \n        result = await hass.config_entries.flow.async_init(\n            DOMAIN, context={\"source\": config_entries.SOURCE_IMPORT}\n        )\n\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_ABORT\n        assert result[\"reason\"] == \"incomplete_discovery\"\n\n\nasync def test_options_flow(hass: HomeAssistantType):\n    \n    \n    udn = \"uuid:device_1\"\n    location = \"http://192.168.1.1/desc.xml\"\n    mock_device = MockDevice(udn)\n    discoveries = [\n        {\n            DISCOVERY_LOCATION: location,\n            DISCOVERY_NAME: mock_device.name,\n            DISCOVERY_ST: mock_device.device_type,\n            DISCOVERY_UDN: mock_device.udn,\n            DISCOVERY_UNIQUE_ID: mock_device.unique_id,\n            DISCOVERY_USN: mock_device.usn,\n        }\n    ]\n    config_entry = MockConfigEntry(\n        domain=DOMAIN,\n        data={\n            CONFIG_ENTRY_UDN: mock_device.udn,\n            CONFIG_ENTRY_ST: mock_device.device_type,\n        },\n        options={CONFIG_ENTRY_SCAN_INTERVAL: DEFAULT_SCAN_INTERVAL},\n    )\n    config_entry.add_to_hass(hass)\n\n    config = {\n        \n    }\n    with patch.object(\n        Device, \"async_create_device\", AsyncMock(return_value=mock_device)\n    ), patch.object(Device, \"async_discover\", AsyncMock(return_value=discoveries)):\n        \n        await async_setup_component(hass, \"upnp\", config)\n        await hass.async_block_till_done()\n\n        \n        coordinator = hass.data[DOMAIN][DOMAIN_COORDINATORS][mock_device.udn]\n        assert coordinator.update_interval == timedelta(seconds=DEFAULT_SCAN_INTERVAL)\n\n        \n        result = await hass.config_entries.options.async_init(\n            config_entry.entry_id,\n        )\n        assert result[\"type\"] == data_entry_flow.RESULT_TYPE_FORM\n\n        \n        result2 = await hass.config_entries.options.async_configure(\n            result[\"flow_id\"],\n            user_input={CONFIG_ENTRY_SCAN_INTERVAL: 60},\n        )\n        assert result2[\"type\"] == data_entry_flow.RESULT_TYPE_CREATE_ENTRY\n        assert config_entry.options == {\n            CONFIG_ENTRY_SCAN_INTERVAL: 60,\n        }\n\n        \n        assert coordinator.update_interval == timedelta(seconds=60)\n",
        "summary": "The provided Python code tests various configuration flows for the UPnP component in Home Assistant, including SSDP discovery, manual import, and options management. Each test case simulates different user interactions and verifies that the component behaves as expected, such as creating new entries, updating existing ones, or aborting when conditions are not met."
    },
    {
        "code": "from .BSD500 import BSD500\n__all__ = ('BSD500')\n",
        "summary": "The provided Python code imports a module named `BSD500` from the same directory and then defines a list containing only the string `'BSD500'`, which is assigned to the variable `__all__`. This setup suggests that when this module is imported, it will make `BSD500` available for use."
    },
    {
        "code": "week = [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\"]\nprint(7-week.index(input()))",
        "summary": "The Python code defines a list of days of the week, then prompts the user to input a day, and finally prints the number of days remaining in the week from that inputted day."
    },
    {
        "code": "class MinHeapPriorityQueue():\n    \n\n    def __init__(self, iterable=(), key=lambda x: x):\n        self._key = key\n        decorated = [(key(item), item) for item in iterable]\n        self._pq = [self.Locator(value, item, i) for i, (value, item) in enumerate(decorated)]\n        if len(self._pq) > 1:\n            self._heapify()\n\n    class Locator:\n        \n        __slots__ = '_value', '_item', '_index'\n\n        def __init__(self, value, item, i):\n            self._value = value\n            self._item = item\n            self._index = i\n\n        def __eq__(self, other):\n            return self._value == other._value\n\n        def __lt__(self, other):\n            return self._value < other._value\n\n        def __le__(self, other):\n            return self._value <= other._value\n\n        def __repr__(self):\n            return '{}(value={!r}, item={!r}, index={})'.format(\n                self.__class__.__name__,\n                self._value,\n                self._item,\n                self._index\n            )\n\n    \n    \n    def _parent(self, j):\n        return (j-1) // 2\n\n    def _left(self, j):\n        return 2*j + 1\n\n    def _right(self, j):\n        return 2*j + 2\n\n    def _swap(self, i, j):\n        \n        self._pq[i], self._pq[j] = self._pq[j], self._pq[i]\n        \n        self._pq[i]._index = i\n        self._pq[j]._index = j\n\n    def _upheap(self, i):\n        parent = self._parent(i)\n        if i > 0 and self._pq[i] < self._pq[parent]:\n            self._swap(i, parent)\n            self._upheap(parent)\n\n    def _downheap(self, i):\n        n = len(self._pq)\n        left, right = self._left(i), self._right(i)\n        if left < n:\n            child = left\n            if right < n and self._pq[right] < self._pq[left]:\n                child = right\n            if self._pq[child] < self._pq[i]:\n                self._swap(i, child)\n                self._downheap(child)\n\n    def _fix(self, i):\n        self._upheap(i)\n        self._downheap(i)\n\n    def _heapify(self):\n        start = self._parent(len(self) - 1) \n        for j in range(start, -1, -1):      \n            self._downheap(j)\n\n    \n    \n    def append(self, item):\n        \n        token = self.Locator(self._key(item), item, len(self._pq))\n        self._pq.append(token)\n        self._upheap(len(self._pq) - 1) \n        return token\n\n    def update(self, loc, newval, newitem):\n        \n        j = loc._index\n        if not (0 <= j < len(self) and self._pq[j] is loc):\n            raise ValueError('Invalid locator')\n        loc._value = newval\n        loc._item = newitem\n        self._fix(j)\n\n    def remove(self, loc):\n        \n        j = loc._index\n        if not (0 <= j < len(self) and self._pq[j] is loc):\n            raise ValueError('Invalid locator')\n        if j == len(self) - 1:\n            self._pq.pop()\n        else:\n            self._swap(j, len(self) - 1)\n            self._pq.pop()\n            self._fix(j)\n        return loc._item\n\n    def peek(self):\n        \n        loc = self._pq[0]\n        return loc._item\n\n    def pop(self):\n        \n        self._swap(0, len(self._pq) - 1)\n        loc = self._pq.pop()\n        self._downheap(0)\n        return loc._item\n\n    @property\n    def items(self):\n        return [token._item for token in self._pq]\n\n    def __len__(self):\n        return len(self._pq)\n\n    def __contains__(self, item):\n        return item in self.items\n\n    def __iter__(self):\n        return iter(sorted(self.items))\n\n    def __repr__(self):\n        return '{}({})'.format(self.__class__.__name__, self._pq)\n\nclass MaxHeapPriorityQueue(MinHeapPriorityQueue):\n    \n    \n    \n    def _upheap(self, i):\n        parent = self._parent(i)\n        if i > 0 and self._pq[parent] < self._pq[i]:\n            self._swap(i, parent)\n            self._upheap(parent)\n\n    def _downheap(self, i):\n        n = len(self._pq)\n        left, right = self._left(i), self._right(i)\n        if left < n:\n            child = left\n            if right < n and self._pq[left] < self._pq[right]:\n                child = right\n            if self._pq[i] < self._pq[child]:\n                self._swap(i, child)\n                self._downheap(child)\n\n    def __iter__(self):\n        return iter(sorted(self.items, reverse=True))\n\n__doc__ += \n\nif __name__ == \"__main__\":\n    import doctest\n    doctest.testmod()\n",
        "summary": "The `MinHeapPriorityQueue` class implements a priority queue using a min-heap data structure, allowing efficient insertion and extraction of the smallest element. The `MaxHeapPriorityQueue` class extends this by implementing a max-heap, which efficiently handles the largest element instead. Both classes provide methods for appending items, updating item priorities, removing items, and peeking at the top item without removal."
    },
    {
        "code": "from selenium import webdriver\nfrom fixture.session import SessionHelper\nfrom fixture.group import GroupHelper\nfrom fixture.contact import ContactHelper\n\n\nclass Application:\n\n    def __init__(self, browser, base_url):\n        if browser == \"firefox\":\n            self.wd = webdriver.Firefox()\n        elif browser == \"chrome\":\n            self.wd = webdriver.Chrome()\n        elif browser == \"ie\":\n            self.wd = webdriver.Ie()\n        else:\n            raise ValueError(\"Unrecognized browser %s\" % browser)\n        self.wd.implicitly_wait(5)\n        self.session = SessionHelper(self)\n        self.group = GroupHelper(self)\n        self.contact = ContactHelper(self)\n        self.base_url=base_url\n\n    def is_valid(self):\n        try:\n            self.wd.current_url\n            return True\n        except:\n            return False\n\n\n    def open_home_page(self):\n        wd = self.wd\n        wd.get(self.base_url)\n\n    def destroy(self):\n        self.wd.quit()\n",
        "summary": "The provided Python code defines a class `Application` that encapsulates the functionality for automating web browser interactions using Selenium WebDriver. It initializes with a specified browser and base URL, sets up helper classes for session management, group operations, and contact management, and includes methods to open the home page and destroy the browser instance."
    },
    {
        "code": "import matplotlib.pyplot as plt\nfrom shapely.geometry import MultiLineString\nfrom .route_iterator import RouteIterator\nfrom .graphconverter import GraphConverter\n\n\nclass TramLine(object):\n    \n\n    def __init__(self, number, direction_to, dl):\n        \n        self.number = number  \n        self.direction_to = direction_to\n        self.default_route = dl.load_single_line(number, direction_to)  \n        self.stops = dl.load_tram_stops(self.default_route)  \n        self.current_route = self.default_route\n        self.route_in_order = GraphConverter.find_route_in_order(dl, self)\n\n\n\n",
        "summary": "The `TramLine` class initializes with a tram line number and direction, loads the default route and stops using provided data loaders, and determines the current route in order by converting it with a graph converter."
    },
    {
        "code": "import argparse\nimport asyncio\nimport dataclasses\nimport itertools\nimport json\nimport logging\nimport os\nimport sys\nimport typing\nfrom pathlib import Path\n\nimport paho.mqtt.client as mqtt\n\nimport rhasspyhermes.cli as hermes_cli\n\nfrom . import SnowboyModel, WakeHermesMqtt\n\n_DIR = Path(__file__).parent\n_LOGGER = logging.getLogger(\"rhasspywake_snowboy_hermes\")\n\n\n\n\ndef main():\n    \n    parser = argparse.ArgumentParser(prog=\"rhasspy-wake-snowboy-hermes\")\n    parser.add_argument(\n        \"--model\",\n        required=True,\n        action=\"append\",\n        nargs=\"+\",\n        help=\"Snowboy model settings (model, sensitivity, audio_gain, apply_frontend)\",\n    )\n    parser.add_argument(\n        \"--model-dir\",\n        action=\"append\",\n        default=[],\n        help=\"Directories with snowboy models\",\n    )\n    parser.add_argument(\n        \"--wakeword-id\",\n        action=\"append\",\n        help=\"Wakeword IDs of each keyword (default: use file name)\",\n    )\n    parser.add_argument(\n        \"--stdin-audio\", action=\"store_true\", help=\"Read WAV audio from stdin\"\n    )\n    parser.add_argument(\n        \"--udp-audio\",\n        nargs=3,\n        action=\"append\",\n        help=\"Host/port/siteId for UDP audio input\",\n    )\n    parser.add_argument(\"--lang\", help=\"Set lang in hotword detected message\")\n\n    hermes_cli.add_hermes_args(parser)\n    args = parser.parse_args()\n\n    hermes_cli.setup_logging(args)\n    _LOGGER.debug(args)\n\n    if args.model_dir:\n        args.model_dir = [Path(d) for d in args.model_dir]\n\n    \n    args.model_dir.append(_DIR / \"models\")\n\n    \n    models: typing.List[SnowboyModel] = []\n\n    for model_settings in args.model:\n        model_path = Path(model_settings[0])\n\n        if not model_path.is_file():\n            \n            for model_dir in args.model_dir:\n                maybe_path = model_dir / model_path.name\n                if maybe_path.is_file():\n                    model_path = maybe_path\n                    break\n\n        _LOGGER.debug(\"Loading model from %s\", str(model_path))\n        model = SnowboyModel(model_path=model_path)\n\n        if len(model_settings) > 1:\n            model.sensitivity = model_settings[1]\n\n        if len(model_settings) > 2:\n            model.audio_gain = float(model_settings[2])\n\n        if len(model_settings) > 3:\n            model.apply_frontend = model_settings[3].strip().lower() == \"true\"\n\n        models.append(model)\n\n    wakeword_ids = [\n        kn[1]\n        for kn in itertools.zip_longest(\n            args.model, args.wakeword_id or [], fillvalue=\"\"\n        )\n    ]\n\n    if args.stdin_audio:\n        \n        client = None\n        hermes = WakeHermesMqtt(client, models, wakeword_ids)\n\n        for site_id in args.site_id:\n            hermes.load_detectors(site_id)\n\n        if os.isatty(sys.stdin.fileno()):\n            print(\"Reading WAV data from stdin...\", file=sys.stderr)\n\n        wav_bytes = sys.stdin.buffer.read()\n\n        \n        for result in hermes.handle_audio_frame(wav_bytes):\n            result_dict = dataclasses.asdict(result)\n            json.dump(result_dict, sys.stdout, ensure_ascii=False)\n\n        return\n\n    udp_audio = []\n    if args.udp_audio:\n        udp_audio = [\n            (host, int(port), site_id) for host, port, site_id in args.udp_audio\n        ]\n\n    \n    client = mqtt.Client()\n    hermes = WakeHermesMqtt(\n        client,\n        models,\n        wakeword_ids,\n        model_dirs=args.model_dir,\n        udp_audio=udp_audio,\n        site_ids=args.site_id,\n        lang=args.lang,\n    )\n\n    for site_id in args.site_id:\n        hermes.load_detectors(site_id)\n\n    _LOGGER.debug(\"Connecting to %s:%s\", args.host, args.port)\n    hermes_cli.connect(client, args)\n\n    client.loop_start()\n\n    try:\n        \n        asyncio.run(hermes.handle_messages_async())\n    except KeyboardInterrupt:\n        pass\n    finally:\n        _LOGGER.debug(\"Shutting down\")\n        client.loop_stop()\n\n\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "summary": "This Python script, named `rhasspy-wake-snowboy-hermes`, is designed to integrate Snowboy wake word detection with Rhasspy Hermes MQTT for voice assistant applications. It allows users to specify multiple Snowboy models and configure their settings through command-line arguments. The script can read audio input from standard input or UDP sockets, process it using the specified models, and publish detected wakewords as MQTT messages."
    },
    {
        "code": "import sys\nimport yaml\n\n\ndef main():\n    args = sys.argv[1:]\n    file = args[0] if args else sys.stdin\n\n    data = yaml.safe_load(file)\n    join_args = data['Fn::Join']\n    contents = join_args[0].join(join_args[1])\n    print(contents, end='')\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n",
        "summary": "The Python script reads a YAML file or standard input, extracts the 'Fn::Join' arguments from it, and joins the second list of strings using the first string as the separator before printing the result."
    },
    {
        "code": "class _GetchWindows:\n    def __init__(self):\n        import msvcrt\n\n    def __call__(self):\n        import msvcrt\n        return msvcrt.getch()\n\ngetch = _GetchWindows()\n\n\nprint (\"Please enter something: \")\n\nx = getch()\n\nprint(x)",
        "summary": "The provided Python code defines a class `_GetchWindows` that encapsulates the functionality to read a single character from the keyboard on a Windows system using the `msvcrt` module. An instance of this class is created and assigned to the variable `getch`, which can be called to retrieve a key press without echoing it to the screen. The script then prompts the user to enter something, reads a single character using the `getch()` method, and prints the character that was pressed."
    },
    {
        "code": "import os\n\nfrom django.conf import settings\nfrom main.tests.test_base import MainTestCase\nfrom odk_viewer.models import ParsedInstance\nfrom odk_viewer.management.commands.remongo import Command\nfrom django.core.management import call_command\nfrom common_tags import USERFORM_ID\n\n\nclass TestRemongo(MainTestCase):\n    def test_remongo_in_batches(self):\n      self._publish_transportation_form()\n      \n      self._make_submissions()\n      self.assertEqual(ParsedInstance.objects.count(), 4)\n      \n      settings.MONGO_DB.instances.drop()\n      c = Command()\n      c.handle(batchsize=3)\n      \n      count = settings.MONGO_DB.instances.count()\n      self.assertEqual(count, 4)\n\n    def test_remongo_with_username_id_string(self):\n        self._publish_transportation_form()\n        \n        s = self.surveys[0]\n        self._make_submission(os.path.join(self.this_directory, 'fixtures',\n                              'transportation', 'instances', s, s + '.xml'))\n        \n        self._logout()\n        self._create_user_and_login(\"harry\", \"harry\")\n        self._publish_transportation_form()\n        s = self.surveys[1]\n        self._make_submission(os.path.join(self.this_directory, 'fixtures',\n                              'transportation', 'instances', s, s + '.xml'))\n        self.assertEqual(ParsedInstance.objects.count(), 2)\n        \n        settings.MONGO_DB.instances.drop()\n        c = Command()\n        c.handle(batchsize=3, username=self.user.username,\n            id_string=self.xform.id_string)\n        \n        count = settings.MONGO_DB.instances.count()\n        self.assertEqual(count, 1)\n\n    def test_indexes_exist(self):\n        \n        call_command('remongo')\n        \n        \n        index_list = [USERFORM_ID]\n        \n        index_info = settings.MONGO_DB.instances.index_information()\n        \n        \n        existing_indexes = [v['key'][0][0] for v in index_info.itervalues() if v['key'][0][1] == 1]\n        all_indexes_found = True\n        for index_item in index_list:\n            if index_item not in existing_indexes:\n                all_indexes_found = False\n                break\n        self.assertTrue(all_indexes_found)\n\n    def test_sync_mongo_with_all_option_deletes_existing_records(self):\n        self._publish_transportation_form()\n        userform_id = \"%s_%s\" % (self.user.username, self.xform.id_string)\n        initial_mongo_count = settings.MONGO_DB.instances.find(\n            {USERFORM_ID: userform_id}).count()\n        for i in range(len(self.surveys)):\n            self._submit_transport_instance(i)\n        mongo_count = settings.MONGO_DB.instances.find(\n            {USERFORM_ID: userform_id}).count()\n        \n        self.assertEqual(mongo_count, initial_mongo_count + len(self.surveys))\n        \n        settings.MONGO_DB.instances.save(\n            {\"_id\": 12345, \"_userform_id\": userform_id})\n        \n        mongo_count = settings.MONGO_DB.instances.find(\n            {USERFORM_ID: userform_id}).count()\n        self.assertEqual(mongo_count,\n                         initial_mongo_count + len(self.surveys) + 1)\n        \n        call_command(\"sync_mongo\", remongo=True)\n        mongo_count = settings.MONGO_DB.instances.find(\n            {USERFORM_ID: userform_id}).count()\n        self.assertEqual(mongo_count,\n            initial_mongo_count + len(self.surveys) + 1)\n        \n        call_command(\"sync_mongo\", remongo=True, update_all=True)\n        \n        mongo_count = settings.MONGO_DB.instances.find(\n            {USERFORM_ID: userform_id}).count()\n        self.assertEqual(mongo_count,\n            initial_mongo_count + len(self.surveys))\n",
        "summary": "The provided Python code defines a test class `TestRemongo` that extends `MainTestCase` and includes several methods to test the functionality of a Django management command named `remongo`. These tests cover scenarios such as processing submissions in batches, filtering by username and ID string, ensuring index existence, and handling synchronization options that can delete existing records."
    },
    {
        "code": "from selenium import webdriver\n\nlink = \"http://selenium1py.pythonanywhere.com/\"\n\n\nclass TestMainPage1():\n\n    @classmethod\n    def setup_class(self):\n        print(\"\\nstart browser for test suite..\")\n        self.browser = webdriver.Chrome()\n\n    @classmethod\n    def teardown_class(self):\n        print(\"quit browser for test suite..\")\n        self.browser.quit()\n\n    def test_guest_should_see_login_link(self):\n        self.browser.get(link)\n        self.browser.find_element_by_css_selector(\"\n\n    def test_guest_should_see_basket_link_on_the_main_page(self):\n        self.browser.get(link)\n        self.browser.find_element_by_css_selector(\n            \".basket-mini .btn-group > a\")\n\n\nclass TestMainPage2():\n\n    def setup_method(self):\n        print(\"start browser for test..\")\n        self.browser = webdriver.Chrome()\n\n    def teardown_method(self):\n        print(\"quit browser for test..\")\n        self.browser.quit()\n\n    def test_guest_should_see_login_link(self):\n        self.browser.get(link)\n        self.browser.find_element_by_css_selector(\"\n\n    def test_guest_should_see_basket_link_on_the_main_page(self):\n        self.browser.get(link)\n        self.browser.find_element_by_css_selector(\n            \".basket-mini .btn-group > a\")\n",
        "summary": "The provided Python code defines two classes, `TestMainPage1` and `TestMainPage2`, each containing methods to test functionalities on a web page using Selenium WebDriver. Both classes include setup and teardown methods for browser management and tests for verifying the presence of login and basket links on the main page."
    },
    {
        "code": "import pytest\n\nimport numpy as np\nimport sklearn.linear_model\nimport sklearn.model_selection\nimport scipy.linalg\n\nfrom himalaya.backend import set_backend\nfrom himalaya.backend import ALL_BACKENDS\nfrom himalaya.utils import assert_array_almost_equal\nfrom himalaya.scoring import r2_score\n\nfrom himalaya.kernel_ridge import solve_multiple_kernel_ridge_random_search\n\n\ndef _create_dataset(backend, n_targets=4):\n    n_featuress = (100, 200)\n    n_samples = 80\n    n_gammas = 3\n\n    Xs = [\n        backend.asarray(backend.randn(n_samples, n_features), backend.float64)\n        for n_features in n_featuress\n    ]\n    Ks = backend.stack([X @ X.T for X in Xs])\n\n    ws = [\n        backend.asarray(backend.randn(n_features, n_targets), backend.float64)\n        for n_features in n_featuress\n    ]\n    Ys = backend.stack([X @ w for X, w in zip(Xs, ws)])\n    Y = Ys.sum(0)\n\n    gammas = backend.asarray(backend.rand(n_gammas, Ks.shape[0]),\n                             backend.float64)\n    gammas /= gammas.sum(1)[:, None]\n\n    return Ks, Y, gammas, Xs\n\n\n@pytest.mark.parametrize('local_alpha', [True, False])\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_local_alphah(\n        backend, local_alpha):\n    _test_solve_multiple_kernel_ridge_random_search(backend=backend,\n                                                    local_alpha=local_alpha)\n\n\n@pytest.mark.parametrize('n_targets_batch', [None, 3])\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_n_targets_batch(\n        backend, n_targets_batch):\n    _test_solve_multiple_kernel_ridge_random_search(\n        backend=backend, n_targets_batch=n_targets_batch)\n\n\n@pytest.mark.parametrize('n_alphas_batch', [None, 2])\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_n_alphas_batch(\n        backend, n_alphas_batch):\n    _test_solve_multiple_kernel_ridge_random_search(\n        backend=backend, n_alphas_batch=n_alphas_batch)\n\n\n@pytest.mark.parametrize('return_weights', ['primal', 'dual'])\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_return_weights(\n        backend, return_weights):\n    _test_solve_multiple_kernel_ridge_random_search(\n        backend=backend, return_weights=return_weights)\n\n\n@pytest.mark.parametrize('diagonalize_method', ['eigh', 'svd'])\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_diagonalize_method(\n        backend, diagonalize_method):\n    _test_solve_multiple_kernel_ridge_random_search(\n        backend=backend, diagonalize_method=diagonalize_method)\n\n\ndef _test_solve_multiple_kernel_ridge_random_search(\n        backend, n_targets_batch=None, n_alphas_batch=None,\n        return_weights=\"dual\", diagonalize_method=\"eigh\", local_alpha=True):\n    backend = set_backend(backend)\n\n    Ks, Y, gammas, Xs = _create_dataset(backend)\n    alphas = backend.asarray_like(backend.logspace(-3, 5, 9), Ks)\n    n_targets = Y.shape[1]\n    cv = sklearn.model_selection.check_cv(10)\n\n    \n    \n    results = solve_multiple_kernel_ridge_random_search(\n        Ks, Y, n_iter=gammas, alphas=alphas, score_func=r2_score, cv=cv,\n        n_targets_batch=n_targets_batch, Xs=Xs, progress_bar=False,\n        return_weights=return_weights, n_alphas_batch=n_alphas_batch,\n        diagonalize_method=diagonalize_method, local_alpha=local_alpha)\n    best_deltas, refit_weights, cv_scores = results\n\n    \n    \n    if local_alpha:  \n        test_scores = []\n        for gamma in backend.sqrt(gammas):\n            X = backend.concatenate([x * g for x, g in zip(Xs, gamma)], 1)\n            for train, test in cv.split(X):\n                for alpha in alphas:\n                    model = sklearn.linear_model.Ridge(\n                        alpha=backend.to_numpy(alpha), fit_intercept=False)\n                    model = model.fit(backend.to_numpy(X[train]),\n                                      backend.to_numpy(Y[train]))\n                    predictions = backend.asarray_like(\n                        model.predict(backend.to_numpy(X[test])), Y)\n                    test_scores.append(r2_score(Y[test], predictions))\n\n        test_scores = backend.stack(test_scores)\n        test_scores = test_scores.reshape(len(gammas), cv.get_n_splits(),\n                                          len(alphas), n_targets)\n        test_scores_mean = backend.max(test_scores.mean(1), 1)\n        assert_array_almost_equal(cv_scores, test_scores_mean, decimal=5)\n\n    \n    \n    for tt in range(n_targets):\n        gamma = backend.exp(best_deltas[:, tt])\n        alpha = 1.0\n\n        if return_weights == 'primal':\n            \n            X = backend.concatenate(\n                [X * backend.sqrt(g) for X, g in zip(Xs, gamma)], 1)\n            model = sklearn.linear_model.Ridge(fit_intercept=False,\n                                               alpha=backend.to_numpy(alpha))\n            w1 = model.fit(backend.to_numpy(X),\n                           backend.to_numpy(Y[:, tt])).coef_\n            w1 = np.split(w1, np.cumsum([X.shape[1] for X in Xs][:-1]), axis=0)\n            w1 = [backend.asarray(w) for w in w1]\n            w1_scaled = backend.concatenate(\n                [w * backend.sqrt(g) for w, g, in zip(w1, gamma)])\n            assert_array_almost_equal(w1_scaled, refit_weights[:, tt],\n                                      decimal=5)\n\n        elif return_weights == 'dual':\n            \n            Ks_64 = backend.asarray(Ks, dtype=backend.float64)\n            gamma_64 = backend.asarray(gamma, dtype=backend.float64)\n            K = backend.matmul(Ks_64.T, gamma_64).T\n            reg = backend.asarray_like(np.eye(K.shape[0]), K) * alpha\n            Y_64 = backend.asarray(Y, dtype=backend.float64)\n            c1 = scipy.linalg.solve(backend.to_numpy(K + reg),\n                                    backend.to_numpy(Y_64[:, tt]))\n            c1 = backend.asarray_like(c1, K)\n            assert_array_almost_equal(c1, refit_weights[:, tt], decimal=5)\n\n\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\ndef test_solve_multiple_kernel_ridge_random_search_single_alpha_numpy(backend):\n    backend = set_backend(backend)\n    \n    Ks, Y, gammas, Xs = _create_dataset(backend)\n    alphas = 1.0\n    \n    Y = backend.to_numpy(Y)\n    results = solve_multiple_kernel_ridge_random_search(\n        Ks, Y, n_iter=gammas, alphas=alphas\n    )\n\n\n@pytest.mark.parametrize('backend', ALL_BACKENDS)\n@pytest.mark.parametrize('n_kernels', [1, 2])\ndef test_solve_multiple_kernel_ridge_random_search_global_alpha(backend, n_kernels):\n    backend = set_backend(backend)\n    \n    Ks, Y, gammas, Xs = _create_dataset(backend, n_targets=20)\n    alphas = backend.asarray_like(backend.logspace(-3, 5, 9), Ks)\n    cv = sklearn.model_selection.check_cv(5)\n\n    deltas, *_, best_alphas = solve_multiple_kernel_ridge_random_search(\n        Ks[:n_kernels],\n        Y,\n        n_iter=50,\n        progress_bar=False,\n        alphas=alphas,\n        cv=cv,\n        local_alpha=False,\n        return_alphas=True\n    )\n    \n    deltas = backend.to_numpy(deltas)\n    if deltas.ndim == 1:\n        assert np.allclose(deltas[0], deltas)\n    else:\n        for dd in deltas:\n            assert np.allclose(dd[0], dd)\n\n    \n    best_alphas = backend.to_numpy(best_alphas)\n    assert np.allclose(best_alphas[0], best_alphas)",
        "summary": "The provided Python code is a test suite for the `solve_multiple_kernel_ridge_random_search` function, which appears to be part of a machine learning library designed for solving multiple kernel ridge regression problems. The tests cover various parameters such as different backends, local alpha settings, batch sizes for targets and alphas, weight return types, diagonalization methods, and single alpha scenarios using NumPy arrays. Each test case validates the correctness of the function's output against expected results, ensuring robustness across different configurations."
    },
    {
        "code": "import logging\nimport os\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom urllib.request import urlretrieve\n\nlogger = logging.getLogger(__name__)\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\nDEFAULT_DIRECTORY = os.path.abspath(os.path.join(HERE, os.pardir, os.pardir, 'data'))\nDATA_DIRECTORY = os.environ.get('REPOSITIONING_COMPARISON_DIRECTORY', DEFAULT_DIRECTORY)\n\n\n\nNODE_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/nodes.tsv'\nEDGE_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/edges.sif.gz'\n\nPERMUTATION1_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-1.json.bz2'\nPERMUTATION2_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-2.json.bz2'\nPERMUTATION3_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-3.json.bz2'\nPERMUTATION4_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-4.json.bz2'\nPERMUTATION5_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-5.json.bz2'\n\nPERMUTATION_DATA_FILE_FMT = 'hetnet_perm-{}.json.bz2'\nPERMUTATION_DATA_URL_FMT = 'https://raw.githubusercontent.com/dhimmel/integrate/master/data/permuted/hetnet_perm-{}.json.bz2'\n\n\n\nTRANSFORMED_FEATURES_URL = 'https://github.com/dhimmel/learn/blob/master/prediction/features/features.tsv.bz2?raw=true'\nVALIDATE_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/learn/master/validate/validation-statuses.tsv'\nSYMPTOMATIC_DATA_URL = 'https://raw.githubusercontent.com/dhimmel/learn/master/prediction/predictions/probabilities.tsv'\n\nREPURPOSE_DATA_URL = 'https://raw.githubusercontent.com/drugrelink/drugrelink/master/notebooks/repurpose_overlap.json'\nREPO_DATA_URL = 'https://raw.githubusercontent.com/drugrelink/drugrelink/master/notebooks/repo_data.csv'\n\n@dataclass\nclass DataPaths:\n    \n\n    node_data_path: str\n    edge_data_path: str\n    transformed_features_path: str\n    validate_data_path: str\n    symptomatic_data_path: str\n    permutation_paths: List[str]\n    data_edge2vec_path: str\n    repurpose_data_path: str\n    repo_data_path: str\n\n\ndef get_data_paths(directory: Optional[str] = None) -> DataPaths:\n    \n    if directory is None:\n        directory = DATA_DIRECTORY\n\n    os.makedirs(directory, exist_ok=True)\n\n    node_data_path = os.path.join(directory, 'nodes.tsv')\n    if not os.path.exists(node_data_path):\n        logger.info(f'downloading {NODE_DATA_URL}')\n        urlretrieve(NODE_DATA_URL, node_data_path)\n\n    edge_data_path = os.path.join(directory, 'edges.sif.gz')\n    if not os.path.exists(edge_data_path):\n        logger.info(f'downloading {EDGE_DATA_URL}')\n        urlretrieve(EDGE_DATA_URL, edge_data_path)\n\n    transformed_features_path = os.path.join(directory, 'transformed-features.tsv.bz2')\n    if not os.path.exists(transformed_features_path):\n        logger.info(f'downloading {TRANSFORMED_FEATURES_URL}')\n        urlretrieve(TRANSFORMED_FEATURES_URL, transformed_features_path)\n\n    validate_data_path = os.path.join(directory, 'validation-statuses.tsv')\n    if not os.path.exists(validate_data_path):\n        logger.info(f'downloading {VALIDATE_DATA_URL}')\n        urlretrieve(VALIDATE_DATA_URL, validate_data_path)\n\n    symptomatic_data_path = os.path.join(directory, 'probabilities.tsv')\n    if not os.path.exists(symptomatic_data_path):\n        logger.info(f'downloading {SYMPTOMATIC_DATA_URL}')\n        urlretrieve(SYMPTOMATIC_DATA_URL, symptomatic_data_path)\n\n    repurpose_data_path = os.path.join(directory,'repurpose_overlap.json')\n    if not os.path.exists(repurpose_data_path):\n        logger.info(f'downloading {REPURPOSE_DATA_URL}')\n        urlretrieve(REPURPOSE_DATA_URL, repurpose_data_path)\n\n    repo_data_path = os.path.join(directory, 'repo_data.csv')\n    if not os.path.exists(repo_data_path):\n        logger.info(f'downloading {REPO_DATA_URL}')\n        urlretrieve(REPO_DATA_URL, repo_data_path)\n\n    permutation_directory = os.path.join(directory, \"permutations\")\n    os.makedirs(permutation_directory, exist_ok=True)\n\n    permutation_paths = []\n    for i in range(5):\n        permutation_data_path = os.path.join(permutation_directory, PERMUTATION_DATA_FILE_FMT.format(i + 1))\n        if not os.path.exists(permutation_data_path):\n            url = PERMUTATION_DATA_URL_FMT.format(i + 1)\n            logger.info(f'downloading {url}')\n            urlretrieve(url, permutation_data_path)\n        permutation_paths.append(permutation_data_path)\n    data_edge2vec_path = os.path.join(directory, 'data_edge2vec')\n\n    return DataPaths(\n        node_data_path=node_data_path,\n        edge_data_path=edge_data_path,\n        transformed_features_path=transformed_features_path,\n        validate_data_path=validate_data_path,\n        symptomatic_data_path=symptomatic_data_path,\n        permutation_paths=permutation_paths,\n        data_edge2vec_path=data_edge2vec_path,\n        repurpose_data_path = repurpose_data_path,\n        repo_data_path = repo_data_path\n    )\n",
        "summary": "The Python script defines a class `DataPaths` to store file paths for various datasets and provides a function `get_data_paths` that downloads these datasets from specified URLs if they are not already present in the designated directory. It uses logging to inform about the download progress and handles the creation of necessary directories."
    },
    {
        "code": "import os\nimport re\n\nfrom oslo_concurrency import processutils as putils\nfrom oslo_log import log as logging\n\nfrom cinder.brick import exception\nfrom cinder.brick import executor\nfrom cinder.i18n import _, _LW, _LE\nfrom cinder.openstack.common import loopingcall\n\nLOG = logging.getLogger(__name__)\n\nMULTIPATH_ERROR_REGEX = re.compile(\"\\w{3} \\d+ \\d\\d:\\d\\d:\\d\\d \\|.*$\")\nMULTIPATH_WWID_REGEX = re.compile(\"\\((?P<wwid>.+)\\)\")\n\n\nclass LinuxSCSI(executor.Executor):\n    def __init__(self, root_helper, execute=putils.execute,\n                 *args, **kwargs):\n        super(LinuxSCSI, self).__init__(root_helper, execute,\n                                        *args, **kwargs)\n\n    def echo_scsi_command(self, path, content):\n        \n\n        args = [\"-a\", path]\n        kwargs = dict(process_input=content,\n                      run_as_root=True,\n                      root_helper=self._root_helper)\n        self._execute('tee', *args, **kwargs)\n\n    def get_name_from_path(self, path):\n        \n\n        name = os.path.realpath(path)\n        if name.startswith(\"/dev/\"):\n            return name\n        else:\n            return None\n\n    def remove_scsi_device(self, device):\n        \n\n        path = \"/sys/block/%s/device/delete\" % device.replace(\"/dev/\", \"\")\n        if os.path.exists(path):\n            \n            self.flush_device_io(device)\n\n            LOG.debug(\"Remove SCSI device(%s) with %s\" % (device, path))\n            self.echo_scsi_command(path, \"1\")\n\n    def wait_for_volume_removal(self, volume_path):\n        \n\n        def _wait_for_volume_removal(volume_path):\n            LOG.debug(\"Waiting for SCSI mount point %s to be removed.\",\n                      volume_path)\n            if os.path.exists(volume_path):\n                if self.tries >= self.scan_attempts:\n                    msg = _LE(\"Exceeded the number of attempts to detect \"\n                              \"volume removal.\")\n                    LOG.error(msg)\n                    raise exception.VolumePathNotRemoved(\n                        volume_path=volume_path)\n\n                LOG.debug(\"%(path)s still exists, rescanning. Try number: \"\n                          \"%(tries)s\",\n                          {'path': volume_path, 'tries': self.tries})\n                self.tries = self.tries + 1\n            else:\n                LOG.debug(\"SCSI mount point %s has been removed.\", volume_path)\n                raise loopingcall.LoopingCallDone()\n\n        \n        \n        self.tries = 0\n        self.scan_attempts = 3\n        timer = loopingcall.FixedIntervalLoopingCall(\n            _wait_for_volume_removal, volume_path)\n        timer.start(interval=2).wait()\n\n    def get_device_info(self, device):\n        (out, _err) = self._execute('sg_scan', device, run_as_root=True,\n                                    root_helper=self._root_helper)\n        dev_info = {'device': device, 'host': None,\n                    'channel': None, 'id': None, 'lun': None}\n        if out:\n            line = out.strip()\n            line = line.replace(device + \": \", \"\")\n            info = line.split(\" \")\n\n            for item in info:\n                if '=' in item:\n                    pair = item.split('=')\n                    dev_info[pair[0]] = pair[1]\n                elif 'scsi' in item:\n                    dev_info['host'] = item.replace('scsi', '')\n\n        return dev_info\n\n    def remove_multipath_device(self, multipath_name):\n        \n\n        LOG.debug(\"remove multipath device %s\" % multipath_name)\n        mpath_dev = self.find_multipath_device(multipath_name)\n        if mpath_dev:\n            devices = mpath_dev['devices']\n            LOG.debug(\"multipath LUNs to remove %s\" % devices)\n            for device in devices:\n                self.remove_scsi_device(device['device'])\n            self.flush_multipath_device(mpath_dev['id'])\n\n    def flush_device_io(self, device):\n        \n        try:\n            LOG.debug(\"Flushing IO for device %s\" % device)\n            self._execute('blockdev', '--flushbufs', device, run_as_root=True,\n                          root_helper=self._root_helper)\n        except putils.ProcessExecutionError as exc:\n            msg = _(\"Failed to flush IO buffers prior to removing\"\n                    \" device: (%(code)s)\") % {'code': exc.exit_code}\n            LOG.warn(msg)\n\n    def flush_multipath_device(self, device):\n        try:\n            LOG.debug(\"Flush multipath device %s\" % device)\n            self._execute('multipath', '-f', device, run_as_root=True,\n                          root_helper=self._root_helper)\n        except putils.ProcessExecutionError as exc:\n            LOG.warn(_LW(\"multipath call failed exit (%(code)s)\")\n                     % {'code': exc.exit_code})\n\n    def flush_multipath_devices(self):\n        try:\n            self._execute('multipath', '-F', run_as_root=True,\n                          root_helper=self._root_helper)\n        except putils.ProcessExecutionError as exc:\n            LOG.warn(_LW(\"multipath call failed exit (%(code)s)\")\n                     % {'code': exc.exit_code})\n\n    def find_multipath_device(self, device):\n        \n\n        mdev = None\n        devices = []\n        out = None\n        try:\n            (out, _err) = self._execute('multipath', '-l', device,\n                                        run_as_root=True,\n                                        root_helper=self._root_helper)\n        except putils.ProcessExecutionError as exc:\n            LOG.warn(_LW(\"multipath call failed exit (%(code)s)\")\n                     % {'code': exc.exit_code})\n            return None\n\n        if out:\n            lines = out.strip()\n            lines = lines.split(\"\\n\")\n            lines = [line for line in lines\n                     if not re.match(MULTIPATH_ERROR_REGEX, line)]\n            if lines:\n\n                \n                \n                \n                \n                mdev_name = lines[0].split(\" \")[0]\n                mdev = '/dev/mapper/%s' % mdev_name\n\n                \n                wwid_search = MULTIPATH_WWID_REGEX.search(lines[0])\n                if wwid_search is not None:\n                    mdev_id = wwid_search.group('wwid')\n                else:\n                    mdev_id = mdev_name\n\n                \n                try:\n                    os.stat(mdev)\n                except OSError:\n                    LOG.warn(_LW(\"Couldn't find multipath device %s\"), mdev)\n                    return None\n\n                LOG.debug(\"Found multipath device = %(mdev)s\"\n                          % {'mdev': mdev})\n                device_lines = lines[3:]\n                for dev_line in device_lines:\n                    if dev_line.find(\"policy\") != -1:\n                        continue\n\n                    dev_line = dev_line.lstrip(' |-`')\n                    dev_info = dev_line.split()\n                    address = dev_info[0].split(\":\")\n\n                    dev = {'device': '/dev/%s' % dev_info[1],\n                           'host': address[0], 'channel': address[1],\n                           'id': address[2], 'lun': address[3]\n                           }\n\n                    devices.append(dev)\n\n        if mdev is not None:\n            info = {\"device\": mdev,\n                    \"id\": mdev_id,\n                    \"name\": mdev_name,\n                    \"devices\": devices}\n            return info\n        return None\n",
        "summary": "The provided Python code defines a class `LinuxSCSI` that extends the `Executor` class from the Cinder project. This class provides methods for managing SCSI and multipath devices on Linux systems, including echoing SCSI commands, removing devices, waiting for volume removals, retrieving device information, and flushing I/O buffers and multipath devices."
    },
    {
        "code": "import pytest\n\nfrom django.conf import settings\nfrom django.contrib import messages\n\nfrom proposals.models import TalkProposal, TutorialProposal\n\n\npytestmark = pytest.mark.skipif(\n    not settings.PROPOSALS_WITHDRAWABLE,\n    reason='proposal withdrawal disabled',\n)\n\n\ndef test_talk_proposal_cancel_login(client):\n    response = client.get('/en-us/proposals/talk/42/cancel/', follow=True)\n    assert response.redirect_chain == [\n        ('/en-us/accounts/login/?next=/en-us/proposals/talk/42/cancel/', 302),\n    ]\n\n\ndef test_tutorial_proposal_cancel_login(client):\n    response = client.get('/en-us/proposals/tutorial/42/cancel/', follow=True)\n    assert response.redirect_chain == [\n        ('/en-us/accounts/login/?next=/en-us/proposals/tutorial/42/cancel/',\n         302),\n    ]\n\n\n@pytest.mark.parametrize('method', ['get', 'post'])\ndef test_talk_proposal_cancel_denied(bare_user_client, method):\n    response = getattr(bare_user_client, method)(\n        '/en-us/proposals/talk/42/cancel/',\n    )\n    assert response.status_code == 403\n\n\n@pytest.mark.parametrize('method', ['get', 'post'])\ndef test_tutorial_proposal_cancel_denied(bare_user_client, method):\n    response = getattr(bare_user_client, method)(\n        '/en-us/proposals/tutorial/42/cancel/',\n    )\n    assert response.status_code == 403\n\n\ndef test_talk_proposal_cancel_get(agreed_user_client, talk_proposal):\n    \n    response = agreed_user_client.get('/en-us/proposals/talk/42/cancel/')\n    assert response.status_code == 405\n\n\ndef test_tutorial_proposal_cancel_get(agreed_user_client, tutorial_proposal):\n    \n    response = agreed_user_client.get('/en-us/proposals/tutorial/42/cancel/')\n    assert response.status_code == 405\n\n\ndef test_talk_proposal_cancel_not_owned(another_agreed_user_client, talk_proposal):\n    response = another_agreed_user_client.post('/en-us/proposals/talk/42/cancel/')\n    assert response.status_code == 404\n\n\ndef test_tutorial_proposal_cancel_not_owned(\n        another_agreed_user_client, tutorial_proposal):\n    response = another_agreed_user_client.post('/en-us/proposals/tutorial/42/cancel/')\n    assert response.status_code == 404\n\n\ndef test_talk_proposal_cancel(agreed_user_client, talk_proposal):\n    assert not talk_proposal.cancelled\n\n    response = agreed_user_client.post('/en-us/proposals/talk/42/cancel/', {\n        'cancelled': True,\n    }, follow=True)\n    assert response.redirect_chain == [('/en-us/dashboard/', 302)], (\n        response.context['form'].errors\n    )\n\n    assert TalkProposal.objects.get(pk=42).cancelled\n\n    msgs = [(m.level, m.message) for m in response.context['messages']]\n    assert msgs == [\n        (messages.INFO,\n         'Talk proposal '\n         '<strong>Beyond the Style Guides&lt;br&gt;</strong> withdrawn.'),\n    ]\n\n\ndef test_talk_proposal_reactivate(agreed_user_client, cancelled_talk_proposal):\n    assert cancelled_talk_proposal.cancelled\n\n    response = agreed_user_client.post('/en-us/proposals/talk/42/cancel/', {\n        'cancelled': '',\n    }, follow=True)\n    assert response.redirect_chain == [('/en-us/dashboard/', 302)], (\n        response.context['form'].errors\n    )\n\n    assert not TalkProposal.objects.get(pk=42).cancelled\n\n    msgs = [(m.level, m.message) for m in response.context['messages']]\n    assert msgs == [\n        (messages.SUCCESS,\n         'Talk proposal '\n         '<strong>Beyond the Style Guides&lt;br&gt;</strong> reactivated.'),\n    ]\n\n\ndef test_tutorial_proposal_cancel(agreed_user_client, tutorial_proposal):\n    assert not tutorial_proposal.cancelled\n\n    response = agreed_user_client.post('/en-us/proposals/tutorial/42/cancel/', {\n        'cancelled': True,\n    }, follow=True)\n    assert response.redirect_chain == [('/en-us/dashboard/', 302)], (\n        response.context['form'].errors\n    )\n\n    assert TutorialProposal.objects.get(pk=42).cancelled\n\n    msgs = [(m.level, m.message) for m in response.context['messages']]\n    assert msgs == [\n        (messages.INFO,\n         'Tutorial proposal '\n         '<strong>Beyond the Style Guides&lt;br&gt;</strong> withdrawn.'),\n    ]\n\n\ndef test_tutorial_proposal_reactivate(\n        agreed_user_client, cancelled_tutorial_proposal):\n    assert cancelled_tutorial_proposal.cancelled\n\n    response = agreed_user_client.post('/en-us/proposals/tutorial/42/cancel/', {\n        'cancelled': '',\n    }, follow=True)\n    assert response.redirect_chain == [('/en-us/dashboard/', 302)], (\n        response.context['form'].errors\n    )\n\n    assert not TutorialProposal.objects.get(pk=42).cancelled\n\n    msgs = [(m.level, m.message) for m in response.context['messages']]\n    assert msgs == [\n        (messages.SUCCESS,\n         'Tutorial proposal '\n         '<strong>Beyond the Style Guides&lt;br&gt;</strong> reactivated.'),\n    ]\n",
        "summary": "The provided Python code is a set of tests using pytest to validate functionality related to canceling and reactivating talk and tutorial proposals in a Django application. The tests cover scenarios such as redirecting unauthenticated users, denying access to non-owners, handling GET requests for cancellation, and ensuring proper cancellation and reactivation of proposals with appropriate messages."
    },
    {
        "code": "import numpy as np\n\n\ndef blit(im1, im2, pos=None, mask=None):\n    \n    if pos is None:\n        pos = (0, 0)  \n    else:\n        \n        pos = tuple(pos)\n    im2.paste(im1, pos, mask)\n    return im2\n\n\ndef color_gradient(\n    size,\n    p1,\n    p2=None,\n    vector=None,\n    radius=None,\n    color_1=0.0,\n    color_2=1.0,\n    shape=\"linear\",\n    offset=0,\n):\n    \n    \n    w, h = size\n\n    color_1 = np.array(color_1).astype(float)\n    color_2 = np.array(color_2).astype(float)\n\n    if shape == \"bilinear\":\n        if vector is None:\n            if p2 is None:\n                raise ValueError(\"You must provide either 'p2' or 'vector'\")\n            vector = np.array(p2) - np.array(p1)\n\n        m1, m2 = [\n            color_gradient(\n                size,\n                p1,\n                vector=v,\n                color_1=1.0,\n                color_2=0.0,\n                shape=\"linear\",\n                offset=offset,\n            )\n            for v in [vector, [-v for v in vector]]\n        ]\n\n        arr = np.maximum(m1, m2)\n        if color_1.size > 1:\n            arr = np.dstack(3 * [arr])\n        return arr * color_1 + (1 - arr) * color_2\n\n    p1 = np.array(p1[::-1]).astype(float)\n\n    M = np.dstack(np.meshgrid(range(w), range(h))[::-1]).astype(float)\n\n    if shape == \"linear\":\n        if vector is None:\n            if p2 is not None:\n                vector = np.array(p2[::-1]) - p1\n            else:\n                raise ValueError(\"You must provide either 'p2' or 'vector'\")\n        else:\n            vector = np.array(vector[::-1])\n\n        norm = np.linalg.norm(vector)\n        n_vec = vector / norm ** 2  \n\n        p1 = p1 + offset * vector\n        arr = (M - p1).dot(n_vec) / (1 - offset)\n        arr = np.minimum(1, np.maximum(0, arr))\n        if color_1.size > 1:\n            arr = np.dstack(3 * [arr])\n        return arr * color_1 + (1 - arr) * color_2\n\n    elif shape == \"radial\":\n        if (radius or 0) == 0:\n            arr = np.ones((h, w))\n        else:\n            arr = (np.sqrt(((M - p1) ** 2).sum(axis=2))) - offset * radius\n            arr = arr / ((1 - offset) * radius)\n            arr = np.minimum(1.0, np.maximum(0, arr))\n\n        if color_1.size > 1:\n            arr = np.dstack(3 * [arr])\n        return (1 - arr) * color_1 + arr * color_2\n    raise ValueError(\"Invalid shape, should be either 'radial', 'linear' or 'bilinear'\")\n\n\ndef color_split(\n    size,\n    x=None,\n    y=None,\n    p1=None,\n    p2=None,\n    vector=None,\n    color_1=0,\n    color_2=1.0,\n    gradient_width=0,\n):\n    \n    if gradient_width or ((x is None) and (y is None)):\n        if p2 is not None:\n            vector = np.array(p2) - np.array(p1)\n        elif x is not None:\n            vector = np.array([0, -1.0])\n            p1 = np.array([x, 0])\n        elif y is not None:\n            vector = np.array([1.0, 0.0])\n            p1 = np.array([0, y])\n\n        x, y = vector\n        vector = np.array([y, -x]).astype(\"float\")\n        norm = np.linalg.norm(vector)\n        vector = max(0.1, gradient_width) * vector / norm\n        return color_gradient(\n            size, p1, vector=vector, color_1=color_1, color_2=color_2, shape=\"linear\"\n        )\n    else:\n        w, h = size\n        shape = (h, w) if np.isscalar(color_1) else (h, w, len(color_1))\n        arr = np.zeros(shape)\n        if x:\n            arr[:, :x] = color_1\n            arr[:, x:] = color_2\n        elif y:\n            arr[:y] = color_1\n            arr[y:] = color_2\n        return arr\n\n\ndef circle(screensize, center, radius, color=1.0, bg_color=0, blur=1):\n    \n    offset = 1.0 * (radius - blur) / radius if radius else 0\n    return color_gradient(\n        screensize,\n        p1=center,\n        radius=radius,\n        color_1=color,\n        color_2=bg_color,\n        shape=\"radial\",\n        offset=offset,\n    )\n",
        "summary": "The provided Python code defines several functions for image manipulation and gradient generation. The `blit` function overlays one image onto another at a specified position, optionally using a mask. The `color_gradient` function creates color gradients of various shapes (linear, radial, bilinear) based on given parameters such as size, positions, colors, and offsets. The `color_split` function generates linear gradients along specified axes or vectors. Lastly, the `circle` function creates a circular gradient with customizable radius, center, and background color."
    },
    {
        "code": "import h5py\nimport pyre\nfrom ..Base import Base\nfrom .Identification import Identification\n\nclass SLC(Base, family='nisar.productreader.slc'):\n    \n   \n    productValidationType = pyre.properties.str(default='SLC')\n    productValidationType.doc = 'Validation tag to ensure correct product type'\n\n    def __init__(self, **kwds):\n        \n\n        \n        super().__init__(**kwds) \n\n\n    def populateIdentification(self):\n        \n        with h5py.File(self.filename, 'r', libver='latest', swmr=True) as f:\n            h5grp = f[self.IdentificationPath]\n            self.identification = Identification(h5grp)\n",
        "summary": "The provided Python code defines a class `SLC` that inherits from `Base`, part of the NISAR product reader family. It includes properties for validating the product type and a method to populate identification data by reading an HDF5 file using the `h5py` library."
    },
    {
        "code": "import os\nimport sys\n\nsys.path.insert(0, os.path.abspath('../..'))\n\n\n\n\nextensions = [\n    'sphinx.ext.autodoc',\n    \n    'stevedore.sphinxext',\n    'oslosphinx'\n]\n\n\n\n\n\n\nsource_suffix = '.rst'\n\n\nmaster_doc = 'index'\n\n\nproject = u'tacker'\ncopyright = u'2013, OpenStack Foundation'\n\n\nadd_function_parentheses = True\n\n\n\nadd_module_names = True\n\n\npygments_style = 'sphinx'\n\n\nmodindex_common_prefix = ['tacker.']\n\n\n\n\n\n\n\n\n\n\nhtmlhelp_basename = '%sdoc' % project\n\n\n\n\nlatex_documents = [\n    ('index',\n     '%s.tex' % project,\n     u'%s Documentation' % project,\n     u'OpenStack Foundation', 'manual'),\n]\n\n\n\n",
        "summary": "This Python script configures Sphinx, a documentation generator for Python projects, to build documentation for the Tacker project. It sets up extensions, specifies source and master documents, and defines various options like HTML help basename and LaTeX document details."
    },
    {
        "code": "import json\nimport sys\nfrom collections import defaultdict\nfrom unidecode import unidecode\nfrom HTMLParser import HTMLParser\n\nh = HTMLParser()\n\n_schema_keys_to_yml = {\n                'description' : 'notes',\n                'image' : 'photo',\n                'recipeCuisine' : 'notes',\n                'recipeInstructions' : 'directions',\n                'recipeCategory' : 'categories',\n                'name' : 'name',\n                'author' : 'source',\n                'ingredients' : 'ingredients',\n                'recipeYield' : 'servings',\n                'prepTime' : 'prep_time',\n                'cookTime' : 'cook_time',\n                'source_url' : 'source_url',\n                'notes' : 'notes',\n            }\n\n_yml_keys = [\n            'name',\n            'servings',\n            'prep_time',\n            'cook_time',\n            'on_favorites',\n            'categories',\n            'ingredients',\n            'directions',\n            'photo',\n            'source_url',\n            'source',\n            'notes',\n            ]\ndef remove_non_ascii(input):\n    \n    return unidecode(unicode(input, encoding = \"utf-8\"))\n\n\n\ndef load_json(filename):\n    \n    with open(filename) as fi:\n        contents = json.load(fi)\n    return contents\n\n\ndef default_yml_recipe():\n    \n    default_recipe = {}\n    for key in _yml_keys:\n        default_recipe[key] = ''\n    return default_recipe\n\n\n\ndef translate_json_to_yml_prep(recipes):\n    \n    all_ymls = []\n    for recipe  in recipes:\n        yml_recipe = defaultdict(str)\n        for key in _schema_keys_to_yml:\n            if key in recipe:\n                handle_key(recipe[key], yml_recipe, key)\n        all_ymls.append(yml_recipe)\n    return all_ymls\n\n\n\ndef handle_key(incomingData, yml_out, key):\n    \n    try:\n        if key in (\n                    'description',\n                    'recipeCuisine',\n                    'recipeCategory',\n                    'recipeYield',\n                    'prepTime',\n                    'cookTime',\n                    'source_url',\n                    'name',\n                    'notes',\n                    ):\n            if yml_out[_schema_keys_to_yml[key]]:\n                yml_out[_schema_keys_to_yml[key]] += '\\n'\n            yml_out[_schema_keys_to_yml[key]] += incomingData\n        elif key in ('recipeInstructions','ingredients'):\n            yml_out[_schema_keys_to_yml[key]] = '\\n'.join(incomingData)\n        \n        \n        elif key == 'author':\n            yml_out[_schema_keys_to_yml[key]] = incomingData['name']\n        elif key == 'image':\n            yml_out[_schema_keys_to_yml[key]] = ''\n    except:\n        pass\n\n\n\n\n_yml_keys = [\n            'name',\n            'servings',\n            'prep_time',\n            'cook_time',\n            'on_favorites',\n            'categories',\n            'ingredients',\n            'directions',\n            'photo',\n            'source_url',\n            'source',\n            'notes',\n            ]\n\ndef yml_prep_to_yml(list_of_yml_dicts):\n    \n    strList = []\n    for item in list_of_yml_dicts:\n        strList.append('- ')\n        for key in _yml_keys:\n            if not item[key]:\n                continue\n\t    line = item[key].replace('<br>','\\n').replace('<br/>','\\n').replace('\\n','\\r\\n    ')\n\t    line = line.replace('<span class=\"fn\">','').replace('</span>','')\n\t    line = line.replace('<span class =\"fn\">','').replace('<span class = \"fn\">','').replace('<span class= \"fn\">','')\n\t    line = h.unescape(line)\n\t    line = remove_non_ascii(line.encode('UTF-8'))\n\t    line = line.replace('\\n','')\n            strList.append(key)\n            strList.append(': ')\n            if key in ('ingredients', 'directions','notes'):\n                strList.append('|\\r\\n    ')\n            strList.append(line)\n            strList.append('\\r\\n  ')\n        del(strList[-1])\n        strList.append('\\r\\n')\n    print u''.join(strList).encode('utf-8').strip()\n            \n\n\n\n\n\nif __name__ == '__main__':\n    json_data = load_json(sys.argv[1])\n    yml_prep = translate_json_to_yml_prep(json_data)\n    yml_prep_to_yml(yml_prep)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "summary": "The provided Python script reads JSON data from a file, translates it into YAML format based on predefined key mappings and transformations, and then prints the resulting YAML content. It handles various types of data such as text, lists, and nested objects, ensuring proper formatting and escaping for YAML output."
    },
    {
        "code": "import logging, time\n\nfrom django.core.management.base import BaseCommand\n\nfrom ...models import get_broker\nfrom ...renewals import (create_charges_for_balance, complete_charges,\n    extend_subscriptions, recognize_income, trigger_expiration_notices)\nfrom ...utils import datetime_or_now\nfrom ... import settings\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Command(BaseCommand):\n    help = \n\n    def add_arguments(self, parser):\n        parser.add_argument('--dry-run', action='store_true',\n            dest='dry_run', default=False,\n            help='Do not commit transactions nor submit charges to processor')\n        parser.add_argument('--no-charges', action='store_true',\n            dest='no_charges', default=False,\n            help='Do not submit charges to processor')\n        parser.add_argument('--at-time', action='store',\n            dest='at_time', default=None,\n            help='Specifies the time at which the command runs')\n\n    def handle(self, *args, **options):\n        \n        dry_run = options['dry_run']\n        no_charges = options['no_charges']\n        end_period = datetime_or_now(options['at_time'])\n        if dry_run:\n            LOGGER.warning(\"dry_run: no changes will be committed.\")\n        if no_charges:\n            LOGGER.warning(\"no_charges: no charges will be submitted.\")\n        try:\n            recognize_income(end_period, dry_run=dry_run)\n        except Exception as err:\n            LOGGER.exception(\"recognize_income: %s\", err)\n        try:\n            extend_subscriptions(end_period, dry_run=dry_run)\n        except Exception as err:\n            LOGGER.exception(\"extend_subscriptions: %s\", err)\n        try:\n            create_charges_for_balance(\n                end_period, dry_run=dry_run or no_charges)\n        except Exception as err:\n            LOGGER.exception(\n                \"Unable to create charges for balance on broker '%s'\",\n                get_broker())\n        if not (dry_run or no_charges):\n            \n            \n            time.sleep(30)\n            complete_charges()\n\n        \n        expiration_periods = settings.EXPIRE_NOTICE_DAYS\n        for period in expiration_periods:\n            trigger_expiration_notices(\n                end_period, nb_days=period, dry_run=dry_run)\n",
        "summary": "This Django management command processes financial transactions and subscriptions, handling income recognition, subscription extensions, charge creation, and expiration notices. It includes options for dry runs and specific time execution, logging errors encountered during processing."
    },
    {
        "code": "from __future__ import print_function\nimport datetime\nimport pickle\nimport os.path\nfrom googleapiclient.discovery import build\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\n\n\nSCOPES = ['https://www.googleapis.com/auth/calendar']\n\ndef main():\n    \n    creds = None\n    \n    \n    \n    if os.path.exists('token.pickle'):\n        with open('token.pickle', 'rb') as token:\n            creds = pickle.load(token)\n    \n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                'credentials.json', SCOPES)\n            print(flow)\n            creds = flow.run_local_server(port=0)\n        \n        with open('token.pickle', 'wb') as token:\n            pickle.dump(creds, token)\n\n    service = build('calendar', 'v3', credentials=creds)\n\n    \n    \n    \n    \n                                       \n                                       \n    \n\n    \n     \n    \n    \n     \n\n    print(\"Creating events\")\n    \n\n\n\n\n\n    event = {\n      'summary': 'Google I/O 2019',\n      'location': '800 Howard St., San Francisco, CA 94103',\n      'description': 'A chance to hear more about Google\\'s developer products.',\n      'start': {\n        'dateTime': '2019-08-28T09:00:00-07:00',\n        'timeZone': 'America/Los_Angeles',\n      },\n      'end': {\n        'dateTime': '2019-09-01T17:00:00-07:00',\n        'timeZone': 'America/Los_Angeles',\n      },\n      'recurrence': [\n        'RRULE:FREQ=DAILY;COUNT=2'\n      ],\n      'attendees': [\n        {'email': 'lpage@example.com'},\n        {'email': 'sbrin@example.com'},\n      ],\n      'reminders': {\n        'useDefault': False,\n        'overrides': [\n          {'method': 'email', 'minutes': 24 * 60},\n          {'method': 'popup', 'minutes': 10},\n        ],\n      },\n    }\n\n    event = service.events().insert(calendarId='primary', body=event).execute()\n    print ('Event created: %s' % (event.get('htmlLink')))\n\n\n\nif __name__ == '__main__':\n    main()",
        "summary": "The provided Python script uses the Google Calendar API to authenticate and create a new event in the user's primary calendar. It handles token management, including refreshing expired tokens, and constructs an event with details such as summary, location, description, start and end times, recurrence, attendees, and reminders before inserting it into the calendar and printing the event link."
    },
    {
        "code": "import database\n\n\ndef load_shard_from_db(conf):\n    \n    shards = database.load_shard(conf)\n\n    return shards\n\n\ndef get_shard(shards, url):\n    \n    if not shards:\n        return {}\n    else:\n        return shards[hash(str(url['hostname'])+str(url['port'])+str(url['path'])) % len(shards)]\n\n",
        "summary": "The provided Python code defines two functions: `load_shard_from_db` which loads shard configurations from a database using the given configuration, and `get_shard` which retrieves a specific shard based on a URL's hostname, port, and path, distributing them evenly across available shards."
    },
    {
        "code": "from nonebot import on_command, CommandSession\n\n@on_command('help', aliases=('h', '\u5e2e\u52a9'), only_to_me=False)\nasync def manual(session: CommandSession):\n\n    await session.send(f'[CQ:image,file=/admin/manual.png]')\n\n@manual.args_parser\nasync def _(session: CommandSession):\n    \n    return",
        "summary": "The provided Python code defines a command handler for the \"help\" command in a bot using the NoneBot framework. When the command is invoked, it sends an image file located at \"/admin/manual.png\". The `args_parser` function associated with this command does not perform any actions and returns immediately."
    },
    {
        "code": "from django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('pipeline', '0004_hospital'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='hospital',\n            name='sv_name',\n        ),\n    ]\n",
        "summary": "This Python code defines a Django migration that removes the `sv_name` field from the `Hospital` model in the `pipeline` app, following the dependency on another migration file named '0004_hospital'."
    },
    {
        "code": "from django.conf.urls import url\r\nfrom app.models import Poll\r\n\r\nimport app.views\r\n\r\nurlpatterns = [\r\n    url(r'^$',\r\n        app.views.PollListView.as_view(\r\n            queryset=Poll.objects.order_by('-pub_date')[:5],\r\n            context_object_name='latest_poll_list',\r\n            template_name='app/index.html',),\r\n        name='home'),\r\n    url(r'^(?P<pk>\\d+)/$',\r\n        app.views.PollDetailView.as_view(\r\n            template_name='app/details.html'),\r\n        name='detail'),\r\n    url(r'^(?P<pk>\\d+)/results/$',\r\n        app.views.PollResultsView.as_view(\r\n            template_name='app/results.html'),\r\n        name='results'),\r\n    url(r'^(?P<poll_id>\\d+)/vote/$', app.views.vote, name='vote'),\r\n]\r\n",
        "summary": "The provided Python code defines URL patterns for a Django application, mapping URLs to specific views that handle requests related to polls. It includes routes for displaying a list of the latest polls, viewing details of a single poll, seeing results of a poll, and casting votes on a poll."
    },
    {
        "code": "import dataclasses\n\nfrom acme.adders import reverb as adders_reverb\nimport numpy as np\n\n\n@dataclasses.dataclass\nclass DQNConfig:\n  \n  epsilon: float = 0.05  \n  \n  seed: int = 1  \n\n  \n  learning_rate: float = 1e-3  \n  adam_eps: float = 1e-8  \n  discount: float = 0.99  \n  n_step: int = 5  \n  target_update_period: int = 100  \n  max_gradient_norm: float = np.inf  \n\n  \n  batch_size: int = 256  \n  min_replay_size: int = 1_000  \n  max_replay_size: int = 1_000_000  \n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  importance_sampling_exponent: float = 0.2  \n  priority_exponent: float = 0.6  \n  prefetch_size: int = 4  \n  samples_per_insert: float = 0.5  \n  \n  \n  samples_per_insert_tolerance_rate: float = 0.1\n\n  \n  num_sgd_steps_per_step: int = 1\n\n@dataclasses.dataclass\nclass DQNEmpowermentConfig:\n  \n  epsilon: float = 0.05  \n  \n  seed: int = 1  \n\n  \n  learning_rate: float = 1e-3  \n  adam_eps: float = 1e-8  \n  discount: float = 0.99  \n  n_step: int = 5  \n  target_update_period: int = 100  \n  max_gradient_norm: float = np.inf  \n\n  \n  batch_size: int = 256  \n  min_replay_size: int = 1_000  \n  max_replay_size: int = 1_000_000  \n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  importance_sampling_exponent: float = 0.2  \n  priority_exponent: float = 0.6  \n  prefetch_size: int = 4  \n  samples_per_insert: float = 0.5  \n  sequence_length: int = 10\n  prefetch_size: int = 4\n  sequence_period: int = 2\n\n  \n  \n  samples_per_insert_tolerance_rate: float = 0.1\n\n  \n  num_sgd_steps_per_step: int = 1\n",
        "summary": "The provided Python code defines two data classes, `DQNConfig` and `DQNEmpowermentConfig`, using the `dataclasses` module from the standard library. Both classes encapsulate configuration parameters for a Deep Q-Network (DQN) algorithm, including hyperparameters like learning rate, discount factor, batch size, and replay buffer settings. The `DQNEmpowermentConfig` class extends `DQNConfig` by adding an additional parameter `sequence_length`, which is specific to empowered DQN configurations."
    },
    {
        "code": "import torch\nfrom torch import nn\nfrom torch.distributions import MultivariateNormal\n\nclass Normal(nn.Module):\n    def __init__(self, num_vars=100):\n        super(Normal, self).__init__()\n\n        self.num_vars = num_vars\n\n        self.means = nn.Parameter(torch.zeros(num_vars))\n        self.std = nn.Parameter(torch.eye(num_vars))\n\n    def log_prob(self, x):\n        distr = MultivariateNormal(self.means, self.std)\n        return distr.log_prob(x)\n\n    def sample(self, num_samples):\n        distr = MultivariateNormal(self.means, self.std)\n        return distr.sample_n(num_samples)\n",
        "summary": "The provided Python code defines a PyTorch module named `Normal` that represents a multivariate normal distribution. This module includes methods for calculating the log probability of input data and sampling from the distribution, with parameters for means and standard deviations initialized as zero and an identity matrix respectively."
    },
    {
        "code": "import sys\n\nclass SortableArray():\n\tdef __init__(self, arr):\n\t\tself.arr = arr\n\n\tdef partition(self, left, right):\n\t\t\n\t\tpivot_index = right\n\t\t\n\t\tpivot = self.arr[pivot_index]\n\n\t\tright -= 1\n\t\tprint(f'left orig: {left} right orig: {right}')\n\n\t\twhile True:\n\t\t\t\n\t\t\twhile self.arr[left] < pivot:\n\t\t\t\tprint(f'left: {left}')\n\t\t\t\tleft += 1\n\t\t\tprint('left', left)\n\n\t\t\t\n\t\t\twhile right > 0 and self.arr[right] > pivot:\n\t\t\t\tprint(f'right: {right}')\n\t\t\t\tright -= 1\n\t\t\tprint('right', right)\n\t\t\t\n\t\t\t\n\t\t\tif left >= right:\n\t\t\t\tbreak\n\t\t\t\n\t\t\telse:\n\t\t\t\tself.arr[left], self.arr[right] = self.arr[right], self.arr[left]\n\t\t\t\tleft += 1\t\n\t\t\n\t\tself.arr[left], self.arr[pivot_index] = self.arr[pivot_index], self.arr[left]\n\t\tprint(self.arr)\n\t\treturn left\n\t\n\tdef quicksort(self, left, right):\n\t\t\n\t\tif right - left <= 0:\n\t\t\treturn\n\t\t\n\t\tpivot_index = self.partition(left, right)\n\t\t\n\t\t\n\t\tself.quicksort(left, pivot_index - 1)\n\n\t\t\n\t\tself.quicksort(pivot_index + 1, right)\t\t\t \n\t\t\t\n\tdef quickselect(self, kth_lowest_num, left, right):\n\t\t\n\t\tif right - left <= 0:\n\t\t\treturn self.arr[left]\n\t\t\n\t\tpivot_index = self.partition(left, right)\n\t\t\n\t\t\n\t\tif kth_lowest_num < pivot_index:\n\t\t\tself.quickselect(kth_lowest_num, left, pivot_index - 1)\n\n\t\t\n\t\telif kth_lowest_num > pivot_index:\n\t\t\tself.quickselect(kth_lowest_num, pivot_index + 1, right)\t\t\t \n\t\t\n\t\telse:\n\t\t\t\n\t\t\tprint(f'kth {kth_lowest_num}: {self.arr[pivot_index]}')\n\t\t\treturn self.arr[pivot_index] \n\n\t\t\t\ndef main(arr, kth):\n\tsortable = SortableArray(arr)\n\tprint(sortable.quickselect(kth, 0, len(arr) - 1))\n\tsortable.quicksort(0, len(arr) - 1)\n\tprint(f'final sorted array: {sortable.arr}')\n\nif __name__ == '__main__':\n\tmain([int(x) for x in sys.argv[1].split(',')], int(sys.argv[2]))\n",
        "summary": "The provided Python code defines a class `SortableArray` that implements the QuickSort and QuickSelect algorithms. The `quicksort` method sorts an array, while the `quickselect` method finds the kth smallest element in an unsorted array. The `main` function initializes an instance of `SortableArray`, uses `quickselect` to find a specific element, sorts the array using `quicksort`, and prints both the sorted array and the found element."
    },
    {
        "code": "from types import MethodType\n\nimport numpy as np\n\nfrom ..exceptions import InvalidBase\n\n\n__all__ = (\n    'get_ops',\n    'LinearOperations',\n    'LogOperations',\n)\n\n\n\nacceptable_base_strings = {'linear', 'e'}\n\n\ndef get_ops(base):\n    \n    \n    if base in cache:\n        ops = cache[base]\n    else:\n        \n        ops = LogOperations(base)\n        cache[base] = ops\n    return ops\n\n\ndef exp_func(b):\n    \n    from dit.utils import is_string_like\n\n    if is_string_like(b) and b not in acceptable_base_strings:\n        raise InvalidBase(msg=b)\n\n    if b == 'linear':\n        exp = lambda x: x  \n    elif b == 2:\n        exp = np.exp2\n    elif b == 10:\n        exp = lambda x: 10**x\n    elif b == 'e' or np.isclose(b, np.e):\n        exp = np.exp\n    else:\n        if b <= 0 or b == 1:\n            raise InvalidBase(b)\n\n        def exp(x, base=b):\n            \n            return base**np.asarray(x)\n\n    return exp\n\n\ndef log_func(b):\n    \n    from dit.utils import is_string_like\n\n    if is_string_like(b) and b not in acceptable_base_strings:\n        raise InvalidBase(msg=b)\n\n    if b == 'linear':\n        log = lambda x: x  \n    elif b == 2:\n        log = np.log2\n    elif b == 10:\n        log = np.log10\n    elif b == 'e' or np.isclose(b, np.e):\n        log = np.log\n    else:\n        if b <= 0 or b == 1:\n            raise InvalidBase(b)\n\n        Z = np.log(b)\n\n        def log(x, func=np.log):\n            \n            return func(x) / Z\n\n    return log\n\n\nclass Operations(object):\n    \n\n    \n    \n    \n\n    one = None\n    zero = None\n    base = None\n    exp = None\n    log = None\n\n    def get_base(self, numerical=False):\n        \n        if numerical and self.base == 'e':\n            base = np.exp(1)\n        else:\n            base = self.base\n        return base\n\n    def is_null(self, p):\n        \n        return np.isclose(self.zero, p)\n\n    def is_null_exact(self, p):\n        \n        return self.zero == p\n\n    def add(self, x, y):\n        \n        raise NotImplementedError\n\n    def add_inplace(self, x, y):\n        \n        raise NotImplementedError\n\n    def add_reduce(self, x):\n        \n        raise NotImplementedError\n\n    def mult(self, x, y):\n        \n        raise NotImplementedError\n\n    def mult_inplace(self, x, y):\n        \n        raise NotImplementedError\n\n    def mult_reduce(self, x):\n        \n        raise NotImplementedError\n\n    def invert(self, x):\n        \n        raise NotImplementedError\n\n    def normalize(self, x):\n        \n        raise NotImplementedError\n\n\nclass LinearOperations(Operations):\n    \n\n    one = 1\n    zero = 0\n    base = 'linear'\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    exp = staticmethod(exp_func(base))\n    log = staticmethod(log_func(base))\n\n    def add(self, x, y):\n        \n        z = x + y\n        return z\n\n    def add_inplace(self, x, y):\n        \n        x += y\n        return x\n\n    def add_reduce(self, x, axis=None):\n        \n        z = x.sum(axis=axis)\n        return z\n\n    def mult(self, x, y):\n        \n        z = x * y\n        return z\n\n    def mult_inplace(self, x, y):\n        \n        x *= y\n        return x\n\n    def mult_reduce(self, x, axis=None):\n        \n        z = np.prod(x, axis=axis)\n        return z\n\n    def invert(self, x):\n        \n        z = 1 / x\n        return z\n\n    def normalize(self, x, axis=None):\n        \n        z = x / x.sum(axis=None)\n        return z\n\n\ndef set_add(ops):\n    \n    \n    \n    \n\n    \n    \n    base = ops.base\n    if base == 2:\n        def add(self, x, y, func=np.logaddexp2):\n            return func(x, y)\n    elif base == 'e' or np.isclose(base, np.e):\n        def add(self, x, y, func=np.logaddexp):\n            return func(x, y)\n    else:\n        \n        def add(self, x, y):\n            \n            x2 = x * np.log2(base)\n            y2 = y * np.log2(base)\n            z = np.logaddexp2(x2, y2)\n            \n            z *= self.log(2)\n            return z\n\n    add.__doc__ = \n    ops.add = MethodType(add, ops)\n\n\ndef set_add_inplace(ops):\n    \n    base = ops.base\n    if base == 2:\n        def add_inplace(self, x, y, func=np.logaddexp2):\n            return func(x, y, x)\n    elif base == 'e' or np.isclose(base, np.e):\n        def add_inplace(self, x, y, func=np.logaddexp):\n            return func(x, y, x)\n    else:\n        def add_inplace(self, x, y):\n            x *= np.log2(base)\n            y2 = y * np.log2(base)\n            np.logaddexp2(x, y2, x)\n            x *= self.log(2)\n            return x\n\n    add_inplace.__doc__ = \n    ops.add_inplace = MethodType(add_inplace, ops)\n\n\ndef set_add_reduce(ops):\n    \n    \n    base = ops.base\n    if base == 2:\n        def add_reduce(self, x, axis=None, func=np.logaddexp2):\n            if len(x) == 0:\n                \n                z = self.zero\n            else:\n                \n                z = func.reduce(x, axis=axis, dtype=float)\n            return z\n\n    elif base == 'e' or np.isclose(base, np.e):\n        def add_reduce(self, x, axis=None, func=np.logaddexp):\n            if len(x) == 0:\n                \n                z = self.zero\n            else:\n                \n                z = func.reduce(x, axis=axis, dtype=float)\n            return z\n\n    else:\n        def add_reduce(self, x, axis=None):\n            if len(x) == 0:\n                \n                z = self.zero\n            else:\n                \n                \n                x2 = x * np.log2(base)\n                z = np.logaddexp2.reduce(x2, axis=axis, dtype=float)\n                z /= np.log2(base)\n            return z\n\n    add_reduce.__doc__ = \n    ops.add_reduce = MethodType(add_reduce, ops)\n\n\nclass LogOperations(Operations):\n\n    one = None\n    zero = None\n    base = None\n    exp = None\n    log = None\n\n    def __init__(self, base):\n        \n        self.set_base(base)\n\n    def set_base(self, base):\n        \n        self.base = base\n        self.exp = exp_func(base)\n        self.log = log_func(base)\n        \n        self.one = self.log(1)\n        self.zero = self.log(0)\n\n        \n        set_add(self)\n        set_add_inplace(self)\n        set_add_reduce(self)\n\n    def mult(self, x, y):\n        \n        z = x + y\n        return z\n\n    def mult_inplace(self, x, y):\n        \n        x += y\n        return x\n\n    def mult_reduce(self, x, axis=None):\n        \n        \n        \n        \n        z = x.sum(axis=axis)\n        return z\n\n    def invert(self, x):\n        \n        z = -x\n        return z\n\n    def normalize(self, x, axis=None):\n        \n        \n        \n        z = x - self.add_reduce(x, axis=axis)\n        return z\n\n\ncache = {\n    'linear': LinearOperations(),\n    2: LogOperations(2),\n    'e': LogOperations('e')\n}\n",
        "summary": "The provided Python code defines a framework for handling different mathematical operations based on the base specified (either linear, exponential, or logarithmic). It includes classes `LinearOperations` and `LogOperations`, each implementing basic arithmetic operations like addition, multiplication, and normalization. The code also features caching mechanisms to store precomputed operation objects for efficiency."
    },
    {
        "code": "import os\nimport argparse\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport nibabel as nib\nimport torch\nfrom scipy.interpolate import RegularGridInterpolator\nfrom astropy.coordinates import cartesian_to_spherical, spherical_to_cartesian\n\n\nos.environ['VXM_BACKEND'] = 'sphere'\nimport voxelmorph as vxm  \nimport math\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--moving', required=True, help='moving image (source) filename')\nparser.add_argument('--fixed', required=True, help='fixed image (target) filename')\nparser.add_argument('--moved', help='warped image output filename')\nparser.add_argument('--model', required=True, help='pytorch model for nonlinear registration')\n\nparser.add_argument('--warp', help='output warp deformation filename')\nparser.add_argument('--sphere_sub', help='sphere_sub image filename')\nparser.add_argument('--sphere_atlas', help='sphere_atlas image filename')\nparser.add_argument('--sphere_reg', help='sphere.reg image output filename')\nparser.add_argument('--sulc_sub', help='silc_sub image filename')\nparser.add_argument('--sulc_atlas', help='silc_atlas image filename')\nparser.add_argument('--sphere_freesurfer', help='sphere_freesurfer image filename')\nparser.add_argument('--plot_image', help='show time image output filename')\nparser.add_argument('--plot_image_dif_1', help='show dif image output filename')\nparser.add_argument('--plot_image_dif_2', help='show dif image output filename')\nparser.add_argument('-g', '--gpu', help='GPU number(s) - if not supplied, CPU is used')\nparser.add_argument('--multichannel', action='store_true',\n                    help='specify that data has multiple channels')\n\nargs = parser.parse_args()\n\n\ndef meannormalize(sub_data):\n    mean = np.mean(sub_data)\n    std = np.std(sub_data)\n    norm = (sub_data - mean) / std\n    return norm, mean, std\n\n\ndef backmeannormalize(input, mean, std):\n    output = input * std + mean\n    return output\n\n\ndef minmaxnormalize(sub_data):\n    zeros = sub_data == 0\n    max = np.max(sub_data)\n    min = np.min(sub_data)\n    norm = (sub_data - min) / (max - min)\n    norm[zeros] = 0\n    return norm\n\n\ndef backminmaxnormalize(input, max, min):\n    output = input * (max - min) + min\n    return output\n\n\ndef domainnorm(sub_data):\n    domain = 33\n    norm = sub_data / domain\n    return norm\n\n\ndef backdomainnorm(sub_data):\n    domain = 33\n    output = sub_data * domain\n    return output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef interpolate(warp_file, lh_sphere):\n    x = np.linspace(-128, 128, 256)  \n    y = np.linspace(0, 512, 512)  \n\n    \n    warp = warp_file.squeeze()\n    warp = warp.permute(0, 2, 1)\n    warp = warp.detach().numpy()\n    \n    \n\n    interpolate_function_x = RegularGridInterpolator((x, y), -warp[0])  \n    interpolate_function_y = RegularGridInterpolator((x, y), -warp[1])  \n\n    coords, faces = nib.freesurfer.read_geometry(lh_sphere)\n    r, phi, theta = cartesian_to_spherical(coords[:, 0], coords[:, 1], coords[:, 2])\n    p = phi.degree\n    t = theta.degree\n\n    theta_bins = 512\n    phi_bins = 256\n    theta_width = math.degrees(2 * np.pi) / theta_bins\n    t /= theta_width\n    phi_width = math.degrees(np.pi) / phi_bins\n    p /= phi_width\n    t = t.reshape(-1, 1)\n    p = p.reshape(-1, 1)\n    pts = np.concatenate((p, t), axis=1)\n\n    new_pts_x = interpolate_function_x(pts)\n    new_pts_y = interpolate_function_y(pts)\n    x_prime = pts.T[0] + new_pts_x\n    y_prime = pts.T[1] + new_pts_y\n\n    x_prime *= phi_width\n    y_prime *= theta_width\n    y_prime = np.clip(y_prime, 0, 360)\n    x_prime = np.clip(x_prime, -90, 90)\n\n    t_prime = [math.radians(i) for i in y_prime]\n    p_prime = [math.radians(i) for i in x_prime]\n    t_prime = np.array(t_prime)\n    p_prime = np.array(p_prime)\n\n    return r, p_prime, t_prime\n\n\ndef save4image(lh_sphere_sub, lh_sphere_atlas, lh_sulc_sub, lh_sulc_atlas, lh_sphere_freesurfer, phi_prime, theta_prime,\n               imagesavefilename):\n    lh_morph_sulc_sub = nib.freesurfer.read_morph_data(lh_sulc_sub)\n    lh_morph_sulc_atlas = nib.freesurfer.read_morph_data(lh_sulc_atlas)\n\n    coords_sub, faces_sub = nib.freesurfer.read_geometry(lh_sphere_sub)\n    r_sub, phi_sub, theta_sub = cartesian_to_spherical(coords_sub[:, 0], coords_sub[:, 1], coords_sub[:, 2])\n    coords_atlas, faces_atlas = nib.freesurfer.read_geometry(lh_sphere_atlas)\n    r_atlas, phi_atlas, theta_atlas = cartesian_to_spherical(coords_atlas[:, 0], coords_atlas[:, 1], coords_atlas[:, 2])\n    coords_freesurfer, faces_freesurfer = nib.freesurfer.read_geometry(lh_sphere_freesurfer)\n    r_reg, phi_reg, theta_reg = cartesian_to_spherical(coords_freesurfer[:, 0], coords_freesurfer[:, 1],\n                                                       coords_freesurfer[:, 2])\n\n    fig = plt.figure(figsize=(14, 7))\n    ax = fig.add_subplot(141)\n    ax.scatter(phi_sub.degree, theta_sub.degree, s=0.1,\n               c=lh_morph_sulc_sub)  \n    plt.title('Moving')\n\n    ax = fig.add_subplot(142)\n    ax.scatter(phi_atlas.degree, theta_atlas.degree, s=0.1, c=lh_morph_sulc_atlas)\n    plt.title('Fixed')\n\n    ax = fig.add_subplot(143)\n    phi_prime = [math.degrees(p) for p in phi_prime]\n    thtea_prime = [math.degrees(t) for t in theta_prime]\n    ax.scatter(phi_prime, thtea_prime, s=0.1, c=lh_morph_sulc_sub)  \n    plt.title('Moved')\n\n    ax = fig.add_subplot(144)\n    ax.scatter(phi_reg.degree, theta_reg.degree, s=0.1, c=lh_morph_sulc_sub)  \n    plt.title('Moved FreeSurfer')\n\n    plt.savefig(imagesavefilename)\n\n\ndef xyz2degree(lh_sphere, lh_sulc):\n    \n    \n    coords, faces = nib.freesurfer.read_geometry(lh_sphere)\n\n    \n    r, phi, theta = cartesian_to_spherical(coords[:, 0], coords[:, 1], coords[:, 2])\n\n    lat = phi.degree + 90\n    lon = theta.degree\n    \n    y_bins = 512\n    x_bins = 256\n    y_width = math.degrees(2 * np.pi) / y_bins\n    ys = lon // y_width\n    x_width = math.degrees(np.pi) / x_bins\n    xs = lat // x_width\n\n    ys = np.clip(ys, 0, 511)\n    xs = np.clip(xs, 0, 255)\n\n    \n    lh_morph_sulc = nib.freesurfer.read_morph_data(lh_sulc)\n    xs = xs.astype(np.int32)\n    ys = ys.astype(np.int32)\n\n    \n    values = np.zeros((512, 256))\n    values[ys, xs] = lh_morph_sulc\n    \n\n    return values\n\ndef xyz2degree2(phi, theta, lh_sulc):\n\n    lat = phi + 90\n    lon = theta\n    \n    y_bins = 512\n    x_bins = 256\n    y_width = math.degrees(2 * np.pi) / y_bins\n    ys = lon // y_width\n    x_width = math.degrees(np.pi) / x_bins\n    xs = lat // x_width\n\n    ys = np.clip(ys, 0, 511)\n    xs = np.clip(xs, 0, 255)\n\n    \n    lh_morph_sulc = nib.freesurfer.read_morph_data(lh_sulc)\n    xs = xs.astype(np.int32)\n    ys = ys.astype(np.int32)\n\n    \n    values = np.zeros((512, 256))\n    values[ys, xs] = lh_morph_sulc\n    \n\n    return values\n\n\nif args.gpu and (args.gpu != '-1'):\n    device = 'cuda'\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\nelse:\n    device = 'cpu'\n    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n\nadd_feat_axis = not args.multichannel\nmoving = vxm.py.utils.load_volfile(args.moving, add_batch_axis=True, add_feat_axis=add_feat_axis)\nfixed, fixed_affine = vxm.py.utils.load_volfile(\n    args.fixed, add_batch_axis=True, add_feat_axis=add_feat_axis, ret_affine=True)\n\n\nmodel = vxm.networks.VxmDense.load(args.model, device)\nmodel.to(device)\nmodel.eval()\n\n\n\n\n\n\n\n\n\n\nmoving = minmaxnormalize(moving)\nfixed = minmaxnormalize(fixed)\n\ninput_moving = torch.from_numpy(moving).to(device).float().permute(0, 3, 1, 2)\ninput_fixed = torch.from_numpy(fixed).to(device).float().permute(0, 3, 1, 2)\n\n\nmoved, warp = model(input_moving, input_fixed, registration=True)\n\n\n\n\nif args.sphere_sub:\n    c, faces = nib.freesurfer.read_geometry(args.sphere_sub)\n    coords = np.empty(shape=c.shape)\n    r, phi_prime, theta_prime = interpolate(warp, args.sphere_sub)\n    coords[:, 0], coords[:, 1], coords[:, 2] = spherical_to_cartesian(r, phi_prime, theta_prime)\n    nib.freesurfer.io.write_geometry(args.sphere_reg, coords, faces)\n\nif args.plot_image:\n    lh_sphere_sub = args.sphere_sub\n    lh_sphere_atlas = args.sphere_atlas\n    lh_sulc_sub = args.sulc_sub\n    lh_sulc_atlas = args.sulc_atlas\n    lh_sphere_freesurfer = args.sphere_freesurfer\n    imagesavefilename = args.plot_image\n    save4image(lh_sphere_sub, lh_sphere_atlas, lh_sulc_sub, lh_sulc_atlas, lh_sphere_freesurfer, phi_prime, theta_prime,\n               imagesavefilename)\nif args.plot_image_dif_1 or args.plot_image_dif_2:\n    imagesavefilenamedif_1 = args.plot_image_dif_1\n    imagesavefilenamedif_2 = args.plot_image_dif_2\n    dif_moving = xyz2degree(lh_sphere_sub, lh_sulc_sub)\n    dif_moved = xyz2degree2(phi_prime, theta_prime, lh_sulc_sub)\n    dif_freesurfer = xyz2degree(lh_sphere_freesurfer, lh_sulc_sub)\n    dif_moved_moving = dif_moved - dif_moving\n    print(np.nanmax(dif_moved_moving), np.nanmin(dif_moved_moving), np.nanmean(dif_moved_moving))\n    dif_freesurfer_moved = dif_freesurfer - dif_moved\n\n    plt.figure(figsize=(14, 7))\n    plt.imshow(dif_moved_moving)\n    plt.title('moved_moving')\n    plt.colorbar()\n    plt.savefig(imagesavefilenamedif_1)\n\n    plt.figure(figsize=(14, 7))\n    plt.imshow(dif_freesurfer_moved)\n    plt.title('freesurfer_moved')\n    plt.colorbar()\n    plt.savefig(imagesavefilenamedif_2)\n\n\n\nif args.moved:\n    moved = moved.detach().cpu().numpy().squeeze()\n    vxm.py.utils.save_volfile(moved, args.moved, fixed_affine)\n\n\nif args.warp:\n    warp = warp.detach().cpu().numpy().squeeze()\n    vxm.py.utils.save_volfile(warp, args.warp, fixed_affine)\n",
        "summary": "This script is a Python program that uses the VoxelMorph library to perform image registration between two volumes. The main steps are:\n\n1. Load the moving and fixed images from files.\n2. Normalize the pixel values of both images to be in the range [0, 1].\n3. Convert the images to PyTorch tensors and move them to the specified device (CPU or GPU).\n4. Load a pre-trained VoxelMorph model for dense registration.\n5. Perform the registration using the loaded model.\n6. If requested, interpolate the deformation field onto a new sphere geometry and save the resulting coordinates.\n7. If requested, plot images showing differences between different registrations or deformations.\n\nThe script also includes functions to convert between spherical and Cartesian coordinates, as well as to normalize pixel values.\n\nSome key command-line arguments allow users to specify:\n- Paths to input moving and fixed volumes\n- Path to a pre-trained VoxelMorph model\n- Output paths for registered volume and deformation field\n- Optional paths to sphere geometries and sulcal depth maps for visualization\n- Whether to use GPU acceleration\n\nThe script is designed to be flexible, allowing users to customize the registration process by specifying different input data and output options."
    },
    {
        "code": "import numpy as np\n\nfrom pymor.algorithms.image import estimate_image_hierarchical\nfrom pymor.algorithms.projection import project, project_to_subbasis\nfrom pymor.core.base import BasicObject\nfrom pymor.core.exceptions import ImageCollectionError\nfrom pymor.operators.constructions import ZeroOperator\nfrom pymor.operators.interface import Operator\n\n\nclass ResidualReductor(BasicObject):\n    \n\n    def __init__(self, RB, operator, rhs=None, product=None, riesz_representatives=False):\n        assert RB in operator.source\n        assert rhs is None \\\n            or (rhs.source.is_scalar and rhs.range == operator.range and rhs.linear)\n        assert product is None or product.source == product.range == operator.range\n\n        self.__auto_init(locals())\n        self.residual_range = operator.range.empty()\n        self.residual_range_dims = []\n\n    def reduce(self):\n        if self.residual_range is not False:\n            with self.logger.block('Estimating residual range ...'):\n                try:\n                    self.residual_range, self.residual_range_dims = \\\n                        estimate_image_hierarchical([self.operator], [self.rhs],\n                                                    self.RB,\n                                                    (self.residual_range, self.residual_range_dims),\n                                                    orthonormalize=True, product=self.product,\n                                                    riesz_representatives=self.riesz_representatives)\n                except ImageCollectionError as e:\n                    self.logger.warning(f'Cannot compute range of {e.op}. Evaluation will be slow.')\n                    self.residual_range = False\n\n        if self.residual_range is False:\n            operator = project(self.operator, None, self.RB)\n            return NonProjectedResidualOperator(operator, self.rhs, self.riesz_representatives, self.product)\n\n        with self.logger.block('Projecting residual operator ...'):\n            if self.riesz_representatives:\n                operator = project(self.operator, self.residual_range, self.RB, product=None)  \n                rhs = project(self.rhs, self.residual_range, None, product=None)\n            else:\n                operator = project(self.operator, self.residual_range, self.RB, product=self.product)\n                rhs = project(self.rhs, self.residual_range, None, product=self.product)\n\n        return ResidualOperator(operator, rhs)\n\n    def reconstruct(self, u):\n        \n        if self.residual_range is False:\n            if self.product:\n                return u * (u.norm() / u.norm(self.product))[0]\n            else:\n                return u\n        else:\n            return self.residual_range[:u.dim].lincomb(u.to_numpy())\n\n\nclass ResidualOperator(Operator):\n    \n\n    def __init__(self, operator, rhs, name=None):\n        self.__auto_init(locals())\n        self.source = operator.source\n        self.range = operator.range\n        self.linear = operator.linear\n        self.rhs_vector = rhs.as_range_array() if rhs and not rhs.parametric else None\n\n    def apply(self, U, mu=None):\n        V = self.operator.apply(U, mu=mu)\n        if self.rhs:\n            F = self.rhs_vector or self.rhs.as_range_array(mu)\n            if len(V) > 1:\n                V -= F[[0]*len(V)]\n            else:\n                V -= F\n        return V\n\n    def projected_to_subbasis(self, dim_range=None, dim_source=None, name=None):\n        return ResidualOperator(project_to_subbasis(self.operator, dim_range, dim_source),\n                                project_to_subbasis(self.rhs, dim_range, None),\n                                name=name)\n\n\nclass NonProjectedResidualOperator(ResidualOperator):\n    \n\n    def __init__(self, operator, rhs, riesz_representatives, product):\n        super().__init__(operator, rhs)\n        self.__auto_init(locals())\n\n    def apply(self, U, mu=None):\n        R = super().apply(U, mu=mu)\n        if self.product:\n            if self.riesz_representatives:\n                R_riesz = self.product.apply_inverse(R)\n                \n                inversel2 = 1./R_riesz.norm()\n                inversel2 = np.nan_to_num(inversel2)\n                R_riesz.scal(np.sqrt(R_riesz.pairwise_inner(R)) * inversel2)\n                return R_riesz\n            else:\n                \n                inversel2 = 1./R.norm()\n                inversel2 = np.nan_to_num(inversel2)\n                R.scal(np.sqrt(self.product.pairwise_apply2(R, R)) * inversel2)\n                return R\n        else:\n            return R\n\n    def projected_to_subbasis(self, dim_range=None, dim_source=None, name=None):\n        return self.with_(operator=project_to_subbasis(self.operator, None, dim_source))\n\n\nclass ImplicitEulerResidualReductor(BasicObject):\n    \n\n    def __init__(self, RB, operator, mass, dt, rhs=None, product=None):\n        assert RB in operator.source\n        assert rhs.source.is_scalar and rhs.range == operator.range and rhs.linear\n        assert product is None or product.source == product.range == operator.range\n\n        self.__auto_init(locals())\n        self.residual_range = operator.range.empty()\n        self.residual_range_dims = []\n\n    def reduce(self):\n        if self.residual_range is not False:\n            with self.logger.block('Estimating residual range ...'):\n                try:\n                    self.residual_range, self.residual_range_dims = \\\n                        estimate_image_hierarchical([self.operator, self.mass], [self.rhs],\n                                                    self.RB,\n                                                    (self.residual_range, self.residual_range_dims),\n                                                    orthonormalize=True, product=self.product,\n                                                    riesz_representatives=True)\n                except ImageCollectionError as e:\n                    self.logger.warning(f'Cannot compute range of {e.op}. Evaluation will be slow.')\n                    self.residual_range = False\n\n        if self.residual_range is False:\n            operator = project(self.operator, None, self.RB)\n            mass = project(self.mass, None, self.RB)\n            return NonProjectedImplicitEulerResidualOperator(operator, mass, self.rhs, self.dt, self.product)\n\n        with self.logger.block('Projecting residual operator ...'):\n            \n            operator = project(self.operator, self.residual_range, self.RB, product=None)\n            mass = project(self.mass, self.residual_range, self.RB, product=None)\n            rhs = project(self.rhs, self.residual_range, None, product=None)\n\n        return ImplicitEulerResidualOperator(operator, mass, rhs, self.dt)\n\n    def reconstruct(self, u):\n        \n        if self.residual_range is False:\n            if self.product:\n                return u * (u.norm() / u.norm(self.product))[0]\n            else:\n                return u\n        else:\n            return self.residual_range[:u.dim].lincomb(u.to_numpy())\n\n\nclass ImplicitEulerResidualOperator(Operator):\n    \n\n    def __init__(self, operator, mass, rhs, dt, name=None):\n        self.__auto_init(locals())\n        self.source = operator.source\n        self.range = operator.range\n        self.linear = operator.linear\n        self.rhs_vector = rhs.as_range_array() if not rhs.parametric else None\n\n    def apply(self, U, U_old, mu=None):\n        V = self.operator.apply(U, mu=mu)\n        V.axpy(1./self.dt, self.mass.apply(U, mu=mu))\n        V.axpy(-1./self.dt, self.mass.apply(U_old, mu=mu))\n        if not isinstance(self.rhs, ZeroOperator):\n            F = self.rhs_vector or self.rhs.as_range_array(mu)\n            if len(V) > 1:\n                V -= F[[0]*len(V)]\n            else:\n                V -= F\n        return V\n\n    def projected_to_subbasis(self, dim_range=None, dim_source=None, name=None):\n        return ImplicitEulerResidualOperator(project_to_subbasis(self.operator, dim_range, dim_source),\n                                             project_to_subbasis(self.mass, dim_range, dim_source),\n                                             project_to_subbasis(self.rhs, dim_range, None),\n                                             self.dt,\n                                             name=name)\n\n\nclass NonProjectedImplicitEulerResidualOperator(ImplicitEulerResidualOperator):\n    \n\n    def __init__(self, operator, mass, rhs, dt, product):\n        super().__init__(operator, mass, rhs, dt)\n        self.product = product\n\n    def apply(self, U, U_old, mu=None):\n        R = super().apply(U, U_old, mu=mu)\n        if self.product:\n            R_riesz = self.product.apply_inverse(R)\n            \n            inversel2 = 1./R_riesz.norm()\n            inversel2 = np.nan_to_num(inversel2)\n            R_riesz.scal(np.sqrt(R_riesz.pairwise_inner(R)) * inversel2)\n            return R_riesz\n        else:\n            return R\n\n    def projected_to_subbasis(self, dim_range=None, dim_source=None, name=None):\n        return self.with_(operator=project_to_subbasis(self.operator, None, dim_source),\n                          mass=project_to_subbasis(self.mass, None, dim_source))\n",
        "summary": "The provided Python code defines several classes for residual-based model reduction in the context of numerical simulations, including `ResidualReductor`, `ImplicitEulerResidualReductor`, and their respective operator classes. These classes facilitate the projection of operators and right-hand sides onto a reduced basis (`RB`), estimate the range of the residual operator, and apply time integration schemes like Implicit Euler for handling dynamic systems. The code also includes methods for reconstructing solutions in the full space from those in the reduced space."
    },
    {
        "code": "import logging\nimport hmac\nfrom hashlib import sha256\nimport os\nimport urllib\nfrom datetime import datetime\n\nlog = logging.getLogger(__name__)\n\n\n\n\ndef prepend_bucketname(name):\n\n    prefix = os.getenv('BUCKETNAME_PREFIX', \"gsfc-ngap-{}-\".format(os.getenv('MATURITY', 'DEV')[0:1].lower()))\n    return \"{}{}\".format(prefix, name)\n\n\ndef hmacsha256(key, string):\n\n    return hmac.new(key, string.encode('utf-8'), sha256)\n\n\ndef get_presigned_url(session, bucket_name, object_name, region_name, expire_seconds, user_id, method='GET'):\n\n    timez = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n    datez = timez[:8]\n    hostname = \"{0}.s3{1}.amazonaws.com\".format(bucket_name, \".\"+region_name if region_name != \"us-east-1\" else \"\")\n\n    cred   = session['Credentials']['AccessKeyId']\n    secret = session['Credentials']['SecretAccessKey']\n    token  = session['Credentials']['SessionToken']\n\n    aws4_request = \"/\".join([datez, region_name, \"s3\", \"aws4_request\"])\n    cred_string = \"{0}/{1}\".format(cred, aws4_request)\n\n    \n    parts = [\"A-userid={0}\".format(user_id),\n             \"X-Amz-Algorithm=AWS4-HMAC-SHA256\",\n             \"X-Amz-Credential=\"+urllib.parse.quote_plus(cred_string),\n             \"X-Amz-Date=\"+timez,\n             \"X-Amz-Expires={0}\".format(expire_seconds),\n             \"X-Amz-Security-Token=\"+urllib.parse.quote_plus(token),\n             \"X-Amz-SignedHeaders=host\"]\n\n    can_query_string = \"&\".join(parts)\n\n    \n    can_req = method + \"\\n/\" + object_name + \"\\n\" + can_query_string + \"\\nhost:\" + hostname + \"\\n\\nhost\\nUNSIGNED-PAYLOAD\"\n    can_req_hash = sha256(can_req.encode('utf-8')).hexdigest()\n\n    \n    stringtosign = \"\\n\".join([\"AWS4-HMAC-SHA256\", timez, aws4_request, can_req_hash])\n\n    \n    StepOne =    hmacsha256( \"AWS4{0}\".format(secret).encode('utf-8'), datez).digest()\n    StepTwo =    hmacsha256( StepOne, region_name ).digest()\n    StepThree =  hmacsha256( StepTwo, \"s3\").digest()\n    SigningKey = hmacsha256( StepThree, \"aws4_request\").digest()\n\n\n    \n    Signature = hmacsha256(SigningKey, stringtosign).hexdigest()\n\n    \n    url = \"https://\" + hostname + \"/\" + object_name + \"?\" + can_query_string + \"&X-Amz-Signature=\" + Signature\n    return url\n\n\ndef get_bucket_dynamic_path(path_list, b_map):\n\n    \n    if 'MAP' in b_map:\n        map_dict = b_map['MAP']\n    else:\n        map_dict = b_map\n\n    mapping = []\n\n    log.debug(\"Pathparts is {0}\".format(\", \".join(path_list)))\n    \n    for path_part in path_list:\n        \n        if (mapping and isinstance(map_dict, str)) or 'bucket' in map_dict: \n            customheaders = {}\n            if isinstance(map_dict, dict) and 'bucket' in map_dict:\n                bucketname = map_dict['bucket']\n                if 'headers' in map_dict:\n                    customheaders = map_dict['headers']\n            else:\n                bucketname = map_dict\n\n            log.debug(f'mapping: {mapping}')\n            \n            for _ in mapping:\n                path_list.pop(0)\n\n            \n            object_name = \"/\".join(path_list)\n            bucket_path = \"/\".join(mapping)\n\n            log.info(\"Bucket mapping was {0}, object was {1}\".format(bucket_path, object_name))\n            return prepend_bucketname(bucketname), bucket_path, object_name, customheaders\n\n        if path_part in map_dict:\n            map_dict = map_dict[path_part]\n            mapping.append(path_part)\n            log.debug(\"Found {0}, Mapping is now {1}\".format(path_part, \"/\".join(mapping)))\n\n        else:\n            log.warning(\"Could not find {0} in bucketmap\".format(path_part))\n            log.debug('said bucketmap: {}'.format(map_dict))\n            return False, False, False, {}\n\n    \n    return False, False, False, {}\n\n\ndef process_varargs(varargs: list, b_map: dict):\n    \n    log.warning('Deprecated process_varargs() called.')\n    path, bucket, object_name, _ = process_request(varargs, b_map)\n    return path, bucket, object_name\n\n\ndef process_request(varargs, b_map):\n\n    varargs = varargs.split(\"/\")\n\n    \n    if len(varargs) < 2:\n        return \"/\".join(varargs), None, None, []\n\n    \n    if len(varargs) == 3:\n        if os.getenv('USE_REVERSE_BUCKET_MAP', 'FALSE').lower() == 'true':\n            varargs[0], varargs[1] = varargs[1], varargs[0]\n\n    \n    bucket, path, object_name, headers = get_bucket_dynamic_path(varargs, b_map)\n\n    \n    if not bucket:\n        object_name = varargs.pop(-1)\n        path = \"/\".join(varargs)\n\n    return path, bucket, object_name, headers\n\ndef bucket_prefix_match(bucket_check, bucket_map, object_name=\"\"):\n    log.debug(f\"bucket_prefix_match(): checking if {bucket_check} matches {bucket_map} w/ optional obj '{object_name}'\")\n    if bucket_check == bucket_map.split('/')[0] and object_name.startswith(\"/\".join(bucket_map.split('/')[1:])):\n        log.debug(f\"Prefixed Bucket Map matched: s3://{bucket_check}/{object_name} => {bucket_map}\")\n        return True\n    return False\n\n\n\ndef get_sorted_bucket_list(b_map, bucket_group):\n    if bucket_group not in b_map:\n        \n        log.warning(f\"Bucket map does not contain bucket group '{bucket_group}'\")\n        return []\n\n    \n    if  isinstance(b_map[bucket_group], dict):\n        return sorted(list(b_map[bucket_group].keys()), key=lambda e: e.count(\"/\"), reverse=True )\n    if isinstance(b_map[bucket_group], list):\n        return sorted(list(b_map[bucket_group]), key=lambda e: e.count(\"/\"), reverse=True )\n\n    \n    return []\n\ndef check_private_bucket(bucket, b_map, object_name=\"\"):\n\n    log.debug('check_private_buckets(): bucket: {}'.format(bucket))\n\n    \n    if 'PRIVATE_BUCKETS' in b_map:\n        \n        sorted_buckets = get_sorted_bucket_list(b_map, 'PRIVATE_BUCKETS')\n        log.debug(f\"Sorted PRIVATE buckets are {sorted_buckets}\")\n        for priv_bucket in sorted_buckets:\n            if bucket_prefix_match(bucket, prepend_bucketname(priv_bucket), object_name):\n                \n                return b_map['PRIVATE_BUCKETS'][priv_bucket]\n\n    return False\n\ndef check_public_bucket(bucket, b_map, object_name=\"\"):\n\n    \n    if 'PUBLIC_BUCKETS' in b_map:\n        sorted_buckets = get_sorted_bucket_list(b_map, 'PUBLIC_BUCKETS')\n        log.debug(f\"Sorted PUBLIC buckets are {sorted_buckets}\")\n        for pub_bucket in sorted_buckets:\n            if bucket_prefix_match(bucket, prepend_bucketname(pub_bucket), object_name):\n                \n                log.debug(\"found a public, we'll take it\")\n                return True\n\n    \n    log.debug('we did not find a public bucket for {}'.format(bucket))\n    return False\n",
        "summary": "The provided Python code includes functions for generating presigned URLs using AWS4-HMAC-SHA256, handling dynamic bucket paths based on mappings, and checking if buckets are private or public. It utilizes various cryptographic and string manipulation techniques to ensure secure and efficient operations."
    },
    {
        "code": "from flask import render_template,request,redirect,url_for\nfrom . import main\nfrom ..requests import get_sources,get_articles\nfrom ..models import Sources\n\n\n@main.route('/')\ndef index():\n\t\n\tsources = get_sources('business')\n\tsports_sources = get_sources('sports')\n\ttechnology_sources = get_sources('technology')\n\tentertainment_sources = get_sources('entertainment')\n\ttitle = \"News Of  The Day\"\n\n\treturn render_template('index.html',title = title, sources = sources,sports_sources = sports_sources,technology_sources = technology_sources,entertainment_sources = entertainment_sources)\n\n@main.route('/sources/<id>')\ndef articles(id):\n\t\n\tarticles = get_articles(id)\n\ttitle = f'NH | {id}'\n\n\treturn render_template('articles.html',title= title,articles = articles)",
        "summary": "The provided Python code defines routes for a Flask application that fetches news sources and articles from an external API. The `index` route displays a homepage with various categories of news sources, while the `articles` route shows articles related to a specific source identified by its ID."
    },
    {
        "code": "from __future__ import absolute_import, division, print_function\n\nfrom marshmallow import fields\n\nfrom polyaxon_schemas.ml.layers.base import BaseLayerConfig, BaseLayerSchema\n\n\nclass WrapperSchema(BaseLayerSchema):\n    layer = fields.Nested('LayerSchema')\n\n    @staticmethod\n    def schema_config():\n        return WrapperConfig\n\n\nclass WrapperConfig(BaseLayerConfig):\n    \n    IDENTIFIER = 'Wrapper'\n    SCHEMA = WrapperSchema\n\n    def __init__(self, layer, **kwargs):\n        super(WrapperConfig, self).__init__(**kwargs)\n        self.layer = layer\n\n\nclass TimeDistributedSchema(WrapperSchema):\n    @staticmethod\n    def schema_config():\n        return TimeDistributedConfig\n\n\nclass TimeDistributedConfig(WrapperConfig):\n    \n    IDENTIFIER = 'TimeDistributed'\n    SCHEMA = TimeDistributedSchema\n\n\nclass BidirectionalSchema(WrapperSchema):\n\n    @staticmethod\n    def schema_config():\n        return BidirectionalConfig\n\n\nclass BidirectionalConfig(WrapperConfig):\n    \n    IDENTIFIER = 'Bidirectional'\n    SCHEMA = BidirectionalSchema\n",
        "summary": "The code defines a set of classes and schemas for configuring different types of layers in machine learning models, including `Wrapper`, `TimeDistributed`, and `Bidirectional`. Each class extends a base configuration class (`BaseLayerConfig`) and includes a corresponding schema class (`WrapperSchema`, `TimeDistributedSchema`, `BidirectionalSchema`) that specifies the structure of the layer configuration."
    },
    {
        "code": "import pytest\nfrom pages.aplication import Application\n\n\ndef pytest_addoption(parser):\n    parser.addoption('--browser_name', action='store', default=\"chrome\", help=\"Choose browser: chrome or firefox\")\n    parser.addoption('--base_url', action='store', default='https://prodoctorov.ru/new/rate/doctor/12/'\n                     , help=\"Choose base_url\")\n\n\n@pytest.fixture\ndef app(request):\n    browser_name = request.config.getoption(\"--browser_name\")  \n    base_url = request.config.getoption(\"--base_url\")\n    fixture = Application(browser_name=browser_name, base_url=base_url)\n    yield fixture\n    print(\"\\nquit browser..\")\n    fixture.destroy()\n    return fixture\n\n",
        "summary": "The provided Python code sets up a pytest framework for web application testing. It defines command-line options to specify the browser and base URL, and creates a fixture that initializes an `Application` object with these settings, manages its lifecycle by yielding it during tests, and cleans up after all tests are run."
    }
]